{"config":{"lang":["en","ja","ko","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenAI Agents SDK","text":"<p>The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm. The Agents SDK has a very small set of primitives:</p> <ul> <li>Agents, which are LLMs equipped with instructions and tools</li> <li>Agents as tools / Handoffs, which allow agents to delegate to other agents for specific tasks</li> <li>Guardrails, which enable validation of agent inputs and outputs</li> </ul> <p>In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in tracing that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.</p>"},{"location":"#why-use-the-agents-sdk","title":"Why use the Agents SDK","text":"<p>The SDK has two driving design principles:</p> <ol> <li>Enough features to be worth using, but few enough primitives to make it quick to learn.</li> <li>Works great out of the box, but you can customize exactly what happens.</li> </ol> <p>Here are the main features of the SDK:</p> <ul> <li>Agent loop: A built-in agent loop that handles tool invocation, sends results back to the LLM, and continues until the task is complete.</li> <li>Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.</li> <li>Agents as tools / Handoffs: A powerful mechanism for coordinating and delegating work across multiple agents.</li> <li>Guardrails: Run input validation and safety checks in parallel with agent execution, and fail fast when checks do not pass.</li> <li>Function tools: Turn any Python function into a tool with automatic schema generation and Pydantic-powered validation.</li> <li>MCP server tool calling: Built-in MCP server tool integration that works the same way as function tools.</li> <li>Sessions: A persistent memory layer for maintaining working context within an agent loop.</li> <li>Human in the loop: Built-in mechanisms for involving humans across agent runs.</li> <li>Tracing: Built-in tracing for visualizing, debugging, and monitoring workflows, with support for the OpenAI suite of evaluation, fine-tuning, and distillation tools.</li> <li>Realtime Agents: Build powerful voice agents with features such as automatic interruption detection, context management, guardrails, and more.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install openai-agents\n</code></pre>"},{"location":"#hello-world-example","title":"Hello world example","text":"<pre><code>from agents import Agent, Runner\n\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\nprint(result.final_output)\n\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\n</code></pre> <p>(If running this, ensure you set the <code>OPENAI_API_KEY</code> environment variable)</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"agents/","title":"Agents","text":"<p>Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.</p>"},{"location":"agents/#basic-configuration","title":"Basic configuration","text":"<p>The most common properties of an agent you'll configure are:</p> <ul> <li><code>name</code>: A required string that identifies your agent.</li> <li><code>instructions</code>: also known as a developer message or system prompt.</li> <li><code>model</code>: which LLM to use, and optional <code>model_settings</code> to configure model tuning parameters like temperature, top_p, etc.</li> <li><code>prompt</code>: Reference a prompt template by id (and variables) when using OpenAI's Responses API.</li> <li><code>tools</code>: Tools that the agent can use to achieve its tasks.</li> <li><code>mcp_servers</code>: MCP servers that provide tools to the agent. See the MCP guide.</li> <li><code>reset_tool_choice</code>: Whether to reset <code>tool_choice</code> after a tool call (default: <code>True</code>) to avoid tool-use loops. See Forcing tool use.</li> </ul> <pre><code>from agents import Agent, ModelSettings, function_tool\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"returns weather info for the specified city.\"\"\"\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Haiku agent\",\n    instructions=\"Always respond in haiku form\",\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\n</code></pre>"},{"location":"agents/#prompt-templates","title":"Prompt templates","text":"<p>You can reference a prompt template created in the OpenAI platform by setting <code>prompt</code>. This works with OpenAI models using the Responses API.</p> <p>To use it, please:</p> <ol> <li>Go to https://platform.openai.com/playground/prompts</li> <li>Create a new prompt variable, <code>poem_style</code>.</li> <li> <p>Create a system prompt with the content:</p> <pre><code>Write a poem in {{poem_style}}\n</code></pre> </li> <li> <p>Run the example with the <code>--prompt-id</code> flag.</p> </li> </ol> <pre><code>from agents import Agent\n\nagent = Agent(\n    name=\"Prompted assistant\",\n    prompt={\n        \"id\": \"pmpt_123\",\n        \"version\": \"1\",\n        \"variables\": {\"poem_style\": \"haiku\"},\n    },\n)\n</code></pre> <p>You can also generate the prompt dynamically at run time:</p> <pre><code>from dataclasses import dataclass\n\nfrom agents import Agent, GenerateDynamicPromptData, Runner\n\n@dataclass\nclass PromptContext:\n    prompt_id: str\n    poem_style: str\n\n\nasync def build_prompt(data: GenerateDynamicPromptData):\n    ctx: PromptContext = data.context.context\n    return {\n        \"id\": ctx.prompt_id,\n        \"version\": \"1\",\n        \"variables\": {\"poem_style\": ctx.poem_style},\n    }\n\n\nagent = Agent(name=\"Prompted assistant\", prompt=build_prompt)\nresult = await Runner.run(\n    agent,\n    \"Say hello\",\n    context=PromptContext(prompt_id=\"pmpt_123\", poem_style=\"limerick\"),\n)\n</code></pre>"},{"location":"agents/#context","title":"Context","text":"<p>Agents are generic on their <code>context</code> type. Context is a dependency-injection tool: it's an object you create and pass to <code>Runner.run()</code>, that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.</p> <pre><code>@dataclass\nclass UserContext:\n    name: str\n    uid: str\n    is_pro_user: bool\n\n    async def fetch_purchases() -&gt; list[Purchase]:\n        return ...\n\nagent = Agent[UserContext](\n    ...,\n)\n</code></pre>"},{"location":"agents/#output-types","title":"Output types","text":"<p>By default, agents produce plain text (i.e. <code>str</code>) outputs. If you want the agent to produce a particular type of output, you can use the <code>output_type</code> parameter. A common choice is to use Pydantic objects, but we support any type that can be wrapped in a Pydantic TypeAdapter - dataclasses, lists, TypedDict, etc.</p> <pre><code>from pydantic import BaseModel\nfrom agents import Agent\n\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\nagent = Agent(\n    name=\"Calendar extractor\",\n    instructions=\"Extract calendar events from text\",\n    output_type=CalendarEvent,\n)\n</code></pre> <p>Note</p> <p>When you pass an <code>output_type</code>, that tells the model to use structured outputs instead of regular plain text responses.</p>"},{"location":"agents/#multi-agent-system-design-patterns","title":"Multi-agent system design patterns","text":"<p>There are many ways to design multi\u2011agent systems, but we commonly see two broadly applicable patterns:</p> <ol> <li>Manager (agents as tools): A central manager/orchestrator invokes specialized sub\u2011agents as tools and retains control of the conversation.</li> <li>Handoffs: Peer agents hand off control to a specialized agent that takes over the conversation. This is decentralized.</li> </ol> <p>See our practical guide to building agents for more details.</p>"},{"location":"agents/#manager-agents-as-tools","title":"Manager (agents as tools)","text":"<p>The <code>customer_facing_agent</code> handles all user interaction and invokes specialized sub\u2011agents exposed as tools. Read more in the tools documentation.</p> <pre><code>from agents import Agent\n\nbooking_agent = Agent(...)\nrefund_agent = Agent(...)\n\ncustomer_facing_agent = Agent(\n    name=\"Customer-facing agent\",\n    instructions=(\n        \"Handle all direct user communication. \"\n        \"Call the relevant tools when specialized expertise is needed.\"\n    ),\n    tools=[\n        booking_agent.as_tool(\n            tool_name=\"booking_expert\",\n            tool_description=\"Handles booking questions and requests.\",\n        ),\n        refund_agent.as_tool(\n            tool_name=\"refund_expert\",\n            tool_description=\"Handles refund questions and requests.\",\n        )\n    ],\n)\n</code></pre>"},{"location":"agents/#handoffs","title":"Handoffs","text":"<p>Handoffs are sub\u2011agents the agent can delegate to. When a handoff occurs, the delegated agent receives the conversation history and takes over the conversation. This pattern enables modular, specialized agents that excel at a single task. Read more in the handoffs documentation.</p> <pre><code>from agents import Agent\n\nbooking_agent = Agent(...)\nrefund_agent = Agent(...)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=(\n        \"Help the user with their questions. \"\n        \"If they ask about booking, hand off to the booking agent. \"\n        \"If they ask about refunds, hand off to the refund agent.\"\n    ),\n    handoffs=[booking_agent, refund_agent],\n)\n</code></pre>"},{"location":"agents/#dynamic-instructions","title":"Dynamic instructions","text":"<p>In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and <code>async</code> functions are accepted.</p> <pre><code>def dynamic_instructions(\n    context: RunContextWrapper[UserContext], agent: Agent[UserContext]\n) -&gt; str:\n    return f\"The user's name is {context.context.name}. Help them with their questions.\"\n\n\nagent = Agent[UserContext](\n    name=\"Triage agent\",\n    instructions=dynamic_instructions,\n)\n</code></pre>"},{"location":"agents/#lifecycle-events-hooks","title":"Lifecycle events (hooks)","text":"<p>Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the <code>hooks</code> property. Subclass the <code>AgentHooks</code> class, and override the methods you're interested in.</p>"},{"location":"agents/#guardrails","title":"Guardrails","text":"<p>Guardrails allow you to run checks/validations on user input in parallel to the agent running, and on the agent's output once it is produced. For example, you could screen the user's input and agent's output for relevance. Read more in the guardrails documentation.</p>"},{"location":"agents/#cloningcopying-agents","title":"Cloning/copying agents","text":"<p>By using the <code>clone()</code> method on an agent, you can duplicate an Agent, and optionally change any properties you like.</p> <pre><code>pirate_agent = Agent(\n    name=\"Pirate\",\n    instructions=\"Write like a pirate\",\n    model=\"gpt-5.2\",\n)\n\nrobot_agent = pirate_agent.clone(\n    name=\"Robot\",\n    instructions=\"Write like a robot\",\n)\n</code></pre>"},{"location":"agents/#forcing-tool-use","title":"Forcing tool use","text":"<p>Supplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting <code>ModelSettings.tool_choice</code>. Valid values are:</p> <ol> <li><code>auto</code>, which allows the LLM to decide whether or not to use a tool.</li> <li><code>required</code>, which requires the LLM to use a tool (but it can intelligently decide which tool).</li> <li><code>none</code>, which requires the LLM to not use a tool.</li> <li>Setting a specific string e.g. <code>my_tool</code>, which requires the LLM to use that specific tool.</li> </ol> <pre><code>from agents import Agent, Runner, function_tool, ModelSettings\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Returns weather info for the specified city.\"\"\"\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Weather Agent\",\n    instructions=\"Retrieve weather details.\",\n    tools=[get_weather],\n    model_settings=ModelSettings(tool_choice=\"get_weather\")\n)\n</code></pre>"},{"location":"agents/#tool-use-behavior","title":"Tool Use Behavior","text":"<p>The <code>tool_use_behavior</code> parameter in the <code>Agent</code> configuration controls how tool outputs are handled:</p> <ul> <li><code>\"run_llm_again\"</code>: The default. Tools are run, and the LLM processes the results to produce a final response.</li> <li><code>\"stop_on_first_tool\"</code>: The output of the first tool call is used as the final response, without further LLM processing.</li> </ul> <pre><code>from agents import Agent, Runner, function_tool, ModelSettings\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Returns weather info for the specified city.\"\"\"\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Weather Agent\",\n    instructions=\"Retrieve weather details.\",\n    tools=[get_weather],\n    tool_use_behavior=\"stop_on_first_tool\"\n)\n</code></pre> <ul> <li><code>StopAtTools(stop_at_tool_names=[...])</code>: Stops if any specified tool is called, using its output as the final response.</li> </ul> <pre><code>from agents import Agent, Runner, function_tool\nfrom agents.agent import StopAtTools\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Returns weather info for the specified city.\"\"\"\n    return f\"The weather in {city} is sunny\"\n\n@function_tool\ndef sum_numbers(a: int, b: int) -&gt; int:\n    \"\"\"Adds two numbers.\"\"\"\n    return a + b\n\nagent = Agent(\n    name=\"Stop At Stock Agent\",\n    instructions=\"Get weather or sum numbers.\",\n    tools=[get_weather, sum_numbers],\n    tool_use_behavior=StopAtTools(stop_at_tool_names=[\"get_weather\"])\n)\n</code></pre> <ul> <li><code>ToolsToFinalOutputFunction</code>: A custom function that processes tool results and decides whether to stop or continue with the LLM.</li> </ul> <pre><code>from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper\nfrom agents.agent import ToolsToFinalOutputResult\nfrom typing import List, Any\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Returns weather info for the specified city.\"\"\"\n    return f\"The weather in {city} is sunny\"\n\ndef custom_tool_handler(\n    context: RunContextWrapper[Any],\n    tool_results: List[FunctionToolResult]\n) -&gt; ToolsToFinalOutputResult:\n    \"\"\"Processes tool results to decide final output.\"\"\"\n    for result in tool_results:\n        if result.output and \"sunny\" in result.output:\n            return ToolsToFinalOutputResult(\n                is_final_output=True,\n                final_output=f\"Final weather: {result.output}\"\n            )\n    return ToolsToFinalOutputResult(\n        is_final_output=False,\n        final_output=None\n    )\n\nagent = Agent(\n    name=\"Weather Agent\",\n    instructions=\"Retrieve weather details.\",\n    tools=[get_weather],\n    tool_use_behavior=custom_tool_handler\n)\n</code></pre> <p>Note</p> <p>To prevent infinite loops, the framework automatically resets <code>tool_choice</code> to \"auto\" after a tool call. This behavior is configurable via <code>agent.reset_tool_choice</code>. The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of <code>tool_choice</code>, ad infinitum.</p>"},{"location":"config/","title":"Configuring the SDK","text":""},{"location":"config/#api-keys-and-clients","title":"API keys and clients","text":"<p>By default, the SDK looks for the <code>OPENAI_API_KEY</code> environment variable for LLM requests and tracing, as soon as it is imported. If you are unable to set that environment variable before your app starts, you can use the set_default_openai_key() function to set the key.</p> <pre><code>from agents import set_default_openai_key\n\nset_default_openai_key(\"sk-...\")\n</code></pre> <p>Alternatively, you can also configure an OpenAI client to be used. By default, the SDK creates an <code>AsyncOpenAI</code> instance, using the API key from the environment variable or the default key set above. You can change this by using the set_default_openai_client() function.</p> <pre><code>from openai import AsyncOpenAI\nfrom agents import set_default_openai_client\n\ncustom_client = AsyncOpenAI(base_url=\"...\", api_key=\"...\")\nset_default_openai_client(custom_client)\n</code></pre> <p>Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the set_default_openai_api() function.</p> <pre><code>from agents import set_default_openai_api\n\nset_default_openai_api(\"chat_completions\")\n</code></pre>"},{"location":"config/#tracing","title":"Tracing","text":"<p>Tracing is enabled by default. It uses the OpenAI API keys from the section above by default (i.e. the environment variable or the default key you set). You can specifically set the API key used for tracing by using the <code>set_tracing_export_api_key</code> function.</p> <pre><code>from agents import set_tracing_export_api_key\n\nset_tracing_export_api_key(\"sk-...\")\n</code></pre> <p>If you need to attribute traces to a specific organization or project when using the default exporter, set these environment variables before your app starts:</p> <pre><code>export OPENAI_ORG_ID=\"org_...\"\nexport OPENAI_PROJECT_ID=\"proj_...\"\n</code></pre> <p>You can also set a tracing API key per run without changing the global exporter.</p> <pre><code>from agents import Runner, RunConfig\n\nawait Runner.run(\n    agent,\n    input=\"Hello\",\n    run_config=RunConfig(tracing={\"api_key\": \"sk-tracing-123\"}),\n)\n</code></pre> <p>You can also disable tracing entirely by using the <code>set_tracing_disabled()</code> function.</p> <pre><code>from agents import set_tracing_disabled\n\nset_tracing_disabled(True)\n</code></pre>"},{"location":"config/#debug-logging","title":"Debug logging","text":"<p>The SDK has two Python loggers without any handlers set. By default, this means that warnings and errors are sent to <code>stdout</code>, but other logs are suppressed.</p> <p>To enable verbose logging, use the <code>enable_verbose_stdout_logging()</code> function.</p> <pre><code>from agents import enable_verbose_stdout_logging\n\nenable_verbose_stdout_logging()\n</code></pre> <p>Alternatively, you can customize the logs by adding handlers, filters, formatters, etc. You can read more in the Python logging guide.</p> <pre><code>import logging\n\nlogger = logging.getLogger(\"openai.agents\") # or openai.agents.tracing for the Tracing logger\n\n# To make all logs show up\nlogger.setLevel(logging.DEBUG)\n# To make info and above show up\nlogger.setLevel(logging.INFO)\n# To make warning and above show up\nlogger.setLevel(logging.WARNING)\n# etc\n\n# You can customize this as needed, but this will output to `stderr` by default\nlogger.addHandler(logging.StreamHandler())\n</code></pre>"},{"location":"config/#sensitive-data-in-logs","title":"Sensitive data in logs","text":"<p>Certain logs may contain sensitive data (for example, user data). If you want to disable this data from being logged, set the following environment variables.</p> <p>To disable logging LLM inputs and outputs:</p> <pre><code>export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1\n</code></pre> <p>To disable logging tool inputs and outputs:</p> <pre><code>export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1\n</code></pre>"},{"location":"context/","title":"Context management","text":"<p>Context is an overloaded term. There are two main classes of context you might care about:</p> <ol> <li>Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like <code>on_handoff</code>, in lifecycle hooks, etc.</li> <li>Context available to LLMs: this is data the LLM sees when generating a response.</li> </ol>"},{"location":"context/#local-context","title":"Local context","text":"<p>This is represented via the <code>RunContextWrapper</code> class and the <code>context</code> property within it. The way this works is:</p> <ol> <li>You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.</li> <li>You pass that object to the various run methods (e.g. <code>Runner.run(..., context=whatever)</code>).</li> <li>All your tool calls, lifecycle hooks etc will be passed a wrapper object, <code>RunContextWrapper[T]</code>, where <code>T</code> represents your context object type which you can access via <code>wrapper.context</code>.</li> </ol> <p>The most important thing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same type of context.</p> <p>You can use the context for things like:</p> <ul> <li>Contextual data for your run (e.g. things like a username/uid or other information about the user)</li> <li>Dependencies (e.g. logger objects, data fetchers, etc)</li> <li>Helper functions</li> </ul> <p>Note</p> <p>The context object is not sent to the LLM. It is purely a local object that you can read from, write to and call methods on it.</p> <pre><code>import asyncio\nfrom dataclasses import dataclass\n\nfrom agents import Agent, RunContextWrapper, Runner, function_tool\n\n@dataclass\nclass UserInfo:  # (1)!\n    name: str\n    uid: int\n\n@function_tool\nasync def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -&gt; str:  # (2)!\n    \"\"\"Fetch the age of the user. Call this function to get user's age information.\"\"\"\n    return f\"The user {wrapper.context.name} is 47 years old\"\n\nasync def main():\n    user_info = UserInfo(name=\"John\", uid=123)\n\n    agent = Agent[UserInfo](  # (3)!\n        name=\"Assistant\",\n        tools=[fetch_user_age],\n    )\n\n    result = await Runner.run(  # (4)!\n        starting_agent=agent,\n        input=\"What is the age of the user?\",\n        context=user_info,\n    )\n\n    print(result.final_output)  # (5)!\n    # The user John is 47 years old.\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li>This is the context object. We've used a dataclass here, but you can use any type.</li> <li>This is a tool. You can see it takes a <code>RunContextWrapper[UserInfo]</code>. The tool implementation reads from the context.</li> <li>We mark the agent with the generic <code>UserInfo</code>, so that the typechecker can catch errors (for example, if we tried to pass a tool that took a different context type).</li> <li>The context is passed to the <code>run</code> function.</li> <li>The agent correctly calls the tool and gets the age.</li> </ol>"},{"location":"context/#advanced-toolcontext","title":"Advanced: <code>ToolContext</code>","text":"<p>In some cases, you might want to access extra metadata about the tool being executed \u2014 such as its name, call ID, or raw argument string. For this, you can use the <code>ToolContext</code> class, which extends <code>RunContextWrapper</code>.</p> <pre><code>from typing import Annotated\nfrom pydantic import BaseModel, Field\nfrom agents import Agent, Runner, function_tool\nfrom agents.tool_context import ToolContext\n\nclass WeatherContext(BaseModel):\n    user_id: str\n\nclass Weather(BaseModel):\n    city: str = Field(description=\"The city name\")\n    temperature_range: str = Field(description=\"The temperature range in Celsius\")\n    conditions: str = Field(description=\"The weather conditions\")\n\n@function_tool\ndef get_weather(ctx: ToolContext[WeatherContext], city: Annotated[str, \"The city to get the weather for\"]) -&gt; Weather:\n    print(f\"[debug] Tool context: (name: {ctx.tool_name}, call_id: {ctx.tool_call_id}, args: {ctx.tool_arguments})\")\n    return Weather(city=city, temperature_range=\"14-20C\", conditions=\"Sunny with wind.\")\n\nagent = Agent(\n    name=\"Weather Agent\",\n    instructions=\"You are a helpful agent that can tell the weather of a given city.\",\n    tools=[get_weather],\n)\n</code></pre> <p><code>ToolContext</code> provides the same <code>.context</code> property as <code>RunContextWrapper</code>, plus additional fields specific to the current tool call:</p> <ul> <li><code>tool_name</code> \u2013 the name of the tool being invoked  </li> <li><code>tool_call_id</code> \u2013 a unique identifier for this tool call  </li> <li><code>tool_arguments</code> \u2013 the raw argument string passed to the tool  </li> </ul> <p>Use <code>ToolContext</code> when you need tool-level metadata during execution. For general context sharing between agents and tools, <code>RunContextWrapper</code> remains sufficient.</p>"},{"location":"context/#agentllm-context","title":"Agent/LLM context","text":"<p>When an LLM is called, the only data it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:</p> <ol> <li>You can add it to the Agent <code>instructions</code>. This is also known as a \"system prompt\" or \"developer message\". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).</li> <li>Add it to the <code>input</code> when calling the <code>Runner.run</code> functions. This is similar to the <code>instructions</code> tactic, but allows you to have messages that are lower in the chain of command.</li> <li>Expose it via function tools. This is useful for on-demand context - the LLM decides when it needs some data, and can call the tool to fetch that data.</li> <li>Use retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for \"grounding\" the response in relevant contextual data.</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>Check out a variety of sample implementations of the SDK in the examples section of the repo. The examples are organized into several categories that demonstrate different patterns and capabilities.</p>"},{"location":"examples/#categories","title":"Categories","text":"<ul> <li> <p>agent_patterns:     Examples in this category illustrate common agent design patterns, such as</p> <ul> <li>Deterministic workflows</li> <li>Agents as tools</li> <li>Parallel agent execution</li> <li>Conditional tool usage</li> <li>Input/output guardrails</li> <li>LLM as a judge</li> <li>Routing</li> <li>Streaming guardrails</li> </ul> </li> <li> <p>basic:     These examples showcase foundational capabilities of the SDK, such as</p> <ul> <li>Hello world examples (Default model, GPT-5, open-weight model)</li> <li>Agent lifecycle management</li> <li>Dynamic system prompts</li> <li>Streaming outputs (text, items, function call args)</li> <li>Prompt templates</li> <li>File handling (local and remote, images and PDFs)</li> <li>Usage tracking</li> <li>Non-strict output types</li> <li>Previous response ID usage</li> </ul> </li> <li> <p>customer_service:     Example customer service system for an airline.</p> </li> <li> <p>financial_research_agent:     A financial research agent that demonstrates structured research workflows with agents and tools for financial data analysis.</p> </li> <li> <p>handoffs:     See practical examples of agent handoffs with message filtering.</p> </li> <li> <p>hosted_mcp:     Examples demonstrating how to use hosted MCP (Model Context Protocol) connectors and approvals.</p> </li> <li> <p>mcp:     Learn how to build agents with MCP (Model Context Protocol), including:</p> <ul> <li>Filesystem examples</li> <li>Git examples</li> <li>MCP prompt server examples</li> <li>SSE (Server-Sent Events) examples</li> <li>Streamable HTTP examples</li> </ul> </li> <li> <p>memory:     Examples of different memory implementations for agents, including:</p> <ul> <li>SQLite session storage</li> <li>Advanced SQLite session storage</li> <li>Redis session storage</li> <li>SQLAlchemy session storage</li> <li>Encrypted session storage</li> <li>OpenAI session storage</li> </ul> </li> <li> <p>model_providers:     Explore how to use non-OpenAI models with the SDK, including custom providers and LiteLLM integration.</p> </li> <li> <p>realtime:     Examples showing how to build real-time experiences using the SDK, including:</p> <ul> <li>Web applications</li> <li>Command-line interfaces</li> <li>Twilio integration</li> </ul> </li> <li> <p>reasoning_content:     Examples demonstrating how to work with reasoning content and structured outputs.</p> </li> <li> <p>research_bot:     Simple deep research clone that demonstrates complex multi-agent research workflows.</p> </li> <li> <p>tools:     Learn how to implement OAI hosted tools and experimental Codex tooling such as:</p> <ul> <li>Web search and web search with filters</li> <li>File search</li> <li>Code interpreter</li> <li>Computer use</li> <li>Image generation</li> <li>Experimental Codex tool workflows (<code>examples/tools/codex.py</code>)</li> </ul> </li> <li> <p>voice:     See examples of voice agents, using our TTS and STT models, including streamed voice examples.</p> </li> </ul>"},{"location":"guardrails/","title":"Guardrails","text":"<p>Guardrails enable you to do checks and validations of user input and agent output. For example, imagine you have an agent that uses a very smart (and hence slow/expensive) model to help with customer requests. You wouldn't want malicious users to ask the model to help them with their math homework. So, you can run a guardrail with a fast/cheap model. If the guardrail detects malicious usage, it can immediately raise an error and prevent the expensive model from running, saving you time and money (when using blocking guardrails; for parallel guardrails, the expensive model may have already started running before the guardrail completes. See \"Execution modes\" below for details).</p> <p>There are two kinds of guardrails:</p> <ol> <li>Input guardrails run on the initial user input</li> <li>Output guardrails run on the final agent output</li> </ol>"},{"location":"guardrails/#input-guardrails","title":"Input guardrails","text":"<p>Input guardrails run in 3 steps:</p> <ol> <li>First, the guardrail receives the same input passed to the agent.</li> <li>Next, the guardrail function runs to produce a <code>GuardrailFunctionOutput</code>, which is then wrapped in an <code>InputGuardrailResult</code></li> <li>Finally, we check if <code>.tripwire_triggered</code> is true. If true, an <code>InputGuardrailTripwireTriggered</code> exception is raised, so you can appropriately respond to the user or handle the exception.</li> </ol> <p>Note</p> <p>Input guardrails are intended to run on user input, so an agent's guardrails only run if the agent is the first agent. You might wonder, why is the <code>guardrails</code> property on the agent instead of passed to <code>Runner.run</code>? It's because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.</p>"},{"location":"guardrails/#execution-modes","title":"Execution modes","text":"<p>Input guardrails support two execution modes:</p> <ul> <li> <p>Parallel execution (default, <code>run_in_parallel=True</code>): The guardrail runs concurrently with the agent's execution. This provides the best latency since both start at the same time. However, if the guardrail fails, the agent may have already consumed tokens and executed tools before being cancelled.</p> </li> <li> <p>Blocking execution (<code>run_in_parallel=False</code>): The guardrail runs and completes before the agent starts. If the guardrail tripwire is triggered, the agent never executes, preventing token consumption and tool execution. This is ideal for cost optimization and when you want to avoid potential side effects from tool calls.</p> </li> </ul>"},{"location":"guardrails/#output-guardrails","title":"Output guardrails","text":"<p>Output guardrails run in 3 steps:</p> <ol> <li>First, the guardrail receives the output produced by the agent.</li> <li>Next, the guardrail function runs to produce a <code>GuardrailFunctionOutput</code>, which is then wrapped in an <code>OutputGuardrailResult</code></li> <li>Finally, we check if <code>.tripwire_triggered</code> is true. If true, an <code>OutputGuardrailTripwireTriggered</code> exception is raised, so you can appropriately respond to the user or handle the exception.</li> </ol> <p>Note</p> <p>Output guardrails are intended to run on the final agent output, so an agent's guardrails only run if the agent is the last agent. Similar to the input guardrails, we do this because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.</p> <p>Output guardrails always run after the agent completes, so they don't support the <code>run_in_parallel</code> parameter.</p>"},{"location":"guardrails/#tool-guardrails","title":"Tool guardrails","text":"<p>Tool guardrails wrap function tools and let you validate or block tool calls before and after execution. They are configured on the tool itself and run every time that tool is invoked.</p> <ul> <li>Input tool guardrails run before the tool executes and can skip the call, replace the output with a message, or raise a tripwire.</li> <li>Output tool guardrails run after the tool executes and can replace the output or raise a tripwire.</li> <li>Tool guardrails apply only to function tools created with <code>function_tool</code>; hosted tools (<code>WebSearchTool</code>, <code>FileSearchTool</code>, <code>HostedMCPTool</code>, <code>CodeInterpreterTool</code>, <code>ImageGenerationTool</code>) and local runtime tools (<code>ComputerTool</code>, <code>ShellTool</code>, <code>ApplyPatchTool</code>, <code>LocalShellTool</code>) do not use this guardrail pipeline.</li> </ul> <p>See the code snippet below for details.</p>"},{"location":"guardrails/#tripwires","title":"Tripwires","text":"<p>If the input or output fails the guardrail, the Guardrail can signal this with a tripwire. As soon as we see a guardrail that has triggered the tripwires, we immediately raise a <code>{Input,Output}GuardrailTripwireTriggered</code> exception and halt the Agent execution.</p>"},{"location":"guardrails/#implementing-a-guardrail","title":"Implementing a guardrail","text":"<p>You need to provide a function that receives input, and returns a <code>GuardrailFunctionOutput</code>. In this example, we'll do this by running an Agent under the hood.</p> <pre><code>from pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    InputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    TResponseInputItem,\n    input_guardrail,\n)\n\nclass MathHomeworkOutput(BaseModel):\n    is_math_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent( # (1)!\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking you to do their math homework.\",\n    output_type=MathHomeworkOutput,\n)\n\n\n@input_guardrail\nasync def math_guardrail( # (2)!\n    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n) -&gt; GuardrailFunctionOutput:\n    result = await Runner.run(guardrail_agent, input, context=ctx.context)\n\n    return GuardrailFunctionOutput(\n        output_info=result.final_output, # (3)!\n        tripwire_triggered=result.final_output.is_math_homework,\n    )\n\n\nagent = Agent(  # (4)!\n    name=\"Customer support agent\",\n    instructions=\"You are a customer support agent. You help customers with their questions.\",\n    input_guardrails=[math_guardrail],\n)\n\nasync def main():\n    # This should trip the guardrail\n    try:\n        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n        print(\"Guardrail didn't trip - this is unexpected\")\n\n    except InputGuardrailTripwireTriggered:\n        print(\"Math homework guardrail tripped\")\n</code></pre> <ol> <li>We'll use this agent in our guardrail function.</li> <li>This is the guardrail function that receives the agent's input/context, and returns the result.</li> <li>We can include extra information in the guardrail result.</li> <li>This is the actual agent that defines the workflow.</li> </ol> <p>Output guardrails are similar.</p> <pre><code>from pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    OutputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    output_guardrail,\n)\nclass MessageOutput(BaseModel): # (1)!\n    response: str\n\nclass MathOutput(BaseModel): # (2)!\n    reasoning: str\n    is_math: bool\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the output includes any math.\",\n    output_type=MathOutput,\n)\n\n@output_guardrail\nasync def math_guardrail(  # (3)!\n    ctx: RunContextWrapper, agent: Agent, output: MessageOutput\n) -&gt; GuardrailFunctionOutput:\n    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)\n\n    return GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_math,\n    )\n\nagent = Agent( # (4)!\n    name=\"Customer support agent\",\n    instructions=\"You are a customer support agent. You help customers with their questions.\",\n    output_guardrails=[math_guardrail],\n    output_type=MessageOutput,\n)\n\nasync def main():\n    # This should trip the guardrail\n    try:\n        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n        print(\"Guardrail didn't trip - this is unexpected\")\n\n    except OutputGuardrailTripwireTriggered:\n        print(\"Math output guardrail tripped\")\n</code></pre> <ol> <li>This is the actual agent's output type.</li> <li>This is the guardrail's output type.</li> <li>This is the guardrail function that receives the agent's output, and returns the result.</li> <li>This is the actual agent that defines the workflow.</li> </ol> <p>Lastly, here are examples of tool guardrails.</p> <pre><code>import json\nfrom agents import (\n    Agent,\n    Runner,\n    ToolGuardrailFunctionOutput,\n    function_tool,\n    tool_input_guardrail,\n    tool_output_guardrail,\n)\n\n@tool_input_guardrail\ndef block_secrets(data):\n    args = json.loads(data.context.tool_arguments or \"{}\")\n    if \"sk-\" in json.dumps(args):\n        return ToolGuardrailFunctionOutput.reject_content(\n            \"Remove secrets before calling this tool.\"\n        )\n    return ToolGuardrailFunctionOutput.allow()\n\n\n@tool_output_guardrail\ndef redact_output(data):\n    text = str(data.output or \"\")\n    if \"sk-\" in text:\n        return ToolGuardrailFunctionOutput.reject_content(\"Output contained sensitive data.\")\n    return ToolGuardrailFunctionOutput.allow()\n\n\n@function_tool(\n    tool_input_guardrails=[block_secrets],\n    tool_output_guardrails=[redact_output],\n)\ndef classify_text(text: str) -&gt; str:\n    \"\"\"Classify text for internal routing.\"\"\"\n    return f\"length:{len(text)}\"\n\n\nagent = Agent(name=\"Classifier\", tools=[classify_text])\nresult = Runner.run_sync(agent, \"hello world\")\nprint(result.final_output)\n</code></pre>"},{"location":"handoffs/","title":"Handoffs","text":"<p>Handoffs allow an agent to delegate tasks to another agent. This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc.</p> <p>Handoffs are represented as tools to the LLM. So if there's a handoff to an agent named <code>Refund Agent</code>, the tool would be called <code>transfer_to_refund_agent</code>.</p>"},{"location":"handoffs/#creating-a-handoff","title":"Creating a handoff","text":"<p>All agents have a <code>handoffs</code> param, which can either take an <code>Agent</code> directly, or a <code>Handoff</code> object that customizes the Handoff.</p> <p>If you pass plain <code>Agent</code> instances, their <code>handoff_description</code> (when set) is appended to the default tool description. Use it to hint when the model should pick that handoff without writing a full <code>handoff()</code> object.</p> <p>You can create a handoff using the <code>handoff()</code> function provided by the Agents SDK. This function allows you to specify the agent to hand off to, along with optional overrides and input filters.</p>"},{"location":"handoffs/#basic-usage","title":"Basic Usage","text":"<p>Here's how you can create a simple handoff:</p> <pre><code>from agents import Agent, handoff\n\nbilling_agent = Agent(name=\"Billing agent\")\nrefund_agent = Agent(name=\"Refund agent\")\n\n# (1)!\ntriage_agent = Agent(name=\"Triage agent\", handoffs=[billing_agent, handoff(refund_agent)])\n</code></pre> <ol> <li>You can use the agent directly (as in <code>billing_agent</code>), or you can use the <code>handoff()</code> function.</li> </ol>"},{"location":"handoffs/#customizing-handoffs-via-the-handoff-function","title":"Customizing handoffs via the <code>handoff()</code> function","text":"<p>The <code>handoff()</code> function lets you customize things.</p> <ul> <li><code>agent</code>: This is the agent to which things will be handed off.</li> <li><code>tool_name_override</code>: By default, the <code>Handoff.default_tool_name()</code> function is used, which resolves to <code>transfer_to_&lt;agent_name&gt;</code>. You can override this.</li> <li><code>tool_description_override</code>: Override the default tool description from <code>Handoff.default_tool_description()</code></li> <li><code>on_handoff</code>: A callback function executed when the handoff is invoked. This is useful for things like kicking off some data fetching as soon as you know a handoff is being invoked. This function receives the agent context, and can optionally also receive LLM generated input. The input data is controlled by the <code>input_type</code> param.</li> <li><code>input_type</code>: The type of input expected by the handoff (optional).</li> <li><code>input_filter</code>: This lets you filter the input received by the next agent. See below for more.</li> <li><code>is_enabled</code>: Whether the handoff is enabled. This can be a boolean or a function that returns a boolean, allowing you to dynamically enable or disable the handoff at runtime.</li> </ul> <pre><code>from agents import Agent, handoff, RunContextWrapper\n\ndef on_handoff(ctx: RunContextWrapper[None]):\n    print(\"Handoff called\")\n\nagent = Agent(name=\"My agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    tool_name_override=\"custom_handoff_tool\",\n    tool_description_override=\"Custom description\",\n)\n</code></pre>"},{"location":"handoffs/#handoff-inputs","title":"Handoff inputs","text":"<p>In certain situations, you want the LLM to provide some data when it calls a handoff. For example, imagine a handoff to an \"Escalation agent\". You might want a reason to be provided, so you can log it.</p> <pre><code>from pydantic import BaseModel\n\nfrom agents import Agent, handoff, RunContextWrapper\n\nclass EscalationData(BaseModel):\n    reason: str\n\nasync def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):\n    print(f\"Escalation agent called with reason: {input_data.reason}\")\n\nagent = Agent(name=\"Escalation agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    input_type=EscalationData,\n)\n</code></pre>"},{"location":"handoffs/#input-filters","title":"Input filters","text":"<p>When a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history. If you want to change this, you can set an <code>input_filter</code>. An input filter is a function that receives the existing input via a <code>HandoffInputData</code>, and must return a new <code>HandoffInputData</code>.</p> <p>Nested handoffs are available as an opt-in beta and are disabled by default while we stabilize them. When you enable <code>RunConfig.nest_handoff_history</code>, the runner collapses the prior transcript into a single assistant summary message and wraps it in a <code>&lt;CONVERSATION HISTORY&gt;</code> block that keeps appending new turns when multiple handoffs happen during the same run. You can provide your own mapping function via <code>RunConfig.handoff_history_mapper</code> to replace the generated message without writing a full <code>input_filter</code>. The opt-in only applies when neither the handoff nor the run supplies an explicit <code>input_filter</code>, so existing code that already customizes the payload (including the examples in this repository) keeps its current behavior without changes. You can override the nesting behaviour for a single handoff by passing <code>nest_handoff_history=True</code> or <code>False</code> to <code>handoff(...)</code>, which sets <code>Handoff.nest_handoff_history</code>. If you just need to change the wrapper text for the generated summary, call <code>set_conversation_history_wrappers</code> (and optionally <code>reset_conversation_history_wrappers</code>) before running your agents.</p> <p>There are some common patterns (for example removing all tool calls from the history), which are implemented for you in <code>agents.extensions.handoff_filters</code></p> <pre><code>from agents import Agent, handoff\nfrom agents.extensions import handoff_filters\n\nagent = Agent(name=\"FAQ agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    input_filter=handoff_filters.remove_all_tools, # (1)!\n)\n</code></pre> <ol> <li>This will automatically remove all tools from the history when <code>FAQ agent</code> is called.</li> </ol>"},{"location":"handoffs/#recommended-prompts","title":"Recommended prompts","text":"<p>To make sure that LLMs understand handoffs properly, we recommend including information about handoffs in your agents. We have a suggested prefix in <code>agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX</code>, or you can call <code>agents.extensions.handoff_prompt.prompt_with_handoff_instructions</code> to automatically add recommended data to your prompts.</p> <pre><code>from agents import Agent\nfrom agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n\nbilling_agent = Agent(\n    name=\"Billing agent\",\n    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n    &lt;Fill in the rest of your prompt here&gt;.\"\"\",\n)\n</code></pre>"},{"location":"human_in_the_loop/","title":"Human-in-the-loop","text":"<p>Use the human-in-the-loop (HITL) flow to pause agent execution until a person approves or rejects sensitive tool calls. Tools declare when they need approval, run results surface pending approvals as interruptions, and <code>RunState</code> lets you serialize and resume runs after decisions are made.</p>"},{"location":"human_in_the_loop/#marking-tools-that-need-approval","title":"Marking tools that need approval","text":"<p>Set <code>needs_approval</code> to <code>True</code> to always require approval or provide an async function that decides per call. The callable receives the run context, parsed tool parameters, and the tool call ID.</p> <pre><code>from agents import Agent, Runner, function_tool\n\n\n@function_tool(needs_approval=True)\nasync def cancel_order(order_id: int) -&gt; str:\n    return f\"Cancelled order {order_id}\"\n\n\nasync def requires_review(_ctx, params, _call_id) -&gt; bool:\n    return \"refund\" in params.get(\"subject\", \"\").lower()\n\n\n@function_tool(needs_approval=requires_review)\nasync def send_email(subject: str, body: str) -&gt; str:\n    return f\"Sent '{subject}'\"\n\n\nagent = Agent(\n    name=\"Support agent\",\n    instructions=\"Handle tickets and ask for approval when needed.\",\n    tools=[cancel_order, send_email],\n)\n</code></pre> <p><code>needs_approval</code> is available on <code>function_tool</code>, <code>Agent.as_tool</code>, <code>ShellTool</code>, and <code>ApplyPatchTool</code>. Local MCP servers also support approvals through <code>require_approval</code> on <code>MCPServerStdio</code>, <code>MCPServerSse</code>, and <code>MCPServerStreamableHttp</code>. Hosted MCP servers support approvals via <code>HostedMCPTool</code> with <code>tool_config={\"require_approval\": \"always\"}</code> and an optional <code>on_approval_request</code> callback. Shell and apply_patch tools accept an <code>on_approval</code> callback if you want to auto-approve or auto-reject without surfacing an interruption.</p>"},{"location":"human_in_the_loop/#how-the-approval-flow-works","title":"How the approval flow works","text":"<ol> <li>When the model emits a tool call, the runner evaluates <code>needs_approval</code>.</li> <li>If an approval decision for that tool call is already stored in the <code>RunContextWrapper</code> (for example, from <code>always_approve=True</code>), the runner proceeds without prompting. Per-call approvals are scoped to the specific call ID; use <code>always_approve=True</code> to allow future calls automatically.</li> <li>Otherwise, execution pauses and <code>RunResult.interruptions</code> (or <code>RunResultStreaming.interruptions</code>) contains <code>ToolApprovalItem</code> entries with details such as <code>agent.name</code>, <code>name</code>, and <code>arguments</code>.</li> <li>Convert the result to a <code>RunState</code> with <code>result.to_state()</code>, call <code>state.approve(...)</code> or <code>state.reject(...)</code> (optionally passing <code>always_approve</code> or <code>always_reject</code>), and then resume with <code>Runner.run(agent, state)</code> or <code>Runner.run_streamed(agent, state)</code>.</li> <li>The resumed run continues where it left off and will re-enter this flow if new approvals are needed.</li> </ol>"},{"location":"human_in_the_loop/#example-pause-approve-resume","title":"Example: pause, approve, resume","text":"<p>The snippet below mirrors the JavaScript HITL guide: it pauses when a tool needs approval, persists state to disk, reloads it, and resumes after collecting a decision.</p> <pre><code>import asyncio\nimport json\nfrom pathlib import Path\n\nfrom agents import Agent, Runner, RunState, function_tool\n\n\nasync def needs_oakland_approval(_ctx, params, _call_id) -&gt; bool:\n    return \"Oakland\" in params.get(\"city\", \"\")\n\n\n@function_tool(needs_approval=needs_oakland_approval)\nasync def get_temperature(city: str) -&gt; str:\n    return f\"The temperature in {city} is 20\u00b0 Celsius\"\n\n\nagent = Agent(\n    name=\"Weather assistant\",\n    instructions=\"Answer weather questions with the provided tools.\",\n    tools=[get_temperature],\n)\n\nSTATE_PATH = Path(\".cache/hitl_state.json\")\n\n\ndef prompt_approval(tool_name: str, arguments: str | None) -&gt; bool:\n    answer = input(f\"Approve {tool_name} with {arguments}? [y/N]: \").strip().lower()\n    return answer in {\"y\", \"yes\"}\n\n\nasync def main() -&gt; None:\n    result = await Runner.run(agent, \"What is the temperature in Oakland?\")\n\n    while result.interruptions:\n        # Persist the paused state.\n        state = result.to_state()\n        STATE_PATH.parent.mkdir(parents=True, exist_ok=True)\n        STATE_PATH.write_text(state.to_string())\n\n        # Load the state later (could be a different process).\n        stored = json.loads(STATE_PATH.read_text())\n        state = await RunState.from_json(agent, stored)\n\n        for interruption in result.interruptions:\n            approved = await asyncio.get_running_loop().run_in_executor(\n                None, prompt_approval, interruption.name or \"unknown_tool\", interruption.arguments\n            )\n            if approved:\n                state.approve(interruption, always_approve=False)\n            else:\n                state.reject(interruption)\n\n        result = await Runner.run(agent, state)\n\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>In this example, <code>prompt_approval</code> is synchronous because it uses <code>input()</code> and is executed with <code>run_in_executor(...)</code>. If your approval source is already asynchronous (for example, an HTTP request or async database query), you can use an <code>async def</code> function and <code>await</code> it directly instead.</p> <p>To stream output while waiting for approvals, call <code>Runner.run_streamed</code>, consume <code>result.stream_events()</code> until it completes, and then follow the same <code>result.to_state()</code> and resume steps shown above.</p>"},{"location":"human_in_the_loop/#other-patterns-in-this-repository","title":"Other patterns in this repository","text":"<ul> <li>Streaming approvals: <code>examples/agent_patterns/human_in_the_loop_stream.py</code> shows how to drain <code>stream_events()</code> and then approve pending tool calls before resuming with <code>Runner.run_streamed(agent, state)</code>.</li> <li>Agent as tool approvals: <code>Agent.as_tool(..., needs_approval=...)</code> applies the same interruption flow when delegated agent tasks need review.</li> <li>Shell and apply_patch tools: <code>ShellTool</code> and <code>ApplyPatchTool</code> also support <code>needs_approval</code>. Use <code>state.approve(interruption, always_approve=True)</code> or <code>state.reject(..., always_reject=True)</code> to cache the decision for future calls. For automatic decisions, provide <code>on_approval</code> (see <code>examples/tools/shell.py</code>); for manual decisions, handle interruptions (see <code>examples/tools/shell_human_in_the_loop.py</code>).</li> <li>Local MCP servers: Use <code>require_approval</code> on <code>MCPServerStdio</code> / <code>MCPServerSse</code> / <code>MCPServerStreamableHttp</code> to gate MCP tool calls (see <code>examples/mcp/get_all_mcp_tools_example/main.py</code> and <code>examples/mcp/tool_filter_example/main.py</code>).</li> <li>Hosted MCP servers: Set <code>require_approval</code> to <code>\"always\"</code> on <code>HostedMCPTool</code> to force HITL, optionally providing <code>on_approval_request</code> to auto-approve or reject (see <code>examples/hosted_mcp/human_in_the_loop.py</code> and <code>examples/hosted_mcp/on_approval.py</code>). Use <code>\"never\"</code> for trusted servers (<code>examples/hosted_mcp/simple.py</code>).</li> <li>Sessions and memory: Pass a session to <code>Runner.run</code> so approvals and conversation history survive multiple turns. SQLite and OpenAI Conversations session variants are in <code>examples/memory/memory_session_hitl_example.py</code> and <code>examples/memory/openai_session_hitl_example.py</code>.</li> <li>Realtime agents: The realtime demo exposes WebSocket messages that approve or reject tool calls via <code>approve_tool_call</code> / <code>reject_tool_call</code> on the <code>RealtimeSession</code> (see <code>examples/realtime/app/server.py</code> for the server-side handlers).</li> </ul>"},{"location":"human_in_the_loop/#long-running-approvals","title":"Long-running approvals","text":"<p><code>RunState</code> is designed to be durable. Use <code>state.to_json()</code> or <code>state.to_string()</code> to store pending work in a database or queue and recreate it later with <code>RunState.from_json(...)</code> or <code>RunState.from_string(...)</code>. Pass <code>context_override</code> if you do not want to persist sensitive context data in the serialized payload.</p>"},{"location":"human_in_the_loop/#versioning-pending-tasks","title":"Versioning pending tasks","text":"<p>If approvals may sit for a while, store a version marker for your agent definitions or SDK alongside the serialized state. You can then route deserialization to the matching code path to avoid incompatibilities when models, prompts, or tool definitions change.</p>"},{"location":"mcp/","title":"Model context protocol (MCP)","text":"<p>The Model context protocol (MCP) standardises how applications expose tools and context to language models. From the official documentation:</p> <p>MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p> <p>The Agents Python SDK understands multiple MCP transports. This lets you reuse existing MCP servers or build your own to expose filesystem, HTTP, or connector backed tools to an agent.</p>"},{"location":"mcp/#choosing-an-mcp-integration","title":"Choosing an MCP integration","text":"<p>Before wiring an MCP server into an agent decide where the tool calls should execute and which transports you can reach. The matrix below summarises the options that the Python SDK supports.</p> What you need Recommended option Let OpenAI's Responses API call a publicly reachable MCP server on the model's behalf Hosted MCP server tools via <code>HostedMCPTool</code> Connect to Streamable HTTP servers that you run locally or remotely Streamable HTTP MCP servers via <code>MCPServerStreamableHttp</code> Talk to servers that implement HTTP with Server-Sent Events HTTP with SSE MCP servers via <code>MCPServerSse</code> Launch a local process and communicate over stdin/stdout stdio MCP servers via <code>MCPServerStdio</code> <p>The sections below walk through each option, how to configure it, and when to prefer one transport over another.</p>"},{"location":"mcp/#agent-level-mcp-configuration","title":"Agent-level MCP configuration","text":"<p>In addition to choosing a transport, you can tune how MCP tools are prepared by setting <code>Agent.mcp_config</code>.</p> <pre><code>from agents import Agent\n\nagent = Agent(\n    name=\"Assistant\",\n    mcp_servers=[server],\n    mcp_config={\n        # Try to convert MCP tool schemas to strict JSON schema.\n        \"convert_schemas_to_strict\": True,\n        # If None, MCP tool failures are raised as exceptions instead of\n        # returning model-visible error text.\n        \"failure_error_function\": None,\n    },\n)\n</code></pre> <p>Notes:</p> <ul> <li><code>convert_schemas_to_strict</code> is best-effort. If a schema cannot be converted, the original schema is used.</li> <li><code>failure_error_function</code> controls how MCP tool call failures are surfaced to the model.</li> <li>When <code>failure_error_function</code> is unset, the SDK uses the default tool error formatter.</li> <li>Server-level <code>failure_error_function</code> overrides <code>Agent.mcp_config[\"failure_error_function\"]</code> for that server.</li> </ul>"},{"location":"mcp/#1-hosted-mcp-server-tools","title":"1. Hosted MCP server tools","text":"<p>Hosted tools push the entire tool round-trip into OpenAI's infrastructure. Instead of your code listing and calling tools, the <code>HostedMCPTool</code> forwards a server label (and optional connector metadata) to the Responses API. The model lists the remote server's tools and invokes them without an extra callback to your Python process. Hosted tools currently work with OpenAI models that support the Responses API's hosted MCP integration.</p>"},{"location":"mcp/#basic-hosted-mcp-tool","title":"Basic hosted MCP tool","text":"<p>Create a hosted tool by adding a <code>HostedMCPTool</code> to the agent's <code>tools</code> list. The <code>tool_config</code> dict mirrors the JSON you would send to the REST API:</p> <pre><code>import asyncio\n\nfrom agents import Agent, HostedMCPTool, Runner\n\nasync def main() -&gt; None:\n    agent = Agent(\n        name=\"Assistant\",\n        tools=[\n            HostedMCPTool(\n                tool_config={\n                    \"type\": \"mcp\",\n                    \"server_label\": \"gitmcp\",\n                    \"server_url\": \"https://gitmcp.io/openai/codex\",\n                    \"require_approval\": \"never\",\n                }\n            )\n        ],\n    )\n\n    result = await Runner.run(agent, \"Which language is this repository written in?\")\n    print(result.final_output)\n\nasyncio.run(main())\n</code></pre> <p>The hosted server exposes its tools automatically; you do not add it to <code>mcp_servers</code>.</p>"},{"location":"mcp/#streaming-hosted-mcp-results","title":"Streaming hosted MCP results","text":"<p>Hosted tools support streaming results in exactly the same way as function tools. Pass <code>stream=True</code> to <code>Runner.run_streamed</code> to consume incremental MCP output while the model is still working:</p> <pre><code>result = Runner.run_streamed(agent, \"Summarise this repository's top languages\")\nasync for event in result.stream_events():\n    if event.type == \"run_item_stream_event\":\n        print(f\"Received: {event.item}\")\nprint(result.final_output)\n</code></pre>"},{"location":"mcp/#optional-approval-flows","title":"Optional approval flows","text":"<p>If a server can perform sensitive operations you can require human or programmatic approval before each tool execution. Configure <code>require_approval</code> in the <code>tool_config</code> with either a single policy (<code>\"always\"</code>, <code>\"never\"</code>) or a dict mapping tool names to policies. To make the decision inside Python, provide an <code>on_approval_request</code> callback.</p> <pre><code>from agents import MCPToolApprovalFunctionResult, MCPToolApprovalRequest\n\nSAFE_TOOLS = {\"read_project_metadata\"}\n\ndef approve_tool(request: MCPToolApprovalRequest) -&gt; MCPToolApprovalFunctionResult:\n    if request.data.name in SAFE_TOOLS:\n        return {\"approve\": True}\n    return {\"approve\": False, \"reason\": \"Escalate to a human reviewer\"}\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[\n        HostedMCPTool(\n            tool_config={\n                \"type\": \"mcp\",\n                \"server_label\": \"gitmcp\",\n                \"server_url\": \"https://gitmcp.io/openai/codex\",\n                \"require_approval\": \"always\",\n            },\n            on_approval_request=approve_tool,\n        )\n    ],\n)\n</code></pre> <p>The callback can be synchronous or asynchronous and is invoked whenever the model needs approval data to keep running.</p>"},{"location":"mcp/#connector-backed-hosted-servers","title":"Connector-backed hosted servers","text":"<p>Hosted MCP also supports OpenAI connectors. Instead of specifying a <code>server_url</code>, supply a <code>connector_id</code> and an access token. The Responses API handles authentication and the hosted server exposes the connector's tools.</p> <pre><code>import os\n\nHostedMCPTool(\n    tool_config={\n        \"type\": \"mcp\",\n        \"server_label\": \"google_calendar\",\n        \"connector_id\": \"connector_googlecalendar\",\n        \"authorization\": os.environ[\"GOOGLE_CALENDAR_AUTHORIZATION\"],\n        \"require_approval\": \"never\",\n    }\n)\n</code></pre> <p>Fully working hosted tool samples\u2014including streaming, approvals, and connectors\u2014live in <code>examples/hosted_mcp</code>.</p>"},{"location":"mcp/#2-streamable-http-mcp-servers","title":"2. Streamable HTTP MCP servers","text":"<p>When you want to manage the network connection yourself, use <code>MCPServerStreamableHttp</code>. Streamable HTTP servers are ideal when you control the transport or want to run the server inside your own infrastructure while keeping latency low.</p> <pre><code>import asyncio\nimport os\n\nfrom agents import Agent, Runner\nfrom agents.mcp import MCPServerStreamableHttp\nfrom agents.model_settings import ModelSettings\n\nasync def main() -&gt; None:\n    token = os.environ[\"MCP_SERVER_TOKEN\"]\n    async with MCPServerStreamableHttp(\n        name=\"Streamable HTTP Python Server\",\n        params={\n            \"url\": \"http://localhost:8000/mcp\",\n            \"headers\": {\"Authorization\": f\"Bearer {token}\"},\n            \"timeout\": 10,\n        },\n        cache_tools_list=True,\n        max_retry_attempts=3,\n    ) as server:\n        agent = Agent(\n            name=\"Assistant\",\n            instructions=\"Use the MCP tools to answer the questions.\",\n            mcp_servers=[server],\n            model_settings=ModelSettings(tool_choice=\"required\"),\n        )\n\n        result = await Runner.run(agent, \"Add 7 and 22.\")\n        print(result.final_output)\n\nasyncio.run(main())\n</code></pre> <p>The constructor accepts additional options:</p> <ul> <li><code>client_session_timeout_seconds</code> controls HTTP read timeouts.</li> <li><code>use_structured_content</code> toggles whether <code>tool_result.structured_content</code> is preferred over textual output.</li> <li><code>max_retry_attempts</code> and <code>retry_backoff_seconds_base</code> add automatic retries for <code>list_tools()</code> and <code>call_tool()</code>.</li> <li><code>tool_filter</code> lets you expose only a subset of tools (see Tool filtering).</li> <li><code>require_approval</code> enables human-in-the-loop approval policies on local MCP tools.</li> <li><code>failure_error_function</code> customizes model-visible MCP tool failure messages; set it to <code>None</code> to raise errors instead.</li> <li><code>tool_meta_resolver</code> injects per-call MCP <code>_meta</code> payloads before <code>call_tool()</code>.</li> </ul>"},{"location":"mcp/#approval-policies-for-local-mcp-servers","title":"Approval policies for local MCP servers","text":"<p><code>MCPServerStdio</code>, <code>MCPServerSse</code>, and <code>MCPServerStreamableHttp</code> all accept <code>require_approval</code>.</p> <p>Supported forms:</p> <ul> <li><code>\"always\"</code> or <code>\"never\"</code> for all tools.</li> <li><code>True</code> / <code>False</code> (equivalent to always/never).</li> <li>A per-tool map, for example <code>{\"delete_file\": \"always\", \"read_file\": \"never\"}</code>.</li> <li>A grouped object:   <code>{\"always\": {\"tool_names\": [...]}, \"never\": {\"tool_names\": [...]}}</code>.</li> </ul> <pre><code>async with MCPServerStreamableHttp(\n    name=\"Filesystem MCP\",\n    params={\"url\": \"http://localhost:8000/mcp\"},\n    require_approval={\"always\": {\"tool_names\": [\"delete_file\"]}},\n) as server:\n    ...\n</code></pre> <p>For a full pause/resume flow, see Human-in-the-loop and <code>examples/mcp/get_all_mcp_tools_example/main.py</code>.</p>"},{"location":"mcp/#per-call-metadata-with-tool_meta_resolver","title":"Per-call metadata with <code>tool_meta_resolver</code>","text":"<p>Use <code>tool_meta_resolver</code> when your MCP server expects request metadata in <code>_meta</code> (for example, tenant IDs or trace context). The example below assumes you pass a <code>dict</code> as <code>context</code> to <code>Runner.run(...)</code>.</p> <pre><code>from agents.mcp import MCPServerStreamableHttp, MCPToolMetaContext\n\n\ndef resolve_meta(context: MCPToolMetaContext) -&gt; dict[str, str] | None:\n    run_context_data = context.run_context.context or {}\n    tenant_id = run_context_data.get(\"tenant_id\")\n    if tenant_id is None:\n        return None\n    return {\"tenant_id\": str(tenant_id), \"source\": \"agents-sdk\"}\n\n\nserver = MCPServerStreamableHttp(\n    name=\"Metadata-aware MCP\",\n    params={\"url\": \"http://localhost:8000/mcp\"},\n    tool_meta_resolver=resolve_meta,\n)\n</code></pre> <p>If your run context is a Pydantic model, dataclass, or custom class, read the tenant ID with attribute access instead.</p>"},{"location":"mcp/#mcp-tool-outputs-text-and-images","title":"MCP tool outputs: text and images","text":"<p>When an MCP tool returns image content, the SDK maps it to image tool output entries automatically. Mixed text/image responses are forwarded as a list of output items, so agents can consume MCP image results the same way they consume image output from regular function tools.</p>"},{"location":"mcp/#3-http-with-sse-mcp-servers","title":"3. HTTP with SSE MCP servers","text":"<p>Warning</p> <p>The MCP project has deprecated the Server-Sent Events transport. Prefer Streamable HTTP or stdio for new integrations and keep SSE only for legacy servers.</p> <p>If the MCP server implements the HTTP with SSE transport, instantiate <code>MCPServerSse</code>. Apart from the transport, the API is identical to the Streamable HTTP server.</p> <pre><code>from agents import Agent, Runner\nfrom agents.model_settings import ModelSettings\nfrom agents.mcp import MCPServerSse\n\nworkspace_id = \"demo-workspace\"\n\nasync with MCPServerSse(\n    name=\"SSE Python Server\",\n    params={\n        \"url\": \"http://localhost:8000/sse\",\n        \"headers\": {\"X-Workspace\": workspace_id},\n    },\n    cache_tools_list=True,\n) as server:\n    agent = Agent(\n        name=\"Assistant\",\n        mcp_servers=[server],\n        model_settings=ModelSettings(tool_choice=\"required\"),\n    )\n    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n    print(result.final_output)\n</code></pre>"},{"location":"mcp/#4-stdio-mcp-servers","title":"4. stdio MCP servers","text":"<p>For MCP servers that run as local subprocesses, use <code>MCPServerStdio</code>. The SDK spawns the process, keeps the pipes open, and closes them automatically when the context manager exits. This option is helpful for quick proofs of concept or when the server only exposes a command line entry point.</p> <pre><code>from pathlib import Path\nfrom agents import Agent, Runner\nfrom agents.mcp import MCPServerStdio\n\ncurrent_dir = Path(__file__).parent\nsamples_dir = current_dir / \"sample_files\"\n\nasync with MCPServerStdio(\n    name=\"Filesystem Server via npx\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", str(samples_dir)],\n    },\n) as server:\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"Use the files in the sample directory to answer questions.\",\n        mcp_servers=[server],\n    )\n    result = await Runner.run(agent, \"List the files available to you.\")\n    print(result.final_output)\n</code></pre>"},{"location":"mcp/#5-mcp-server-manager","title":"5. MCP server manager","text":"<p>When you have multiple MCP servers, use <code>MCPServerManager</code> to connect them up front and expose the connected subset to your agents.</p> <pre><code>from agents import Agent, Runner\nfrom agents.mcp import MCPServerManager, MCPServerStreamableHttp\n\nservers = [\n    MCPServerStreamableHttp(name=\"calendar\", params={\"url\": \"http://localhost:8000/mcp\"}),\n    MCPServerStreamableHttp(name=\"docs\", params={\"url\": \"http://localhost:8001/mcp\"}),\n]\n\nasync with MCPServerManager(servers) as manager:\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"Use MCP tools when they help.\",\n        mcp_servers=manager.active_servers,\n    )\n    result = await Runner.run(agent, \"Which MCP tools are available?\")\n    print(result.final_output)\n</code></pre> <p>Key behaviors:</p> <ul> <li><code>active_servers</code> includes only successfully connected servers when <code>drop_failed_servers=True</code> (the default).</li> <li>Failures are tracked in <code>failed_servers</code> and <code>errors</code>.</li> <li>Set <code>strict=True</code> to raise on the first connection failure.</li> <li>Call <code>reconnect(failed_only=True)</code> to retry failed servers, or <code>reconnect(failed_only=False)</code> to restart all servers.</li> <li>Use <code>connect_timeout_seconds</code>, <code>cleanup_timeout_seconds</code>, and <code>connect_in_parallel</code> to tune lifecycle behavior.</li> </ul>"},{"location":"mcp/#tool-filtering","title":"Tool filtering","text":"<p>Each MCP server supports tool filters so that you can expose only the functions that your agent needs. Filtering can happen at construction time or dynamically per run.</p>"},{"location":"mcp/#static-tool-filtering","title":"Static tool filtering","text":"<p>Use <code>create_static_tool_filter</code> to configure simple allow/block lists:</p> <pre><code>from pathlib import Path\n\nfrom agents.mcp import MCPServerStdio, create_static_tool_filter\n\nsamples_dir = Path(\"/path/to/files\")\n\nfilesystem_server = MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", str(samples_dir)],\n    },\n    tool_filter=create_static_tool_filter(allowed_tool_names=[\"read_file\", \"write_file\"]),\n)\n</code></pre> <p>When both <code>allowed_tool_names</code> and <code>blocked_tool_names</code> are supplied the SDK applies the allow-list first and then removes any blocked tools from the remaining set.</p>"},{"location":"mcp/#dynamic-tool-filtering","title":"Dynamic tool filtering","text":"<p>For more elaborate logic pass a callable that receives a <code>ToolFilterContext</code>. The callable can be synchronous or asynchronous and returns <code>True</code> when the tool should be exposed.</p> <pre><code>from pathlib import Path\n\nfrom agents.mcp import MCPServerStdio, ToolFilterContext\n\nsamples_dir = Path(\"/path/to/files\")\n\nasync def context_aware_filter(context: ToolFilterContext, tool) -&gt; bool:\n    if context.agent.name == \"Code Reviewer\" and tool.name.startswith(\"danger_\"):\n        return False\n    return True\n\nasync with MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", str(samples_dir)],\n    },\n    tool_filter=context_aware_filter,\n) as server:\n    ...\n</code></pre> <p>The filter context exposes the active <code>run_context</code>, the <code>agent</code> requesting the tools, and the <code>server_name</code>.</p>"},{"location":"mcp/#prompts","title":"Prompts","text":"<p>MCP servers can also provide prompts that dynamically generate agent instructions. Servers that support prompts expose two methods:</p> <ul> <li><code>list_prompts()</code> enumerates the available prompt templates.</li> <li><code>get_prompt(name, arguments)</code> fetches a concrete prompt, optionally with parameters.</li> </ul> <pre><code>from agents import Agent\n\nprompt_result = await server.get_prompt(\n    \"generate_code_review_instructions\",\n    {\"focus\": \"security vulnerabilities\", \"language\": \"python\"},\n)\ninstructions = prompt_result.messages[0].content.text\n\nagent = Agent(\n    name=\"Code Reviewer\",\n    instructions=instructions,\n    mcp_servers=[server],\n)\n</code></pre>"},{"location":"mcp/#caching","title":"Caching","text":"<p>Every agent run calls <code>list_tools()</code> on each MCP server. Remote servers can introduce noticeable latency, so all of the MCP server classes expose a <code>cache_tools_list</code> option. Set it to <code>True</code> only if you are confident that the tool definitions do not change frequently. To force a fresh list later, call <code>invalidate_tools_cache()</code> on the server instance.</p>"},{"location":"mcp/#tracing","title":"Tracing","text":"<p>Tracing automatically captures MCP activity, including:</p> <ol> <li>Calls to the MCP server to list tools.</li> <li>MCP-related information on tool calls.</li> </ol> <p></p>"},{"location":"mcp/#further-reading","title":"Further reading","text":"<ul> <li>Model Context Protocol \u2013 the specification and design guides.</li> <li>examples/mcp \u2013 runnable stdio, SSE, and Streamable HTTP samples.</li> <li>examples/hosted_mcp \u2013 complete hosted MCP demonstrations including approvals and connectors.</li> </ul>"},{"location":"multi_agent/","title":"Orchestrating multiple agents","text":"<p>Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:</p> <ol> <li>Allowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.</li> <li>Orchestrating via code: determining the flow of agents via your code.</li> </ol> <p>You can mix and match these patterns. Each has their own tradeoffs, described below.</p>"},{"location":"multi_agent/#orchestrating-via-llm","title":"Orchestrating via LLM","text":"<p>An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like:</p> <ul> <li>Web search to find information online</li> <li>File search and retrieval to search through proprietary data and connections</li> <li>Computer use to take actions on a computer</li> <li>Code execution to do data analysis</li> <li>Handoffs to specialized agents that are great at planning, report writing and more.</li> </ul> <p>This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are:</p> <ol> <li>Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.</li> <li>Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts.</li> <li>Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.</li> <li>Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.</li> <li>Invest in evals. This lets you train your agents to improve and get better at tasks.</li> </ol>"},{"location":"multi_agent/#orchestrating-via-code","title":"Orchestrating via code","text":"<p>While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are:</p> <ul> <li>Using structured outputs to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.</li> <li>Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.</li> <li>Running the agent that performs the task in a <code>while</code> loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.</li> <li>Running multiple agents in parallel, e.g. via Python primitives like <code>asyncio.gather</code>. This is useful for speed when you have multiple tasks that don't depend on each other.</li> </ul> <p>We have a number of examples in <code>examples/agent_patterns</code>.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#create-a-project-and-virtual-environment","title":"Create a project and virtual environment","text":"<p>You'll only need to do this once.</p> <pre><code>mkdir my_project\ncd my_project\npython -m venv .venv\n</code></pre>"},{"location":"quickstart/#activate-the-virtual-environment","title":"Activate the virtual environment","text":"<p>Do this every time you start a new terminal session.</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"quickstart/#install-the-agents-sdk","title":"Install the Agents SDK","text":"<pre><code>pip install openai-agents # or `uv add openai-agents`, etc\n</code></pre>"},{"location":"quickstart/#set-an-openai-api-key","title":"Set an OpenAI API key","text":"<p>If you don't have one, follow these instructions to create an OpenAI API key.</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"quickstart/#create-your-first-agent","title":"Create your first agent","text":"<p>Agents are defined with instructions, a name, and optional config (such as <code>model_config</code>)</p> <pre><code>from agents import Agent\n\nagent = Agent(\n    name=\"Math Tutor\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n</code></pre>"},{"location":"quickstart/#add-a-few-more-agents","title":"Add a few more agents","text":"<p>Additional agents can be defined in the same way. <code>handoff_descriptions</code> provide additional context for determining handoff routing</p> <pre><code>from agents import Agent\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n</code></pre>"},{"location":"quickstart/#define-your-handoffs","title":"Define your handoffs","text":"<p>On each agent, you can define an inventory of outgoing handoff options that the agent can choose from to decide how to make progress on their task.</p> <pre><code>triage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"You determine which agent to use based on the user's homework question\",\n    handoffs=[history_tutor_agent, math_tutor_agent]\n)\n</code></pre>"},{"location":"quickstart/#run-the-agent-orchestration","title":"Run the agent orchestration","text":"<p>Let's check that the workflow runs and the triage agent correctly routes between the two specialist agents.</p> <pre><code>from agents import Runner\n\nasync def main():\n    result = await Runner.run(triage_agent, \"who was the first president of the united states?\")\n    print(result.final_output)\n</code></pre>"},{"location":"quickstart/#add-a-guardrail","title":"Add a guardrail","text":"<p>You can define custom guardrails to run on the input or output.</p> <pre><code>from agents import GuardrailFunctionOutput, Agent, Runner\nfrom pydantic import BaseModel\n\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n    final_output = result.final_output_as(HomeworkOutput)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n</code></pre>"},{"location":"quickstart/#put-it-all-together","title":"Put it all together","text":"<p>Let's put it all together and run the entire workflow, using handoffs and the input guardrail.</p> <pre><code>from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner\nfrom agents.exceptions import InputGuardrailTripwireTriggered\nfrom pydantic import BaseModel\nimport asyncio\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n    final_output = result.final_output_as(HomeworkOutput)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"You determine which agent to use based on the user's homework question\",\n    handoffs=[history_tutor_agent, math_tutor_agent],\n    input_guardrails=[\n        InputGuardrail(guardrail_function=homework_guardrail),\n    ],\n)\n\nasync def main():\n    # Example 1: History question\n    try:\n        result = await Runner.run(triage_agent, \"who was the first president of the united states?\")\n        print(result.final_output)\n    except InputGuardrailTripwireTriggered as e:\n        print(\"Guardrail blocked this input:\", e)\n\n    # Example 2: General/philosophical question\n    try:\n        result = await Runner.run(triage_agent, \"What is the meaning of life?\")\n        print(result.final_output)\n    except InputGuardrailTripwireTriggered as e:\n        print(\"Guardrail blocked this input:\", e)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#view-your-traces","title":"View your traces","text":"<p>To review what happened during your agent run, navigate to the Trace viewer in the OpenAI Dashboard to view traces of your agent runs.</p>"},{"location":"quickstart/#next-steps","title":"Next steps","text":"<p>Learn how to build more complex agentic flows:</p> <ul> <li>Learn about how to configure Agents.</li> <li>Learn about running agents.</li> <li>Learn about tools, guardrails and models.</li> </ul>"},{"location":"release/","title":"Release process/changelog","text":"<p>The project follows a slightly modified version of semantic versioning using the form <code>0.Y.Z</code>. The leading <code>0</code> indicates the SDK is still evolving rapidly. Increment the components as follows:</p>"},{"location":"release/#minor-y-versions","title":"Minor (<code>Y</code>) versions","text":"<p>We will increase minor versions <code>Y</code> for breaking changes to any public interfaces that are not marked as beta. For example, going from <code>0.0.x</code> to <code>0.1.x</code> might include breaking changes.</p> <p>If you don't want breaking changes, we recommend pinning to <code>0.0.x</code> versions in your project.</p>"},{"location":"release/#patch-z-versions","title":"Patch (<code>Z</code>) versions","text":"<p>We will increment <code>Z</code> for non-breaking changes:</p> <ul> <li>Bug fixes</li> <li>New features</li> <li>Changes to private interfaces</li> <li>Updates to beta features</li> </ul>"},{"location":"release/#breaking-change-changelog","title":"Breaking change changelog","text":""},{"location":"release/#080","title":"0.8.0","text":"<p>In this version, two runtime behavior changes may require migration work:</p> <ul> <li>Function tools wrapping synchronous Python callables now execute on worker threads via <code>asyncio.to_thread(...)</code> instead of running on the event loop thread. If your tool logic depends on thread-local state or thread-affine resources, migrate to an async tool implementation or make thread affinity explicit in your tool code.</li> <li>Local MCP tool failure handling is now configurable, and the default behavior can return model-visible error output instead of failing the whole run. If you rely on fail-fast semantics, set <code>mcp_config={\"failure_error_function\": None}</code>. Server-level <code>failure_error_function</code> values override the agent-level setting, so set <code>failure_error_function=None</code> on each local MCP server that has an explicit handler.</li> </ul>"},{"location":"release/#070","title":"0.7.0","text":"<p>In this version, there were a few behavior changes that can affect existing applications:</p> <ul> <li>Nested handoff history is now opt-in (disabled by default). If you depended on the v0.6.x default nested behavior, explicitly set <code>RunConfig(nest_handoff_history=True)</code>.</li> <li>The default <code>reasoning.effort</code> for <code>gpt-5.1</code> / <code>gpt-5.2</code> changed to <code>\"none\"</code> (from the previous default <code>\"low\"</code> configured by SDK defaults). If your prompts or quality/cost profile relied on <code>\"low\"</code>, set it explicitly in <code>model_settings</code>.</li> </ul>"},{"location":"release/#060","title":"0.6.0","text":"<p>In this version, the default handoff history is now packaged into a single assistant message instead of exposing the raw user/assistant turns, giving downstream agents a concise, predictable recap - The existing single-message handoff transcript now by default starts with \"For context, here is the conversation so far between the user and the previous agent:\" before the <code>&lt;CONVERSATION HISTORY&gt;</code> block, so downstream agents get a clearly labeled recap</p>"},{"location":"release/#050","title":"0.5.0","text":"<p>This version doesn\u2019t introduce any visible breaking changes, but it includes new features and a few significant updates under the hood:</p> <ul> <li>Added support for <code>RealtimeRunner</code> to handle SIP protocol connections</li> <li>Significantly revised the internal logic of <code>Runner#run_sync</code> for Python 3.14 compatibility</li> </ul>"},{"location":"release/#040","title":"0.4.0","text":"<p>In this version, openai package v1.x versions are no longer supported. Please use openai v2.x along with this SDK.</p>"},{"location":"release/#030","title":"0.3.0","text":"<p>In this version, the Realtime API support migrates to gpt-realtime model and its API interface (GA version).</p>"},{"location":"release/#020","title":"0.2.0","text":"<p>In this version, a few places that used to take <code>Agent</code> as an arg, now take <code>AgentBase</code> as an arg instead. For example, the <code>list_tools()</code> call in MCP servers. This is a purely typing change, you will still receive <code>Agent</code> objects. To update, just fix type errors by replacing <code>Agent</code> with <code>AgentBase</code>.</p>"},{"location":"release/#010","title":"0.1.0","text":"<p>In this version, <code>MCPServer.list_tools()</code> has two new params: <code>run_context</code> and <code>agent</code>. You'll need to add these params to any classes that subclass <code>MCPServer</code>.</p>"},{"location":"repl/","title":"REPL utility","text":"<p>The SDK provides <code>run_demo_loop</code> for quick, interactive testing of an agent's behavior directly in your terminal.</p> <pre><code>import asyncio\nfrom agents import Agent, run_demo_loop\n\nasync def main() -&gt; None:\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant.\")\n    await run_demo_loop(agent)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p><code>run_demo_loop</code> prompts for user input in a loop, keeping the conversation history between turns. By default, it streams model output as it is produced. When you run the example above, run_demo_loop starts an interactive chat session. It continuously asks for your input, remembers the entire conversation history between turns (so your agent knows what's been discussed) and automatically streams the agent's responses to you in real-time as they are generated.</p> <p>To end this chat session, simply type <code>quit</code> or <code>exit</code> (and press Enter) or use the <code>Ctrl-D</code> keyboard\u00a0shortcut.</p>"},{"location":"results/","title":"Results","text":"<p>When you call the <code>Runner.run</code> methods, you either get a:</p> <ul> <li><code>RunResult</code> if you call <code>run</code> or <code>run_sync</code></li> <li><code>RunResultStreaming</code> if you call <code>run_streamed</code></li> </ul> <p>Both of these inherit from <code>RunResultBase</code>, which is where most useful information is present.</p>"},{"location":"results/#final-output","title":"Final output","text":"<p>The <code>final_output</code> property contains the final output of the last agent that ran. This is either:</p> <ul> <li>a <code>str</code>, if the last agent didn't have an <code>output_type</code> defined</li> <li>an object of type <code>last_agent.output_type</code>, if the agent had an output type defined.</li> </ul> <p>Note</p> <p><code>final_output</code> is of type <code>Any</code>. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.</p>"},{"location":"results/#inputs-for-the-next-turn","title":"Inputs for the next turn","text":"<p>You can use <code>result.to_input_list()</code> to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.</p>"},{"location":"results/#last-agent","title":"Last agent","text":"<p>The <code>last_agent</code> property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.</p>"},{"location":"results/#new-items","title":"New items","text":"<p>The <code>new_items</code> property contains the new items generated during the run. The items are <code>RunItem</code>s. A run item wraps the raw item generated by the LLM.</p> <ul> <li><code>MessageOutputItem</code> indicates a message from the LLM. The raw item is the message generated.</li> <li><code>HandoffCallItem</code> indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.</li> <li><code>HandoffOutputItem</code> indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.</li> <li><code>ToolCallItem</code> indicates that the LLM invoked a tool.</li> <li><code>ToolCallOutputItem</code> indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.</li> <li><code>ReasoningItem</code> indicates a reasoning item from the LLM. The raw item is the reasoning generated.</li> </ul>"},{"location":"results/#other-information","title":"Other information","text":""},{"location":"results/#guardrail-results","title":"Guardrail results","text":"<p>The <code>input_guardrail_results</code> and <code>output_guardrail_results</code> properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.</p> <p>Tool guardrail results are available separately as <code>tool_input_guardrail_results</code> and <code>tool_output_guardrail_results</code>. These guardrails can be attached to tools, and those tool calls execute the guardrails during the agent workflow.</p>"},{"location":"results/#raw-responses","title":"Raw responses","text":"<p>The <code>raw_responses</code> property contains the <code>ModelResponse</code>s generated by the LLM.</p>"},{"location":"results/#original-input","title":"Original input","text":"<p>The <code>input</code> property contains the original input you provided to the <code>run</code> method. In most cases you won't need this, but it's available in case you do.</p>"},{"location":"results/#interruptions-and-resuming-runs","title":"Interruptions and resuming runs","text":"<p>If a run pauses for tool approval, pending approvals are exposed in [<code>interruptions</code>][agents.result.RunResultBase.interruptions]. Convert the result into a <code>RunState</code> with <code>to_state()</code>, approve or reject the interruption(s), and resume with <code>Runner.run(...)</code> or <code>Runner.run_streamed(...)</code>.</p> <pre><code>from agents import Agent, Runner\n\nagent = Agent(name=\"Assistant\", instructions=\"Use tools when needed.\")\nresult = await Runner.run(agent, \"Delete temp files that are no longer needed.\")\n\nif result.interruptions:\n    state = result.to_state()\n    for interruption in result.interruptions:\n        state.approve(interruption)\n    result = await Runner.run(agent, state)\n</code></pre> <p>Both <code>RunResult</code> and <code>RunResultStreaming</code> support <code>to_state()</code>.</p>"},{"location":"results/#convenience-helpers","title":"Convenience helpers","text":"<p><code>RunResultBase</code> includes a few helper methods/properties that are useful in production flows:</p> <ul> <li><code>final_output_as(...)</code> casts final output to a specific type (optionally with runtime type checking).</li> <li><code>last_response_id</code> returns the latest model response ID, useful for response chaining.</li> <li><code>release_agents(...)</code> drops strong references to agents when you want to reduce memory pressure after inspecting results.</li> </ul>"},{"location":"running_agents/","title":"Running agents","text":"<p>You can run agents via the <code>Runner</code> class. You have 3 options:</p> <ol> <li><code>Runner.run()</code>, which runs async and returns a <code>RunResult</code>.</li> <li><code>Runner.run_sync()</code>, which is a sync method and just runs <code>.run()</code> under the hood.</li> <li><code>Runner.run_streamed()</code>, which runs async and returns a <code>RunResultStreaming</code>. It calls the LLM in streaming mode, and streams those events to you as they are received.</li> </ol> <pre><code>from agents import Agent, Runner\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n\n    result = await Runner.run(agent, \"Write a haiku about recursion in programming.\")\n    print(result.final_output)\n    # Code within the code,\n    # Functions calling themselves,\n    # Infinite loop's dance\n</code></pre> <p>Read more in the results guide.</p>"},{"location":"running_agents/#the-agent-loop","title":"The agent loop","text":"<p>When you use the run method in <code>Runner</code>, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.</p> <p>The runner then runs a loop:</p> <ol> <li>We call the LLM for the current agent, with the current input.</li> <li>The LLM produces its output.<ol> <li>If the LLM returns a <code>final_output</code>, the loop ends and we return the result.</li> <li>If the LLM does a handoff, we update the current agent and input, and re-run the loop.</li> <li>If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.</li> </ol> </li> <li>If we exceed the <code>max_turns</code> passed, we raise a <code>MaxTurnsExceeded</code> exception.</li> </ol> <p>Note</p> <p>The rule for whether the LLM output is considered as a \"final output\" is that it produces text output with the desired type, and there are no tool calls.</p>"},{"location":"running_agents/#streaming","title":"Streaming","text":"<p>Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the <code>RunResultStreaming</code> will contain the complete information about the run, including all the new outputs produced. You can call <code>.stream_events()</code> for the streaming events. Read more in the streaming guide.</p>"},{"location":"running_agents/#run-config","title":"Run config","text":"<p>The <code>run_config</code> parameter lets you configure some global settings for the agent run:</p> <ul> <li><code>model</code>: Allows setting a global LLM model to use, irrespective of what <code>model</code> each Agent has.</li> <li><code>model_provider</code>: A model provider for looking up model names, which defaults to OpenAI.</li> <li><code>model_settings</code>: Overrides agent-specific settings. For example, you can set a global <code>temperature</code> or <code>top_p</code>.</li> <li><code>session_settings</code>: Overrides session-level defaults (for example, <code>SessionSettings(limit=...)</code>) when retrieving history during a run.</li> <li><code>input_guardrails</code>, <code>output_guardrails</code>: A list of input or output guardrails to include on all runs.</li> <li><code>handoff_input_filter</code>: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in <code>Handoff.input_filter</code> for more details.</li> <li><code>nest_handoff_history</code>: Opt-in beta that collapses the prior transcript into a single assistant message before invoking the next agent. This is disabled by default while we stabilize nested handoffs; set to <code>True</code> to enable or leave <code>False</code> to pass through the raw transcript. All Runner methods automatically create a <code>RunConfig</code> when you do not pass one, so the quickstarts and examples keep the default off, and any explicit <code>Handoff.input_filter</code> callbacks continue to override it. Individual handoffs can override this setting via <code>Handoff.nest_handoff_history</code>.</li> <li><code>handoff_history_mapper</code>: Optional callable that receives the normalized transcript (history + handoff items) whenever you opt in to <code>nest_handoff_history</code>. It must return the exact list of input items to forward to the next agent, allowing you to replace the built-in summary without writing a full handoff filter.</li> <li><code>tracing_disabled</code>: Allows you to disable tracing for the entire run.</li> <li><code>tracing</code>: Pass a <code>TracingConfig</code> to override exporters, processors, or tracing metadata for this run.</li> <li><code>trace_include_sensitive_data</code>: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.</li> <li><code>workflow_name</code>, <code>trace_id</code>, <code>group_id</code>: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting <code>workflow_name</code>. The group ID is an optional field that lets you link traces across multiple runs.</li> <li><code>trace_metadata</code>: Metadata to include on all traces.</li> <li><code>session_input_callback</code>: Customize how new user input is merged with session history before each turn when using Sessions.</li> <li><code>call_model_input_filter</code>: Hook to edit the fully prepared model input (instructions and input items) immediately before the model call, e.g., to trim history or inject a system prompt.</li> <li><code>tool_error_formatter</code>: Customize the model-visible message when a tool call is rejected during approval flows.</li> </ul> <p>Nested handoffs are available as an opt-in beta. Enable the collapsed-transcript behavior by passing <code>RunConfig(nest_handoff_history=True)</code> or set <code>handoff(..., nest_handoff_history=True)</code> to turn it on for a specific handoff. If you prefer to keep the raw transcript (the default), leave the flag unset or provide a <code>handoff_input_filter</code> (or <code>handoff_history_mapper</code>) that forwards the conversation exactly as you need. To change the wrapper text used in the generated summary without writing a custom mapper, call <code>set_conversation_history_wrappers</code> (and <code>reset_conversation_history_wrappers</code> to restore the defaults).</p>"},{"location":"running_agents/#conversationschat-threads","title":"Conversations/chat threads","text":"<p>Calling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:</p> <ol> <li>User turn: user enter text</li> <li>Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.</li> </ol> <p>At the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again.</p>"},{"location":"running_agents/#manual-conversation-management","title":"Manual conversation management","text":"<p>You can manually manage conversation history using the <code>RunResultBase.to_input_list()</code> method to get the inputs for the next turn:</p> <pre><code>async def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    thread_id = \"thread_123\"  # Example thread ID\n    with trace(workflow_name=\"Conversation\", group_id=thread_id):\n        # First turn\n        result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\")\n        print(result.final_output)\n        # San Francisco\n\n        # Second turn\n        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": \"What state is it in?\"}]\n        result = await Runner.run(agent, new_input)\n        print(result.final_output)\n        # California\n</code></pre>"},{"location":"running_agents/#automatic-conversation-management-with-sessions","title":"Automatic conversation management with Sessions","text":"<p>For a simpler approach, you can use Sessions to automatically handle conversation history without manually calling <code>.to_input_list()</code>:</p> <pre><code>from agents import Agent, Runner, SQLiteSession\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    # Create session instance\n    session = SQLiteSession(\"conversation_123\")\n\n    thread_id = \"thread_123\"  # Example thread ID\n    with trace(workflow_name=\"Conversation\", group_id=thread_id):\n        # First turn\n        result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\", session=session)\n        print(result.final_output)\n        # San Francisco\n\n        # Second turn - agent automatically remembers previous context\n        result = await Runner.run(agent, \"What state is it in?\", session=session)\n        print(result.final_output)\n        # California\n</code></pre> <p>Sessions automatically:</p> <ul> <li>Retrieves conversation history before each run</li> <li>Stores new messages after each run</li> <li>Maintains separate conversations for different session IDs</li> </ul> <p>See the Sessions documentation for more details.</p>"},{"location":"running_agents/#server-managed-conversations","title":"Server-managed conversations","text":"<p>You can also let the OpenAI conversation state feature manage conversation state on the server side, instead of handling it locally with <code>to_input_list()</code> or <code>Sessions</code>. This allows you to preserve conversation history without manually resending all past messages. See the OpenAI Conversation state guide for more details.</p> <p>OpenAI provides two ways to track state across turns:</p>"},{"location":"running_agents/#1-using-conversation_id","title":"1. Using <code>conversation_id</code>","text":"<p>You first create a conversation using the OpenAI Conversations API and then reuse its ID for every subsequent call:</p> <pre><code>from agents import Agent, Runner\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    # Create a server-managed conversation\n    conversation = await client.conversations.create()\n    conv_id = conversation.id\n\n    while True:\n        user_input = input(\"You: \")\n        result = await Runner.run(agent, user_input, conversation_id=conv_id)\n        print(f\"Assistant: {result.final_output}\")\n</code></pre>"},{"location":"running_agents/#2-using-previous_response_id","title":"2. Using <code>previous_response_id</code>","text":"<p>Another option is response chaining, where each turn links explicitly to the response ID from the previous turn.</p> <pre><code>from agents import Agent, Runner\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    previous_response_id = None\n\n    while True:\n        user_input = input(\"You: \")\n\n        # Setting auto_previous_response_id=True enables response chaining automatically\n        # for the first turn, even when there's no actual previous response ID yet.\n        result = await Runner.run(\n            agent,\n            user_input,\n            previous_response_id=previous_response_id,\n            auto_previous_response_id=True,\n        )\n        previous_response_id = result.last_response_id\n        print(f\"Assistant: {result.final_output}\")\n</code></pre>"},{"location":"running_agents/#call-model-input-filter","title":"Call model input filter","text":"<p>Use <code>call_model_input_filter</code> to edit the model input right before the model call. The hook receives the current agent, context, and the combined input items (including session history when present) and returns a new <code>ModelInputData</code>.</p> <pre><code>from agents import Agent, Runner, RunConfig\nfrom agents.run import CallModelData, ModelInputData\n\ndef drop_old_messages(data: CallModelData[None]) -&gt; ModelInputData:\n    # Keep only the last 5 items and preserve existing instructions.\n    trimmed = data.model_data.input[-5:]\n    return ModelInputData(input=trimmed, instructions=data.model_data.instructions)\n\nagent = Agent(name=\"Assistant\", instructions=\"Answer concisely.\")\nresult = Runner.run_sync(\n    agent,\n    \"Explain quines\",\n    run_config=RunConfig(call_model_input_filter=drop_old_messages),\n)\n</code></pre> <p>Set the hook per run via <code>run_config</code> or as a default on your <code>Runner</code> to redact sensitive data, trim long histories, or inject additional system guidance.</p>"},{"location":"running_agents/#error-handlers","title":"Error handlers","text":"<p>All <code>Runner</code> entry points accept <code>error_handlers</code>, a dict keyed by error kind. Today, the supported key is <code>\"max_turns\"</code>. Use it when you want to return a controlled final output instead of raising <code>MaxTurnsExceeded</code>.</p> <pre><code>from agents import (\n    Agent,\n    RunErrorHandlerInput,\n    RunErrorHandlerResult,\n    Runner,\n)\n\nagent = Agent(name=\"Assistant\", instructions=\"Be concise.\")\n\n\ndef on_max_turns(_data: RunErrorHandlerInput[None]) -&gt; RunErrorHandlerResult:\n    return RunErrorHandlerResult(\n        final_output=\"I couldn't finish within the turn limit. Please narrow the request.\",\n        include_in_history=False,\n    )\n\n\nresult = Runner.run_sync(\n    agent,\n    \"Analyze this long transcript\",\n    max_turns=3,\n    error_handlers={\"max_turns\": on_max_turns},\n)\nprint(result.final_output)\n</code></pre> <p>Set <code>include_in_history=False</code> when you do not want the fallback output appended to conversation history.</p>"},{"location":"running_agents/#long-running-agents-human-in-the-loop","title":"Long running agents &amp; human-in-the-loop","text":"<p>For tool approval pause/resume patterns, see the dedicated Human-in-the-loop guide.</p>"},{"location":"running_agents/#temporal","title":"Temporal","text":"<p>You can use the Agents SDK Temporal integration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks in this video, and view docs here. </p>"},{"location":"running_agents/#restate","title":"Restate","text":"<p>You can use the Agents SDK Restate integration for lightweight, durable agents, including human approval, handoffs, and session management. The integration requires Restate's single-binary runtime as a dependency, and supports running agents as processes/containers or serverless functions. Read the overview or view the docs for more details.</p>"},{"location":"running_agents/#dbos","title":"DBOS","text":"<p>You can use the Agents SDK DBOS integration to run reliable agents that preserves progress across failures and restarts. It supports long-running agents, human-in-the-loop workflows, and handoffs. It supports both sync and async methods. The integration requires only a SQLite or Postgres database. View the integration repo and the docs for more details.</p>"},{"location":"running_agents/#exceptions","title":"Exceptions","text":"<p>The SDK raises exceptions in certain cases. The full list is in <code>agents.exceptions</code>. As an overview:</p> <ul> <li><code>AgentsException</code>: This is the base class for all exceptions raised within the SDK. It serves as a generic type from which all other specific exceptions are derived.</li> <li><code>MaxTurnsExceeded</code>: This exception is raised when the agent's run exceeds the <code>max_turns</code> limit passed to the <code>Runner.run</code>, <code>Runner.run_sync</code>, or <code>Runner.run_streamed</code> methods. It indicates that the agent could not complete its task within the specified number of interaction turns.</li> <li><code>ModelBehaviorError</code>: This exception occurs when the underlying model (LLM) produces unexpected or invalid outputs. This can include:<ul> <li>Malformed JSON: When the model provides a malformed JSON structure for tool calls or in its direct output, especially if a specific <code>output_type</code> is defined.</li> <li>Unexpected tool-related failures: When the model fails to use tools in an expected manner</li> </ul> </li> <li><code>UserError</code>: This exception is raised when you (the person writing code using the SDK) make an error while using the SDK. This typically results from incorrect code implementation, invalid configuration, or misuse of the SDK's API.</li> <li><code>InputGuardrailTripwireTriggered</code>, <code>OutputGuardrailTripwireTriggered</code>: This exception is raised when the conditions of an input guardrail or output guardrail are met, respectively. Input guardrails check incoming messages before processing, while output guardrails check the agent's final response before delivery.</li> </ul>"},{"location":"streaming/","title":"Streaming","text":"<p>Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.</p> <p>To stream, you can call <code>Runner.run_streamed()</code>, which will give you a <code>RunResultStreaming</code>. Calling <code>result.stream_events()</code> gives you an async stream of <code>StreamEvent</code> objects, which are described below.</p>"},{"location":"streaming/#raw-response-events","title":"Raw response events","text":"<p><code>RawResponsesStreamEvent</code> are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like <code>response.created</code>, <code>response.output_text.delta</code>, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.</p> <p>For example, this will output the text generated by the LLM token-by-token.</p> <pre><code>import asyncio\nfrom openai.types.responses import ResponseTextDeltaEvent\nfrom agents import Agent, Runner\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n            print(event.data.delta, end=\"\", flush=True)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"streaming/#run-item-events-and-agent-events","title":"Run item events and agent events","text":"<p><code>RunItemStreamEvent</code>s are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of \"message generated\", \"tool ran\", etc, instead of each token. Similarly, <code>AgentUpdatedStreamEvent</code> gives you updates when the current agent changes (e.g. as the result of a handoff).</p> <p>For example, this will ignore raw events and stream updates to the user.</p> <pre><code>import asyncio\nimport random\nfrom agents import Agent, ItemHelpers, Runner, function_tool\n\n@function_tool\ndef how_many_jokes() -&gt; int:\n    return random.randint(1, 10)\n\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n        tools=[how_many_jokes],\n    )\n\n    result = Runner.run_streamed(\n        agent,\n        input=\"Hello\",\n    )\n    print(\"=== Run starting ===\")\n\n    async for event in result.stream_events():\n        # We'll ignore the raw responses event deltas\n        if event.type == \"raw_response_event\":\n            continue\n        # When the agent updates, print that\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n            continue\n        # When items are generated, print them\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n            else:\n                pass  # Ignore other event types\n\n    print(\"=== Run complete ===\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p>Tools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. The SDK supports five categories:</p> <ul> <li>Hosted OpenAI tools: run alongside the model on OpenAI servers.</li> <li>Local runtime tools: run in your environment (computer use, shell, apply patch).</li> <li>Function calling: wrap any Python function as a tool.</li> <li>Agents as tools: expose an agent as a callable tool without a full handoff.</li> <li>Experimental: Codex tool: run workspace-scoped Codex tasks from a tool call.</li> </ul>"},{"location":"tools/#hosted-tools","title":"Hosted tools","text":"<p>OpenAI offers a few built-in tools when using the <code>OpenAIResponsesModel</code>:</p> <ul> <li>The <code>WebSearchTool</code> lets an agent search the web.</li> <li>The <code>FileSearchTool</code> allows retrieving information from your OpenAI Vector Stores.</li> <li>The <code>CodeInterpreterTool</code> lets the LLM execute code in a sandboxed environment.</li> <li>The <code>HostedMCPTool</code> exposes a remote MCP server's tools to the model.</li> <li>The <code>ImageGenerationTool</code> generates images from a prompt.</li> </ul> <pre><code>from agents import Agent, FileSearchTool, Runner, WebSearchTool\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[\n        WebSearchTool(),\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[\"VECTOR_STORE_ID\"],\n        ),\n    ],\n)\n\nasync def main():\n    result = await Runner.run(agent, \"Which coffee shop should I go to, taking into account my preferences and the weather today in SF?\")\n    print(result.final_output)\n</code></pre>"},{"location":"tools/#local-runtime-tools","title":"Local runtime tools","text":"<p>Local runtime tools execute in your environment and require you to supply implementations:</p> <ul> <li><code>ComputerTool</code>: implement the <code>Computer</code> or <code>AsyncComputer</code> interface to enable GUI/browser automation.</li> <li><code>ShellTool</code> or <code>LocalShellTool</code>: provide a shell executor to run commands.</li> <li><code>ApplyPatchTool</code>: implement <code>ApplyPatchEditor</code> to apply diffs locally.</li> </ul> <pre><code>from agents import Agent, ApplyPatchTool, ShellTool\nfrom agents.computer import AsyncComputer\nfrom agents.editor import ApplyPatchResult, ApplyPatchOperation, ApplyPatchEditor\n\n\nclass NoopComputer(AsyncComputer):\n    environment = \"browser\"\n    dimensions = (1024, 768)\n    async def screenshot(self): return \"\"\n    async def click(self, x, y, button): ...\n    async def double_click(self, x, y): ...\n    async def scroll(self, x, y, scroll_x, scroll_y): ...\n    async def type(self, text): ...\n    async def wait(self): ...\n    async def move(self, x, y): ...\n    async def keypress(self, keys): ...\n    async def drag(self, path): ...\n\n\nclass NoopEditor(ApplyPatchEditor):\n    async def create_file(self, op: ApplyPatchOperation): return ApplyPatchResult(status=\"completed\")\n    async def update_file(self, op: ApplyPatchOperation): return ApplyPatchResult(status=\"completed\")\n    async def delete_file(self, op: ApplyPatchOperation): return ApplyPatchResult(status=\"completed\")\n\n\nasync def run_shell(request):\n    return \"shell output\"\n\n\nagent = Agent(\n    name=\"Local tools agent\",\n    tools=[\n        ShellTool(executor=run_shell),\n        ApplyPatchTool(editor=NoopEditor()),\n        # ComputerTool expects a Computer/AsyncComputer implementation; omitted here for brevity.\n    ],\n)\n</code></pre>"},{"location":"tools/#function-tools","title":"Function tools","text":"<p>You can use any Python function as a tool. The Agents SDK will setup the tool automatically:</p> <ul> <li>The name of the tool will be the name of the Python function (or you can provide a name)</li> <li>Tool description will be taken from the docstring of the function (or you can provide a description)</li> <li>The schema for the function inputs is automatically created from the function's arguments</li> <li>Descriptions for each input are taken from the docstring of the function, unless disabled</li> </ul> <p>We use Python's <code>inspect</code> module to extract the function signature, along with <code>griffe</code> to parse docstrings and <code>pydantic</code> for schema creation.</p> <pre><code>import json\n\nfrom typing_extensions import TypedDict, Any\n\nfrom agents import Agent, FunctionTool, RunContextWrapper, function_tool\n\n\nclass Location(TypedDict):\n    lat: float\n    long: float\n\n@function_tool  # (1)!\nasync def fetch_weather(location: Location) -&gt; str:\n    # (2)!\n    \"\"\"Fetch the weather for a given location.\n\n    Args:\n        location: The location to fetch the weather for.\n    \"\"\"\n    # In real life, we'd fetch the weather from a weather API\n    return \"sunny\"\n\n\n@function_tool(name_override=\"fetch_data\")  # (3)!\ndef read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -&gt; str:\n    \"\"\"Read the contents of a file.\n\n    Args:\n        path: The path to the file to read.\n        directory: The directory to read the file from.\n    \"\"\"\n    # In real life, we'd read the file from the file system\n    return \"&lt;file contents&gt;\"\n\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[fetch_weather, read_file],  # (4)!\n)\n\nfor tool in agent.tools:\n    if isinstance(tool, FunctionTool):\n        print(tool.name)\n        print(tool.description)\n        print(json.dumps(tool.params_json_schema, indent=2))\n        print()\n</code></pre> <ol> <li>You can use any Python types as arguments to your functions, and the function can be sync or async.</li> <li>Docstrings, if present, are used to capture descriptions and argument descriptions</li> <li>Functions can optionally take the <code>context</code> (must be the first argument). You can also set overrides, like the name of the tool, description, which docstring style to use, etc.</li> <li>You can pass the decorated functions to the list of tools.</li> </ol> Expand to see output <pre><code>fetch_weather\nFetch the weather for a given location.\n{\n\"$defs\": {\n  \"Location\": {\n    \"properties\": {\n      \"lat\": {\n        \"title\": \"Lat\",\n        \"type\": \"number\"\n      },\n      \"long\": {\n        \"title\": \"Long\",\n        \"type\": \"number\"\n      }\n    },\n    \"required\": [\n      \"lat\",\n      \"long\"\n    ],\n    \"title\": \"Location\",\n    \"type\": \"object\"\n  }\n},\n\"properties\": {\n  \"location\": {\n    \"$ref\": \"#/$defs/Location\",\n    \"description\": \"The location to fetch the weather for.\"\n  }\n},\n\"required\": [\n  \"location\"\n],\n\"title\": \"fetch_weather_args\",\n\"type\": \"object\"\n}\n\nfetch_data\nRead the contents of a file.\n{\n\"properties\": {\n  \"path\": {\n    \"description\": \"The path to the file to read.\",\n    \"title\": \"Path\",\n    \"type\": \"string\"\n  },\n  \"directory\": {\n    \"anyOf\": [\n      {\n        \"type\": \"string\"\n      },\n      {\n        \"type\": \"null\"\n      }\n    ],\n    \"default\": null,\n    \"description\": \"The directory to read the file from.\",\n    \"title\": \"Directory\"\n  }\n},\n\"required\": [\n  \"path\"\n],\n\"title\": \"fetch_data_args\",\n\"type\": \"object\"\n}\n</code></pre>"},{"location":"tools/#returning-images-or-files-from-function-tools","title":"Returning images or files from function tools","text":"<p>In addition to returning text outputs, you can return one or many images or files as the output of a function tool. To do so, you can return any of:</p> <ul> <li>Images: <code>ToolOutputImage</code> (or the TypedDict version, <code>ToolOutputImageDict</code>)</li> <li>Files: <code>ToolOutputFileContent</code> (or the TypedDict version, <code>ToolOutputFileContentDict</code>)</li> <li>Text: either a string or stringable objects, or <code>ToolOutputText</code> (or the TypedDict version, <code>ToolOutputTextDict</code>)</li> </ul>"},{"location":"tools/#custom-function-tools","title":"Custom function tools","text":"<p>Sometimes, you don't want to use a Python function as a tool. You can directly create a <code>FunctionTool</code> if you prefer. You'll need to provide:</p> <ul> <li><code>name</code></li> <li><code>description</code></li> <li><code>params_json_schema</code>, which is the JSON schema for the arguments</li> <li><code>on_invoke_tool</code>, which is an async function that receives a <code>ToolContext</code> and the arguments as a JSON string, and must return the tool output as a string.</li> </ul> <pre><code>from typing import Any\n\nfrom pydantic import BaseModel\n\nfrom agents import RunContextWrapper, FunctionTool\n\n\n\ndef do_some_work(data: str) -&gt; str:\n    return \"done\"\n\n\nclass FunctionArgs(BaseModel):\n    username: str\n    age: int\n\n\nasync def run_function(ctx: RunContextWrapper[Any], args: str) -&gt; str:\n    parsed = FunctionArgs.model_validate_json(args)\n    return do_some_work(data=f\"{parsed.username} is {parsed.age} years old\")\n\n\ntool = FunctionTool(\n    name=\"process_user\",\n    description=\"Processes extracted user data\",\n    params_json_schema=FunctionArgs.model_json_schema(),\n    on_invoke_tool=run_function,\n)\n</code></pre>"},{"location":"tools/#automatic-argument-and-docstring-parsing","title":"Automatic argument and docstring parsing","text":"<p>As mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that:</p> <ol> <li>The signature parsing is done via the <code>inspect</code> module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more.</li> <li>We use <code>griffe</code> to parse docstrings. Supported docstring formats are <code>google</code>, <code>sphinx</code> and <code>numpy</code>. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling <code>function_tool</code>. You can also disable docstring parsing by setting <code>use_docstring_info</code> to <code>False</code>.</li> </ol> <p>The code for the schema extraction lives in <code>agents.function_schema</code>.</p>"},{"location":"tools/#agents-as-tools","title":"Agents as tools","text":"<p>In some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools.</p> <pre><code>from agents import Agent, Runner\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You translate the user's message to Spanish\",\n)\n\nfrench_agent = Agent(\n    name=\"French agent\",\n    instructions=\"You translate the user's message to French\",\n)\n\norchestrator_agent = Agent(\n    name=\"orchestrator_agent\",\n    instructions=(\n        \"You are a translation agent. You use the tools given to you to translate.\"\n        \"If asked for multiple translations, you call the relevant tools.\"\n    ),\n    tools=[\n        spanish_agent.as_tool(\n            tool_name=\"translate_to_spanish\",\n            tool_description=\"Translate the user's message to Spanish\",\n        ),\n        french_agent.as_tool(\n            tool_name=\"translate_to_french\",\n            tool_description=\"Translate the user's message to French\",\n        ),\n    ],\n)\n\nasync def main():\n    result = await Runner.run(orchestrator_agent, input=\"Say 'Hello, how are you?' in Spanish.\")\n    print(result.final_output)\n</code></pre>"},{"location":"tools/#customizing-tool-agents","title":"Customizing tool-agents","text":"<p>The <code>agent.as_tool</code> function is a convenience method to make it easy to turn an agent into a tool. It supports common runtime options such as <code>max_turns</code>, <code>run_config</code>, <code>hooks</code>, <code>previous_response_id</code>, <code>conversation_id</code>, <code>session</code>, and <code>needs_approval</code>. It also supports structured input with <code>parameters</code>, <code>input_builder</code>, and <code>include_input_schema</code>. For advanced orchestration (for example, conditional retries, fallback behavior, or chaining multiple agent calls), use <code>Runner.run</code> directly in your tool implementation:</p> <pre><code>@function_tool\nasync def run_my_agent() -&gt; str:\n    \"\"\"A tool that runs the agent with custom configs\"\"\"\n\n    agent = Agent(name=\"My agent\", instructions=\"...\")\n\n    result = await Runner.run(\n        agent,\n        input=\"...\",\n        max_turns=5,\n        run_config=...\n    )\n\n    return str(result.final_output)\n</code></pre>"},{"location":"tools/#structured-input-for-tool-agents","title":"Structured input for tool-agents","text":"<p>By default, <code>Agent.as_tool()</code> expects a single string input (<code>{\"input\": \"...\"}</code>), but you can expose a structured schema by passing <code>parameters</code> (a Pydantic model or dataclass type).</p> <p>Additional options:</p> <ul> <li><code>include_input_schema=True</code> includes the full JSON Schema in the generated nested input.</li> <li><code>input_builder=...</code> lets you fully customize how structured tool arguments become nested agent input.</li> <li><code>RunContextWrapper.tool_input</code> contains the parsed structured payload inside the nested run context.</li> </ul> <pre><code>from pydantic import BaseModel, Field\n\n\nclass TranslationInput(BaseModel):\n    text: str = Field(description=\"Text to translate.\")\n    source: str = Field(description=\"Source language.\")\n    target: str = Field(description=\"Target language.\")\n\n\ntranslator_tool = translator_agent.as_tool(\n    tool_name=\"translate_text\",\n    tool_description=\"Translate text between languages.\",\n    parameters=TranslationInput,\n    include_input_schema=True,\n)\n</code></pre> <p>See <code>examples/agent_patterns/agents_as_tools_structured.py</code> for a complete runnable example.</p>"},{"location":"tools/#approval-gates-for-tool-agents","title":"Approval gates for tool-agents","text":"<p><code>Agent.as_tool(..., needs_approval=...)</code> uses the same approval flow as <code>function_tool</code>. If approval is required, the run pauses and pending items appear in <code>result.interruptions</code>; then use <code>result.to_state()</code> and resume after calling <code>state.approve(...)</code> or <code>state.reject(...)</code>. See the Human-in-the-loop guide for the full pause/resume pattern.</p>"},{"location":"tools/#custom-output-extraction","title":"Custom output extraction","text":"<p>In certain cases, you might want to modify the output of the tool-agents before returning it to the central agent. This may be useful if you want to:</p> <ul> <li>Extract a specific piece of information (e.g., a JSON payload) from the sub-agent's chat history.</li> <li>Convert or reformat the agent\u2019s final answer (e.g., transform Markdown into plain text or CSV).</li> <li>Validate the output or provide a fallback value when the agent\u2019s response is missing or malformed.</li> </ul> <p>You can do this by supplying the <code>custom_output_extractor</code> argument to the <code>as_tool</code> method:</p> <pre><code>async def extract_json_payload(run_result: RunResult) -&gt; str:\n    # Scan the agent\u2019s outputs in reverse order until we find a JSON-like message from a tool call.\n    for item in reversed(run_result.new_items):\n        if isinstance(item, ToolCallOutputItem) and item.output.strip().startswith(\"{\"):\n            return item.output.strip()\n    # Fallback to an empty JSON object if nothing was found\n    return \"{}\"\n\n\njson_tool = data_agent.as_tool(\n    tool_name=\"get_data_json\",\n    tool_description=\"Run the data agent and return only its JSON payload\",\n    custom_output_extractor=extract_json_payload,\n)\n</code></pre>"},{"location":"tools/#streaming-nested-agent-runs","title":"Streaming nested agent runs","text":"<p>Pass an <code>on_stream</code> callback to <code>as_tool</code> to listen to streaming events emitted by the nested agent while still returning its final output once the stream completes.</p> <pre><code>from agents import AgentToolStreamEvent\n\n\nasync def handle_stream(event: AgentToolStreamEvent) -&gt; None:\n    # Inspect the underlying StreamEvent along with agent metadata.\n    print(f\"[stream] {event['agent'].name} :: {event['event'].type}\")\n\n\nbilling_agent_tool = billing_agent.as_tool(\n    tool_name=\"billing_helper\",\n    tool_description=\"Answer billing questions.\",\n    on_stream=handle_stream,  # Can be sync or async.\n)\n</code></pre> <p>What to expect:</p> <ul> <li>Event types mirror <code>StreamEvent[\"type\"]</code>: <code>raw_response_event</code>, <code>run_item_stream_event</code>, <code>agent_updated_stream_event</code>.</li> <li>Providing <code>on_stream</code> automatically runs the nested agent in streaming mode and drains the stream before returning the final output.</li> <li>The handler may be synchronous or asynchronous; each event is delivered in order as it arrives.</li> <li><code>tool_call</code> is present when the tool is invoked via a model tool call; direct calls may leave it <code>None</code>.</li> <li>See <code>examples/agent_patterns/agents_as_tools_streaming.py</code> for a complete runnable sample.</li> </ul>"},{"location":"tools/#conditional-tool-enabling","title":"Conditional tool enabling","text":"<p>You can conditionally enable or disable agent tools at runtime using the <code>is_enabled</code> parameter. This allows you to dynamically filter which tools are available to the LLM based on context, user preferences, or runtime conditions.</p> <pre><code>import asyncio\nfrom agents import Agent, AgentBase, Runner, RunContextWrapper\nfrom pydantic import BaseModel\n\nclass LanguageContext(BaseModel):\n    language_preference: str = \"french_spanish\"\n\ndef french_enabled(ctx: RunContextWrapper[LanguageContext], agent: AgentBase) -&gt; bool:\n    \"\"\"Enable French for French+Spanish preference.\"\"\"\n    return ctx.context.language_preference == \"french_spanish\"\n\n# Create specialized agents\nspanish_agent = Agent(\n    name=\"spanish_agent\",\n    instructions=\"You respond in Spanish. Always reply to the user's question in Spanish.\",\n)\n\nfrench_agent = Agent(\n    name=\"french_agent\",\n    instructions=\"You respond in French. Always reply to the user's question in French.\",\n)\n\n# Create orchestrator with conditional tools\norchestrator = Agent(\n    name=\"orchestrator\",\n    instructions=(\n        \"You are a multilingual assistant. You use the tools given to you to respond to users. \"\n        \"You must call ALL available tools to provide responses in different languages. \"\n        \"You never respond in languages yourself, you always use the provided tools.\"\n    ),\n    tools=[\n        spanish_agent.as_tool(\n            tool_name=\"respond_spanish\",\n            tool_description=\"Respond to the user's question in Spanish\",\n            is_enabled=True,  # Always enabled\n        ),\n        french_agent.as_tool(\n            tool_name=\"respond_french\",\n            tool_description=\"Respond to the user's question in French\",\n            is_enabled=french_enabled,\n        ),\n    ],\n)\n\nasync def main():\n    context = RunContextWrapper(LanguageContext(language_preference=\"french_spanish\"))\n    result = await Runner.run(orchestrator, \"How are you?\", context=context.context)\n    print(result.final_output)\n\nasyncio.run(main())\n</code></pre> <p>The <code>is_enabled</code> parameter accepts:</p> <ul> <li>Boolean values: <code>True</code> (always enabled) or <code>False</code> (always disabled)</li> <li>Callable functions: Functions that take <code>(context, agent)</code> and return a boolean</li> <li>Async functions: Async functions for complex conditional logic</li> </ul> <p>Disabled tools are completely hidden from the LLM at runtime, making this useful for:</p> <ul> <li>Feature gating based on user permissions</li> <li>Environment-specific tool availability (dev vs prod)</li> <li>A/B testing different tool configurations</li> <li>Dynamic tool filtering based on runtime state</li> </ul>"},{"location":"tools/#experimental-codex-tool","title":"Experimental: Codex tool","text":"<p>The <code>codex_tool</code> wraps the Codex CLI so an agent can run workspace-scoped tasks (shell, file edits, MCP tools) during a tool call. This surface is experimental and may change.</p> <pre><code>from agents import Agent\nfrom agents.extensions.experimental.codex import ThreadOptions, TurnOptions, codex_tool\n\nagent = Agent(\n    name=\"Codex Agent\",\n    instructions=\"Use the codex tool to inspect the workspace and answer the question.\",\n    tools=[\n        codex_tool(\n            sandbox_mode=\"workspace-write\",\n            working_directory=\"/path/to/repo\",\n            default_thread_options=ThreadOptions(\n                model=\"gpt-5.2-codex\",\n                model_reasoning_effort=\"low\",\n                network_access_enabled=True,\n                web_search_mode=\"disabled\",\n                approval_policy=\"never\",\n            ),\n            default_turn_options=TurnOptions(\n                idle_timeout_seconds=60,\n            ),\n            persist_session=True,\n        )\n    ],\n)\n</code></pre> <p>What to know:</p> <ul> <li>Auth: set <code>CODEX_API_KEY</code> (preferred) or <code>OPENAI_API_KEY</code>, or pass <code>codex_options={\"api_key\": \"...\"}</code>.</li> <li>Runtime: <code>codex_options.base_url</code> overrides the CLI base URL.</li> <li>Binary resolution: set <code>codex_options.codex_path_override</code> (or <code>CODEX_PATH</code>) to pin the CLI path. Otherwise the SDK resolves <code>codex</code> from <code>PATH</code>, then falls back to the bundled vendor binary.</li> <li>Environment: <code>codex_options.env</code> fully controls the subprocess environment. When it is provided, the subprocess does not inherit <code>os.environ</code>.</li> <li>Stream limits: <code>codex_options.codex_subprocess_stream_limit_bytes</code> (or <code>OPENAI_AGENTS_CODEX_SUBPROCESS_STREAM_LIMIT_BYTES</code>) controls stdout/stderr reader limits. Valid range is <code>65536</code> to <code>67108864</code>; default is <code>8388608</code>.</li> <li>Inputs: tool calls must include at least one item in <code>inputs</code> with <code>{ \"type\": \"text\", \"text\": ... }</code> or <code>{ \"type\": \"local_image\", \"path\": ... }</code>.</li> <li>Thread defaults: configure <code>default_thread_options</code> for <code>model_reasoning_effort</code>, <code>web_search_mode</code> (preferred over legacy <code>web_search_enabled</code>), <code>approval_policy</code>, and <code>additional_directories</code>.</li> <li>Turn defaults: configure <code>default_turn_options</code> for <code>idle_timeout_seconds</code> and cancellation <code>signal</code>.</li> <li>Safety: pair <code>sandbox_mode</code> with <code>working_directory</code>; set <code>skip_git_repo_check=True</code> outside Git repos.</li> <li>Behavior: <code>persist_session=True</code> reuses a single Codex thread and returns its <code>thread_id</code>.</li> <li>Streaming: <code>on_stream</code> receives Codex events (reasoning, command execution, MCP tool calls, file changes, web search).</li> <li>Outputs: results include <code>response</code>, <code>usage</code>, and <code>thread_id</code>; usage is added to <code>RunContextWrapper.usage</code>.</li> <li>Structure: <code>output_schema</code> enforces structured Codex responses when you need typed outputs.</li> <li>See <code>examples/tools/codex.py</code> for a complete runnable sample.</li> </ul>"},{"location":"tools/#handling-errors-in-function-tools","title":"Handling errors in function tools","text":"<p>When you create a function tool via <code>@function_tool</code>, you can pass a <code>failure_error_function</code>. This is a function that provides an error response to the LLM in case the tool call crashes.</p> <ul> <li>By default (i.e. if you don't pass anything), it runs a <code>default_tool_error_function</code> which tells the LLM an error occurred.</li> <li>If you pass your own error function, it runs that instead, and sends the response to the LLM.</li> <li>If you explicitly pass <code>None</code>, then any tool call errors will be re-raised for you to handle. This could be a <code>ModelBehaviorError</code> if the model produced invalid JSON, or a <code>UserError</code> if your code crashed, etc.</li> </ul> <pre><code>from agents import function_tool, RunContextWrapper\nfrom typing import Any\n\ndef my_custom_error_function(context: RunContextWrapper[Any], error: Exception) -&gt; str:\n    \"\"\"A custom function to provide a user-friendly error message.\"\"\"\n    print(f\"A tool call failed with the following error: {error}\")\n    return \"An internal server error occurred. Please try again later.\"\n\n@function_tool(failure_error_function=my_custom_error_function)\ndef get_user_profile(user_id: str) -&gt; str:\n    \"\"\"Fetches a user profile from a mock API.\n     This function demonstrates a 'flaky' or failing API call.\n    \"\"\"\n    if user_id == \"user_123\":\n        return \"User profile for user_123 successfully retrieved.\"\n    else:\n        raise ValueError(f\"Could not retrieve profile for user_id: {user_id}. API returned an error.\")\n</code></pre> <p>If you are manually creating a <code>FunctionTool</code> object, then you must handle errors inside the <code>on_invoke_tool</code> function.</p>"},{"location":"tracing/","title":"Tracing","text":"<p>The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the Traces dashboard, you can debug, visualize, and monitor your workflows during development and in production.</p> <p>Note</p> <p>Tracing is enabled by default. There are two ways to disable tracing:</p> <ol> <li>You can globally disable tracing by setting the env var <code>OPENAI_AGENTS_DISABLE_TRACING=1</code></li> <li>You can disable tracing for a single run by setting <code>agents.run.RunConfig.tracing_disabled</code> to <code>True</code></li> </ol> <p>For organizations operating under a Zero Data Retention (ZDR) policy using OpenAI's APIs, tracing is unavailable.</p>"},{"location":"tracing/#traces-and-spans","title":"Traces and spans","text":"<ul> <li>Traces represent a single end-to-end operation of a \"workflow\". They're composed of Spans. Traces have the following properties:<ul> <li><code>workflow_name</code>: This is the logical workflow or app. For example \"Code generation\" or \"Customer service\".</li> <li><code>trace_id</code>: A unique ID for the trace. Automatically generated if you don't pass one. Must have the format <code>trace_&lt;32_alphanumeric&gt;</code>.</li> <li><code>group_id</code>: Optional group ID, to link multiple traces from the same conversation. For example, you might use a chat thread ID.</li> <li><code>disabled</code>: If True, the trace will not be recorded.</li> <li><code>metadata</code>: Optional metadata for the trace.</li> </ul> </li> <li>Spans represent operations that have a start and end time. Spans have:<ul> <li><code>started_at</code> and <code>ended_at</code> timestamps.</li> <li><code>trace_id</code>, to represent the trace they belong to</li> <li><code>parent_id</code>, which points to the parent Span of this Span (if any)</li> <li><code>span_data</code>, which is information about the Span. For example, <code>AgentSpanData</code> contains information about the Agent, <code>GenerationSpanData</code> contains information about the LLM generation, etc.</li> </ul> </li> </ul>"},{"location":"tracing/#default-tracing","title":"Default tracing","text":"<p>By default, the SDK traces the following:</p> <ul> <li>The entire <code>Runner.{run, run_sync, run_streamed}()</code> is wrapped in a <code>trace()</code>.</li> <li>Each time an agent runs, it is wrapped in <code>agent_span()</code></li> <li>LLM generations are wrapped in <code>generation_span()</code></li> <li>Function tool calls are each wrapped in <code>function_span()</code></li> <li>Guardrails are wrapped in <code>guardrail_span()</code></li> <li>Handoffs are wrapped in <code>handoff_span()</code></li> <li>Audio inputs (speech-to-text) are wrapped in a <code>transcription_span()</code></li> <li>Audio outputs (text-to-speech) are wrapped in a <code>speech_span()</code></li> <li>Related audio spans may be parented under a <code>speech_group_span()</code></li> </ul> <p>By default, the trace is named \"Agent workflow\". You can set this name if you use <code>trace</code>, or you can configure the name and other properties with the <code>RunConfig</code>.</p> <p>In addition, you can set up custom trace processors to push traces to other destinations (as a replacement, or secondary destination).</p>"},{"location":"tracing/#higher-level-traces","title":"Higher level traces","text":"<p>Sometimes, you might want multiple calls to <code>run()</code> to be part of a single trace. You can do this by wrapping the entire code in a <code>trace()</code>.</p> <pre><code>from agents import Agent, Runner, trace\n\nasync def main():\n    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n\n    with trace(\"Joke workflow\"): # (1)!\n        first_result = await Runner.run(agent, \"Tell me a joke\")\n        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\")\n        print(f\"Joke: {first_result.final_output}\")\n        print(f\"Rating: {second_result.final_output}\")\n</code></pre> <ol> <li>Because the two calls to <code>Runner.run</code> are wrapped in a <code>with trace()</code>, the individual runs will be part of the overall trace rather than creating two traces.</li> </ol>"},{"location":"tracing/#creating-traces","title":"Creating traces","text":"<p>You can use the <code>trace()</code> function to create a trace. Traces need to be started and finished. You have two options to do so:</p> <ol> <li>Recommended: use the trace as a context manager, i.e. <code>with trace(...) as my_trace</code>. This will automatically start and end the trace at the right time.</li> <li>You can also manually call <code>trace.start()</code> and <code>trace.finish()</code>.</li> </ol> <p>The current trace is tracked via a Python <code>contextvar</code>. This means that it works with concurrency automatically. If you manually start/end a trace, you'll need to pass <code>mark_as_current</code> and <code>reset_current</code> to <code>start()</code>/<code>finish()</code> to update the current trace.</p>"},{"location":"tracing/#creating-spans","title":"Creating spans","text":"<p>You can use the various <code>*_span()</code> methods to create a span. In general, you don't need to manually create spans. A <code>custom_span()</code> function is available for tracking custom span information.</p> <p>Spans are automatically part of the current trace, and are nested under the nearest current span, which is tracked via a Python <code>contextvar</code>.</p>"},{"location":"tracing/#sensitive-data","title":"Sensitive data","text":"<p>Certain spans may capture potentially sensitive data.</p> <p>The <code>generation_span()</code> stores the inputs/outputs of the LLM generation, and <code>function_span()</code> stores the inputs/outputs of function calls. These may contain sensitive data, so you can disable capturing that data via <code>RunConfig.trace_include_sensitive_data</code>.</p> <p>Similarly, Audio spans include base64-encoded PCM data for input and output audio by default. You can disable capturing this audio data by configuring <code>VoicePipelineConfig.trace_include_sensitive_audio_data</code>.</p> <p>By default, <code>trace_include_sensitive_data</code> is <code>True</code>. You can set the default without code by exporting the <code>OPENAI_AGENTS_TRACE_INCLUDE_SENSITIVE_DATA</code> environment variable to <code>true/1</code> or <code>false/0</code> before running your app.</p>"},{"location":"tracing/#custom-tracing-processors","title":"Custom tracing processors","text":"<p>The high level architecture for tracing is:</p> <ul> <li>At initialization, we create a global <code>TraceProvider</code>, which is responsible for creating traces.</li> <li>We configure the <code>TraceProvider</code> with a <code>BatchTraceProcessor</code> that sends traces/spans in batches to a <code>BackendSpanExporter</code>, which exports the spans and traces to the OpenAI backend in batches.</li> </ul> <p>To customize this default setup, to send traces to alternative or additional backends or modifying exporter behavior, you have two options:</p> <ol> <li><code>add_trace_processor()</code> lets you add an additional trace processor that will receive traces and spans as they are ready. This lets you do your own processing in addition to sending traces to OpenAI's backend.</li> <li><code>set_trace_processors()</code> lets you replace the default processors with your own trace processors. This means traces will not be sent to the OpenAI backend unless you include a <code>TracingProcessor</code> that does so.</li> </ol>"},{"location":"tracing/#tracing-with-non-openai-models","title":"Tracing with Non-OpenAI Models","text":"<p>You can use an OpenAI API key with non-OpenAI Models to enable free tracing in the OpenAI Traces dashboard without needing to disable tracing.</p> <pre><code>import os\nfrom agents import set_tracing_export_api_key, Agent, Runner\nfrom agents.extensions.models.litellm_model import LitellmModel\n\ntracing_api_key = os.environ[\"OPENAI_API_KEY\"]\nset_tracing_export_api_key(tracing_api_key)\n\nmodel = LitellmModel(\n    model=\"your-model-name\",\n    api_key=\"your-api-key\",\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n)\n</code></pre> <p>If you only need a different tracing key for a single run, pass it via <code>RunConfig</code> instead of changing the global exporter.</p> <pre><code>from agents import Runner, RunConfig\n\nawait Runner.run(\n    agent,\n    input=\"Hello\",\n    run_config=RunConfig(tracing={\"api_key\": \"sk-tracing-123\"}),\n)\n</code></pre>"},{"location":"tracing/#notes","title":"Notes","text":"<ul> <li>View free traces at Openai Traces dashboard.</li> </ul>"},{"location":"tracing/#external-tracing-processors-list","title":"External tracing processors list","text":"<ul> <li>Weights &amp; Biases</li> <li>Arize-Phoenix</li> <li>Future AGI</li> <li>MLflow (self-hosted/OSS)</li> <li>MLflow (Databricks hosted)</li> <li>Braintrust</li> <li>Pydantic Logfire</li> <li>AgentOps</li> <li>Scorecard</li> <li>Keywords AI</li> <li>LangSmith</li> <li>Maxim AI</li> <li>Comet Opik</li> <li>Langfuse</li> <li>Langtrace</li> <li>Okahu-Monocle</li> <li>Galileo</li> <li>Portkey AI</li> <li>LangDB AI</li> <li>Agenta</li> <li>PostHog</li> <li>Traccia</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>The Agents SDK automatically tracks token usage for every run. You can access it from the run context and use it to monitor costs, enforce limits, or record analytics.</p>"},{"location":"usage/#what-is-tracked","title":"What is tracked","text":"<ul> <li>requests: number of LLM API calls made</li> <li>input_tokens: total input tokens sent</li> <li>output_tokens: total output tokens received</li> <li>total_tokens: input + output</li> <li>request_usage_entries: list of per-request usage breakdowns</li> <li>details:</li> <li><code>input_tokens_details.cached_tokens</code></li> <li><code>output_tokens_details.reasoning_tokens</code></li> </ul>"},{"location":"usage/#accessing-usage-from-a-run","title":"Accessing usage from a run","text":"<p>After <code>Runner.run(...)</code>, access usage via <code>result.context_wrapper.usage</code>.</p> <pre><code>result = await Runner.run(agent, \"What's the weather in Tokyo?\")\nusage = result.context_wrapper.usage\n\nprint(\"Requests:\", usage.requests)\nprint(\"Input tokens:\", usage.input_tokens)\nprint(\"Output tokens:\", usage.output_tokens)\nprint(\"Total tokens:\", usage.total_tokens)\n</code></pre> <p>Usage is aggregated across all model calls during the run (including tool calls and handoffs).</p>"},{"location":"usage/#enabling-usage-with-litellm-models","title":"Enabling usage with LiteLLM models","text":"<p>LiteLLM providers do not report usage metrics by default. When you are using <code>LitellmModel</code>, pass <code>ModelSettings(include_usage=True)</code> to your agent so that LiteLLM responses populate <code>result.context_wrapper.usage</code>.</p> <pre><code>from agents import Agent, ModelSettings, Runner\nfrom agents.extensions.models.litellm_model import LitellmModel\n\nagent = Agent(\n    name=\"Assistant\",\n    model=LitellmModel(model=\"your/model\", api_key=\"...\"),\n    model_settings=ModelSettings(include_usage=True),\n)\n\nresult = await Runner.run(agent, \"What's the weather in Tokyo?\")\nprint(result.context_wrapper.usage.total_tokens)\n</code></pre>"},{"location":"usage/#per-request-usage-tracking","title":"Per-request usage tracking","text":"<p>The SDK automatically tracks usage for each API request in <code>request_usage_entries</code>, useful for detailed cost calculation and monitoring context window consumption.</p> <pre><code>result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n\nfor i, request in enumerate(result.context_wrapper.usage.request_usage_entries):\n    print(f\"Request {i + 1}: {request.input_tokens} in, {request.output_tokens} out\")\n</code></pre>"},{"location":"usage/#accessing-usage-with-sessions","title":"Accessing usage with sessions","text":"<p>When you use a <code>Session</code> (e.g., <code>SQLiteSession</code>), each call to <code>Runner.run(...)</code> returns usage for that specific run. Sessions maintain conversation history for context, but each run's usage is independent.</p> <pre><code>session = SQLiteSession(\"my_conversation\")\n\nfirst = await Runner.run(agent, \"Hi!\", session=session)\nprint(first.context_wrapper.usage.total_tokens)  # Usage for first run\n\nsecond = await Runner.run(agent, \"Can you elaborate?\", session=session)\nprint(second.context_wrapper.usage.total_tokens)  # Usage for second run\n</code></pre> <p>Note that while sessions preserve conversation context between runs, the usage metrics returned by each <code>Runner.run()</code> call represent only that particular execution. In sessions, previous messages may be re-fed as input to each run, which affects the input token count in consequent turns.</p>"},{"location":"usage/#using-usage-in-hooks","title":"Using usage in hooks","text":"<p>If you're using <code>RunHooks</code>, the <code>context</code> object passed to each hook contains <code>usage</code>. This lets you log usage at key lifecycle moments.</p> <pre><code>class MyHooks(RunHooks):\n    async def on_agent_end(self, context: RunContextWrapper, agent: Agent, output: Any) -&gt; None:\n        u = context.usage\n        print(f\"{agent.name} \u2192 {u.requests} requests, {u.total_tokens} total tokens\")\n</code></pre>"},{"location":"usage/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see:</p> <ul> <li><code>Usage</code> - Usage tracking data structure</li> <li><code>RequestUsage</code> - Per-request usage details</li> <li><code>RunContextWrapper</code> - Access usage from run context</li> <li><code>RunHooks</code> - Hook into usage tracking lifecycle</li> </ul>"},{"location":"visualization/","title":"Agent Visualization","text":"<p>Agent visualization allows you to generate a structured graphical representation of agents and their relationships using Graphviz. This is useful for understanding how agents, tools, and handoffs interact within an application.</p>"},{"location":"visualization/#installation","title":"Installation","text":"<p>Install the optional <code>viz</code> dependency group:</p> <pre><code>pip install \"openai-agents[viz]\"\n</code></pre>"},{"location":"visualization/#generating-a-graph","title":"Generating a Graph","text":"<p>You can generate an agent visualization using the <code>draw_graph</code> function. This function creates a directed graph where:</p> <ul> <li>Agents are represented as yellow boxes.</li> <li>MCP Servers are represented as grey boxes.</li> <li>Tools are represented as green ellipses.</li> <li>Handoffs are directed edges from one agent to another.</li> </ul>"},{"location":"visualization/#example-usage","title":"Example Usage","text":"<pre><code>import os\n\nfrom agents import Agent, function_tool\nfrom agents.mcp.server import MCPServerStdio\nfrom agents.extensions.visualization import draw_graph\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny.\"\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n)\n\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\nsamples_dir = os.path.join(current_dir, \"sample_files\")\nmcp_server = MCPServerStdio(\n    name=\"Filesystem Server, via npx\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    },\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    tools=[get_weather],\n    mcp_servers=[mcp_server],\n)\n\ndraw_graph(triage_agent)\n</code></pre> <p>This generates a graph that visually represents the structure of the triage agent and its connections to sub-agents and tools.</p>"},{"location":"visualization/#understanding-the-visualization","title":"Understanding the Visualization","text":"<p>The generated graph includes:</p> <ul> <li>A start node (<code>__start__</code>) indicating the entry point.</li> <li>Agents represented as rectangles with yellow fill.</li> <li>Tools represented as ellipses with green fill.</li> <li>MCP Servers represented as rectangles with grey fill.</li> <li>Directed edges indicating interactions:</li> <li>Solid arrows for agent-to-agent handoffs.</li> <li>Dotted arrows for tool invocations.</li> <li>Dashed arrows for MCP server invocations.</li> <li>An end node (<code>__end__</code>) indicating where execution terminates.</li> </ul> <p>Note: MCP servers are rendered in recent versions of the <code>agents</code> package (verified in v0.2.8). If you don\u2019t see MCP boxes in your visualization, upgrade to the latest release.</p>"},{"location":"visualization/#customizing-the-graph","title":"Customizing the Graph","text":""},{"location":"visualization/#showing-the-graph","title":"Showing the Graph","text":"<p>By default, <code>draw_graph</code> displays the graph inline. To show the graph in a separate window, write the following:</p> <pre><code>draw_graph(triage_agent).view()\n</code></pre>"},{"location":"visualization/#saving-the-graph","title":"Saving the Graph","text":"<p>By default, <code>draw_graph</code> displays the graph inline. To save it as a file, specify a filename:</p> <pre><code>draw_graph(triage_agent, filename=\"agent_graph\")\n</code></pre> <p>This will generate <code>agent_graph.png</code> in the working directory.</p>"},{"location":"models/","title":"Models","text":"<p>The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:</p> <ul> <li>Recommended: the <code>OpenAIResponsesModel</code>, which calls OpenAI APIs using the new Responses API.</li> <li>The <code>OpenAIChatCompletionsModel</code>, which calls OpenAI APIs using the Chat Completions API.</li> </ul>"},{"location":"models/#openai-models","title":"OpenAI models","text":"<p>When you don't specify a model when initializing an <code>Agent</code>, the default model will be used. The default is currently <code>gpt-4.1</code> for compatibility and low latency. If you have access, we recommend setting your agents to <code>gpt-5.2</code> for higher quality while keeping explicit <code>model_settings</code>.</p> <p>If you want to switch to other models like <code>gpt-5.2</code>, there are two ways to configure your agents.</p>"},{"location":"models/#default-model","title":"Default model","text":"<p>First, if you want to consistently use a specific model for all agents that do not set a custom model, set the <code>OPENAI_DEFAULT_MODEL</code> environment variable before running your agents.</p> <pre><code>export OPENAI_DEFAULT_MODEL=gpt-5.2\npython3 my_awesome_agent.py\n</code></pre> <p>Second, you can set a default model for a run via <code>RunConfig</code>. If you don't set a model for an agent, this run's model will be used.</p> <pre><code>from agents import Agent, RunConfig, Runner\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You're a helpful agent.\",\n)\n\nresult = await Runner.run(\n    agent,\n    \"Hello\",\n    run_config=RunConfig(model=\"gpt-5.2\"),\n)\n</code></pre>"},{"location":"models/#gpt-5x-models","title":"GPT-5.x models","text":"<p>When you use any GPT-5.x model such as <code>gpt-5.2</code> in this way, the SDK applies default <code>ModelSettings</code>. It sets the ones that work the best for most use cases. To adjust the reasoning effort for the default model, pass your own <code>ModelSettings</code>:</p> <pre><code>from openai.types.shared import Reasoning\nfrom agents import Agent, ModelSettings\n\nmy_agent = Agent(\n    name=\"My Agent\",\n    instructions=\"You're a helpful agent.\",\n    # If OPENAI_DEFAULT_MODEL=gpt-5.2 is set, passing only model_settings works.\n    # It's also fine to pass a GPT-5.x model name explicitly:\n    model=\"gpt-5.2\",\n    model_settings=ModelSettings(reasoning=Reasoning(effort=\"high\"), verbosity=\"low\")\n)\n</code></pre> <p>For lower latency, using <code>reasoning.effort=\"none\"</code> with <code>gpt-5.2</code> is recommended. The gpt-4.1 family (including mini and nano variants) also remains a solid choice for building interactive agent apps.</p>"},{"location":"models/#non-gpt-5-models","title":"Non-GPT-5 models","text":"<p>If you pass a non\u2013GPT-5 model name without custom <code>model_settings</code>, the SDK reverts to generic <code>ModelSettings</code> compatible with any model.</p>"},{"location":"models/#non-openai-models","title":"Non-OpenAI models","text":"<p>You can use most other non-OpenAI models via the LiteLLM integration. First, install the litellm dependency group:</p> <pre><code>pip install \"openai-agents[litellm]\"\n</code></pre> <p>Then, use any of the supported models with the <code>litellm/</code> prefix:</p> <pre><code>claude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", ...)\ngemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", ...)\n</code></pre>"},{"location":"models/#other-ways-to-use-non-openai-models","title":"Other ways to use non-OpenAI models","text":"<p>You can integrate other LLM providers in 3 more ways (examples here):</p> <ol> <li><code>set_default_openai_client</code> is useful in cases where you want to globally use an instance of <code>AsyncOpenAI</code> as the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the <code>base_url</code> and <code>api_key</code>. See a configurable example in examples/model_providers/custom_example_global.py.</li> <li><code>ModelProvider</code> is at the <code>Runner.run</code> level. This lets you say \"use a custom model provider for all agents in this run\". See a configurable example in examples/model_providers/custom_example_provider.py.</li> <li><code>Agent.model</code> lets you specify the model on a specific Agent instance. This enables you to mix and match different providers for different agents. See a configurable example in examples/model_providers/custom_example_agent.py. An easy way to use most available models is via the LiteLLM integration.</li> </ol> <p>In cases where you do not have an API key from <code>platform.openai.com</code>, we recommend disabling tracing via <code>set_tracing_disabled()</code>, or setting up a different tracing processor.</p> <p>Note</p> <p>In these examples, we use the Chat Completions API/model, because most LLM providers don't yet support the Responses API. If your LLM provider does support it, we recommend using Responses.</p>"},{"location":"models/#mixing-and-matching-models","title":"Mixing and matching models","text":"<p>Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an <code>Agent</code>, you can select a specific model by either:</p> <ol> <li>Passing the name of a model.</li> <li>Passing any model name + a <code>ModelProvider</code> that can map that name to a Model instance.</li> <li>Directly providing a <code>Model</code> implementation.</li> </ol> <p>Note</p> <p>While our SDK supports both the <code>OpenAIResponsesModel</code> and the <code>OpenAIChatCompletionsModel</code> shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.</p> <pre><code>from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n    model=\"gpt-5-mini\", # (1)!\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=OpenAIChatCompletionsModel( # (2)!\n        model=\"gpt-5-nano\",\n        openai_client=AsyncOpenAI()\n    ),\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    model=\"gpt-5\",\n)\n\nasync def main():\n    result = await Runner.run(triage_agent, input=\"Hola, \u00bfc\u00f3mo est\u00e1s?\")\n    print(result.final_output)\n</code></pre> <ol> <li>Sets the name of an OpenAI model directly.</li> <li>Provides a <code>Model</code> implementation.</li> </ol> <p>When you want to further configure the model used for an agent, you can pass <code>ModelSettings</code>, which provides optional model configuration parameters such as temperature.</p> <pre><code>from agents import Agent, ModelSettings\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=\"gpt-4.1\",\n    model_settings=ModelSettings(temperature=0.1),\n)\n</code></pre> <p>Also, when you use OpenAI's Responses API, there are a few other optional parameters (e.g., <code>user</code>, <code>service_tier</code>, and so on). If they are not available at the top level, you can use <code>extra_args</code> to pass them as well.</p> <pre><code>from agents import Agent, ModelSettings\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=\"gpt-4.1\",\n    model_settings=ModelSettings(\n        temperature=0.1,\n        extra_args={\"service_tier\": \"flex\", \"user\": \"user_12345\"},\n    ),\n)\n</code></pre>"},{"location":"models/#common-issues-with-using-other-llm-providers","title":"Common issues with using other LLM providers","text":""},{"location":"models/#tracing-client-error-401","title":"Tracing client error 401","text":"<p>If you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:</p> <ol> <li>Disable tracing entirely: <code>set_tracing_disabled(True)</code>.</li> <li>Set an OpenAI key for tracing: <code>set_tracing_export_api_key(...)</code>. This API key will only be used for uploading traces, and must be from platform.openai.com.</li> <li>Use a non-OpenAI trace processor. See the tracing docs.</li> </ol>"},{"location":"models/#responses-api-support","title":"Responses API support","text":"<p>The SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:</p> <ol> <li>Call <code>set_default_openai_api(\"chat_completions\")</code>. This works if you are setting <code>OPENAI_API_KEY</code> and <code>OPENAI_BASE_URL</code> via environment vars.</li> <li>Use <code>OpenAIChatCompletionsModel</code>. There are examples here.</li> </ol>"},{"location":"models/#structured-outputs-support","title":"Structured outputs support","text":"<p>Some model providers don't have support for structured outputs. This sometimes results in an error that looks something like this:</p> <pre><code>BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n</code></pre> <p>This is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the <code>json_schema</code> to use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.</p>"},{"location":"models/#mixing-models-across-providers","title":"Mixing models across providers","text":"<p>You need to be aware of feature differences between model providers, or you may run into errors. For example, OpenAI supports structured outputs, multimodal input, and hosted file search and web search, but many other providers don't support these features. Be aware of these limitations:</p> <ul> <li>Don't send unsupported <code>tools</code> to providers that don't understand them</li> <li>Filter out multimodal inputs before calling models that are text-only</li> <li>Be aware that providers that don't support structured JSON outputs will occasionally produce invalid JSON.</li> </ul>"},{"location":"models/litellm/","title":"Using any model via LiteLLM","text":"<p>Note</p> <p>The LiteLLM integration is in beta. You may run into issues with some model providers, especially smaller ones. Please report any issues via Github issues and we'll fix quickly.</p> <p>LiteLLM is a library that allows you to use 100+ models via a single interface. We've added a LiteLLM integration to allow you to use any AI model in the Agents SDK.</p>"},{"location":"models/litellm/#setup","title":"Setup","text":"<p>You'll need to ensure <code>litellm</code> is available. You can do this by installing the optional <code>litellm</code> dependency group:</p> <pre><code>pip install \"openai-agents[litellm]\"\n</code></pre> <p>Once done, you can use <code>LitellmModel</code> in any agent.</p>"},{"location":"models/litellm/#example","title":"Example","text":"<p>This is a fully working example. When you run it, you'll be prompted for a model name and API key. For example, you could enter:</p> <ul> <li><code>openai/gpt-4.1</code> for the model, and your OpenAI API key</li> <li><code>anthropic/claude-3-5-sonnet-20240620</code> for the model, and your Anthropic API key</li> <li>etc</li> </ul> <p>For a full list of models supported in LiteLLM, see the litellm providers docs.</p> <pre><code>from __future__ import annotations\n\nimport asyncio\n\nfrom agents import Agent, Runner, function_tool, set_tracing_disabled\nfrom agents.extensions.models.litellm_model import LitellmModel\n\n@function_tool\ndef get_weather(city: str):\n    print(f\"[debug] getting weather for {city}\")\n    return f\"The weather in {city} is sunny.\"\n\n\nasync def main(model: str, api_key: str):\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You only respond in haikus.\",\n        model=LitellmModel(model=model, api_key=api_key),\n        tools=[get_weather],\n    )\n\n    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    # First try to get model/api key from args\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, required=False)\n    parser.add_argument(\"--api-key\", type=str, required=False)\n    args = parser.parse_args()\n\n    model = args.model\n    if not model:\n        model = input(\"Enter a model name for Litellm: \")\n\n    api_key = args.api_key\n    if not api_key:\n        api_key = input(\"Enter an API key for Litellm: \")\n\n    asyncio.run(main(model, api_key))\n</code></pre>"},{"location":"models/litellm/#tracking-usage-data","title":"Tracking usage data","text":"<p>If you want LiteLLM responses to populate the Agents SDK usage metrics, pass <code>ModelSettings(include_usage=True)</code> when creating your agent.</p> <pre><code>from agents import Agent, ModelSettings\nfrom agents.extensions.models.litellm_model import LitellmModel\n\nagent = Agent(\n    name=\"Assistant\",\n    model=LitellmModel(model=\"your/model\", api_key=\"...\"),\n    model_settings=ModelSettings(include_usage=True),\n)\n</code></pre> <p>With <code>include_usage=True</code>, LiteLLM requests report token and request counts through <code>result.context_wrapper.usage</code> just like the built-in OpenAI models.</p>"},{"location":"models/litellm/#troubleshooting","title":"Troubleshooting","text":"<p>If you see Pydantic serializer warnings from LiteLLM responses, enable a small compatibility patch by setting:</p> <pre><code>export OPENAI_AGENTS_ENABLE_LITELLM_SERIALIZER_PATCH=true\n</code></pre> <p>This opt-in flag suppresses known LiteLLM serializer warnings while preserving normal behavior. Turn it off (unset or <code>false</code>) if you do not need it.</p>"},{"location":"realtime/guide/","title":"Guide","text":"<p>This guide provides an in-depth look at building voice-enabled AI agents using the OpenAI Agents SDK's realtime capabilities.</p> <p>Beta feature</p> <p>Realtime agents are in beta. Expect some breaking changes as we improve the implementation.</p>"},{"location":"realtime/guide/#overview","title":"Overview","text":"<p>Realtime agents allow for conversational flows, processing audio and text inputs in real time and responding with realtime audio. They maintain persistent connections with OpenAI's Realtime API, enabling natural voice conversations with low latency and the ability to handle interruptions gracefully.</p>"},{"location":"realtime/guide/#architecture","title":"Architecture","text":""},{"location":"realtime/guide/#core-components","title":"Core Components","text":"<p>The realtime system consists of several key components:</p> <ul> <li>RealtimeAgent: An agent, configured with instructions, tools and handoffs.</li> <li>RealtimeRunner: Manages configuration. You can call <code>runner.run()</code> to get a session.</li> <li>RealtimeSession: A single interaction session. You typically create one each time a user starts a conversation, and keep it alive until the conversation is done.</li> <li>RealtimeModel: The underlying model interface (typically OpenAI's WebSocket implementation)</li> </ul>"},{"location":"realtime/guide/#session-flow","title":"Session flow","text":"<p>A typical realtime session follows this flow:</p> <ol> <li>Create your RealtimeAgent(s) with instructions, tools and handoffs.</li> <li>Set up a RealtimeRunner with the agent and configuration options</li> <li>Start the session using <code>await runner.run()</code> which returns a RealtimeSession.</li> <li>Send audio or text messages to the session using <code>send_audio()</code> or <code>send_message()</code></li> <li>Listen for events by iterating over the session - events include audio output, transcripts, tool calls, handoffs, and errors</li> <li>Handle interruptions when users speak over the agent, which automatically stops current audio generation</li> </ol> <p>The session maintains the conversation history and manages the persistent connection with the realtime model.</p>"},{"location":"realtime/guide/#agent-configuration","title":"Agent configuration","text":"<p>RealtimeAgent works similarly to the regular Agent class with some key differences. For full API details, see the <code>RealtimeAgent</code> API reference.</p> <p>Key differences from regular agents:</p> <ul> <li>Model choice is configured at the session level, not the agent level.</li> <li>No structured output support (<code>outputType</code> is not supported).</li> <li>Voice can be configured per agent but cannot be changed after the first agent speaks.</li> <li>All other features like tools, handoffs, and instructions work the same way.</li> </ul>"},{"location":"realtime/guide/#session-configuration","title":"Session configuration","text":""},{"location":"realtime/guide/#model-settings","title":"Model settings","text":"<p>The session configuration allows you to control the underlying realtime model behavior. You can configure the model name (such as <code>gpt-realtime</code>), voice selection (alloy, echo, fable, onyx, nova, shimmer), and supported modalities (text and/or audio). Audio formats can be set for both input and output, with PCM16 being the default.</p>"},{"location":"realtime/guide/#audio-configuration","title":"Audio configuration","text":"<p>Audio settings control how the session handles voice input and output. You can configure input audio transcription using models like Whisper, set language preferences, and provide transcription prompts to improve accuracy for domain-specific terms. Turn detection settings control when the agent should start and stop responding, with options for voice activity detection thresholds, silence duration, and padding around detected speech.</p>"},{"location":"realtime/guide/#tools-and-functions","title":"Tools and Functions","text":""},{"location":"realtime/guide/#adding-tools","title":"Adding Tools","text":"<p>Just like regular agents, realtime agents support function tools that execute during conversations:</p> <pre><code>from agents import function_tool\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    # Your weather API logic here\n    return f\"The weather in {city} is sunny, 72\u00b0F\"\n\n@function_tool\ndef book_appointment(date: str, time: str, service: str) -&gt; str:\n    \"\"\"Book an appointment.\"\"\"\n    # Your booking logic here\n    return f\"Appointment booked for {service} on {date} at {time}\"\n\nagent = RealtimeAgent(\n    name=\"Assistant\",\n    instructions=\"You can help with weather and appointments.\",\n    tools=[get_weather, book_appointment],\n)\n</code></pre>"},{"location":"realtime/guide/#handoffs","title":"Handoffs","text":""},{"location":"realtime/guide/#creating-handoffs","title":"Creating Handoffs","text":"<p>Handoffs allow transferring conversations between specialized agents.</p> <pre><code>from agents.realtime import realtime_handoff\n\n# Specialized agents\nbilling_agent = RealtimeAgent(\n    name=\"Billing Support\",\n    instructions=\"You specialize in billing and payment issues.\",\n)\n\ntechnical_agent = RealtimeAgent(\n    name=\"Technical Support\",\n    instructions=\"You handle technical troubleshooting.\",\n)\n\n# Main agent with handoffs\nmain_agent = RealtimeAgent(\n    name=\"Customer Service\",\n    instructions=\"You are the main customer service agent. Hand off to specialists when needed.\",\n    handoffs=[\n        realtime_handoff(billing_agent, tool_description=\"Transfer to billing support\"),\n        realtime_handoff(technical_agent, tool_description=\"Transfer to technical support\"),\n    ]\n)\n</code></pre>"},{"location":"realtime/guide/#event-handling","title":"Event handling","text":"<p>The session streams events that you can listen to by iterating over the session object. Events include audio output chunks, transcription results, tool execution start and end, agent handoffs, and errors. Key events to handle include:</p> <ul> <li>audio: Raw audio data from the agent's response</li> <li>audio_end: Agent finished speaking</li> <li>audio_interrupted: User interrupted the agent</li> <li>tool_start/tool_end: Tool execution lifecycle</li> <li>handoff: Agent handoff occurred</li> <li>error: Error occurred during processing</li> </ul> <p>For complete event details, see <code>RealtimeSessionEvent</code>.</p>"},{"location":"realtime/guide/#guardrails","title":"Guardrails","text":"<p>Only output guardrails are supported for realtime agents. These guardrails are debounced and run periodically (not on every word) to avoid performance issues during real-time generation. The default debounce length is 100 characters, but this is configurable.</p> <p>Guardrails can be attached directly to a <code>RealtimeAgent</code> or provided via the session's <code>run_config</code>. Guardrails from both sources run together.</p> <pre><code>from agents.guardrail import GuardrailFunctionOutput, OutputGuardrail\n\ndef sensitive_data_check(context, agent, output):\n    return GuardrailFunctionOutput(\n        tripwire_triggered=\"password\" in output,\n        output_info=None,\n    )\n\nagent = RealtimeAgent(\n    name=\"Assistant\",\n    instructions=\"...\",\n    output_guardrails=[OutputGuardrail(guardrail_function=sensitive_data_check)],\n)\n</code></pre> <p>When a guardrail is triggered, it generates a <code>guardrail_tripped</code> event and can interrupt the agent's current response. The debounce behavior helps balance safety with real-time performance requirements. Unlike text agents, realtime agents do not raise an Exception when guardrails are tripped.</p>"},{"location":"realtime/guide/#audio-processing","title":"Audio processing","text":"<p>Send audio to the session using <code>session.send_audio(audio_bytes)</code> or send text using <code>session.send_message()</code>.</p> <p>For audio output, listen for <code>audio</code> events and play the audio data through your preferred audio library. Make sure to listen for <code>audio_interrupted</code> events to stop playback immediately and clear any queued audio when the user interrupts the agent.</p>"},{"location":"realtime/guide/#sip-integration","title":"SIP integration","text":"<p>You can attach realtime agents to phone calls that arrive via the Realtime Calls API. The SDK provides <code>OpenAIRealtimeSIPModel</code>, which reuses the same agent flow while negotiating media over SIP.</p> <p>To use it, pass the model instance to the runner and supply the SIP <code>call_id</code> when starting the session. The call ID is delivered by the webhook that signals an incoming call.</p> <pre><code>from agents.realtime import RealtimeAgent, RealtimeRunner\nfrom agents.realtime.openai_realtime import OpenAIRealtimeSIPModel\n\nrunner = RealtimeRunner(\n    starting_agent=agent,\n    model=OpenAIRealtimeSIPModel(),\n)\n\nasync with await runner.run(\n    model_config={\n        \"call_id\": call_id_from_webhook,\n        \"initial_model_settings\": {\n            \"turn_detection\": {\"type\": \"semantic_vad\", \"interrupt_response\": True},\n        },\n    },\n) as session:\n    async for event in session:\n        ...\n</code></pre> <p>When the caller hangs up, the SIP session ends and the realtime connection closes automatically. For a complete telephony example, see <code>examples/realtime/twilio_sip</code>.</p>"},{"location":"realtime/guide/#direct-model-access","title":"Direct model access","text":"<p>You can access the underlying model to add custom listeners or perform advanced operations:</p> <pre><code># Add a custom listener to the model\nsession.model.add_listener(my_custom_listener)\n</code></pre> <p>This gives you direct access to the <code>RealtimeModel</code> interface for advanced use cases where you need lower-level control over the connection.</p>"},{"location":"realtime/guide/#examples","title":"Examples","text":"<p>For complete working examples, check out the examples/realtime directory which includes demos with and without UI components.</p>"},{"location":"realtime/quickstart/","title":"Quickstart","text":"<p>Realtime agents enable voice conversations with your AI agents using OpenAI's Realtime API. This guide walks you through creating your first realtime voice agent.</p> <p>Beta feature</p> <p>Realtime agents are in beta. Expect some breaking changes as we improve the implementation.</p>"},{"location":"realtime/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>OpenAI API key</li> <li>Basic familiarity with the OpenAI Agents SDK</li> </ul>"},{"location":"realtime/quickstart/#installation","title":"Installation","text":"<p>If you haven't already, install the OpenAI Agents SDK:</p> <pre><code>pip install openai-agents\n</code></pre>"},{"location":"realtime/quickstart/#creating-your-first-realtime-agent","title":"Creating your first realtime agent","text":""},{"location":"realtime/quickstart/#1-import-required-components","title":"1. Import required components","text":"<pre><code>import asyncio\nfrom agents.realtime import RealtimeAgent, RealtimeRunner\n</code></pre>"},{"location":"realtime/quickstart/#2-create-a-realtime-agent","title":"2. Create a realtime agent","text":"<pre><code>agent = RealtimeAgent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful voice assistant. Keep your responses conversational and friendly.\",\n)\n</code></pre>"},{"location":"realtime/quickstart/#3-set-up-the-runner","title":"3. Set up the runner","text":"<pre><code>runner = RealtimeRunner(\n    starting_agent=agent,\n    config={\n        \"model_settings\": {\n            \"model_name\": \"gpt-realtime\",\n            \"voice\": \"ash\",\n            \"modalities\": [\"audio\"],\n            \"input_audio_format\": \"pcm16\",\n            \"output_audio_format\": \"pcm16\",\n            \"input_audio_transcription\": {\"model\": \"gpt-4o-mini-transcribe\"},\n            \"turn_detection\": {\"type\": \"semantic_vad\", \"interrupt_response\": True},\n        }\n    }\n)\n</code></pre>"},{"location":"realtime/quickstart/#4-start-a-session","title":"4. Start a session","text":"<pre><code># Start the session\nsession = await runner.run()\n\nasync with session:\n    print(\"Session started! The agent will stream audio responses in real-time.\")\n    # Process events\n    async for event in session:\n        try:\n            if event.type == \"agent_start\":\n                print(f\"Agent started: {event.agent.name}\")\n            elif event.type == \"agent_end\":\n                print(f\"Agent ended: {event.agent.name}\")\n            elif event.type == \"handoff\":\n                print(f\"Handoff from {event.from_agent.name} to {event.to_agent.name}\")\n            elif event.type == \"tool_start\":\n                print(f\"Tool started: {event.tool.name}\")\n            elif event.type == \"tool_end\":\n                print(f\"Tool ended: {event.tool.name}; output: {event.output}\")\n            elif event.type == \"audio_end\":\n                print(\"Audio ended\")\n            elif event.type == \"audio\":\n                # Enqueue audio for callback-based playback with metadata\n                # Non-blocking put; queue is unbounded, so drops won\u2019t occur.\n                pass\n            elif event.type == \"audio_interrupted\":\n                print(\"Audio interrupted\")\n                # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.\n            elif event.type == \"error\":\n                print(f\"Error: {event.error}\")\n            elif event.type == \"history_updated\":\n                pass  # Skip these frequent events\n            elif event.type == \"history_added\":\n                pass  # Skip these frequent events\n            elif event.type == \"raw_model_event\":\n                print(f\"Raw model event: {_truncate_str(str(event.data), 200)}\")\n            else:\n                print(f\"Unknown event type: {event.type}\")\n        except Exception as e:\n            print(f\"Error processing event: {_truncate_str(str(e), 200)}\")\n\ndef _truncate_str(s: str, max_length: int) -&gt; str:\n    if len(s) &gt; max_length:\n        return s[:max_length] + \"...\"\n    return s\n</code></pre>"},{"location":"realtime/quickstart/#complete-example","title":"Complete example","text":"<p>Here's a complete working example:</p> <pre><code>import asyncio\nfrom agents.realtime import RealtimeAgent, RealtimeRunner\n\nasync def main():\n    # Create the agent\n    agent = RealtimeAgent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful voice assistant. Keep responses brief and conversational.\",\n    )\n    # Set up the runner with configuration\n    runner = RealtimeRunner(\n        starting_agent=agent,\n        config={\n            \"model_settings\": {\n                \"model_name\": \"gpt-realtime\",\n                \"voice\": \"ash\",\n                \"modalities\": [\"audio\"],\n                \"input_audio_format\": \"pcm16\",\n                \"output_audio_format\": \"pcm16\",\n                \"input_audio_transcription\": {\"model\": \"gpt-4o-mini-transcribe\"},\n                \"turn_detection\": {\"type\": \"semantic_vad\", \"interrupt_response\": True},\n            }\n        },\n    )\n    # Start the session\n    session = await runner.run()\n\n    async with session:\n        print(\"Session started! The agent will stream audio responses in real-time.\")\n        # Process events\n        async for event in session:\n            try:\n                if event.type == \"agent_start\":\n                    print(f\"Agent started: {event.agent.name}\")\n                elif event.type == \"agent_end\":\n                    print(f\"Agent ended: {event.agent.name}\")\n                elif event.type == \"handoff\":\n                    print(f\"Handoff from {event.from_agent.name} to {event.to_agent.name}\")\n                elif event.type == \"tool_start\":\n                    print(f\"Tool started: {event.tool.name}\")\n                elif event.type == \"tool_end\":\n                    print(f\"Tool ended: {event.tool.name}; output: {event.output}\")\n                elif event.type == \"audio_end\":\n                    print(\"Audio ended\")\n                elif event.type == \"audio\":\n                    # Enqueue audio for callback-based playback with metadata\n                    # Non-blocking put; queue is unbounded, so drops won\u2019t occur.\n                    pass\n                elif event.type == \"audio_interrupted\":\n                    print(\"Audio interrupted\")\n                    # Begin graceful fade + flush in the audio callback and rebuild jitter buffer.\n                elif event.type == \"error\":\n                    print(f\"Error: {event.error}\")\n                elif event.type == \"history_updated\":\n                    pass  # Skip these frequent events\n                elif event.type == \"history_added\":\n                    pass  # Skip these frequent events\n                elif event.type == \"raw_model_event\":\n                    print(f\"Raw model event: {_truncate_str(str(event.data), 200)}\")\n                else:\n                    print(f\"Unknown event type: {event.type}\")\n            except Exception as e:\n                print(f\"Error processing event: {_truncate_str(str(e), 200)}\")\n\ndef _truncate_str(s: str, max_length: int) -&gt; str:\n    if len(s) &gt; max_length:\n        return s[:max_length] + \"...\"\n    return s\n\nif __name__ == \"__main__\":\n    # Run the session\n    asyncio.run(main())\n</code></pre>"},{"location":"realtime/quickstart/#configuration-options","title":"Configuration options","text":""},{"location":"realtime/quickstart/#model-settings","title":"Model settings","text":"<ul> <li><code>model_name</code>: Choose from available realtime models (e.g., <code>gpt-realtime</code>)</li> <li><code>voice</code>: Select voice (<code>alloy</code>, <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, <code>shimmer</code>)</li> <li><code>modalities</code>: Enable text or audio (<code>[\"text\"]</code> or <code>[\"audio\"]</code>)</li> </ul>"},{"location":"realtime/quickstart/#audio-settings","title":"Audio settings","text":"<ul> <li><code>input_audio_format</code>: Format for input audio (<code>pcm16</code>, <code>g711_ulaw</code>, <code>g711_alaw</code>)</li> <li><code>output_audio_format</code>: Format for output audio</li> <li><code>input_audio_transcription</code>: Transcription configuration</li> </ul>"},{"location":"realtime/quickstart/#turn-detection","title":"Turn detection","text":"<ul> <li><code>type</code>: Detection method (<code>server_vad</code>, <code>semantic_vad</code>)</li> <li><code>threshold</code>: Voice activity threshold (0.0-1.0)</li> <li><code>silence_duration_ms</code>: Silence duration to detect turn end</li> <li><code>prefix_padding_ms</code>: Audio padding before speech</li> </ul>"},{"location":"realtime/quickstart/#next-steps","title":"Next steps","text":"<ul> <li>Learn more about realtime agents</li> <li>Check out working examples in the examples/realtime folder</li> <li>Add tools to your agent</li> <li>Implement handoffs between agents</li> <li>Set up guardrails for safety</li> </ul>"},{"location":"realtime/quickstart/#authentication","title":"Authentication","text":"<p>Make sure your OpenAI API key is set in your environment:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Or pass it directly when creating the session:</p> <pre><code>session = await runner.run(model_config={\"api_key\": \"your-api-key\"})\n</code></pre>"},{"location":"ref/","title":"Agents module","text":""},{"location":"ref/#agents.set_default_openai_key","title":"set_default_openai_key","text":"<pre><code>set_default_openai_key(\n    key: str, use_for_tracing: bool = True\n) -&gt; None\n</code></pre> <p>Set the default OpenAI API key to use for LLM requests (and optionally tracing()). This is only necessary if the OPENAI_API_KEY environment variable is not already set.</p> <p>If provided, this key will be used instead of the OPENAI_API_KEY environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The OpenAI key to use.</p> required <code>use_for_tracing</code> <code>bool</code> <p>Whether to also use this key to send traces to OpenAI. Defaults to True If False, you'll either need to set the OPENAI_API_KEY environment variable or call set_tracing_export_api_key() with the API key you want to use for tracing.</p> <code>True</code> Source code in <code>src/agents/__init__.py</code> <pre><code>def set_default_openai_key(key: str, use_for_tracing: bool = True) -&gt; None:\n    \"\"\"Set the default OpenAI API key to use for LLM requests (and optionally tracing()). This is\n    only necessary if the OPENAI_API_KEY environment variable is not already set.\n\n    If provided, this key will be used instead of the OPENAI_API_KEY environment variable.\n\n    Args:\n        key: The OpenAI key to use.\n        use_for_tracing: Whether to also use this key to send traces to OpenAI. Defaults to True\n            If False, you'll either need to set the OPENAI_API_KEY environment variable or call\n            set_tracing_export_api_key() with the API key you want to use for tracing.\n    \"\"\"\n    _config.set_default_openai_key(key, use_for_tracing)\n</code></pre>"},{"location":"ref/#agents.set_default_openai_client","title":"set_default_openai_client","text":"<pre><code>set_default_openai_client(\n    client: AsyncOpenAI, use_for_tracing: bool = True\n) -&gt; None\n</code></pre> <p>Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this client will be used instead of the default OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>The OpenAI client to use.</p> required <code>use_for_tracing</code> <code>bool</code> <p>Whether to use the API key from this client for uploading traces. If False, you'll either need to set the OPENAI_API_KEY environment variable or call set_tracing_export_api_key() with the API key you want to use for tracing.</p> <code>True</code> Source code in <code>src/agents/__init__.py</code> <pre><code>def set_default_openai_client(client: AsyncOpenAI, use_for_tracing: bool = True) -&gt; None:\n    \"\"\"Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this\n    client will be used instead of the default OpenAI client.\n\n    Args:\n        client: The OpenAI client to use.\n        use_for_tracing: Whether to use the API key from this client for uploading traces. If False,\n            you'll either need to set the OPENAI_API_KEY environment variable or call\n            set_tracing_export_api_key() with the API key you want to use for tracing.\n    \"\"\"\n    _config.set_default_openai_client(client, use_for_tracing)\n</code></pre>"},{"location":"ref/#agents.set_default_openai_api","title":"set_default_openai_api","text":"<pre><code>set_default_openai_api(\n    api: Literal[\"chat_completions\", \"responses\"],\n) -&gt; None\n</code></pre> <p>Set the default API to use for OpenAI LLM requests. By default, we will use the responses API but you can set this to use the chat completions API instead.</p> Source code in <code>src/agents/__init__.py</code> <pre><code>def set_default_openai_api(api: Literal[\"chat_completions\", \"responses\"]) -&gt; None:\n    \"\"\"Set the default API to use for OpenAI LLM requests. By default, we will use the responses API\n    but you can set this to use the chat completions API instead.\n    \"\"\"\n    _config.set_default_openai_api(api)\n</code></pre>"},{"location":"ref/#agents.set_tracing_export_api_key","title":"set_tracing_export_api_key","text":"<pre><code>set_tracing_export_api_key(api_key: str) -&gt; None\n</code></pre> <p>Set the OpenAI API key for the backend exporter.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_export_api_key(api_key: str) -&gt; None:\n    \"\"\"\n    Set the OpenAI API key for the backend exporter.\n    \"\"\"\n    default_exporter().set_api_key(api_key)\n</code></pre>"},{"location":"ref/#agents.set_tracing_disabled","title":"set_tracing_disabled","text":"<pre><code>set_tracing_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Set whether tracing is globally disabled.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_disabled(disabled: bool) -&gt; None:\n    \"\"\"\n    Set whether tracing is globally disabled.\n    \"\"\"\n    get_trace_provider().set_disabled(disabled)\n</code></pre>"},{"location":"ref/#agents.set_trace_processors","title":"set_trace_processors","text":"<pre><code>set_trace_processors(\n    processors: list[TracingProcessor],\n) -&gt; None\n</code></pre> <p>Set the list of trace processors. This will replace the current list of processors.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_trace_processors(processors: list[TracingProcessor]) -&gt; None:\n    \"\"\"\n    Set the list of trace processors. This will replace the current list of processors.\n    \"\"\"\n    get_trace_provider().set_processors(processors)\n</code></pre>"},{"location":"ref/#agents.enable_verbose_stdout_logging","title":"enable_verbose_stdout_logging","text":"<pre><code>enable_verbose_stdout_logging()\n</code></pre> <p>Enables verbose logging to stdout. This is useful for debugging.</p> Source code in <code>src/agents/__init__.py</code> <pre><code>def enable_verbose_stdout_logging():\n    \"\"\"Enables verbose logging to stdout. This is useful for debugging.\"\"\"\n    logger = logging.getLogger(\"openai.agents\")\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(logging.StreamHandler(sys.stdout))\n</code></pre>"},{"location":"ref/agent/","title":"<code>Agents</code>","text":""},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputFunction","title":"ToolsToFinalOutputFunction  <code>module-attribute</code>","text":"<pre><code>ToolsToFinalOutputFunction: TypeAlias = Callable[\n    [RunContextWrapper[TContext], list[FunctionToolResult]],\n    MaybeAwaitable[ToolsToFinalOutputResult],\n]\n</code></pre> <p>A function that takes a run context and a list of tool results, and returns a <code>ToolsToFinalOutputResult</code>.</p>"},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputResult","title":"ToolsToFinalOutputResult  <code>dataclass</code>","text":"Source code in <code>src/agents/agent.py</code> <pre><code>@dataclass\nclass ToolsToFinalOutputResult:\n    is_final_output: bool\n    \"\"\"Whether this is the final output. If False, the LLM will run again and receive the tool call\n    output.\n    \"\"\"\n\n    final_output: Any | None = None\n    \"\"\"The final output. Can be None if `is_final_output` is False, otherwise must match the\n    `output_type` of the agent.\n    \"\"\"\n</code></pre>"},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputResult.is_final_output","title":"is_final_output  <code>instance-attribute</code>","text":"<pre><code>is_final_output: bool\n</code></pre> <p>Whether this is the final output. If False, the LLM will run again and receive the tool call output.</p>"},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputResult.final_output","title":"final_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>final_output: Any | None = None\n</code></pre> <p>The final output. Can be None if <code>is_final_output</code> is False, otherwise must match the <code>output_type</code> of the agent.</p>"},{"location":"ref/agent/#agents.agent.AgentToolStreamEvent","title":"AgentToolStreamEvent","text":"<p>               Bases: <code>TypedDict</code></p> <p>Streaming event emitted when an agent is invoked as a tool.</p> Source code in <code>src/agents/agent.py</code> <pre><code>class AgentToolStreamEvent(TypedDict):\n    \"\"\"Streaming event emitted when an agent is invoked as a tool.\"\"\"\n\n    event: StreamEvent\n    \"\"\"The streaming event from the nested agent run.\"\"\"\n\n    agent: Agent[Any]\n    \"\"\"The nested agent emitting the event.\"\"\"\n\n    tool_call: ResponseFunctionToolCall | None\n    \"\"\"The originating tool call, if available.\"\"\"\n</code></pre>"},{"location":"ref/agent/#agents.agent.AgentToolStreamEvent.event","title":"event  <code>instance-attribute</code>","text":"<pre><code>event: StreamEvent\n</code></pre> <p>The streaming event from the nested agent run.</p>"},{"location":"ref/agent/#agents.agent.AgentToolStreamEvent.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The nested agent emitting the event.</p>"},{"location":"ref/agent/#agents.agent.AgentToolStreamEvent.tool_call","title":"tool_call  <code>instance-attribute</code>","text":"<pre><code>tool_call: ResponseFunctionToolCall | None\n</code></pre> <p>The originating tool call, if available.</p>"},{"location":"ref/agent/#agents.agent.StopAtTools","title":"StopAtTools","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/agents/agent.py</code> <pre><code>class StopAtTools(TypedDict):\n    stop_at_tool_names: list[str]\n    \"\"\"A list of tool names, any of which will stop the agent from running further.\"\"\"\n</code></pre>"},{"location":"ref/agent/#agents.agent.StopAtTools.stop_at_tool_names","title":"stop_at_tool_names  <code>instance-attribute</code>","text":"<pre><code>stop_at_tool_names: list[str]\n</code></pre> <p>A list of tool names, any of which will stop the agent from running further.</p>"},{"location":"ref/agent/#agents.agent.MCPConfig","title":"MCPConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>class MCPConfig(TypedDict):\n    \"\"\"Configuration for MCP servers.\"\"\"\n\n    convert_schemas_to_strict: NotRequired[bool]\n    \"\"\"If True, we will attempt to convert the MCP schemas to strict-mode schemas. This is a\n    best-effort conversion, so some schemas may not be convertible. Defaults to False.\n    \"\"\"\n\n    failure_error_function: NotRequired[ToolErrorFunction | None]\n    \"\"\"Optional function to convert MCP tool failures into model-visible messages. If explicitly\n    set to None, tool errors will be raised instead. If unset, defaults to\n    default_tool_error_function.\n    \"\"\"\n</code></pre>"},{"location":"ref/agent/#agents.agent.MCPConfig.convert_schemas_to_strict","title":"convert_schemas_to_strict  <code>instance-attribute</code>","text":"<pre><code>convert_schemas_to_strict: NotRequired[bool]\n</code></pre> <p>If True, we will attempt to convert the MCP schemas to strict-mode schemas. This is a best-effort conversion, so some schemas may not be convertible. Defaults to False.</p>"},{"location":"ref/agent/#agents.agent.MCPConfig.failure_error_function","title":"failure_error_function  <code>instance-attribute</code>","text":"<pre><code>failure_error_function: NotRequired[\n    ToolErrorFunction | None\n]\n</code></pre> <p>Optional function to convert MCP tool failures into model-visible messages. If explicitly set to None, tool errors will be raised instead. If unset, defaults to default_tool_error_function.</p>"},{"location":"ref/agent/#agents.agent.AgentBase","title":"AgentBase  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Base class for <code>Agent</code> and <code>RealtimeAgent</code>.</p> Source code in <code>src/agents/agent.py</code> <pre><code>@dataclass\nclass AgentBase(Generic[TContext]):\n    \"\"\"Base class for `Agent` and `RealtimeAgent`.\"\"\"\n\n    name: str\n    \"\"\"The name of the agent.\"\"\"\n\n    handoff_description: str | None = None\n    \"\"\"A description of the agent. This is used when the agent is used as a handoff, so that an\n    LLM knows what it does and when to invoke it.\n    \"\"\"\n\n    tools: list[Tool] = field(default_factory=list)\n    \"\"\"A list of tools that the agent can use.\"\"\"\n\n    mcp_servers: list[MCPServer] = field(default_factory=list)\n    \"\"\"A list of [Model Context Protocol](https://modelcontextprotocol.io/) servers that\n    the agent can use. Every time the agent runs, it will include tools from these servers in the\n    list of available tools.\n\n    NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call\n    `server.connect()` before passing it to the agent, and `server.cleanup()` when the server is no\n    longer needed. Consider using `MCPServerManager` from `agents.mcp` to keep connect/cleanup\n    in the same task.\n    \"\"\"\n\n    mcp_config: MCPConfig = field(default_factory=lambda: MCPConfig())\n    \"\"\"Configuration for MCP servers.\"\"\"\n\n    async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n        \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n        convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n        failure_error_function = self.mcp_config.get(\n            \"failure_error_function\", default_tool_error_function\n        )\n        return await MCPUtil.get_all_function_tools(\n            self.mcp_servers,\n            convert_schemas_to_strict,\n            run_context,\n            self,\n            failure_error_function=failure_error_function,\n        )\n\n    async def get_all_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n        \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n        mcp_tools = await self.get_mcp_tools(run_context)\n\n        async def _check_tool_enabled(tool: Tool) -&gt; bool:\n            if not isinstance(tool, FunctionTool):\n                return True\n\n            attr = tool.is_enabled\n            if isinstance(attr, bool):\n                return attr\n            res = attr(run_context, self)\n            if inspect.isawaitable(res):\n                return bool(await res)\n            return bool(res)\n\n        results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n        enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n        return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/agent/#agents.agent.AgentBase.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the agent.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.handoff_description","title":"handoff_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_description: str | None = None\n</code></pre> <p>A description of the agent. This is used when the agent is used as a handoff, so that an LLM knows what it does and when to invoke it.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.tools","title":"tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools: list[Tool] = field(default_factory=list)\n</code></pre> <p>A list of tools that the agent can use.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.mcp_servers","title":"mcp_servers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_servers: list[MCPServer] = field(default_factory=list)\n</code></pre> <p>A list of Model Context Protocol servers that the agent can use. Every time the agent runs, it will include tools from these servers in the list of available tools.</p> <p>NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call <code>server.connect()</code> before passing it to the agent, and <code>server.cleanup()</code> when the server is no longer needed. Consider using <code>MCPServerManager</code> from <code>agents.mcp</code> to keep connect/cleanup in the same task.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.mcp_config","title":"mcp_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_config: MCPConfig = field(\n    default_factory=lambda: MCPConfig()\n)\n</code></pre> <p>Configuration for MCP servers.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.get_mcp_tools","title":"get_mcp_tools  <code>async</code>","text":"<pre><code>get_mcp_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>Fetches the available tools from the MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n    convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n    failure_error_function = self.mcp_config.get(\n        \"failure_error_function\", default_tool_error_function\n    )\n    return await MCPUtil.get_all_function_tools(\n        self.mcp_servers,\n        convert_schemas_to_strict,\n        run_context,\n        self,\n        failure_error_function=failure_error_function,\n    )\n</code></pre>"},{"location":"ref/agent/#agents.agent.AgentBase.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>All agent tools, including MCP tools and function tools.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_all_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n    mcp_tools = await self.get_mcp_tools(run_context)\n\n    async def _check_tool_enabled(tool: Tool) -&gt; bool:\n        if not isinstance(tool, FunctionTool):\n            return True\n\n        attr = tool.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(run_context, self)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n    enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n    return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent","title":"Agent  <code>dataclass</code>","text":"<p>               Bases: <code>AgentBase</code>, <code>Generic[TContext]</code></p> <p>An agent is an AI model configured with instructions, tools, guardrails, handoffs and more.</p> <p>We strongly recommend passing <code>instructions</code>, which is the \"system prompt\" for the agent. In addition, you can pass <code>handoff_description</code>, which is a human-readable description of the agent, used when the agent is used inside tools/handoffs.</p> <p>Agents are generic on the context type. The context is a (mutable) object you create. It is passed to tool functions, handoffs, guardrails, etc.</p> <p>See <code>AgentBase</code> for base parameters that are shared with <code>RealtimeAgent</code>s.</p> Source code in <code>src/agents/agent.py</code> <pre><code>@dataclass\nclass Agent(AgentBase, Generic[TContext]):\n    \"\"\"An agent is an AI model configured with instructions, tools, guardrails, handoffs and more.\n\n    We strongly recommend passing `instructions`, which is the \"system prompt\" for the agent. In\n    addition, you can pass `handoff_description`, which is a human-readable description of the\n    agent, used when the agent is used inside tools/handoffs.\n\n    Agents are generic on the context type. The context is a (mutable) object you create. It is\n    passed to tool functions, handoffs, guardrails, etc.\n\n    See `AgentBase` for base parameters that are shared with `RealtimeAgent`s.\n    \"\"\"\n\n    instructions: (\n        str\n        | Callable[\n            [RunContextWrapper[TContext], Agent[TContext]],\n            MaybeAwaitable[str],\n        ]\n        | None\n    ) = None\n    \"\"\"The instructions for the agent. Will be used as the \"system prompt\" when this agent is\n    invoked. Describes what the agent should do, and how it responds.\n\n    Can either be a string, or a function that dynamically generates instructions for the agent. If\n    you provide a function, it will be called with the context and the agent instance. It must\n    return a string.\n    \"\"\"\n\n    prompt: Prompt | DynamicPromptFunction | None = None\n    \"\"\"A prompt object (or a function that returns a Prompt). Prompts allow you to dynamically\n    configure the instructions, tools and other config for an agent outside of your code. Only\n    usable with OpenAI models, using the Responses API.\n    \"\"\"\n\n    handoffs: list[Agent[Any] | Handoff[TContext, Any]] = field(default_factory=list)\n    \"\"\"Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs,\n    and the agent can choose to delegate to them if relevant. Allows for separation of concerns and\n    modularity.\n    \"\"\"\n\n    model: str | Model | None = None\n    \"\"\"The model implementation to use when invoking the LLM.\n\n    By default, if not set, the agent will use the default model configured in\n    `agents.models.get_default_model()` (currently \"gpt-4.1\").\n    \"\"\"\n\n    model_settings: ModelSettings = field(default_factory=get_default_model_settings)\n    \"\"\"Configures model-specific tuning parameters (e.g. temperature, top_p).\n    \"\"\"\n\n    input_guardrails: list[InputGuardrail[TContext]] = field(default_factory=list)\n    \"\"\"A list of checks that run in parallel to the agent's execution, before generating a\n    response. Runs only if the agent is the first agent in the chain.\n    \"\"\"\n\n    output_guardrails: list[OutputGuardrail[TContext]] = field(default_factory=list)\n    \"\"\"A list of checks that run on the final output of the agent, after generating a response.\n    Runs only if the agent produces a final output.\n    \"\"\"\n\n    output_type: type[Any] | AgentOutputSchemaBase | None = None\n    \"\"\"The type of the output object. If not provided, the output will be `str`. In most cases,\n    you should pass a regular Python type (e.g. a dataclass, Pydantic model, TypedDict, etc).\n    You can customize this in two ways:\n    1. If you want non-strict schemas, pass `AgentOutputSchema(MyClass, strict_json_schema=False)`.\n    2. If you want to use a custom JSON schema (i.e. without using the SDK's automatic schema)\n       creation, subclass and pass an `AgentOutputSchemaBase` subclass.\n    \"\"\"\n\n    hooks: AgentHooks[TContext] | None = None\n    \"\"\"A class that receives callbacks on various lifecycle events for this agent.\n    \"\"\"\n\n    tool_use_behavior: (\n        Literal[\"run_llm_again\", \"stop_on_first_tool\"] | StopAtTools | ToolsToFinalOutputFunction\n    ) = \"run_llm_again\"\n    \"\"\"\n    This lets you configure how tool use is handled.\n    - \"run_llm_again\": The default behavior. Tools are run, and then the LLM receives the results\n        and gets to respond.\n    - \"stop_on_first_tool\": The output from the first tool call is treated as the final result.\n        In other words, it isn\u2019t sent back to the LLM for further processing but is used directly\n        as the final output.\n    - A StopAtTools object: The agent will stop running if any of the tools listed in\n        `stop_at_tool_names` is called.\n        The final output will be the output of the first matching tool call.\n        The LLM does not process the result of the tool call.\n    - A function: If you pass a function, it will be called with the run context and the list of\n      tool results. It must return a `ToolsToFinalOutputResult`, which determines whether the tool\n      calls result in a final output.\n\n      NOTE: This configuration is specific to FunctionTools. Hosted tools, such as file search,\n      web search, etc. are always processed by the LLM.\n    \"\"\"\n\n    reset_tool_choice: bool = True\n    \"\"\"Whether to reset the tool choice to the default value after a tool has been called. Defaults\n    to True. This ensures that the agent doesn't enter an infinite loop of tool usage.\"\"\"\n\n    def __post_init__(self):\n        from typing import get_origin\n\n        if not isinstance(self.name, str):\n            raise TypeError(f\"Agent name must be a string, got {type(self.name).__name__}\")\n\n        if self.handoff_description is not None and not isinstance(self.handoff_description, str):\n            raise TypeError(\n                f\"Agent handoff_description must be a string or None, \"\n                f\"got {type(self.handoff_description).__name__}\"\n            )\n\n        if not isinstance(self.tools, list):\n            raise TypeError(f\"Agent tools must be a list, got {type(self.tools).__name__}\")\n\n        if not isinstance(self.mcp_servers, list):\n            raise TypeError(\n                f\"Agent mcp_servers must be a list, got {type(self.mcp_servers).__name__}\"\n            )\n\n        if not isinstance(self.mcp_config, dict):\n            raise TypeError(\n                f\"Agent mcp_config must be a dict, got {type(self.mcp_config).__name__}\"\n            )\n\n        if (\n            self.instructions is not None\n            and not isinstance(self.instructions, str)\n            and not callable(self.instructions)\n        ):\n            raise TypeError(\n                f\"Agent instructions must be a string, callable, or None, \"\n                f\"got {type(self.instructions).__name__}\"\n            )\n\n        if (\n            self.prompt is not None\n            and not callable(self.prompt)\n            and not hasattr(self.prompt, \"get\")\n        ):\n            raise TypeError(\n                f\"Agent prompt must be a Prompt, DynamicPromptFunction, or None, \"\n                f\"got {type(self.prompt).__name__}\"\n            )\n\n        if not isinstance(self.handoffs, list):\n            raise TypeError(f\"Agent handoffs must be a list, got {type(self.handoffs).__name__}\")\n\n        if self.model is not None and not isinstance(self.model, str):\n            from .models.interface import Model\n\n            if not isinstance(self.model, Model):\n                raise TypeError(\n                    f\"Agent model must be a string, Model, or None, got {type(self.model).__name__}\"\n                )\n\n        if not isinstance(self.model_settings, ModelSettings):\n            raise TypeError(\n                f\"Agent model_settings must be a ModelSettings instance, \"\n                f\"got {type(self.model_settings).__name__}\"\n            )\n\n        if (\n            # The user sets a non-default model\n            self.model is not None\n            and (\n                # The default model is gpt-5\n                is_gpt_5_default() is True\n                # However, the specified model is not a gpt-5 model\n                and (\n                    isinstance(self.model, str) is False\n                    or gpt_5_reasoning_settings_required(self.model) is False  # type: ignore\n                )\n                # The model settings are not customized for the specified model\n                and self.model_settings == get_default_model_settings()\n            )\n        ):\n            # In this scenario, we should use a generic model settings\n            # because non-gpt-5 models are not compatible with the default gpt-5 model settings.\n            # This is a best-effort attempt to make the agent work with non-gpt-5 models.\n            self.model_settings = ModelSettings()\n\n        if not isinstance(self.input_guardrails, list):\n            raise TypeError(\n                f\"Agent input_guardrails must be a list, got {type(self.input_guardrails).__name__}\"\n            )\n\n        if not isinstance(self.output_guardrails, list):\n            raise TypeError(\n                f\"Agent output_guardrails must be a list, \"\n                f\"got {type(self.output_guardrails).__name__}\"\n            )\n\n        if self.output_type is not None:\n            from .agent_output import AgentOutputSchemaBase\n\n            if not (\n                isinstance(self.output_type, (type, AgentOutputSchemaBase))\n                or get_origin(self.output_type) is not None\n            ):\n                raise TypeError(\n                    f\"Agent output_type must be a type, AgentOutputSchemaBase, or None, \"\n                    f\"got {type(self.output_type).__name__}\"\n                )\n\n        if self.hooks is not None:\n            from .lifecycle import AgentHooksBase\n\n            if not isinstance(self.hooks, AgentHooksBase):\n                raise TypeError(\n                    f\"Agent hooks must be an AgentHooks instance or None, \"\n                    f\"got {type(self.hooks).__name__}\"\n                )\n\n        if (\n            not (\n                isinstance(self.tool_use_behavior, str)\n                and self.tool_use_behavior in [\"run_llm_again\", \"stop_on_first_tool\"]\n            )\n            and not isinstance(self.tool_use_behavior, dict)\n            and not callable(self.tool_use_behavior)\n        ):\n            raise TypeError(\n                f\"Agent tool_use_behavior must be 'run_llm_again', 'stop_on_first_tool', \"\n                f\"StopAtTools dict, or callable, got {type(self.tool_use_behavior).__name__}\"\n            )\n\n        if not isinstance(self.reset_tool_choice, bool):\n            raise TypeError(\n                f\"Agent reset_tool_choice must be a boolean, \"\n                f\"got {type(self.reset_tool_choice).__name__}\"\n            )\n\n    def clone(self, **kwargs: Any) -&gt; Agent[TContext]:\n        \"\"\"Make a copy of the agent, with the given arguments changed.\n        Notes:\n            - Uses `dataclasses.replace`, which performs a **shallow copy**.\n            - Mutable attributes like `tools` and `handoffs` are shallow-copied:\n              new list objects are created only if overridden, but their contents\n              (tool functions and handoff objects) are shared with the original.\n            - To modify these independently, pass new lists when calling `clone()`.\n        Example:\n            ```python\n            new_agent = agent.clone(instructions=\"New instructions\")\n            ```\n        \"\"\"\n        return dataclasses.replace(self, **kwargs)\n\n    def as_tool(\n        self,\n        tool_name: str | None,\n        tool_description: str | None,\n        custom_output_extractor: (\n            Callable[[RunResult | RunResultStreaming], Awaitable[str]] | None\n        ) = None,\n        is_enabled: bool\n        | Callable[[RunContextWrapper[Any], AgentBase[Any]], MaybeAwaitable[bool]] = True,\n        on_stream: Callable[[AgentToolStreamEvent], MaybeAwaitable[None]] | None = None,\n        run_config: RunConfig | None = None,\n        max_turns: int | None = None,\n        hooks: RunHooks[TContext] | None = None,\n        previous_response_id: str | None = None,\n        conversation_id: str | None = None,\n        session: Session | None = None,\n        failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n        needs_approval: bool\n        | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]] = False,\n        parameters: type[Any] | None = None,\n        input_builder: StructuredToolInputBuilder | None = None,\n        include_input_schema: bool = False,\n    ) -&gt; Tool:\n        \"\"\"Transform this agent into a tool, callable by other agents.\n\n        This is different from handoffs in two ways:\n        1. In handoffs, the new agent receives the conversation history. In this tool, the new agent\n           receives generated input.\n        2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is\n           called as a tool, and the conversation is continued by the original agent.\n\n        Args:\n            tool_name: The name of the tool. If not provided, the agent's name will be used.\n            tool_description: The description of the tool, which should indicate what it does and\n                when to use it.\n            custom_output_extractor: A function that extracts the output from the agent. If not\n                provided, the last message from the agent will be used.\n            is_enabled: Whether the tool is enabled. Can be a bool or a callable that takes the run\n                context and agent and returns whether the tool is enabled. Disabled tools are hidden\n                from the LLM at runtime.\n            on_stream: Optional callback (sync or async) to receive streaming events from the nested\n                agent run. The callback receives an `AgentToolStreamEvent` containing the nested\n                agent, the originating tool call (when available), and each stream event. When\n                provided, the nested agent is executed in streaming mode.\n            failure_error_function: If provided, generate an error message when the tool (agent) run\n                fails. The message is sent to the LLM. If None, the exception is raised instead.\n            needs_approval: Bool or callable to decide if this agent tool should pause for approval.\n            parameters: Structured input type for the tool arguments (dataclass or Pydantic model).\n            input_builder: Optional function to build the nested agent input from structured data.\n            include_input_schema: Whether to include the full JSON schema in structured input.\n        \"\"\"\n\n        def _is_supported_parameters(value: Any) -&gt; bool:\n            if not isinstance(value, type):\n                return False\n            if dataclasses.is_dataclass(value):\n                return True\n            return issubclass(value, BaseModel)\n\n        tool_name_resolved = tool_name or _transforms.transform_string_function_style(self.name)\n        tool_description_resolved = tool_description or \"\"\n        has_custom_parameters = parameters is not None\n        include_schema = bool(include_input_schema and has_custom_parameters)\n        should_capture_tool_input = bool(\n            has_custom_parameters or include_schema or input_builder is not None\n        )\n\n        if parameters is None:\n            params_adapter = TypeAdapter(AgentAsToolInput)\n            params_schema = ensure_strict_json_schema(params_adapter.json_schema())\n        else:\n            if not _is_supported_parameters(parameters):\n                raise TypeError(\"Agent tool parameters must be a dataclass or Pydantic model type.\")\n            params_adapter = TypeAdapter(parameters)\n            params_schema = ensure_strict_json_schema(params_adapter.json_schema())\n\n        schema_info = build_structured_input_schema_info(\n            params_schema,\n            include_json_schema=include_schema,\n        )\n\n        def _normalize_tool_input(parsed: Any) -&gt; Any:\n            # Prefer JSON mode so structured params (datetime/UUID/Decimal, etc.) serialize cleanly.\n            try:\n                return params_adapter.dump_python(parsed, mode=\"json\")\n            except Exception as exc:\n                raise ModelBehaviorError(\n                    f\"Failed to serialize structured tool input for {tool_name_resolved}: {exc}\"\n                ) from exc\n\n        async def _run_agent_impl(context: ToolContext, input_json: str) -&gt; Any:\n            from .run import DEFAULT_MAX_TURNS, Runner\n            from .tool_context import ToolContext\n\n            try:\n                json_data = json.loads(input_json) if input_json else {}\n            except Exception as exc:\n                if _debug.DONT_LOG_TOOL_DATA:\n                    logger.debug(f\"Invalid JSON input for tool {tool_name_resolved}\")\n                else:\n                    logger.debug(f\"Invalid JSON input for tool {tool_name_resolved}: {input_json}\")\n                raise ModelBehaviorError(\n                    f\"Invalid JSON input for tool {tool_name_resolved}: {input_json}\"\n                ) from exc\n\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Invoking tool {tool_name_resolved}\")\n            else:\n                logger.debug(f\"Invoking tool {tool_name_resolved} with input {input_json}\")\n\n            try:\n                parsed_params = params_adapter.validate_python(json_data)\n            except ValidationError as exc:\n                raise ModelBehaviorError(\n                    f\"Invalid JSON input for tool {tool_name_resolved}: {exc}\"\n                ) from exc\n\n            params_data = _normalize_tool_input(parsed_params)\n            resolved_input = await resolve_agent_tool_input(\n                params=params_data,\n                schema_info=schema_info if should_capture_tool_input else None,\n                input_builder=input_builder,\n            )\n            if not isinstance(resolved_input, str) and not isinstance(resolved_input, list):\n                raise ModelBehaviorError(\"Agent tool called with invalid input\")\n\n            resolved_max_turns = max_turns if max_turns is not None else DEFAULT_MAX_TURNS\n            if isinstance(context, ToolContext):\n                # Use a fresh ToolContext to avoid sharing approval state with parent runs.\n                nested_context = ToolContext(\n                    context=context.context,\n                    usage=context.usage,\n                    tool_name=context.tool_name,\n                    tool_call_id=context.tool_call_id,\n                    tool_arguments=context.tool_arguments,\n                    tool_call=context.tool_call,\n                )\n                if should_capture_tool_input:\n                    nested_context.tool_input = params_data\n            elif isinstance(context, RunContextWrapper):\n                if should_capture_tool_input:\n                    nested_context = RunContextWrapper(context=context.context)\n                    nested_context.tool_input = params_data\n                else:\n                    nested_context = context.context\n            else:\n                if should_capture_tool_input:\n                    nested_context = RunContextWrapper(context=context)\n                    nested_context.tool_input = params_data\n                else:\n                    nested_context = context\n            run_result: RunResult | RunResultStreaming | None = None\n            resume_state: RunState | None = None\n            should_record_run_result = True\n\n            def _nested_approvals_status(\n                interruptions: list[ToolApprovalItem],\n            ) -&gt; Literal[\"approved\", \"pending\", \"rejected\"]:\n                has_pending = False\n                has_decision = False\n                for interruption in interruptions:\n                    call_id = interruption.call_id\n                    if not call_id:\n                        has_pending = True\n                        continue\n                    status = context.get_approval_status(\n                        interruption.tool_name or \"\", call_id, existing_pending=interruption\n                    )\n                    if status is False:\n                        return \"rejected\"\n                    if status is True:\n                        has_decision = True\n                    if status is None:\n                        has_pending = True\n                if has_decision:\n                    return \"approved\"\n                if has_pending:\n                    return \"pending\"\n                return \"approved\"\n\n            def _apply_nested_approvals(\n                nested_context: RunContextWrapper[Any],\n                parent_context: RunContextWrapper[Any],\n                interruptions: list[ToolApprovalItem],\n            ) -&gt; None:\n                for interruption in interruptions:\n                    call_id = interruption.call_id\n                    if not call_id:\n                        continue\n                    tool_name = RunContextWrapper._resolve_tool_name(interruption)\n                    status = parent_context.get_approval_status(\n                        tool_name, call_id, existing_pending=interruption\n                    )\n                    if status is None:\n                        continue\n                    approval_record = parent_context._approvals.get(tool_name)\n                    if status is True:\n                        always_approve = bool(approval_record and approval_record.approved is True)\n                        nested_context.approve_tool(\n                            interruption,\n                            always_approve=always_approve,\n                        )\n                    else:\n                        always_reject = bool(approval_record and approval_record.rejected is True)\n                        nested_context.reject_tool(\n                            interruption,\n                            always_reject=always_reject,\n                        )\n\n            if isinstance(context, ToolContext) and context.tool_call is not None:\n                pending_run_result = peek_agent_tool_run_result(context.tool_call)\n                if pending_run_result and getattr(pending_run_result, \"interruptions\", None):\n                    status = _nested_approvals_status(pending_run_result.interruptions)\n                    if status == \"pending\":\n                        run_result = pending_run_result\n                        should_record_run_result = False\n                    elif status in (\"approved\", \"rejected\"):\n                        resume_state = pending_run_result.to_state()\n                        if resume_state._context is not None:\n                            # Apply only explicit parent approvals to the nested resumed run.\n                            _apply_nested_approvals(\n                                resume_state._context,\n                                context,\n                                pending_run_result.interruptions,\n                            )\n                        consume_agent_tool_run_result(context.tool_call)\n\n            if run_result is None:\n                if on_stream is not None:\n                    run_result_streaming = Runner.run_streamed(\n                        starting_agent=cast(Agent[Any], self),\n                        input=resume_state or resolved_input,\n                        context=None if resume_state is not None else cast(Any, nested_context),\n                        run_config=run_config,\n                        max_turns=resolved_max_turns,\n                        hooks=hooks,\n                        previous_response_id=None\n                        if resume_state is not None\n                        else previous_response_id,\n                        conversation_id=None if resume_state is not None else conversation_id,\n                        session=session,\n                    )\n                    # Dispatch callbacks in the background so slow handlers do not block\n                    # event consumption.\n                    event_queue: asyncio.Queue[AgentToolStreamEvent | None] = asyncio.Queue()\n\n                    async def _run_handler(payload: AgentToolStreamEvent) -&gt; None:\n                        \"\"\"Execute the user callback while capturing exceptions.\"\"\"\n                        try:\n                            maybe_result = on_stream(payload)\n                            if inspect.isawaitable(maybe_result):\n                                await maybe_result\n                        except Exception:\n                            logger.exception(\n                                \"Error while handling on_stream event for agent tool %s.\",\n                                self.name,\n                            )\n\n                    async def dispatch_stream_events() -&gt; None:\n                        while True:\n                            payload = await event_queue.get()\n                            is_sentinel = payload is None  # None marks the end of the stream.\n                            try:\n                                if payload is not None:\n                                    await _run_handler(payload)\n                            finally:\n                                event_queue.task_done()\n\n                            if is_sentinel:\n                                break\n\n                    dispatch_task = asyncio.create_task(dispatch_stream_events())\n\n                    try:\n                        from .stream_events import AgentUpdatedStreamEvent\n\n                        current_agent = run_result_streaming.current_agent\n                        async for event in run_result_streaming.stream_events():\n                            if isinstance(event, AgentUpdatedStreamEvent):\n                                current_agent = event.new_agent\n\n                            payload: AgentToolStreamEvent = {\n                                \"event\": event,\n                                \"agent\": current_agent,\n                                \"tool_call\": context.tool_call,\n                            }\n                            await event_queue.put(payload)\n                    finally:\n                        await event_queue.put(None)\n                        await event_queue.join()\n                        await dispatch_task\n                    run_result = run_result_streaming\n                else:\n                    run_result = await Runner.run(\n                        starting_agent=cast(Agent[Any], self),\n                        input=resume_state or resolved_input,\n                        context=None if resume_state is not None else cast(Any, nested_context),\n                        run_config=run_config,\n                        max_turns=resolved_max_turns,\n                        hooks=hooks,\n                        previous_response_id=None\n                        if resume_state is not None\n                        else previous_response_id,\n                        conversation_id=None if resume_state is not None else conversation_id,\n                        session=session,\n                    )\n            assert run_result is not None\n\n            # Store the run result by tool call identity so nested interruptions can be read later.\n            interruptions = getattr(run_result, \"interruptions\", None)\n            if isinstance(context, ToolContext) and context.tool_call is not None and interruptions:\n                if should_record_run_result:\n                    record_agent_tool_run_result(context.tool_call, run_result)\n\n            if custom_output_extractor:\n                return await custom_output_extractor(run_result)\n\n            return run_result.final_output\n\n        async def _run_agent_tool(context: ToolContext, input_json: str) -&gt; Any:\n            try:\n                return await _run_agent_impl(context, input_json)\n            except Exception as exc:\n                if failure_error_function is None:\n                    raise\n\n                result = failure_error_function(context, exc)\n                if inspect.isawaitable(result):\n                    result = await result\n\n                json_decode_error = _extract_tool_argument_json_error(exc)\n                if json_decode_error is not None:\n                    span_error_message = \"Error running tool\"\n                    span_error_detail = str(json_decode_error)\n                else:\n                    span_error_message = \"Error running tool (non-fatal)\"\n                    span_error_detail = str(exc)\n\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=span_error_message,\n                        data={\n                            \"tool_name\": tool_name_resolved,\n                            \"error\": span_error_detail,\n                        },\n                    )\n                )\n                if _debug.DONT_LOG_TOOL_DATA:\n                    logger.debug(f\"Tool {tool_name_resolved} failed\")\n                else:\n                    logger.error(\n                        f\"Tool {tool_name_resolved} failed: {input_json} {exc}\",\n                        exc_info=exc,\n                    )\n                return result\n\n        run_agent_tool = FunctionTool(\n            name=tool_name_resolved,\n            description=tool_description_resolved,\n            params_json_schema=params_schema,\n            on_invoke_tool=_run_agent_tool,\n            strict_json_schema=True,\n            is_enabled=is_enabled,\n            needs_approval=needs_approval,\n        )\n        run_agent_tool._is_agent_tool = True\n        run_agent_tool._agent_instance = self\n\n        return run_agent_tool\n\n    async def get_system_prompt(self, run_context: RunContextWrapper[TContext]) -&gt; str | None:\n        if isinstance(self.instructions, str):\n            return self.instructions\n        elif callable(self.instructions):\n            # Inspect the signature of the instructions function\n            sig = inspect.signature(self.instructions)\n            params = list(sig.parameters.values())\n\n            # Enforce exactly 2 parameters\n            if len(params) != 2:\n                raise TypeError(\n                    f\"'instructions' callable must accept exactly 2 arguments (context, agent), \"\n                    f\"but got {len(params)}: {[p.name for p in params]}\"\n                )\n\n            # Call the instructions function properly\n            if inspect.iscoroutinefunction(self.instructions):\n                return await cast(Awaitable[str], self.instructions(run_context, self))\n            else:\n                return cast(str, self.instructions(run_context, self))\n\n        elif self.instructions is not None:\n            logger.error(\n                f\"Instructions must be a string or a callable function, \"\n                f\"got {type(self.instructions).__name__}\"\n            )\n\n        return None\n\n    async def get_prompt(\n        self, run_context: RunContextWrapper[TContext]\n    ) -&gt; ResponsePromptParam | None:\n        \"\"\"Get the prompt for the agent.\"\"\"\n        return await PromptUtil.to_model_input(self.prompt, run_context, self)\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.instructions","title":"instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instructions: (\n    str\n    | Callable[\n        [RunContextWrapper[TContext], Agent[TContext]],\n        MaybeAwaitable[str],\n    ]\n    | None\n) = None\n</code></pre> <p>The instructions for the agent. Will be used as the \"system prompt\" when this agent is invoked. Describes what the agent should do, and how it responds.</p> <p>Can either be a string, or a function that dynamically generates instructions for the agent. If you provide a function, it will be called with the context and the agent instance. It must return a string.</p>"},{"location":"ref/agent/#agents.agent.Agent.prompt","title":"prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt: Prompt | DynamicPromptFunction | None = None\n</code></pre> <p>A prompt object (or a function that returns a Prompt). Prompts allow you to dynamically configure the instructions, tools and other config for an agent outside of your code. Only usable with OpenAI models, using the Responses API.</p>"},{"location":"ref/agent/#agents.agent.Agent.handoffs","title":"handoffs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoffs: list[Agent[Any] | Handoff[TContext, Any]] = field(\n    default_factory=list\n)\n</code></pre> <p>Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs, and the agent can choose to delegate to them if relevant. Allows for separation of concerns and modularity.</p>"},{"location":"ref/agent/#agents.agent.Agent.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str | Model | None = None\n</code></pre> <p>The model implementation to use when invoking the LLM.</p> <p>By default, if not set, the agent will use the default model configured in <code>agents.models.get_default_model()</code> (currently \"gpt-4.1\").</p>"},{"location":"ref/agent/#agents.agent.Agent.model_settings","title":"model_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_settings: ModelSettings = field(\n    default_factory=get_default_model_settings\n)\n</code></pre> <p>Configures model-specific tuning parameters (e.g. temperature, top_p).</p>"},{"location":"ref/agent/#agents.agent.Agent.input_guardrails","title":"input_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_guardrails: list[InputGuardrail[TContext]] = field(\n    default_factory=list\n)\n</code></pre> <p>A list of checks that run in parallel to the agent's execution, before generating a response. Runs only if the agent is the first agent in the chain.</p>"},{"location":"ref/agent/#agents.agent.Agent.output_guardrails","title":"output_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrails: list[OutputGuardrail[TContext]] = field(\n    default_factory=list\n)\n</code></pre> <p>A list of checks that run on the final output of the agent, after generating a response. Runs only if the agent produces a final output.</p>"},{"location":"ref/agent/#agents.agent.Agent.output_type","title":"output_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_type: type[Any] | AgentOutputSchemaBase | None = None\n</code></pre> <p>The type of the output object. If not provided, the output will be <code>str</code>. In most cases, you should pass a regular Python type (e.g. a dataclass, Pydantic model, TypedDict, etc). You can customize this in two ways: 1. If you want non-strict schemas, pass <code>AgentOutputSchema(MyClass, strict_json_schema=False)</code>. 2. If you want to use a custom JSON schema (i.e. without using the SDK's automatic schema)    creation, subclass and pass an <code>AgentOutputSchemaBase</code> subclass.</p>"},{"location":"ref/agent/#agents.agent.Agent.hooks","title":"hooks  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hooks: AgentHooks[TContext] | None = None\n</code></pre> <p>A class that receives callbacks on various lifecycle events for this agent.</p>"},{"location":"ref/agent/#agents.agent.Agent.tool_use_behavior","title":"tool_use_behavior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_use_behavior: (\n    Literal[\"run_llm_again\", \"stop_on_first_tool\"]\n    | StopAtTools\n    | ToolsToFinalOutputFunction\n) = \"run_llm_again\"\n</code></pre> <p>This lets you configure how tool use is handled. - \"run_llm_again\": The default behavior. Tools are run, and then the LLM receives the results     and gets to respond. - \"stop_on_first_tool\": The output from the first tool call is treated as the final result.     In other words, it isn\u2019t sent back to the LLM for further processing but is used directly     as the final output. - A StopAtTools object: The agent will stop running if any of the tools listed in     <code>stop_at_tool_names</code> is called.     The final output will be the output of the first matching tool call.     The LLM does not process the result of the tool call. - A function: If you pass a function, it will be called with the run context and the list of   tool results. It must return a <code>ToolsToFinalOutputResult</code>, which determines whether the tool   calls result in a final output.</p> <p>NOTE: This configuration is specific to FunctionTools. Hosted tools, such as file search,   web search, etc. are always processed by the LLM.</p>"},{"location":"ref/agent/#agents.agent.Agent.reset_tool_choice","title":"reset_tool_choice  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reset_tool_choice: bool = True\n</code></pre> <p>Whether to reset the tool choice to the default value after a tool has been called. Defaults to True. This ensures that the agent doesn't enter an infinite loop of tool usage.</p>"},{"location":"ref/agent/#agents.agent.Agent.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the agent.</p>"},{"location":"ref/agent/#agents.agent.Agent.handoff_description","title":"handoff_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_description: str | None = None\n</code></pre> <p>A description of the agent. This is used when the agent is used as a handoff, so that an LLM knows what it does and when to invoke it.</p>"},{"location":"ref/agent/#agents.agent.Agent.tools","title":"tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools: list[Tool] = field(default_factory=list)\n</code></pre> <p>A list of tools that the agent can use.</p>"},{"location":"ref/agent/#agents.agent.Agent.mcp_servers","title":"mcp_servers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_servers: list[MCPServer] = field(default_factory=list)\n</code></pre> <p>A list of Model Context Protocol servers that the agent can use. Every time the agent runs, it will include tools from these servers in the list of available tools.</p> <p>NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call <code>server.connect()</code> before passing it to the agent, and <code>server.cleanup()</code> when the server is no longer needed. Consider using <code>MCPServerManager</code> from <code>agents.mcp</code> to keep connect/cleanup in the same task.</p>"},{"location":"ref/agent/#agents.agent.Agent.mcp_config","title":"mcp_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_config: MCPConfig = field(\n    default_factory=lambda: MCPConfig()\n)\n</code></pre> <p>Configuration for MCP servers.</p>"},{"location":"ref/agent/#agents.agent.Agent.clone","title":"clone","text":"<pre><code>clone(**kwargs: Any) -&gt; Agent[TContext]\n</code></pre> <p>Make a copy of the agent, with the given arguments changed. Notes:     - Uses <code>dataclasses.replace</code>, which performs a shallow copy.     - Mutable attributes like <code>tools</code> and <code>handoffs</code> are shallow-copied:       new list objects are created only if overridden, but their contents       (tool functions and handoff objects) are shared with the original.     - To modify these independently, pass new lists when calling <code>clone()</code>. Example:     <pre><code>new_agent = agent.clone(instructions=\"New instructions\")\n</code></pre></p> Source code in <code>src/agents/agent.py</code> <pre><code>def clone(self, **kwargs: Any) -&gt; Agent[TContext]:\n    \"\"\"Make a copy of the agent, with the given arguments changed.\n    Notes:\n        - Uses `dataclasses.replace`, which performs a **shallow copy**.\n        - Mutable attributes like `tools` and `handoffs` are shallow-copied:\n          new list objects are created only if overridden, but their contents\n          (tool functions and handoff objects) are shared with the original.\n        - To modify these independently, pass new lists when calling `clone()`.\n    Example:\n        ```python\n        new_agent = agent.clone(instructions=\"New instructions\")\n        ```\n    \"\"\"\n    return dataclasses.replace(self, **kwargs)\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.as_tool","title":"as_tool","text":"<pre><code>as_tool(\n    tool_name: str | None,\n    tool_description: str | None,\n    custom_output_extractor: Callable[\n        [RunResult | RunResultStreaming], Awaitable[str]\n    ]\n    | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n    on_stream: Callable[\n        [AgentToolStreamEvent], MaybeAwaitable[None]\n    ]\n    | None = None,\n    run_config: RunConfig | None = None,\n    max_turns: int | None = None,\n    hooks: RunHooks[TContext] | None = None,\n    previous_response_id: str | None = None,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n    failure_error_function: ToolErrorFunction\n    | None = default_tool_error_function,\n    needs_approval: bool\n    | Callable[\n        [RunContextWrapper[Any], dict[str, Any], str],\n        Awaitable[bool],\n    ] = False,\n    parameters: type[Any] | None = None,\n    input_builder: StructuredToolInputBuilder | None = None,\n    include_input_schema: bool = False,\n) -&gt; Tool\n</code></pre> <p>Transform this agent into a tool, callable by other agents.</p> <p>This is different from handoffs in two ways: 1. In handoffs, the new agent receives the conversation history. In this tool, the new agent    receives generated input. 2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is    called as a tool, and the conversation is continued by the original agent.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str | None</code> <p>The name of the tool. If not provided, the agent's name will be used.</p> required <code>tool_description</code> <code>str | None</code> <p>The description of the tool, which should indicate what it does and when to use it.</p> required <code>custom_output_extractor</code> <code>Callable[[RunResult | RunResultStreaming], Awaitable[str]] | None</code> <p>A function that extracts the output from the agent. If not provided, the last message from the agent will be used.</p> <code>None</code> <code>is_enabled</code> <code>bool | Callable[[RunContextWrapper[Any], AgentBase[Any]], MaybeAwaitable[bool]]</code> <p>Whether the tool is enabled. Can be a bool or a callable that takes the run context and agent and returns whether the tool is enabled. Disabled tools are hidden from the LLM at runtime.</p> <code>True</code> <code>on_stream</code> <code>Callable[[AgentToolStreamEvent], MaybeAwaitable[None]] | None</code> <p>Optional callback (sync or async) to receive streaming events from the nested agent run. The callback receives an <code>AgentToolStreamEvent</code> containing the nested agent, the originating tool call (when available), and each stream event. When provided, the nested agent is executed in streaming mode.</p> <code>None</code> <code>failure_error_function</code> <code>ToolErrorFunction | None</code> <p>If provided, generate an error message when the tool (agent) run fails. The message is sent to the LLM. If None, the exception is raised instead.</p> <code>default_tool_error_function</code> <code>needs_approval</code> <code>bool | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]]</code> <p>Bool or callable to decide if this agent tool should pause for approval.</p> <code>False</code> <code>parameters</code> <code>type[Any] | None</code> <p>Structured input type for the tool arguments (dataclass or Pydantic model).</p> <code>None</code> <code>input_builder</code> <code>StructuredToolInputBuilder | None</code> <p>Optional function to build the nested agent input from structured data.</p> <code>None</code> <code>include_input_schema</code> <code>bool</code> <p>Whether to include the full JSON schema in structured input.</p> <code>False</code> Source code in <code>src/agents/agent.py</code> <pre><code>def as_tool(\n    self,\n    tool_name: str | None,\n    tool_description: str | None,\n    custom_output_extractor: (\n        Callable[[RunResult | RunResultStreaming], Awaitable[str]] | None\n    ) = None,\n    is_enabled: bool\n    | Callable[[RunContextWrapper[Any], AgentBase[Any]], MaybeAwaitable[bool]] = True,\n    on_stream: Callable[[AgentToolStreamEvent], MaybeAwaitable[None]] | None = None,\n    run_config: RunConfig | None = None,\n    max_turns: int | None = None,\n    hooks: RunHooks[TContext] | None = None,\n    previous_response_id: str | None = None,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n    failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n    needs_approval: bool\n    | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]] = False,\n    parameters: type[Any] | None = None,\n    input_builder: StructuredToolInputBuilder | None = None,\n    include_input_schema: bool = False,\n) -&gt; Tool:\n    \"\"\"Transform this agent into a tool, callable by other agents.\n\n    This is different from handoffs in two ways:\n    1. In handoffs, the new agent receives the conversation history. In this tool, the new agent\n       receives generated input.\n    2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is\n       called as a tool, and the conversation is continued by the original agent.\n\n    Args:\n        tool_name: The name of the tool. If not provided, the agent's name will be used.\n        tool_description: The description of the tool, which should indicate what it does and\n            when to use it.\n        custom_output_extractor: A function that extracts the output from the agent. If not\n            provided, the last message from the agent will be used.\n        is_enabled: Whether the tool is enabled. Can be a bool or a callable that takes the run\n            context and agent and returns whether the tool is enabled. Disabled tools are hidden\n            from the LLM at runtime.\n        on_stream: Optional callback (sync or async) to receive streaming events from the nested\n            agent run. The callback receives an `AgentToolStreamEvent` containing the nested\n            agent, the originating tool call (when available), and each stream event. When\n            provided, the nested agent is executed in streaming mode.\n        failure_error_function: If provided, generate an error message when the tool (agent) run\n            fails. The message is sent to the LLM. If None, the exception is raised instead.\n        needs_approval: Bool or callable to decide if this agent tool should pause for approval.\n        parameters: Structured input type for the tool arguments (dataclass or Pydantic model).\n        input_builder: Optional function to build the nested agent input from structured data.\n        include_input_schema: Whether to include the full JSON schema in structured input.\n    \"\"\"\n\n    def _is_supported_parameters(value: Any) -&gt; bool:\n        if not isinstance(value, type):\n            return False\n        if dataclasses.is_dataclass(value):\n            return True\n        return issubclass(value, BaseModel)\n\n    tool_name_resolved = tool_name or _transforms.transform_string_function_style(self.name)\n    tool_description_resolved = tool_description or \"\"\n    has_custom_parameters = parameters is not None\n    include_schema = bool(include_input_schema and has_custom_parameters)\n    should_capture_tool_input = bool(\n        has_custom_parameters or include_schema or input_builder is not None\n    )\n\n    if parameters is None:\n        params_adapter = TypeAdapter(AgentAsToolInput)\n        params_schema = ensure_strict_json_schema(params_adapter.json_schema())\n    else:\n        if not _is_supported_parameters(parameters):\n            raise TypeError(\"Agent tool parameters must be a dataclass or Pydantic model type.\")\n        params_adapter = TypeAdapter(parameters)\n        params_schema = ensure_strict_json_schema(params_adapter.json_schema())\n\n    schema_info = build_structured_input_schema_info(\n        params_schema,\n        include_json_schema=include_schema,\n    )\n\n    def _normalize_tool_input(parsed: Any) -&gt; Any:\n        # Prefer JSON mode so structured params (datetime/UUID/Decimal, etc.) serialize cleanly.\n        try:\n            return params_adapter.dump_python(parsed, mode=\"json\")\n        except Exception as exc:\n            raise ModelBehaviorError(\n                f\"Failed to serialize structured tool input for {tool_name_resolved}: {exc}\"\n            ) from exc\n\n    async def _run_agent_impl(context: ToolContext, input_json: str) -&gt; Any:\n        from .run import DEFAULT_MAX_TURNS, Runner\n        from .tool_context import ToolContext\n\n        try:\n            json_data = json.loads(input_json) if input_json else {}\n        except Exception as exc:\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Invalid JSON input for tool {tool_name_resolved}\")\n            else:\n                logger.debug(f\"Invalid JSON input for tool {tool_name_resolved}: {input_json}\")\n            raise ModelBehaviorError(\n                f\"Invalid JSON input for tool {tool_name_resolved}: {input_json}\"\n            ) from exc\n\n        if _debug.DONT_LOG_TOOL_DATA:\n            logger.debug(f\"Invoking tool {tool_name_resolved}\")\n        else:\n            logger.debug(f\"Invoking tool {tool_name_resolved} with input {input_json}\")\n\n        try:\n            parsed_params = params_adapter.validate_python(json_data)\n        except ValidationError as exc:\n            raise ModelBehaviorError(\n                f\"Invalid JSON input for tool {tool_name_resolved}: {exc}\"\n            ) from exc\n\n        params_data = _normalize_tool_input(parsed_params)\n        resolved_input = await resolve_agent_tool_input(\n            params=params_data,\n            schema_info=schema_info if should_capture_tool_input else None,\n            input_builder=input_builder,\n        )\n        if not isinstance(resolved_input, str) and not isinstance(resolved_input, list):\n            raise ModelBehaviorError(\"Agent tool called with invalid input\")\n\n        resolved_max_turns = max_turns if max_turns is not None else DEFAULT_MAX_TURNS\n        if isinstance(context, ToolContext):\n            # Use a fresh ToolContext to avoid sharing approval state with parent runs.\n            nested_context = ToolContext(\n                context=context.context,\n                usage=context.usage,\n                tool_name=context.tool_name,\n                tool_call_id=context.tool_call_id,\n                tool_arguments=context.tool_arguments,\n                tool_call=context.tool_call,\n            )\n            if should_capture_tool_input:\n                nested_context.tool_input = params_data\n        elif isinstance(context, RunContextWrapper):\n            if should_capture_tool_input:\n                nested_context = RunContextWrapper(context=context.context)\n                nested_context.tool_input = params_data\n            else:\n                nested_context = context.context\n        else:\n            if should_capture_tool_input:\n                nested_context = RunContextWrapper(context=context)\n                nested_context.tool_input = params_data\n            else:\n                nested_context = context\n        run_result: RunResult | RunResultStreaming | None = None\n        resume_state: RunState | None = None\n        should_record_run_result = True\n\n        def _nested_approvals_status(\n            interruptions: list[ToolApprovalItem],\n        ) -&gt; Literal[\"approved\", \"pending\", \"rejected\"]:\n            has_pending = False\n            has_decision = False\n            for interruption in interruptions:\n                call_id = interruption.call_id\n                if not call_id:\n                    has_pending = True\n                    continue\n                status = context.get_approval_status(\n                    interruption.tool_name or \"\", call_id, existing_pending=interruption\n                )\n                if status is False:\n                    return \"rejected\"\n                if status is True:\n                    has_decision = True\n                if status is None:\n                    has_pending = True\n            if has_decision:\n                return \"approved\"\n            if has_pending:\n                return \"pending\"\n            return \"approved\"\n\n        def _apply_nested_approvals(\n            nested_context: RunContextWrapper[Any],\n            parent_context: RunContextWrapper[Any],\n            interruptions: list[ToolApprovalItem],\n        ) -&gt; None:\n            for interruption in interruptions:\n                call_id = interruption.call_id\n                if not call_id:\n                    continue\n                tool_name = RunContextWrapper._resolve_tool_name(interruption)\n                status = parent_context.get_approval_status(\n                    tool_name, call_id, existing_pending=interruption\n                )\n                if status is None:\n                    continue\n                approval_record = parent_context._approvals.get(tool_name)\n                if status is True:\n                    always_approve = bool(approval_record and approval_record.approved is True)\n                    nested_context.approve_tool(\n                        interruption,\n                        always_approve=always_approve,\n                    )\n                else:\n                    always_reject = bool(approval_record and approval_record.rejected is True)\n                    nested_context.reject_tool(\n                        interruption,\n                        always_reject=always_reject,\n                    )\n\n        if isinstance(context, ToolContext) and context.tool_call is not None:\n            pending_run_result = peek_agent_tool_run_result(context.tool_call)\n            if pending_run_result and getattr(pending_run_result, \"interruptions\", None):\n                status = _nested_approvals_status(pending_run_result.interruptions)\n                if status == \"pending\":\n                    run_result = pending_run_result\n                    should_record_run_result = False\n                elif status in (\"approved\", \"rejected\"):\n                    resume_state = pending_run_result.to_state()\n                    if resume_state._context is not None:\n                        # Apply only explicit parent approvals to the nested resumed run.\n                        _apply_nested_approvals(\n                            resume_state._context,\n                            context,\n                            pending_run_result.interruptions,\n                        )\n                    consume_agent_tool_run_result(context.tool_call)\n\n        if run_result is None:\n            if on_stream is not None:\n                run_result_streaming = Runner.run_streamed(\n                    starting_agent=cast(Agent[Any], self),\n                    input=resume_state or resolved_input,\n                    context=None if resume_state is not None else cast(Any, nested_context),\n                    run_config=run_config,\n                    max_turns=resolved_max_turns,\n                    hooks=hooks,\n                    previous_response_id=None\n                    if resume_state is not None\n                    else previous_response_id,\n                    conversation_id=None if resume_state is not None else conversation_id,\n                    session=session,\n                )\n                # Dispatch callbacks in the background so slow handlers do not block\n                # event consumption.\n                event_queue: asyncio.Queue[AgentToolStreamEvent | None] = asyncio.Queue()\n\n                async def _run_handler(payload: AgentToolStreamEvent) -&gt; None:\n                    \"\"\"Execute the user callback while capturing exceptions.\"\"\"\n                    try:\n                        maybe_result = on_stream(payload)\n                        if inspect.isawaitable(maybe_result):\n                            await maybe_result\n                    except Exception:\n                        logger.exception(\n                            \"Error while handling on_stream event for agent tool %s.\",\n                            self.name,\n                        )\n\n                async def dispatch_stream_events() -&gt; None:\n                    while True:\n                        payload = await event_queue.get()\n                        is_sentinel = payload is None  # None marks the end of the stream.\n                        try:\n                            if payload is not None:\n                                await _run_handler(payload)\n                        finally:\n                            event_queue.task_done()\n\n                        if is_sentinel:\n                            break\n\n                dispatch_task = asyncio.create_task(dispatch_stream_events())\n\n                try:\n                    from .stream_events import AgentUpdatedStreamEvent\n\n                    current_agent = run_result_streaming.current_agent\n                    async for event in run_result_streaming.stream_events():\n                        if isinstance(event, AgentUpdatedStreamEvent):\n                            current_agent = event.new_agent\n\n                        payload: AgentToolStreamEvent = {\n                            \"event\": event,\n                            \"agent\": current_agent,\n                            \"tool_call\": context.tool_call,\n                        }\n                        await event_queue.put(payload)\n                finally:\n                    await event_queue.put(None)\n                    await event_queue.join()\n                    await dispatch_task\n                run_result = run_result_streaming\n            else:\n                run_result = await Runner.run(\n                    starting_agent=cast(Agent[Any], self),\n                    input=resume_state or resolved_input,\n                    context=None if resume_state is not None else cast(Any, nested_context),\n                    run_config=run_config,\n                    max_turns=resolved_max_turns,\n                    hooks=hooks,\n                    previous_response_id=None\n                    if resume_state is not None\n                    else previous_response_id,\n                    conversation_id=None if resume_state is not None else conversation_id,\n                    session=session,\n                )\n        assert run_result is not None\n\n        # Store the run result by tool call identity so nested interruptions can be read later.\n        interruptions = getattr(run_result, \"interruptions\", None)\n        if isinstance(context, ToolContext) and context.tool_call is not None and interruptions:\n            if should_record_run_result:\n                record_agent_tool_run_result(context.tool_call, run_result)\n\n        if custom_output_extractor:\n            return await custom_output_extractor(run_result)\n\n        return run_result.final_output\n\n    async def _run_agent_tool(context: ToolContext, input_json: str) -&gt; Any:\n        try:\n            return await _run_agent_impl(context, input_json)\n        except Exception as exc:\n            if failure_error_function is None:\n                raise\n\n            result = failure_error_function(context, exc)\n            if inspect.isawaitable(result):\n                result = await result\n\n            json_decode_error = _extract_tool_argument_json_error(exc)\n            if json_decode_error is not None:\n                span_error_message = \"Error running tool\"\n                span_error_detail = str(json_decode_error)\n            else:\n                span_error_message = \"Error running tool (non-fatal)\"\n                span_error_detail = str(exc)\n\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=span_error_message,\n                    data={\n                        \"tool_name\": tool_name_resolved,\n                        \"error\": span_error_detail,\n                    },\n                )\n            )\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Tool {tool_name_resolved} failed\")\n            else:\n                logger.error(\n                    f\"Tool {tool_name_resolved} failed: {input_json} {exc}\",\n                    exc_info=exc,\n                )\n            return result\n\n    run_agent_tool = FunctionTool(\n        name=tool_name_resolved,\n        description=tool_description_resolved,\n        params_json_schema=params_schema,\n        on_invoke_tool=_run_agent_tool,\n        strict_json_schema=True,\n        is_enabled=is_enabled,\n        needs_approval=needs_approval,\n    )\n    run_agent_tool._is_agent_tool = True\n    run_agent_tool._agent_instance = self\n\n    return run_agent_tool\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    run_context: RunContextWrapper[TContext],\n) -&gt; ResponsePromptParam | None\n</code></pre> <p>Get the prompt for the agent.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_prompt(\n    self, run_context: RunContextWrapper[TContext]\n) -&gt; ResponsePromptParam | None:\n    \"\"\"Get the prompt for the agent.\"\"\"\n    return await PromptUtil.to_model_input(self.prompt, run_context, self)\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.get_mcp_tools","title":"get_mcp_tools  <code>async</code>","text":"<pre><code>get_mcp_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>Fetches the available tools from the MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n    convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n    failure_error_function = self.mcp_config.get(\n        \"failure_error_function\", default_tool_error_function\n    )\n    return await MCPUtil.get_all_function_tools(\n        self.mcp_servers,\n        convert_schemas_to_strict,\n        run_context,\n        self,\n        failure_error_function=failure_error_function,\n    )\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>All agent tools, including MCP tools and function tools.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_all_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n    mcp_tools = await self.get_mcp_tools(run_context)\n\n    async def _check_tool_enabled(tool: Tool) -&gt; bool:\n        if not isinstance(tool, FunctionTool):\n            return True\n\n        attr = tool.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(run_context, self)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n    enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n    return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/agent_output/","title":"<code>Agent output</code>","text":""},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase","title":"AgentOutputSchemaBase","text":"<p>               Bases: <code>ABC</code></p> <p>An object that captures the JSON schema of the output, as well as validating/parsing JSON produced by the LLM into the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>class AgentOutputSchemaBase(abc.ABC):\n    \"\"\"An object that captures the JSON schema of the output, as well as validating/parsing JSON\n    produced by the LLM into the output type.\n    \"\"\"\n\n    @abc.abstractmethod\n    def is_plain_text(self) -&gt; bool:\n        \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"The name of the output type.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def json_schema(self) -&gt; dict[str, Any]:\n        \"\"\"Returns the JSON schema of the output. Will only be called if the output type is not\n        plain text.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def is_strict_json_schema(self) -&gt; bool:\n        \"\"\"Whether the JSON schema is in strict mode. Strict mode constrains the JSON schema\n        features, but guarantees valid JSON. See here for details:\n        https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def validate_json(self, json_str: str) -&gt; Any:\n        \"\"\"Validate a JSON string against the output type. You must return the validated object,\n        or raise a `ModelBehaviorError` if the JSON is invalid.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.is_plain_text","title":"is_plain_text  <code>abstractmethod</code>","text":"<pre><code>is_plain_text() -&gt; bool\n</code></pre> <p>Whether the output type is plain text (versus a JSON object).</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef is_plain_text(self) -&gt; bool:\n    \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.name","title":"name  <code>abstractmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>The name of the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef name(self) -&gt; str:\n    \"\"\"The name of the output type.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.json_schema","title":"json_schema  <code>abstractmethod</code>","text":"<pre><code>json_schema() -&gt; dict[str, Any]\n</code></pre> <p>Returns the JSON schema of the output. Will only be called if the output type is not plain text.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef json_schema(self) -&gt; dict[str, Any]:\n    \"\"\"Returns the JSON schema of the output. Will only be called if the output type is not\n    plain text.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.is_strict_json_schema","title":"is_strict_json_schema  <code>abstractmethod</code>","text":"<pre><code>is_strict_json_schema() -&gt; bool\n</code></pre> <p>Whether the JSON schema is in strict mode. Strict mode constrains the JSON schema features, but guarantees valid JSON. See here for details: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef is_strict_json_schema(self) -&gt; bool:\n    \"\"\"Whether the JSON schema is in strict mode. Strict mode constrains the JSON schema\n    features, but guarantees valid JSON. See here for details:\n    https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.validate_json","title":"validate_json  <code>abstractmethod</code>","text":"<pre><code>validate_json(json_str: str) -&gt; Any\n</code></pre> <p>Validate a JSON string against the output type. You must return the validated object, or raise a <code>ModelBehaviorError</code> if the JSON is invalid.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef validate_json(self, json_str: str) -&gt; Any:\n    \"\"\"Validate a JSON string against the output type. You must return the validated object,\n    or raise a `ModelBehaviorError` if the JSON is invalid.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema","title":"AgentOutputSchema  <code>dataclass</code>","text":"<p>               Bases: <code>AgentOutputSchemaBase</code></p> <p>An object that captures the JSON schema of the output, as well as validating/parsing JSON produced by the LLM into the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@dataclass(init=False)\nclass AgentOutputSchema(AgentOutputSchemaBase):\n    \"\"\"An object that captures the JSON schema of the output, as well as validating/parsing JSON\n    produced by the LLM into the output type.\n    \"\"\"\n\n    output_type: type[Any]\n    \"\"\"The type of the output.\"\"\"\n\n    _type_adapter: TypeAdapter[Any]\n    \"\"\"A type adapter that wraps the output type, so that we can validate JSON.\"\"\"\n\n    _is_wrapped: bool\n    \"\"\"Whether the output type is wrapped in a dictionary. This is generally done if the base\n    output type cannot be represented as a JSON Schema object.\n    \"\"\"\n\n    _output_schema: dict[str, Any]\n    \"\"\"The JSON schema of the output.\"\"\"\n\n    _strict_json_schema: bool\n    \"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\n    as it increases the likelihood of correct JSON input.\n    \"\"\"\n\n    def __init__(self, output_type: type[Any], strict_json_schema: bool = True):\n        \"\"\"\n        Args:\n            output_type: The type of the output.\n            strict_json_schema: Whether the JSON schema is in strict mode. We **strongly** recommend\n                setting this to True, as it increases the likelihood of correct JSON input.\n        \"\"\"\n        self.output_type = output_type\n        self._strict_json_schema = strict_json_schema\n\n        if output_type is None or output_type is str:\n            self._is_wrapped = False\n            self._type_adapter = TypeAdapter(output_type)\n            self._output_schema = self._type_adapter.json_schema()\n            return\n\n        # We should wrap for things that are not plain text, and for things that would definitely\n        # not be a JSON Schema object.\n        self._is_wrapped = not _is_subclass_of_base_model_or_dict(output_type)\n\n        if self._is_wrapped:\n            OutputType = TypedDict(\n                \"OutputType\",\n                {\n                    _WRAPPER_DICT_KEY: output_type,  # type: ignore\n                },\n            )\n            self._type_adapter = TypeAdapter(OutputType)\n            self._output_schema = self._type_adapter.json_schema()\n        else:\n            self._type_adapter = TypeAdapter(output_type)\n            self._output_schema = self._type_adapter.json_schema()\n\n        if self._strict_json_schema:\n            try:\n                self._output_schema = ensure_strict_json_schema(self._output_schema)\n            except UserError as e:\n                raise UserError(\n                    \"Strict JSON schema is enabled, but the output type is not valid. \"\n                    \"Either make the output type strict, \"\n                    \"or wrap your type with AgentOutputSchema(YourType, strict_json_schema=False)\"\n                ) from e\n\n    def is_plain_text(self) -&gt; bool:\n        \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n        return self.output_type is None or self.output_type is str\n\n    def is_strict_json_schema(self) -&gt; bool:\n        \"\"\"Whether the JSON schema is in strict mode.\"\"\"\n        return self._strict_json_schema\n\n    def json_schema(self) -&gt; dict[str, Any]:\n        \"\"\"The JSON schema of the output type.\"\"\"\n        if self.is_plain_text():\n            raise UserError(\"Output type is plain text, so no JSON schema is available\")\n        return self._output_schema\n\n    def validate_json(self, json_str: str) -&gt; Any:\n        \"\"\"Validate a JSON string against the output type. Returns the validated object, or raises\n        a `ModelBehaviorError` if the JSON is invalid.\n        \"\"\"\n        validated = _json.validate_json(json_str, self._type_adapter, partial=False)\n        if self._is_wrapped:\n            if not isinstance(validated, dict):\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Invalid JSON\",\n                        data={\"details\": f\"Expected a dict, got {type(validated)}\"},\n                    )\n                )\n                raise ModelBehaviorError(\n                    f\"Expected a dict, got {type(validated)} for JSON: {json_str}\"\n                )\n\n            if _WRAPPER_DICT_KEY not in validated:\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Invalid JSON\",\n                        data={\"details\": f\"Could not find key {_WRAPPER_DICT_KEY} in JSON\"},\n                    )\n                )\n                raise ModelBehaviorError(\n                    f\"Could not find key {_WRAPPER_DICT_KEY} in JSON: {json_str}\"\n                )\n            return validated[_WRAPPER_DICT_KEY]\n        return validated\n\n    def name(self) -&gt; str:\n        \"\"\"The name of the output type.\"\"\"\n        return _type_to_str(self.output_type)\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.output_type","title":"output_type  <code>instance-attribute</code>","text":"<pre><code>output_type: type[Any] = output_type\n</code></pre> <p>The type of the output.</p>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.__init__","title":"__init__","text":"<pre><code>__init__(\n    output_type: type[Any], strict_json_schema: bool = True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>type[Any]</code> <p>The type of the output.</p> required <code>strict_json_schema</code> <code>bool</code> <p>Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.</p> <code>True</code> Source code in <code>src/agents/agent_output.py</code> <pre><code>def __init__(self, output_type: type[Any], strict_json_schema: bool = True):\n    \"\"\"\n    Args:\n        output_type: The type of the output.\n        strict_json_schema: Whether the JSON schema is in strict mode. We **strongly** recommend\n            setting this to True, as it increases the likelihood of correct JSON input.\n    \"\"\"\n    self.output_type = output_type\n    self._strict_json_schema = strict_json_schema\n\n    if output_type is None or output_type is str:\n        self._is_wrapped = False\n        self._type_adapter = TypeAdapter(output_type)\n        self._output_schema = self._type_adapter.json_schema()\n        return\n\n    # We should wrap for things that are not plain text, and for things that would definitely\n    # not be a JSON Schema object.\n    self._is_wrapped = not _is_subclass_of_base_model_or_dict(output_type)\n\n    if self._is_wrapped:\n        OutputType = TypedDict(\n            \"OutputType\",\n            {\n                _WRAPPER_DICT_KEY: output_type,  # type: ignore\n            },\n        )\n        self._type_adapter = TypeAdapter(OutputType)\n        self._output_schema = self._type_adapter.json_schema()\n    else:\n        self._type_adapter = TypeAdapter(output_type)\n        self._output_schema = self._type_adapter.json_schema()\n\n    if self._strict_json_schema:\n        try:\n            self._output_schema = ensure_strict_json_schema(self._output_schema)\n        except UserError as e:\n            raise UserError(\n                \"Strict JSON schema is enabled, but the output type is not valid. \"\n                \"Either make the output type strict, \"\n                \"or wrap your type with AgentOutputSchema(YourType, strict_json_schema=False)\"\n            ) from e\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.is_plain_text","title":"is_plain_text","text":"<pre><code>is_plain_text() -&gt; bool\n</code></pre> <p>Whether the output type is plain text (versus a JSON object).</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def is_plain_text(self) -&gt; bool:\n    \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n    return self.output_type is None or self.output_type is str\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.is_strict_json_schema","title":"is_strict_json_schema","text":"<pre><code>is_strict_json_schema() -&gt; bool\n</code></pre> <p>Whether the JSON schema is in strict mode.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def is_strict_json_schema(self) -&gt; bool:\n    \"\"\"Whether the JSON schema is in strict mode.\"\"\"\n    return self._strict_json_schema\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.json_schema","title":"json_schema","text":"<pre><code>json_schema() -&gt; dict[str, Any]\n</code></pre> <p>The JSON schema of the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def json_schema(self) -&gt; dict[str, Any]:\n    \"\"\"The JSON schema of the output type.\"\"\"\n    if self.is_plain_text():\n        raise UserError(\"Output type is plain text, so no JSON schema is available\")\n    return self._output_schema\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.validate_json","title":"validate_json","text":"<pre><code>validate_json(json_str: str) -&gt; Any\n</code></pre> <p>Validate a JSON string against the output type. Returns the validated object, or raises a <code>ModelBehaviorError</code> if the JSON is invalid.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def validate_json(self, json_str: str) -&gt; Any:\n    \"\"\"Validate a JSON string against the output type. Returns the validated object, or raises\n    a `ModelBehaviorError` if the JSON is invalid.\n    \"\"\"\n    validated = _json.validate_json(json_str, self._type_adapter, partial=False)\n    if self._is_wrapped:\n        if not isinstance(validated, dict):\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Invalid JSON\",\n                    data={\"details\": f\"Expected a dict, got {type(validated)}\"},\n                )\n            )\n            raise ModelBehaviorError(\n                f\"Expected a dict, got {type(validated)} for JSON: {json_str}\"\n            )\n\n        if _WRAPPER_DICT_KEY not in validated:\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Invalid JSON\",\n                    data={\"details\": f\"Could not find key {_WRAPPER_DICT_KEY} in JSON\"},\n                )\n            )\n            raise ModelBehaviorError(\n                f\"Could not find key {_WRAPPER_DICT_KEY} in JSON: {json_str}\"\n            )\n        return validated[_WRAPPER_DICT_KEY]\n    return validated\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.name","title":"name","text":"<pre><code>name() -&gt; str\n</code></pre> <p>The name of the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def name(self) -&gt; str:\n    \"\"\"The name of the output type.\"\"\"\n    return _type_to_str(self.output_type)\n</code></pre>"},{"location":"ref/agent_tool_input/","title":"<code>Agent Tool Input</code>","text":""},{"location":"ref/agent_tool_input/#agents.agent_tool_input.AgentAsToolInput","title":"AgentAsToolInput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Default input schema for agent-as-tool calls.</p> Source code in <code>src/agents/agent_tool_input.py</code> <pre><code>class AgentAsToolInput(BaseModel):\n    \"\"\"Default input schema for agent-as-tool calls.\"\"\"\n\n    input: str\n</code></pre>"},{"location":"ref/agent_tool_input/#agents.agent_tool_input.StructuredInputSchemaInfo","title":"StructuredInputSchemaInfo  <code>dataclass</code>","text":"<p>Optional schema details used to build structured tool input.</p> Source code in <code>src/agents/agent_tool_input.py</code> <pre><code>@dataclass(frozen=True)\nclass StructuredInputSchemaInfo:\n    \"\"\"Optional schema details used to build structured tool input.\"\"\"\n\n    summary: str | None = None\n    json_schema: dict[str, Any] | None = None\n</code></pre>"},{"location":"ref/agent_tool_input/#agents.agent_tool_input.StructuredToolInputBuilderOptions","title":"StructuredToolInputBuilderOptions","text":"<p>               Bases: <code>TypedDict</code></p> <p>Options passed to structured tool input builders.</p> Source code in <code>src/agents/agent_tool_input.py</code> <pre><code>class StructuredToolInputBuilderOptions(TypedDict, total=False):\n    \"\"\"Options passed to structured tool input builders.\"\"\"\n\n    params: Any\n    summary: str | None\n    json_schema: dict[str, Any] | None\n</code></pre>"},{"location":"ref/agent_tool_input/#agents.agent_tool_input.default_tool_input_builder","title":"default_tool_input_builder","text":"<pre><code>default_tool_input_builder(\n    options: StructuredToolInputBuilderOptions,\n) -&gt; str\n</code></pre> <p>Build a default message for structured agent tool input.</p> Source code in <code>src/agents/agent_tool_input.py</code> <pre><code>def default_tool_input_builder(options: StructuredToolInputBuilderOptions) -&gt; str:\n    \"\"\"Build a default message for structured agent tool input.\"\"\"\n    sections: list[str] = [STRUCTURED_INPUT_PREAMBLE]\n\n    sections.append(\"## Structured Input Data:\")\n    sections.append(\"\")\n    sections.append(\"```\")\n    sections.append(json.dumps(options.get(\"params\"), indent=2) or \"null\")\n    sections.append(\"```\")\n    sections.append(\"\")\n\n    json_schema = options.get(\"json_schema\")\n    if json_schema is not None:\n        sections.append(\"## Input JSON Schema:\")\n        sections.append(\"\")\n        sections.append(\"```\")\n        sections.append(json.dumps(json_schema, indent=2))\n        sections.append(\"```\")\n        sections.append(\"\")\n    else:\n        summary = options.get(\"summary\")\n        if summary:\n            sections.append(\"## Input Schema Summary:\")\n            sections.append(summary)\n            sections.append(\"\")\n\n    return \"\\n\".join(sections)\n</code></pre>"},{"location":"ref/agent_tool_input/#agents.agent_tool_input.resolve_agent_tool_input","title":"resolve_agent_tool_input  <code>async</code>","text":"<pre><code>resolve_agent_tool_input(\n    *,\n    params: Any,\n    schema_info: StructuredInputSchemaInfo | None = None,\n    input_builder: StructuredToolInputBuilder | None = None,\n) -&gt; str | list[TResponseInputItem]\n</code></pre> <p>Resolve structured tool input into a string or list of input items.</p> Source code in <code>src/agents/agent_tool_input.py</code> <pre><code>async def resolve_agent_tool_input(\n    *,\n    params: Any,\n    schema_info: StructuredInputSchemaInfo | None = None,\n    input_builder: StructuredToolInputBuilder | None = None,\n) -&gt; str | list[TResponseInputItem]:\n    \"\"\"Resolve structured tool input into a string or list of input items.\"\"\"\n    should_build_structured_input = bool(\n        input_builder or (schema_info and (schema_info.summary or schema_info.json_schema))\n    )\n    if should_build_structured_input:\n        builder = input_builder or default_tool_input_builder\n        result = builder(\n            {\n                \"params\": params,\n                \"summary\": schema_info.summary if schema_info else None,\n                \"json_schema\": schema_info.json_schema if schema_info else None,\n            }\n        )\n        if inspect.isawaitable(result):\n            result = await result\n        if isinstance(result, str) or isinstance(result, list):\n            return result\n        return cast(StructuredToolInputResult, result)\n\n    if is_agent_tool_input(params) and _has_only_input_field(params):\n        return cast(str, params[\"input\"])\n\n    return json.dumps(params)\n</code></pre>"},{"location":"ref/agent_tool_input/#agents.agent_tool_input.build_structured_input_schema_info","title":"build_structured_input_schema_info","text":"<pre><code>build_structured_input_schema_info(\n    params_schema: dict[str, Any] | None,\n    *,\n    include_json_schema: bool,\n) -&gt; StructuredInputSchemaInfo\n</code></pre> <p>Build schema details used for structured input rendering.</p> Source code in <code>src/agents/agent_tool_input.py</code> <pre><code>def build_structured_input_schema_info(\n    params_schema: dict[str, Any] | None,\n    *,\n    include_json_schema: bool,\n) -&gt; StructuredInputSchemaInfo:\n    \"\"\"Build schema details used for structured input rendering.\"\"\"\n    if not params_schema:\n        return StructuredInputSchemaInfo()\n    summary = _build_schema_summary(params_schema)\n    json_schema = params_schema if include_json_schema else None\n    return StructuredInputSchemaInfo(summary=summary, json_schema=json_schema)\n</code></pre>"},{"location":"ref/agent_tool_input/#agents.agent_tool_input.is_agent_tool_input","title":"is_agent_tool_input","text":"<pre><code>is_agent_tool_input(value: Any) -&gt; bool\n</code></pre> <p>Return True if the value looks like the default agent tool input.</p> Source code in <code>src/agents/agent_tool_input.py</code> <pre><code>def is_agent_tool_input(value: Any) -&gt; bool:\n    \"\"\"Return True if the value looks like the default agent tool input.\"\"\"\n    return isinstance(value, dict) and isinstance(value.get(\"input\"), str)\n</code></pre>"},{"location":"ref/agent_tool_state/","title":"<code>Agent Tool State</code>","text":""},{"location":"ref/agent_tool_state/#agents.agent_tool_state.record_agent_tool_run_result","title":"record_agent_tool_run_result","text":"<pre><code>record_agent_tool_run_result(\n    tool_call: ResponseFunctionToolCall,\n    run_result: RunResult | RunResultStreaming,\n) -&gt; None\n</code></pre> <p>Store the nested agent run result by tool call identity.</p> Source code in <code>src/agents/agent_tool_state.py</code> <pre><code>def record_agent_tool_run_result(\n    tool_call: ResponseFunctionToolCall, run_result: RunResult | RunResultStreaming\n) -&gt; None:\n    \"\"\"Store the nested agent run result by tool call identity.\"\"\"\n    tool_call_obj_id = id(tool_call)\n    _agent_tool_run_results_by_obj[tool_call_obj_id] = run_result\n    _index_agent_tool_run_result(tool_call, tool_call_obj_id)\n    _register_tool_call_ref(tool_call, tool_call_obj_id)\n</code></pre>"},{"location":"ref/agent_tool_state/#agents.agent_tool_state.consume_agent_tool_run_result","title":"consume_agent_tool_run_result","text":"<pre><code>consume_agent_tool_run_result(\n    tool_call: ResponseFunctionToolCall,\n) -&gt; RunResult | RunResultStreaming | None\n</code></pre> <p>Return and drop the stored nested agent run result for the given tool call.</p> Source code in <code>src/agents/agent_tool_state.py</code> <pre><code>def consume_agent_tool_run_result(\n    tool_call: ResponseFunctionToolCall,\n) -&gt; RunResult | RunResultStreaming | None:\n    \"\"\"Return and drop the stored nested agent run result for the given tool call.\"\"\"\n    obj_id = id(tool_call)\n    run_result = _agent_tool_run_results_by_obj.pop(obj_id, None)\n    if run_result is not None:\n        _drop_agent_tool_run_result(obj_id)\n        return run_result\n\n    signature = _tool_call_signature(tool_call)\n    candidate_ids = _agent_tool_run_results_by_signature.get(signature)\n    if not candidate_ids:\n        return None\n    if len(candidate_ids) != 1:\n        return None\n\n    candidate_id = next(iter(candidate_ids))\n    _agent_tool_run_results_by_signature.pop(signature, None)\n    _agent_tool_run_result_signature_by_obj.pop(candidate_id, None)\n    _agent_tool_call_refs_by_obj.pop(candidate_id, None)\n    return _agent_tool_run_results_by_obj.pop(candidate_id, None)\n</code></pre>"},{"location":"ref/agent_tool_state/#agents.agent_tool_state.peek_agent_tool_run_result","title":"peek_agent_tool_run_result","text":"<pre><code>peek_agent_tool_run_result(\n    tool_call: ResponseFunctionToolCall,\n) -&gt; RunResult | RunResultStreaming | None\n</code></pre> <p>Return the stored nested agent run result without removing it.</p> Source code in <code>src/agents/agent_tool_state.py</code> <pre><code>def peek_agent_tool_run_result(\n    tool_call: ResponseFunctionToolCall,\n) -&gt; RunResult | RunResultStreaming | None:\n    \"\"\"Return the stored nested agent run result without removing it.\"\"\"\n    obj_id = id(tool_call)\n    run_result = _agent_tool_run_results_by_obj.get(obj_id)\n    if run_result is not None:\n        return run_result\n\n    signature = _tool_call_signature(tool_call)\n    candidate_ids = _agent_tool_run_results_by_signature.get(signature)\n    if not candidate_ids:\n        return None\n    if len(candidate_ids) != 1:\n        return None\n\n    candidate_id = next(iter(candidate_ids))\n    return _agent_tool_run_results_by_obj.get(candidate_id)\n</code></pre>"},{"location":"ref/agent_tool_state/#agents.agent_tool_state.drop_agent_tool_run_result","title":"drop_agent_tool_run_result","text":"<pre><code>drop_agent_tool_run_result(\n    tool_call: ResponseFunctionToolCall,\n) -&gt; None\n</code></pre> <p>Drop the stored nested agent run result, if present.</p> Source code in <code>src/agents/agent_tool_state.py</code> <pre><code>def drop_agent_tool_run_result(tool_call: ResponseFunctionToolCall) -&gt; None:\n    \"\"\"Drop the stored nested agent run result, if present.\"\"\"\n    obj_id = id(tool_call)\n    run_result = _agent_tool_run_results_by_obj.pop(obj_id, None)\n    if run_result is not None:\n        _drop_agent_tool_run_result(obj_id)\n        return\n\n    signature = _tool_call_signature(tool_call)\n    candidate_ids = _agent_tool_run_results_by_signature.get(signature)\n    if not candidate_ids:\n        return\n    if len(candidate_ids) != 1:\n        return\n\n    candidate_id = next(iter(candidate_ids))\n    _agent_tool_run_results_by_signature.pop(signature, None)\n    _agent_tool_run_result_signature_by_obj.pop(candidate_id, None)\n    _agent_tool_call_refs_by_obj.pop(candidate_id, None)\n    _agent_tool_run_results_by_obj.pop(candidate_id, None)\n</code></pre>"},{"location":"ref/apply_diff/","title":"<code>Apply Diff</code>","text":"<p>Utility for applying V4A diffs against text inputs.</p>"},{"location":"ref/apply_diff/#agents.apply_diff.apply_diff","title":"apply_diff","text":"<pre><code>apply_diff(\n    input: str, diff: str, mode: ApplyDiffMode = \"default\"\n) -&gt; str\n</code></pre> <p>Apply a V4A diff to the provided text.</p> <p>This parser understands both the create-file syntax (only \"+\" prefixed lines) and the default update syntax that includes context hunks.</p> Source code in <code>src/agents/apply_diff.py</code> <pre><code>def apply_diff(input: str, diff: str, mode: ApplyDiffMode = \"default\") -&gt; str:\n    \"\"\"Apply a V4A diff to the provided text.\n\n    This parser understands both the create-file syntax (only \"+\" prefixed\n    lines) and the default update syntax that includes context hunks.\n    \"\"\"\n    newline = _detect_newline(input, diff, mode)\n    diff_lines = _normalize_diff_lines(diff)\n    if mode == \"create\":\n        return _parse_create_diff(diff_lines, newline=newline)\n\n    normalized_input = _normalize_text_newlines(input)\n    parsed = _parse_update_diff(diff_lines, normalized_input)\n    return _apply_chunks(normalized_input, parsed.chunks, newline=newline)\n</code></pre>"},{"location":"ref/computer/","title":"<code>Computer</code>","text":""},{"location":"ref/computer/#agents.computer.Computer","title":"Computer","text":"<p>               Bases: <code>ABC</code></p> <p>A computer implemented with sync operations. The Computer interface abstracts the operations needed to control a computer or browser.</p> Source code in <code>src/agents/computer.py</code> <pre><code>class Computer(abc.ABC):\n    \"\"\"A computer implemented with sync operations. The Computer interface abstracts the\n    operations needed to control a computer or browser.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def environment(self) -&gt; Environment:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def dimensions(self) -&gt; tuple[int, int]:\n        pass\n\n    @abc.abstractmethod\n    def screenshot(self) -&gt; str:\n        pass\n\n    @abc.abstractmethod\n    def click(self, x: int, y: int, button: Button) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def double_click(self, x: int, y: int) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def scroll(self, x: int, y: int, scroll_x: int, scroll_y: int) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def type(self, text: str) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def wait(self) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def move(self, x: int, y: int) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def keypress(self, keys: list[str]) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def drag(self, path: list[tuple[int, int]]) -&gt; None:\n        pass\n</code></pre>"},{"location":"ref/computer/#agents.computer.AsyncComputer","title":"AsyncComputer","text":"<p>               Bases: <code>ABC</code></p> <p>A computer implemented with async operations. The Computer interface abstracts the operations needed to control a computer or browser.</p> Source code in <code>src/agents/computer.py</code> <pre><code>class AsyncComputer(abc.ABC):\n    \"\"\"A computer implemented with async operations. The Computer interface abstracts the\n    operations needed to control a computer or browser.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def environment(self) -&gt; Environment:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def dimensions(self) -&gt; tuple[int, int]:\n        pass\n\n    @abc.abstractmethod\n    async def screenshot(self) -&gt; str:\n        pass\n\n    @abc.abstractmethod\n    async def click(self, x: int, y: int, button: Button) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    async def double_click(self, x: int, y: int) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    async def scroll(self, x: int, y: int, scroll_x: int, scroll_y: int) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    async def type(self, text: str) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    async def wait(self) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    async def move(self, x: int, y: int) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    async def keypress(self, keys: list[str]) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    async def drag(self, path: list[tuple[int, int]]) -&gt; None:\n        pass\n</code></pre>"},{"location":"ref/editor/","title":"<code>Editor</code>","text":""},{"location":"ref/editor/#agents.editor.ApplyPatchOperation","title":"ApplyPatchOperation  <code>dataclass</code>","text":"<p>Represents a single apply_patch editor operation requested by the model.</p> Source code in <code>src/agents/editor.py</code> <pre><code>@dataclass(**_DATACLASS_KWARGS)\nclass ApplyPatchOperation:\n    \"\"\"Represents a single apply_patch editor operation requested by the model.\"\"\"\n\n    type: ApplyPatchOperationType\n    path: str\n    diff: str | None = None\n    ctx_wrapper: RunContextWrapper | None = None\n</code></pre>"},{"location":"ref/editor/#agents.editor.ApplyPatchResult","title":"ApplyPatchResult  <code>dataclass</code>","text":"<p>Optional metadata returned by editor operations.</p> Source code in <code>src/agents/editor.py</code> <pre><code>@dataclass(**_DATACLASS_KWARGS)\nclass ApplyPatchResult:\n    \"\"\"Optional metadata returned by editor operations.\"\"\"\n\n    status: Literal[\"completed\", \"failed\"] | None = None\n    output: str | None = None\n</code></pre>"},{"location":"ref/editor/#agents.editor.ApplyPatchEditor","title":"ApplyPatchEditor","text":"<p>               Bases: <code>Protocol</code></p> <p>Host-defined editor that applies diffs on disk.</p> Source code in <code>src/agents/editor.py</code> <pre><code>@runtime_checkable\nclass ApplyPatchEditor(Protocol):\n    \"\"\"Host-defined editor that applies diffs on disk.\"\"\"\n\n    def create_file(\n        self, operation: ApplyPatchOperation\n    ) -&gt; MaybeAwaitable[ApplyPatchResult | str | None]: ...\n\n    def update_file(\n        self, operation: ApplyPatchOperation\n    ) -&gt; MaybeAwaitable[ApplyPatchResult | str | None]: ...\n\n    def delete_file(\n        self, operation: ApplyPatchOperation\n    ) -&gt; MaybeAwaitable[ApplyPatchResult | str | None]: ...\n</code></pre>"},{"location":"ref/exceptions/","title":"<code>Exceptions</code>","text":""},{"location":"ref/exceptions/#agents.exceptions.RunErrorDetails","title":"RunErrorDetails  <code>dataclass</code>","text":"<p>Data collected from an agent run when an exception occurs.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>@dataclass\nclass RunErrorDetails:\n    \"\"\"Data collected from an agent run when an exception occurs.\"\"\"\n\n    input: str | list[TResponseInputItem]\n    new_items: list[RunItem]\n    raw_responses: list[ModelResponse]\n    last_agent: Agent[Any]\n    context_wrapper: RunContextWrapper[Any]\n    input_guardrail_results: list[InputGuardrailResult]\n    output_guardrail_results: list[OutputGuardrailResult]\n\n    def __str__(self) -&gt; str:\n        return pretty_print_run_error_details(self)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.AgentsException","title":"AgentsException","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all exceptions in the Agents SDK.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class AgentsException(Exception):\n    \"\"\"Base class for all exceptions in the Agents SDK.\"\"\"\n\n    run_data: RunErrorDetails | None\n\n    def __init__(self, *args: object) -&gt; None:\n        super().__init__(*args)\n        self.run_data = None\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.MaxTurnsExceeded","title":"MaxTurnsExceeded","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the maximum number of turns is exceeded.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class MaxTurnsExceeded(AgentsException):\n    \"\"\"Exception raised when the maximum number of turns is exceeded.\"\"\"\n\n    message: str\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(message)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.ModelBehaviorError","title":"ModelBehaviorError","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the model does something unexpected, e.g. calling a tool that doesn't exist, or providing malformed JSON.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class ModelBehaviorError(AgentsException):\n    \"\"\"Exception raised when the model does something unexpected, e.g. calling a tool that doesn't\n    exist, or providing malformed JSON.\n    \"\"\"\n\n    message: str\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(message)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.UserError","title":"UserError","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the user makes an error using the SDK.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class UserError(AgentsException):\n    \"\"\"Exception raised when the user makes an error using the SDK.\"\"\"\n\n    message: str\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(message)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.InputGuardrailTripwireTriggered","title":"InputGuardrailTripwireTriggered","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when a guardrail tripwire is triggered.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class InputGuardrailTripwireTriggered(AgentsException):\n    \"\"\"Exception raised when a guardrail tripwire is triggered.\"\"\"\n\n    guardrail_result: InputGuardrailResult\n    \"\"\"The result data of the guardrail that was triggered.\"\"\"\n\n    def __init__(self, guardrail_result: InputGuardrailResult):\n        self.guardrail_result = guardrail_result\n        super().__init__(\n            f\"Guardrail {guardrail_result.guardrail.__class__.__name__} triggered tripwire\"\n        )\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.InputGuardrailTripwireTriggered.guardrail_result","title":"guardrail_result  <code>instance-attribute</code>","text":"<pre><code>guardrail_result: InputGuardrailResult = guardrail_result\n</code></pre> <p>The result data of the guardrail that was triggered.</p>"},{"location":"ref/exceptions/#agents.exceptions.OutputGuardrailTripwireTriggered","title":"OutputGuardrailTripwireTriggered","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when a guardrail tripwire is triggered.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class OutputGuardrailTripwireTriggered(AgentsException):\n    \"\"\"Exception raised when a guardrail tripwire is triggered.\"\"\"\n\n    guardrail_result: OutputGuardrailResult\n    \"\"\"The result data of the guardrail that was triggered.\"\"\"\n\n    def __init__(self, guardrail_result: OutputGuardrailResult):\n        self.guardrail_result = guardrail_result\n        super().__init__(\n            f\"Guardrail {guardrail_result.guardrail.__class__.__name__} triggered tripwire\"\n        )\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.OutputGuardrailTripwireTriggered.guardrail_result","title":"guardrail_result  <code>instance-attribute</code>","text":"<pre><code>guardrail_result: OutputGuardrailResult = guardrail_result\n</code></pre> <p>The result data of the guardrail that was triggered.</p>"},{"location":"ref/exceptions/#agents.exceptions.ToolInputGuardrailTripwireTriggered","title":"ToolInputGuardrailTripwireTriggered","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when a tool input guardrail tripwire is triggered.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class ToolInputGuardrailTripwireTriggered(AgentsException):\n    \"\"\"Exception raised when a tool input guardrail tripwire is triggered.\"\"\"\n\n    guardrail: ToolInputGuardrail[Any]\n    \"\"\"The guardrail that was triggered.\"\"\"\n\n    output: ToolGuardrailFunctionOutput\n    \"\"\"The output from the guardrail function.\"\"\"\n\n    def __init__(self, guardrail: ToolInputGuardrail[Any], output: ToolGuardrailFunctionOutput):\n        self.guardrail = guardrail\n        self.output = output\n        super().__init__(f\"Tool input guardrail {guardrail.__class__.__name__} triggered tripwire\")\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.ToolInputGuardrailTripwireTriggered.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: ToolInputGuardrail[Any] = guardrail\n</code></pre> <p>The guardrail that was triggered.</p>"},{"location":"ref/exceptions/#agents.exceptions.ToolInputGuardrailTripwireTriggered.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: ToolGuardrailFunctionOutput = output\n</code></pre> <p>The output from the guardrail function.</p>"},{"location":"ref/exceptions/#agents.exceptions.ToolOutputGuardrailTripwireTriggered","title":"ToolOutputGuardrailTripwireTriggered","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when a tool output guardrail tripwire is triggered.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class ToolOutputGuardrailTripwireTriggered(AgentsException):\n    \"\"\"Exception raised when a tool output guardrail tripwire is triggered.\"\"\"\n\n    guardrail: ToolOutputGuardrail[Any]\n    \"\"\"The guardrail that was triggered.\"\"\"\n\n    output: ToolGuardrailFunctionOutput\n    \"\"\"The output from the guardrail function.\"\"\"\n\n    def __init__(self, guardrail: ToolOutputGuardrail[Any], output: ToolGuardrailFunctionOutput):\n        self.guardrail = guardrail\n        self.output = output\n        super().__init__(f\"Tool output guardrail {guardrail.__class__.__name__} triggered tripwire\")\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.ToolOutputGuardrailTripwireTriggered.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: ToolOutputGuardrail[Any] = guardrail\n</code></pre> <p>The guardrail that was triggered.</p>"},{"location":"ref/exceptions/#agents.exceptions.ToolOutputGuardrailTripwireTriggered.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: ToolGuardrailFunctionOutput = output\n</code></pre> <p>The output from the guardrail function.</p>"},{"location":"ref/function_schema/","title":"<code>Function schema</code>","text":""},{"location":"ref/function_schema/#agents.function_schema.FuncSchema","title":"FuncSchema  <code>dataclass</code>","text":"<p>Captures the schema for a python function, in preparation for sending it to an LLM as a tool.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>@dataclass\nclass FuncSchema:\n    \"\"\"\n    Captures the schema for a python function, in preparation for sending it to an LLM as a tool.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the function.\"\"\"\n    description: str | None\n    \"\"\"The description of the function.\"\"\"\n    params_pydantic_model: type[BaseModel]\n    \"\"\"A Pydantic model that represents the function's parameters.\"\"\"\n    params_json_schema: dict[str, Any]\n    \"\"\"The JSON schema for the function's parameters, derived from the Pydantic model.\"\"\"\n    signature: inspect.Signature\n    \"\"\"The signature of the function.\"\"\"\n    takes_context: bool = False\n    \"\"\"Whether the function takes a RunContextWrapper argument (must be the first argument).\"\"\"\n    strict_json_schema: bool = True\n    \"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\n    as it increases the likelihood of correct JSON input.\"\"\"\n\n    def to_call_args(self, data: BaseModel) -&gt; tuple[list[Any], dict[str, Any]]:\n        \"\"\"\n        Converts validated data from the Pydantic model into (args, kwargs), suitable for calling\n        the original function.\n        \"\"\"\n        positional_args: list[Any] = []\n        keyword_args: dict[str, Any] = {}\n        seen_var_positional = False\n\n        # Use enumerate() so we can skip the first parameter if it's context.\n        for idx, (name, param) in enumerate(self.signature.parameters.items()):\n            # If the function takes a RunContextWrapper and this is the first parameter, skip it.\n            if self.takes_context and idx == 0:\n                continue\n\n            value = getattr(data, name, None)\n            if param.kind == param.VAR_POSITIONAL:\n                # e.g. *args: extend positional args and mark that *args is now seen\n                positional_args.extend(value or [])\n                seen_var_positional = True\n            elif param.kind == param.VAR_KEYWORD:\n                # e.g. **kwargs handling\n                keyword_args.update(value or {})\n            elif param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n                # Before *args, add to positional args. After *args, add to keyword args.\n                if not seen_var_positional:\n                    positional_args.append(value)\n                else:\n                    keyword_args[name] = value\n            else:\n                # For KEYWORD_ONLY parameters, always use keyword args.\n                keyword_args[name] = value\n        return positional_args, keyword_args\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str | None\n</code></pre> <p>The description of the function.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.params_pydantic_model","title":"params_pydantic_model  <code>instance-attribute</code>","text":"<pre><code>params_pydantic_model: type[BaseModel]\n</code></pre> <p>A Pydantic model that represents the function's parameters.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.params_json_schema","title":"params_json_schema  <code>instance-attribute</code>","text":"<pre><code>params_json_schema: dict[str, Any]\n</code></pre> <p>The JSON schema for the function's parameters, derived from the Pydantic model.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.signature","title":"signature  <code>instance-attribute</code>","text":"<pre><code>signature: Signature\n</code></pre> <p>The signature of the function.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.takes_context","title":"takes_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>takes_context: bool = False\n</code></pre> <p>Whether the function takes a RunContextWrapper argument (must be the first argument).</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.strict_json_schema","title":"strict_json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strict_json_schema: bool = True\n</code></pre> <p>Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.to_call_args","title":"to_call_args","text":"<pre><code>to_call_args(\n    data: BaseModel,\n) -&gt; tuple[list[Any], dict[str, Any]]\n</code></pre> <p>Converts validated data from the Pydantic model into (args, kwargs), suitable for calling the original function.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>def to_call_args(self, data: BaseModel) -&gt; tuple[list[Any], dict[str, Any]]:\n    \"\"\"\n    Converts validated data from the Pydantic model into (args, kwargs), suitable for calling\n    the original function.\n    \"\"\"\n    positional_args: list[Any] = []\n    keyword_args: dict[str, Any] = {}\n    seen_var_positional = False\n\n    # Use enumerate() so we can skip the first parameter if it's context.\n    for idx, (name, param) in enumerate(self.signature.parameters.items()):\n        # If the function takes a RunContextWrapper and this is the first parameter, skip it.\n        if self.takes_context and idx == 0:\n            continue\n\n        value = getattr(data, name, None)\n        if param.kind == param.VAR_POSITIONAL:\n            # e.g. *args: extend positional args and mark that *args is now seen\n            positional_args.extend(value or [])\n            seen_var_positional = True\n        elif param.kind == param.VAR_KEYWORD:\n            # e.g. **kwargs handling\n            keyword_args.update(value or {})\n        elif param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n            # Before *args, add to positional args. After *args, add to keyword args.\n            if not seen_var_positional:\n                positional_args.append(value)\n            else:\n                keyword_args[name] = value\n        else:\n            # For KEYWORD_ONLY parameters, always use keyword args.\n            keyword_args[name] = value\n    return positional_args, keyword_args\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation","title":"FuncDocumentation  <code>dataclass</code>","text":"<p>Contains metadata about a Python function, extracted from its docstring.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>@dataclass\nclass FuncDocumentation:\n    \"\"\"Contains metadata about a Python function, extracted from its docstring.\"\"\"\n\n    name: str\n    \"\"\"The name of the function, via `__name__`.\"\"\"\n    description: str | None\n    \"\"\"The description of the function, derived from the docstring.\"\"\"\n    param_descriptions: dict[str, str] | None\n    \"\"\"The parameter descriptions of the function, derived from the docstring.\"\"\"\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function, via <code>__name__</code>.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str | None\n</code></pre> <p>The description of the function, derived from the docstring.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation.param_descriptions","title":"param_descriptions  <code>instance-attribute</code>","text":"<pre><code>param_descriptions: dict[str, str] | None\n</code></pre> <p>The parameter descriptions of the function, derived from the docstring.</p>"},{"location":"ref/function_schema/#agents.function_schema.generate_func_documentation","title":"generate_func_documentation","text":"<pre><code>generate_func_documentation(\n    func: Callable[..., Any],\n    style: DocstringStyle | None = None,\n) -&gt; FuncDocumentation\n</code></pre> <p>Extracts metadata from a function docstring, in preparation for sending it to an LLM as a tool.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to extract documentation from.</p> required <code>style</code> <code>DocstringStyle | None</code> <p>The style of the docstring to use for parsing. If not provided, we will attempt to auto-detect the style.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncDocumentation</code> <p>A FuncDocumentation object containing the function's name, description, and parameter</p> <code>FuncDocumentation</code> <p>descriptions.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>def generate_func_documentation(\n    func: Callable[..., Any], style: DocstringStyle | None = None\n) -&gt; FuncDocumentation:\n    \"\"\"\n    Extracts metadata from a function docstring, in preparation for sending it to an LLM as a tool.\n\n    Args:\n        func: The function to extract documentation from.\n        style: The style of the docstring to use for parsing. If not provided, we will attempt to\n            auto-detect the style.\n\n    Returns:\n        A FuncDocumentation object containing the function's name, description, and parameter\n        descriptions.\n    \"\"\"\n    name = func.__name__\n    doc = inspect.getdoc(func)\n    if not doc:\n        return FuncDocumentation(name=name, description=None, param_descriptions=None)\n\n    with _suppress_griffe_logging():\n        docstring = Docstring(doc, lineno=1, parser=style or _detect_docstring_style(doc))\n        parsed = docstring.parse()\n\n    description: str | None = next(\n        (section.value for section in parsed if section.kind == DocstringSectionKind.text), None\n    )\n\n    param_descriptions: dict[str, str] = {\n        param.name: param.description\n        for section in parsed\n        if section.kind == DocstringSectionKind.parameters\n        for param in section.value\n    }\n\n    return FuncDocumentation(\n        name=func.__name__,\n        description=description,\n        param_descriptions=param_descriptions or None,\n    )\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.function_schema","title":"function_schema","text":"<pre><code>function_schema(\n    func: Callable[..., Any],\n    docstring_style: DocstringStyle | None = None,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    use_docstring_info: bool = True,\n    strict_json_schema: bool = True,\n) -&gt; FuncSchema\n</code></pre> <p>Given a Python function, extracts a <code>FuncSchema</code> from it, capturing the name, description, parameter descriptions, and other metadata.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to extract the schema from.</p> required <code>docstring_style</code> <code>DocstringStyle | None</code> <p>The style of the docstring to use for parsing. If not provided, we will attempt to auto-detect the style.</p> <code>None</code> <code>name_override</code> <code>str | None</code> <p>If provided, use this name instead of the function's <code>__name__</code>.</p> <code>None</code> <code>description_override</code> <code>str | None</code> <p>If provided, use this description instead of the one derived from the docstring.</p> <code>None</code> <code>use_docstring_info</code> <code>bool</code> <p>If True, uses the docstring to generate the description and parameter descriptions.</p> <code>True</code> <code>strict_json_schema</code> <code>bool</code> <p>Whether the JSON schema is in strict mode. If True, we'll ensure that the schema adheres to the \"strict\" standard the OpenAI API expects. We strongly recommend setting this to True, as it increases the likelihood of the LLM producing correct JSON input.</p> <code>True</code> <p>Returns:</p> Type Description <code>FuncSchema</code> <p>A <code>FuncSchema</code> object containing the function's name, description, parameter descriptions,</p> <code>FuncSchema</code> <p>and other metadata.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>def function_schema(\n    func: Callable[..., Any],\n    docstring_style: DocstringStyle | None = None,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    use_docstring_info: bool = True,\n    strict_json_schema: bool = True,\n) -&gt; FuncSchema:\n    \"\"\"\n    Given a Python function, extracts a `FuncSchema` from it, capturing the name, description,\n    parameter descriptions, and other metadata.\n\n    Args:\n        func: The function to extract the schema from.\n        docstring_style: The style of the docstring to use for parsing. If not provided, we will\n            attempt to auto-detect the style.\n        name_override: If provided, use this name instead of the function's `__name__`.\n        description_override: If provided, use this description instead of the one derived from the\n            docstring.\n        use_docstring_info: If True, uses the docstring to generate the description and parameter\n            descriptions.\n        strict_json_schema: Whether the JSON schema is in strict mode. If True, we'll ensure that\n            the schema adheres to the \"strict\" standard the OpenAI API expects. We **strongly**\n            recommend setting this to True, as it increases the likelihood of the LLM producing\n            correct JSON input.\n\n    Returns:\n        A `FuncSchema` object containing the function's name, description, parameter descriptions,\n        and other metadata.\n    \"\"\"\n\n    # 1. Grab docstring info\n    if use_docstring_info:\n        doc_info = generate_func_documentation(func, docstring_style)\n        param_descs = dict(doc_info.param_descriptions or {})\n    else:\n        doc_info = None\n        param_descs = {}\n\n    type_hints_with_extras = get_type_hints(func, include_extras=True)\n    type_hints: dict[str, Any] = {}\n    annotated_param_descs: dict[str, str] = {}\n\n    for name, annotation in type_hints_with_extras.items():\n        if name == \"return\":\n            continue\n\n        stripped_ann, metadata = _strip_annotated(annotation)\n        type_hints[name] = stripped_ann\n\n        description = _extract_description_from_metadata(metadata)\n        if description is not None:\n            annotated_param_descs[name] = description\n\n    for name, description in annotated_param_descs.items():\n        param_descs.setdefault(name, description)\n\n    # Ensure name_override takes precedence even if docstring info is disabled.\n    func_name = name_override or (doc_info.name if doc_info else func.__name__)\n\n    # 2. Inspect function signature and get type hints\n    sig = inspect.signature(func)\n    params = list(sig.parameters.items())\n    takes_context = False\n    filtered_params = []\n\n    if params:\n        first_name, first_param = params[0]\n        # Prefer the evaluated type hint if available\n        ann = type_hints.get(first_name, first_param.annotation)\n        if ann != inspect._empty:\n            origin = get_origin(ann) or ann\n            if origin is RunContextWrapper or origin is ToolContext:\n                takes_context = True  # Mark that the function takes context\n            else:\n                filtered_params.append((first_name, first_param))\n        else:\n            filtered_params.append((first_name, first_param))\n\n    # For parameters other than the first, raise error if any use RunContextWrapper or ToolContext.\n    for name, param in params[1:]:\n        ann = type_hints.get(name, param.annotation)\n        if ann != inspect._empty:\n            origin = get_origin(ann) or ann\n            if origin is RunContextWrapper or origin is ToolContext:\n                raise UserError(\n                    f\"RunContextWrapper/ToolContext param found at non-first position in function\"\n                    f\" {func.__name__}\"\n                )\n        filtered_params.append((name, param))\n\n    # We will collect field definitions for create_model as a dict:\n    #   field_name -&gt; (type_annotation, default_value_or_Field(...))\n    fields: dict[str, Any] = {}\n\n    for name, param in filtered_params:\n        ann = type_hints.get(name, param.annotation)\n        default = param.default\n\n        # If there's no type hint, assume `Any`\n        if ann == inspect._empty:\n            ann = Any\n\n        # If a docstring param description exists, use it\n        field_description = param_descs.get(name, None)\n\n        # Handle different parameter kinds\n        if param.kind == param.VAR_POSITIONAL:\n            # e.g. *args: extend positional args\n            if get_origin(ann) is tuple:\n                # e.g. def foo(*args: tuple[int, ...]) -&gt; treat as List[int]\n                args_of_tuple = get_args(ann)\n                if len(args_of_tuple) == 2 and args_of_tuple[1] is Ellipsis:\n                    ann = list[args_of_tuple[0]]  # type: ignore\n                else:\n                    ann = list[Any]\n            else:\n                # If user wrote *args: int, treat as List[int]\n                ann = list[ann]  # type: ignore\n\n            # Default factory to empty list\n            fields[name] = (\n                ann,\n                Field(default_factory=list, description=field_description),\n            )\n\n        elif param.kind == param.VAR_KEYWORD:\n            # **kwargs handling\n            if get_origin(ann) is dict:\n                # e.g. def foo(**kwargs: dict[str, int])\n                dict_args = get_args(ann)\n                if len(dict_args) == 2:\n                    ann = dict[dict_args[0], dict_args[1]]  # type: ignore\n                else:\n                    ann = dict[str, Any]\n            else:\n                # e.g. def foo(**kwargs: int) -&gt; Dict[str, int]\n                ann = dict[str, ann]  # type: ignore\n\n            fields[name] = (\n                ann,\n                Field(default_factory=dict, description=field_description),\n            )\n\n        else:\n            # Normal parameter\n            if default == inspect._empty:\n                # Required field\n                fields[name] = (\n                    ann,\n                    Field(..., description=field_description),\n                )\n            elif isinstance(default, FieldInfo):\n                # Parameter with a default value that is a Field(...)\n                fields[name] = (\n                    ann,\n                    FieldInfo.merge_field_infos(\n                        default, description=field_description or default.description\n                    ),\n                )\n            else:\n                # Parameter with a default value\n                fields[name] = (\n                    ann,\n                    Field(default=default, description=field_description),\n                )\n\n    # 3. Dynamically build a Pydantic model\n    dynamic_model = create_model(f\"{func_name}_args\", __base__=BaseModel, **fields)\n\n    # 4. Build JSON schema from that model\n    json_schema = dynamic_model.model_json_schema()\n    if strict_json_schema:\n        json_schema = ensure_strict_json_schema(json_schema)\n\n    # 5. Return as a FuncSchema dataclass\n    return FuncSchema(\n        name=func_name,\n        # Ensure description_override takes precedence even if docstring info is disabled.\n        description=description_override or (doc_info.description if doc_info else None),\n        params_pydantic_model=dynamic_model,\n        params_json_schema=json_schema,\n        signature=sig,\n        takes_context=takes_context,\n        strict_json_schema=strict_json_schema,\n    )\n</code></pre>"},{"location":"ref/guardrail/","title":"<code>Guardrails</code>","text":""},{"location":"ref/guardrail/#agents.guardrail.GuardrailFunctionOutput","title":"GuardrailFunctionOutput  <code>dataclass</code>","text":"<p>The output of a guardrail function.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass GuardrailFunctionOutput:\n    \"\"\"The output of a guardrail function.\"\"\"\n\n    output_info: Any\n    \"\"\"\n    Optional information about the guardrail's output. For example, the guardrail could include\n    information about the checks it performed and granular results.\n    \"\"\"\n\n    tripwire_triggered: bool\n    \"\"\"\n    Whether the tripwire was triggered. If triggered, the agent's execution will be halted.\n    \"\"\"\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.GuardrailFunctionOutput.output_info","title":"output_info  <code>instance-attribute</code>","text":"<pre><code>output_info: Any\n</code></pre> <p>Optional information about the guardrail's output. For example, the guardrail could include information about the checks it performed and granular results.</p>"},{"location":"ref/guardrail/#agents.guardrail.GuardrailFunctionOutput.tripwire_triggered","title":"tripwire_triggered  <code>instance-attribute</code>","text":"<pre><code>tripwire_triggered: bool\n</code></pre> <p>Whether the tripwire was triggered. If triggered, the agent's execution will be halted.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrailResult","title":"InputGuardrailResult  <code>dataclass</code>","text":"<p>The result of a guardrail run.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass InputGuardrailResult:\n    \"\"\"The result of a guardrail run.\"\"\"\n\n    guardrail: InputGuardrail[Any]\n    \"\"\"\n    The guardrail that was run.\n    \"\"\"\n\n    output: GuardrailFunctionOutput\n    \"\"\"The output of the guardrail function.\"\"\"\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrailResult.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: InputGuardrail[Any]\n</code></pre> <p>The guardrail that was run.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrailResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: GuardrailFunctionOutput\n</code></pre> <p>The output of the guardrail function.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult","title":"OutputGuardrailResult  <code>dataclass</code>","text":"<p>The result of a guardrail run.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass OutputGuardrailResult:\n    \"\"\"The result of a guardrail run.\"\"\"\n\n    guardrail: OutputGuardrail[Any]\n    \"\"\"\n    The guardrail that was run.\n    \"\"\"\n\n    agent_output: Any\n    \"\"\"\n    The output of the agent that was checked by the guardrail.\n    \"\"\"\n\n    agent: Agent[Any]\n    \"\"\"\n    The agent that was checked by the guardrail.\n    \"\"\"\n\n    output: GuardrailFunctionOutput\n    \"\"\"The output of the guardrail function.\"\"\"\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: OutputGuardrail[Any]\n</code></pre> <p>The guardrail that was run.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.agent_output","title":"agent_output  <code>instance-attribute</code>","text":"<pre><code>agent_output: Any\n</code></pre> <p>The output of the agent that was checked by the guardrail.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent that was checked by the guardrail.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: GuardrailFunctionOutput\n</code></pre> <p>The output of the guardrail function.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrail","title":"InputGuardrail  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Input guardrails are checks that run either in parallel with the agent or before it starts. They can be used to do things like: - Check if input messages are off-topic - Take over control of the agent's execution if an unexpected input is detected</p> <p>You can use the <code>@input_guardrail()</code> decorator to turn a function into an <code>InputGuardrail</code>, or create an <code>InputGuardrail</code> manually.</p> <p>Guardrails return a <code>GuardrailResult</code>. If <code>result.tripwire_triggered</code> is <code>True</code>, the agent's execution will immediately stop, and an <code>InputGuardrailTripwireTriggered</code> exception will be raised</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass InputGuardrail(Generic[TContext]):\n    \"\"\"Input guardrails are checks that run either in parallel with the agent or before it starts.\n    They can be used to do things like:\n    - Check if input messages are off-topic\n    - Take over control of the agent's execution if an unexpected input is detected\n\n    You can use the `@input_guardrail()` decorator to turn a function into an `InputGuardrail`, or\n    create an `InputGuardrail` manually.\n\n    Guardrails return a `GuardrailResult`. If `result.tripwire_triggered` is `True`,\n    the agent's execution will immediately stop, and\n    an `InputGuardrailTripwireTriggered` exception will be raised\n    \"\"\"\n\n    guardrail_function: Callable[\n        [RunContextWrapper[TContext], Agent[Any], str | list[TResponseInputItem]],\n        MaybeAwaitable[GuardrailFunctionOutput],\n    ]\n    \"\"\"A function that receives the agent input and the context, and returns a\n     `GuardrailResult`. The result marks whether the tripwire was triggered, and can optionally\n     include information about the guardrail's output.\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"The name of the guardrail, used for tracing. If not provided, we'll use the guardrail\n    function's name.\n    \"\"\"\n\n    run_in_parallel: bool = True\n    \"\"\"Whether the guardrail runs concurrently with the agent (True, default) or before\n    the agent starts (False).\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        if self.name:\n            return self.name\n\n        return self.guardrail_function.__name__\n\n    async def run(\n        self,\n        agent: Agent[Any],\n        input: str | list[TResponseInputItem],\n        context: RunContextWrapper[TContext],\n    ) -&gt; InputGuardrailResult:\n        if not callable(self.guardrail_function):\n            raise UserError(f\"Guardrail function must be callable, got {self.guardrail_function}\")\n\n        output = self.guardrail_function(context, agent, input)\n        if inspect.isawaitable(output):\n            return InputGuardrailResult(\n                guardrail=self,\n                output=await output,\n            )\n\n        return InputGuardrailResult(\n            guardrail=self,\n            output=output,\n        )\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrail.guardrail_function","title":"guardrail_function  <code>instance-attribute</code>","text":"<pre><code>guardrail_function: Callable[\n    [\n        RunContextWrapper[TContext],\n        Agent[Any],\n        str | list[TResponseInputItem],\n    ],\n    MaybeAwaitable[GuardrailFunctionOutput],\n]\n</code></pre> <p>A function that receives the agent input and the context, and returns a <code>GuardrailResult</code>. The result marks whether the tripwire was triggered, and can optionally include information about the guardrail's output.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrail.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre> <p>The name of the guardrail, used for tracing. If not provided, we'll use the guardrail function's name.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrail.run_in_parallel","title":"run_in_parallel  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_in_parallel: bool = True\n</code></pre> <p>Whether the guardrail runs concurrently with the agent (True, default) or before the agent starts (False).</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrail","title":"OutputGuardrail  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Output guardrails are checks that run on the final output of an agent. They can be used to do check if the output passes certain validation criteria</p> <p>You can use the <code>@output_guardrail()</code> decorator to turn a function into an <code>OutputGuardrail</code>, or create an <code>OutputGuardrail</code> manually.</p> <p>Guardrails return a <code>GuardrailResult</code>. If <code>result.tripwire_triggered</code> is <code>True</code>, an <code>OutputGuardrailTripwireTriggered</code> exception will be raised.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass OutputGuardrail(Generic[TContext]):\n    \"\"\"Output guardrails are checks that run on the final output of an agent.\n    They can be used to do check if the output passes certain validation criteria\n\n    You can use the `@output_guardrail()` decorator to turn a function into an `OutputGuardrail`,\n    or create an `OutputGuardrail` manually.\n\n    Guardrails return a `GuardrailResult`. If `result.tripwire_triggered` is `True`, an\n    `OutputGuardrailTripwireTriggered` exception will be raised.\n    \"\"\"\n\n    guardrail_function: Callable[\n        [RunContextWrapper[TContext], Agent[Any], Any],\n        MaybeAwaitable[GuardrailFunctionOutput],\n    ]\n    \"\"\"A function that receives the final agent, its output, and the context, and returns a\n     `GuardrailResult`. The result marks whether the tripwire was triggered, and can optionally\n     include information about the guardrail's output.\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"The name of the guardrail, used for tracing. If not provided, we'll use the guardrail\n    function's name.\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        if self.name:\n            return self.name\n\n        return self.guardrail_function.__name__\n\n    async def run(\n        self, context: RunContextWrapper[TContext], agent: Agent[Any], agent_output: Any\n    ) -&gt; OutputGuardrailResult:\n        if not callable(self.guardrail_function):\n            raise UserError(f\"Guardrail function must be callable, got {self.guardrail_function}\")\n\n        output = self.guardrail_function(context, agent, agent_output)\n        if inspect.isawaitable(output):\n            return OutputGuardrailResult(\n                guardrail=self,\n                agent=agent,\n                agent_output=agent_output,\n                output=await output,\n            )\n\n        return OutputGuardrailResult(\n            guardrail=self,\n            agent=agent,\n            agent_output=agent_output,\n            output=output,\n        )\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrail.guardrail_function","title":"guardrail_function  <code>instance-attribute</code>","text":"<pre><code>guardrail_function: Callable[\n    [RunContextWrapper[TContext], Agent[Any], Any],\n    MaybeAwaitable[GuardrailFunctionOutput],\n]\n</code></pre> <p>A function that receives the final agent, its output, and the context, and returns a <code>GuardrailResult</code>. The result marks whether the tripwire was triggered, and can optionally include information about the guardrail's output.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrail.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre> <p>The name of the guardrail, used for tracing. If not provided, we'll use the guardrail function's name.</p>"},{"location":"ref/guardrail/#agents.guardrail.input_guardrail","title":"input_guardrail","text":"<pre><code>input_guardrail(\n    func: _InputGuardrailFuncSync[TContext_co],\n) -&gt; InputGuardrail[TContext_co]\n</code></pre><pre><code>input_guardrail(\n    func: _InputGuardrailFuncAsync[TContext_co],\n) -&gt; InputGuardrail[TContext_co]\n</code></pre><pre><code>input_guardrail(\n    *, name: str | None = None, run_in_parallel: bool = True\n) -&gt; Callable[\n    [\n        _InputGuardrailFuncSync[TContext_co]\n        | _InputGuardrailFuncAsync[TContext_co]\n    ],\n    InputGuardrail[TContext_co],\n]\n</code></pre> <pre><code>input_guardrail(\n    func: _InputGuardrailFuncSync[TContext_co]\n    | _InputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n    run_in_parallel: bool = True,\n) -&gt; (\n    InputGuardrail[TContext_co]\n    | Callable[\n        [\n            _InputGuardrailFuncSync[TContext_co]\n            | _InputGuardrailFuncAsync[TContext_co]\n        ],\n        InputGuardrail[TContext_co],\n    ]\n)\n</code></pre> <p>Decorator that transforms a sync or async function into an <code>InputGuardrail</code>. It can be used directly (no parentheses) or with keyword args, e.g.:</p> <pre><code>@input_guardrail\ndef my_sync_guardrail(...): ...\n\n@input_guardrail(name=\"guardrail_name\", run_in_parallel=False)\nasync def my_async_guardrail(...): ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>_InputGuardrailFuncSync[TContext_co] | _InputGuardrailFuncAsync[TContext_co] | None</code> <p>The guardrail function to wrap.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the guardrail. If not provided, uses the function's name.</p> <code>None</code> <code>run_in_parallel</code> <code>bool</code> <p>Whether to run the guardrail concurrently with the agent (True, default) or before the agent starts (False).</p> <code>True</code> Source code in <code>src/agents/guardrail.py</code> <pre><code>def input_guardrail(\n    func: _InputGuardrailFuncSync[TContext_co]\n    | _InputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n    run_in_parallel: bool = True,\n) -&gt; (\n    InputGuardrail[TContext_co]\n    | Callable[\n        [_InputGuardrailFuncSync[TContext_co] | _InputGuardrailFuncAsync[TContext_co]],\n        InputGuardrail[TContext_co],\n    ]\n):\n    \"\"\"\n    Decorator that transforms a sync or async function into an `InputGuardrail`.\n    It can be used directly (no parentheses) or with keyword args, e.g.:\n\n        @input_guardrail\n        def my_sync_guardrail(...): ...\n\n        @input_guardrail(name=\"guardrail_name\", run_in_parallel=False)\n        async def my_async_guardrail(...): ...\n\n    Args:\n        func: The guardrail function to wrap.\n        name: Optional name for the guardrail. If not provided, uses the function's name.\n        run_in_parallel: Whether to run the guardrail concurrently with the agent (True, default)\n            or before the agent starts (False).\n    \"\"\"\n\n    def decorator(\n        f: _InputGuardrailFuncSync[TContext_co] | _InputGuardrailFuncAsync[TContext_co],\n    ) -&gt; InputGuardrail[TContext_co]:\n        return InputGuardrail(\n            guardrail_function=f,\n            # If not set, guardrail name uses the function\u2019s name by default.\n            name=name if name else f.__name__,\n            run_in_parallel=run_in_parallel,\n        )\n\n    if func is not None:\n        # Decorator was used without parentheses\n        return decorator(func)\n\n    # Decorator used with keyword arguments\n    return decorator\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.output_guardrail","title":"output_guardrail","text":"<pre><code>output_guardrail(\n    func: _OutputGuardrailFuncSync[TContext_co],\n) -&gt; OutputGuardrail[TContext_co]\n</code></pre><pre><code>output_guardrail(\n    func: _OutputGuardrailFuncAsync[TContext_co],\n) -&gt; OutputGuardrail[TContext_co]\n</code></pre><pre><code>output_guardrail(\n    *, name: str | None = None\n) -&gt; Callable[\n    [\n        _OutputGuardrailFuncSync[TContext_co]\n        | _OutputGuardrailFuncAsync[TContext_co]\n    ],\n    OutputGuardrail[TContext_co],\n]\n</code></pre> <pre><code>output_guardrail(\n    func: _OutputGuardrailFuncSync[TContext_co]\n    | _OutputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    OutputGuardrail[TContext_co]\n    | Callable[\n        [\n            _OutputGuardrailFuncSync[TContext_co]\n            | _OutputGuardrailFuncAsync[TContext_co]\n        ],\n        OutputGuardrail[TContext_co],\n    ]\n)\n</code></pre> <p>Decorator that transforms a sync or async function into an <code>OutputGuardrail</code>. It can be used directly (no parentheses) or with keyword args, e.g.:</p> <pre><code>@output_guardrail\ndef my_sync_guardrail(...): ...\n\n@output_guardrail(name=\"guardrail_name\")\nasync def my_async_guardrail(...): ...\n</code></pre> Source code in <code>src/agents/guardrail.py</code> <pre><code>def output_guardrail(\n    func: _OutputGuardrailFuncSync[TContext_co]\n    | _OutputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    OutputGuardrail[TContext_co]\n    | Callable[\n        [_OutputGuardrailFuncSync[TContext_co] | _OutputGuardrailFuncAsync[TContext_co]],\n        OutputGuardrail[TContext_co],\n    ]\n):\n    \"\"\"\n    Decorator that transforms a sync or async function into an `OutputGuardrail`.\n    It can be used directly (no parentheses) or with keyword args, e.g.:\n\n        @output_guardrail\n        def my_sync_guardrail(...): ...\n\n        @output_guardrail(name=\"guardrail_name\")\n        async def my_async_guardrail(...): ...\n    \"\"\"\n\n    def decorator(\n        f: _OutputGuardrailFuncSync[TContext_co] | _OutputGuardrailFuncAsync[TContext_co],\n    ) -&gt; OutputGuardrail[TContext_co]:\n        return OutputGuardrail(\n            guardrail_function=f,\n            # Guardrail name defaults to function's name when not specified (None).\n            name=name if name else f.__name__,\n        )\n\n    if func is not None:\n        # Decorator was used without parentheses\n        return decorator(func)\n\n    # Decorator used with keyword arguments\n    return decorator\n</code></pre>"},{"location":"ref/handoffs/","title":"<code>Handoffs</code>","text":""},{"location":"ref/handoffs/#agents.handoffs.HandoffInputFilter","title":"HandoffInputFilter  <code>module-attribute</code>","text":"<pre><code>HandoffInputFilter: TypeAlias = Callable[\n    [HandoffInputData], MaybeAwaitable[HandoffInputData]\n]\n</code></pre> <p>A function that filters the input data passed to the next agent.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffHistoryMapper","title":"HandoffHistoryMapper  <code>module-attribute</code>","text":"<pre><code>HandoffHistoryMapper: TypeAlias = Callable[\n    [list[TResponseInputItem]], list[TResponseInputItem]\n]\n</code></pre> <p>A function that maps the previous transcript to the nested summary payload.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData","title":"HandoffInputData  <code>dataclass</code>","text":"Source code in <code>src/agents/handoffs/__init__.py</code> <pre><code>@dataclass(frozen=True)\nclass HandoffInputData:\n    input_history: str | tuple[TResponseInputItem, ...]\n    \"\"\"\n    The input history before `Runner.run()` was called.\n    \"\"\"\n\n    pre_handoff_items: tuple[RunItem, ...]\n    \"\"\"\n    The items generated before the agent turn where the handoff was invoked.\n    \"\"\"\n\n    new_items: tuple[RunItem, ...]\n    \"\"\"\n    The new items generated during the current agent turn, including the item that triggered the\n    handoff and the tool output message representing the response from the handoff output.\n    \"\"\"\n\n    run_context: RunContextWrapper[Any] | None = None\n    \"\"\"\n    The run context at the time the handoff was invoked. Note that, since this property was added\n    later on, it is optional for backwards compatibility.\n    \"\"\"\n\n    input_items: tuple[RunItem, ...] | None = None\n    \"\"\"\n    Items to include in the next agent's input. When set, these items are used instead of\n    new_items for building the input to the next agent. This allows filtering duplicates\n    from agent input while preserving all items in new_items for session history.\n    \"\"\"\n\n    def clone(self, **kwargs: Any) -&gt; HandoffInputData:\n        \"\"\"\n        Make a copy of the handoff input data, with the given arguments changed. For example, you\n        could do:\n\n        ```\n        new_handoff_input_data = handoff_input_data.clone(new_items=())\n        ```\n        \"\"\"\n\n        return dataclasses_replace(self, **kwargs)\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.input_history","title":"input_history  <code>instance-attribute</code>","text":"<pre><code>input_history: str | tuple[TResponseInputItem, ...]\n</code></pre> <p>The input history before <code>Runner.run()</code> was called.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.pre_handoff_items","title":"pre_handoff_items  <code>instance-attribute</code>","text":"<pre><code>pre_handoff_items: tuple[RunItem, ...]\n</code></pre> <p>The items generated before the agent turn where the handoff was invoked.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: tuple[RunItem, ...]\n</code></pre> <p>The new items generated during the current agent turn, including the item that triggered the handoff and the tool output message representing the response from the handoff output.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.run_context","title":"run_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_context: RunContextWrapper[Any] | None = None\n</code></pre> <p>The run context at the time the handoff was invoked. Note that, since this property was added later on, it is optional for backwards compatibility.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.input_items","title":"input_items  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_items: tuple[RunItem, ...] | None = None\n</code></pre> <p>Items to include in the next agent's input. When set, these items are used instead of new_items for building the input to the next agent. This allows filtering duplicates from agent input while preserving all items in new_items for session history.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.clone","title":"clone","text":"<pre><code>clone(**kwargs: Any) -&gt; HandoffInputData\n</code></pre> <p>Make a copy of the handoff input data, with the given arguments changed. For example, you could do:</p> <pre><code>new_handoff_input_data = handoff_input_data.clone(new_items=())\n</code></pre> Source code in <code>src/agents/handoffs/__init__.py</code> <pre><code>def clone(self, **kwargs: Any) -&gt; HandoffInputData:\n    \"\"\"\n    Make a copy of the handoff input data, with the given arguments changed. For example, you\n    could do:\n\n    ```\n    new_handoff_input_data = handoff_input_data.clone(new_items=())\n    ```\n    \"\"\"\n\n    return dataclasses_replace(self, **kwargs)\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.Handoff","title":"Handoff  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext, TAgent]</code></p> <p>A handoff is when an agent delegates a task to another agent.</p> <p>For example, in a customer support scenario you might have a \"triage agent\" that determines which agent should handle the user's request, and sub-agents that specialize in different areas like billing, account management, etc.</p> Source code in <code>src/agents/handoffs/__init__.py</code> <pre><code>@dataclass\nclass Handoff(Generic[TContext, TAgent]):\n    \"\"\"A handoff is when an agent delegates a task to another agent.\n\n    For example, in a customer support scenario you might have a \"triage agent\" that determines\n    which agent should handle the user's request, and sub-agents that specialize in different areas\n    like billing, account management, etc.\n    \"\"\"\n\n    tool_name: str\n    \"\"\"The name of the tool that represents the handoff.\"\"\"\n\n    tool_description: str\n    \"\"\"The description of the tool that represents the handoff.\"\"\"\n\n    input_json_schema: dict[str, Any]\n    \"\"\"The JSON schema for the handoff input. Can be empty if the handoff does not take an input.\"\"\"\n\n    on_invoke_handoff: Callable[[RunContextWrapper[Any], str], Awaitable[TAgent]]\n    \"\"\"The function that invokes the handoff.\n\n    The parameters passed are: (1) the handoff run context, (2) the arguments from the LLM as a\n    JSON string (or an empty string if ``input_json_schema`` is empty). Must return an agent.\n    \"\"\"\n\n    agent_name: str\n    \"\"\"The name of the agent that is being handed off to.\"\"\"\n\n    input_filter: HandoffInputFilter | None = None\n    \"\"\"A function that filters the inputs that are passed to the next agent.\n\n    By default, the new agent sees the entire conversation history. In some cases, you may want to\n    filter inputs (for example, to remove older inputs or remove tools from existing inputs). The\n    function receives the entire conversation history so far, including the input item that\n    triggered the handoff and a tool call output item representing the handoff tool's output. You\n    are free to modify the input history or new items as you see fit. The next agent receives the\n    input history plus ``input_items`` when provided, otherwise it receives ``new_items``. Use\n    ``input_items`` to filter model input while keeping ``new_items`` intact for session history.\n    IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The\n    items generated before will already have been streamed.\n    \"\"\"\n\n    nest_handoff_history: bool | None = None\n    \"\"\"Override the run-level ``nest_handoff_history`` behavior for this handoff only.\"\"\"\n\n    strict_json_schema: bool = True\n    \"\"\"Whether the input JSON schema is in strict mode. We strongly recommend setting this to True\n    because it increases the likelihood of correct JSON input.\"\"\"\n\n    is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase[Any]], MaybeAwaitable[bool]] = (\n        True\n    )\n    \"\"\"Whether the handoff is enabled.\n\n    Either a bool or a callable that takes the run context and agent and returns whether the\n    handoff is enabled. You can use this to dynamically enable or disable a handoff based on your\n    context or state.\n    \"\"\"\n\n    _agent_ref: weakref.ReferenceType[AgentBase[Any]] | None = field(\n        default=None, init=False, repr=False\n    )\n    \"\"\"Weak reference to the target agent when constructed via `handoff()`.\"\"\"\n\n    def get_transfer_message(self, agent: AgentBase[Any]) -&gt; str:\n        return json.dumps({\"assistant\": agent.name})\n\n    @classmethod\n    def default_tool_name(cls, agent: AgentBase[Any]) -&gt; str:\n        return _transforms.transform_string_function_style(f\"transfer_to_{agent.name}\")\n\n    @classmethod\n    def default_tool_description(cls, agent: AgentBase[Any]) -&gt; str:\n        return (\n            f\"Handoff to the {agent.name} agent to handle the request. \"\n            f\"{agent.handoff_description or ''}\"\n        )\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.tool_name","title":"tool_name  <code>instance-attribute</code>","text":"<pre><code>tool_name: str\n</code></pre> <p>The name of the tool that represents the handoff.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.tool_description","title":"tool_description  <code>instance-attribute</code>","text":"<pre><code>tool_description: str\n</code></pre> <p>The description of the tool that represents the handoff.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.input_json_schema","title":"input_json_schema  <code>instance-attribute</code>","text":"<pre><code>input_json_schema: dict[str, Any]\n</code></pre> <p>The JSON schema for the handoff input. Can be empty if the handoff does not take an input.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.on_invoke_handoff","title":"on_invoke_handoff  <code>instance-attribute</code>","text":"<pre><code>on_invoke_handoff: Callable[\n    [RunContextWrapper[Any], str], Awaitable[TAgent]\n]\n</code></pre> <p>The function that invokes the handoff.</p> <p>The parameters passed are: (1) the handoff run context, (2) the arguments from the LLM as a JSON string (or an empty string if <code>input_json_schema</code> is empty). Must return an agent.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.agent_name","title":"agent_name  <code>instance-attribute</code>","text":"<pre><code>agent_name: str\n</code></pre> <p>The name of the agent that is being handed off to.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.input_filter","title":"input_filter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_filter: HandoffInputFilter | None = None\n</code></pre> <p>A function that filters the inputs that are passed to the next agent.</p> <p>By default, the new agent sees the entire conversation history. In some cases, you may want to filter inputs (for example, to remove older inputs or remove tools from existing inputs). The function receives the entire conversation history so far, including the input item that triggered the handoff and a tool call output item representing the handoff tool's output. You are free to modify the input history or new items as you see fit. The next agent receives the input history plus <code>input_items</code> when provided, otherwise it receives <code>new_items</code>. Use <code>input_items</code> to filter model input while keeping <code>new_items</code> intact for session history. IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The items generated before will already have been streamed.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.nest_handoff_history","title":"nest_handoff_history  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nest_handoff_history: bool | None = None\n</code></pre> <p>Override the run-level <code>nest_handoff_history</code> behavior for this handoff only.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.strict_json_schema","title":"strict_json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strict_json_schema: bool = True\n</code></pre> <p>Whether the input JSON schema is in strict mode. We strongly recommend setting this to True because it increases the likelihood of correct JSON input.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.is_enabled","title":"is_enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_enabled: (\n    bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase[Any]],\n        MaybeAwaitable[bool],\n    ]\n) = True\n</code></pre> <p>Whether the handoff is enabled.</p> <p>Either a bool or a callable that takes the run context and agent and returns whether the handoff is enabled. You can use this to dynamically enable or disable a handoff based on your context or state.</p>"},{"location":"ref/handoffs/#agents.handoffs.default_handoff_history_mapper","title":"default_handoff_history_mapper","text":"<pre><code>default_handoff_history_mapper(\n    transcript: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Return a single assistant message summarizing the transcript.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def default_handoff_history_mapper(\n    transcript: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Return a single assistant message summarizing the transcript.\"\"\"\n\n    summary_message = _build_summary_message(transcript)\n    return [summary_message]\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.get_conversation_history_wrappers","title":"get_conversation_history_wrappers","text":"<pre><code>get_conversation_history_wrappers() -&gt; tuple[str, str]\n</code></pre> <p>Return the current start/end markers used for the nested conversation summary.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def get_conversation_history_wrappers() -&gt; tuple[str, str]:\n    \"\"\"Return the current start/end markers used for the nested conversation summary.\"\"\"\n\n    return (_conversation_history_start, _conversation_history_end)\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.nest_handoff_history","title":"nest_handoff_history","text":"<pre><code>nest_handoff_history(\n    handoff_input_data: HandoffInputData,\n    *,\n    history_mapper: HandoffHistoryMapper | None = None,\n) -&gt; HandoffInputData\n</code></pre> <p>Summarize the previous transcript for the next agent.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def nest_handoff_history(\n    handoff_input_data: HandoffInputData,\n    *,\n    history_mapper: HandoffHistoryMapper | None = None,\n) -&gt; HandoffInputData:\n    \"\"\"Summarize the previous transcript for the next agent.\"\"\"\n\n    normalized_history = _normalize_input_history(handoff_input_data.input_history)\n    flattened_history = _flatten_nested_history_messages(normalized_history)\n\n    # Convert items to plain inputs for the transcript summary.\n    pre_items_as_inputs: list[TResponseInputItem] = []\n    filtered_pre_items: list[RunItem] = []\n    for run_item in handoff_input_data.pre_handoff_items:\n        if isinstance(run_item, ToolApprovalItem):\n            continue\n        plain_input = _run_item_to_plain_input(run_item)\n        pre_items_as_inputs.append(plain_input)\n        if _should_forward_pre_item(plain_input):\n            filtered_pre_items.append(run_item)\n\n    new_items_as_inputs: list[TResponseInputItem] = []\n    filtered_input_items: list[RunItem] = []\n    for run_item in handoff_input_data.new_items:\n        if isinstance(run_item, ToolApprovalItem):\n            continue\n        plain_input = _run_item_to_plain_input(run_item)\n        new_items_as_inputs.append(plain_input)\n        if _should_forward_new_item(plain_input):\n            filtered_input_items.append(run_item)\n\n    transcript = flattened_history + pre_items_as_inputs + new_items_as_inputs\n\n    mapper = history_mapper or default_handoff_history_mapper\n    history_items = mapper(transcript)\n\n    return handoff_input_data.clone(\n        input_history=tuple(deepcopy(item) for item in history_items),\n        pre_handoff_items=tuple(filtered_pre_items),\n        # new_items stays unchanged for session history.\n        input_items=tuple(filtered_input_items),\n    )\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.reset_conversation_history_wrappers","title":"reset_conversation_history_wrappers","text":"<pre><code>reset_conversation_history_wrappers() -&gt; None\n</code></pre> <p>Restore the default <code>&lt;CONVERSATION HISTORY&gt;</code> markers.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def reset_conversation_history_wrappers() -&gt; None:\n    \"\"\"Restore the default ``&lt;CONVERSATION HISTORY&gt;`` markers.\"\"\"\n\n    global _conversation_history_start, _conversation_history_end\n    _conversation_history_start = _DEFAULT_CONVERSATION_HISTORY_START\n    _conversation_history_end = _DEFAULT_CONVERSATION_HISTORY_END\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.set_conversation_history_wrappers","title":"set_conversation_history_wrappers","text":"<pre><code>set_conversation_history_wrappers(\n    *, start: str | None = None, end: str | None = None\n) -&gt; None\n</code></pre> <p>Override the markers that wrap the generated conversation summary.</p> <p>Pass <code>None</code> to leave either side unchanged.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def set_conversation_history_wrappers(\n    *,\n    start: str | None = None,\n    end: str | None = None,\n) -&gt; None:\n    \"\"\"Override the markers that wrap the generated conversation summary.\n\n    Pass ``None`` to leave either side unchanged.\n    \"\"\"\n\n    global _conversation_history_start, _conversation_history_end\n    if start is not None:\n        _conversation_history_start = start\n    if end is not None:\n        _conversation_history_end = end\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.handoff","title":"handoff","text":"<pre><code>handoff(\n    agent: Agent[TContext],\n    *,\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    nest_handoff_history: bool | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre><pre><code>handoff(\n    agent: Agent[TContext],\n    *,\n    on_handoff: OnHandoffWithInput[THandoffInput],\n    input_type: type[THandoffInput],\n    tool_description_override: str | None = None,\n    tool_name_override: str | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    nest_handoff_history: bool | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre><pre><code>handoff(\n    agent: Agent[TContext],\n    *,\n    on_handoff: OnHandoffWithoutInput,\n    tool_description_override: str | None = None,\n    tool_name_override: str | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    nest_handoff_history: bool | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre> <pre><code>handoff(\n    agent: Agent[TContext],\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    on_handoff: OnHandoffWithInput[THandoffInput]\n    | OnHandoffWithoutInput\n    | None = None,\n    input_type: type[THandoffInput] | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    nest_handoff_history: bool | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[TContext]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre> <p>Create a handoff from an agent.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent[TContext]</code> <p>The agent to handoff to.</p> required <code>tool_name_override</code> <code>str | None</code> <p>Optional override for the name of the tool that represents the handoff.</p> <code>None</code> <code>tool_description_override</code> <code>str | None</code> <p>Optional override for the description of the tool that represents the handoff.</p> <code>None</code> <code>on_handoff</code> <code>OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None</code> <p>A function that runs when the handoff is invoked.</p> <code>None</code> <code>input_type</code> <code>type[THandoffInput] | None</code> <p>The type of the input to the handoff. If provided, the input will be validated against this type. Only relevant if you pass a function that takes an input.</p> <code>None</code> <code>input_filter</code> <code>Callable[[HandoffInputData], HandoffInputData] | None</code> <p>A function that filters the inputs that are passed to the next agent.</p> <code>None</code> <code>nest_handoff_history</code> <code>bool | None</code> <p>Optional override for the RunConfig-level <code>nest_handoff_history</code> flag. If <code>None</code> we fall back to the run's configuration.</p> <code>None</code> <code>is_enabled</code> <code>bool | Callable[[RunContextWrapper[Any], Agent[TContext]], MaybeAwaitable[bool]]</code> <p>Whether the handoff is enabled. Can be a bool or a callable that takes the run context and agent and returns whether the handoff is enabled. Disabled handoffs are hidden from the LLM at runtime.</p> <code>True</code> Source code in <code>src/agents/handoffs/__init__.py</code> <pre><code>def handoff(\n    agent: Agent[TContext],\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    on_handoff: OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None = None,\n    input_type: type[THandoffInput] | None = None,\n    input_filter: Callable[[HandoffInputData], HandoffInputData] | None = None,\n    nest_handoff_history: bool | None = None,\n    is_enabled: bool\n    | Callable[[RunContextWrapper[Any], Agent[TContext]], MaybeAwaitable[bool]] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]:\n    \"\"\"Create a handoff from an agent.\n\n    Args:\n        agent: The agent to handoff to.\n        tool_name_override: Optional override for the name of the tool that represents the handoff.\n        tool_description_override: Optional override for the description of the tool that\n            represents the handoff.\n        on_handoff: A function that runs when the handoff is invoked.\n        input_type: The type of the input to the handoff. If provided, the input will be validated\n            against this type. Only relevant if you pass a function that takes an input.\n        input_filter: A function that filters the inputs that are passed to the next agent.\n        nest_handoff_history: Optional override for the RunConfig-level ``nest_handoff_history``\n            flag. If ``None`` we fall back to the run's configuration.\n        is_enabled: Whether the handoff is enabled. Can be a bool or a callable that takes the run\n            context and agent and returns whether the handoff is enabled. Disabled handoffs are\n            hidden from the LLM at runtime.\n    \"\"\"\n\n    assert (on_handoff and input_type) or not (on_handoff and input_type), (\n        \"You must provide either both on_handoff and input_type, or neither\"\n    )\n    type_adapter: TypeAdapter[Any] | None\n    if input_type is not None:\n        assert callable(on_handoff), \"on_handoff must be callable\"\n        sig = inspect.signature(on_handoff)\n        if len(sig.parameters) != 2:\n            raise UserError(\"on_handoff must take two arguments: context and input\")\n\n        type_adapter = TypeAdapter(input_type)\n        input_json_schema = type_adapter.json_schema()\n    else:\n        type_adapter = None\n        input_json_schema = {}\n        if on_handoff is not None:\n            sig = inspect.signature(on_handoff)\n            if len(sig.parameters) != 1:\n                raise UserError(\"on_handoff must take one argument: context\")\n\n    async def _invoke_handoff(\n        ctx: RunContextWrapper[Any], input_json: str | None = None\n    ) -&gt; Agent[TContext]:\n        if input_type is not None and type_adapter is not None:\n            if input_json is None:\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Handoff function expected non-null input, but got None\",\n                        data={\"details\": \"input_json is None\"},\n                    )\n                )\n                raise ModelBehaviorError(\"Handoff function expected non-null input, but got None\")\n\n            validated_input = _json.validate_json(\n                json_str=input_json,\n                type_adapter=type_adapter,\n                partial=False,\n            )\n            input_func = cast(OnHandoffWithInput[THandoffInput], on_handoff)\n            if inspect.iscoroutinefunction(input_func):\n                await input_func(ctx, validated_input)\n            else:\n                input_func(ctx, validated_input)\n        elif on_handoff is not None:\n            no_input_func = cast(OnHandoffWithoutInput, on_handoff)\n            if inspect.iscoroutinefunction(no_input_func):\n                await no_input_func(ctx)\n            else:\n                no_input_func(ctx)\n\n        return agent\n\n    tool_name = tool_name_override or Handoff.default_tool_name(agent)\n    tool_description = tool_description_override or Handoff.default_tool_description(agent)\n\n    # Always ensure the input JSON schema is in strict mode. If needed, we can make this\n    # configurable in the future.\n    input_json_schema = ensure_strict_json_schema(input_json_schema)\n\n    async def _is_enabled(ctx: RunContextWrapper[Any], agent_base: AgentBase[Any]) -&gt; bool:\n        from ..agent import Agent\n\n        assert callable(is_enabled), \"is_enabled must be callable here\"\n        assert isinstance(agent_base, Agent), \"Can't handoff to a non-Agent\"\n        result = is_enabled(ctx, agent_base)\n        if inspect.isawaitable(result):\n            return await result\n        return bool(result)\n\n    handoff_obj = Handoff(\n        tool_name=tool_name,\n        tool_description=tool_description,\n        input_json_schema=input_json_schema,\n        on_invoke_handoff=_invoke_handoff,\n        input_filter=input_filter,\n        nest_handoff_history=nest_handoff_history,\n        agent_name=agent.name,\n        is_enabled=_is_enabled if callable(is_enabled) else is_enabled,\n    )\n    handoff_obj._agent_ref = weakref.ref(agent)\n    return handoff_obj\n</code></pre>"},{"location":"ref/items/","title":"<code>Items</code>","text":""},{"location":"ref/items/#agents.items.TResponse","title":"TResponse  <code>module-attribute</code>","text":"<pre><code>TResponse = Response\n</code></pre> <p>A type alias for the Response type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.TResponseInputItem","title":"TResponseInputItem  <code>module-attribute</code>","text":"<pre><code>TResponseInputItem = ResponseInputItemParam\n</code></pre> <p>A type alias for the ResponseInputItemParam type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.TResponseOutputItem","title":"TResponseOutputItem  <code>module-attribute</code>","text":"<pre><code>TResponseOutputItem = ResponseOutputItem\n</code></pre> <p>A type alias for the ResponseOutputItem type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.TResponseStreamEvent","title":"TResponseStreamEvent  <code>module-attribute</code>","text":"<pre><code>TResponseStreamEvent = ResponseStreamEvent\n</code></pre> <p>A type alias for the ResponseStreamEvent type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.ToolCallItemTypes","title":"ToolCallItemTypes  <code>module-attribute</code>","text":"<pre><code>ToolCallItemTypes: TypeAlias = Union[\n    ResponseFunctionToolCall,\n    ResponseComputerToolCall,\n    ResponseFileSearchToolCall,\n    ResponseFunctionWebSearch,\n    McpCall,\n    ResponseCodeInterpreterToolCall,\n    ImageGenerationCall,\n    LocalShellCall,\n    dict[str, Any],\n]\n</code></pre> <p>A type that represents a tool call item.</p>"},{"location":"ref/items/#agents.items.RunItem","title":"RunItem  <code>module-attribute</code>","text":"<pre><code>RunItem: TypeAlias = Union[\n    MessageOutputItem,\n    HandoffCallItem,\n    HandoffOutputItem,\n    ToolCallItem,\n    ToolCallOutputItem,\n    CompactionItem,\n    ReasoningItem,\n    MCPListToolsItem,\n    MCPApprovalRequestItem,\n    MCPApprovalResponseItem,\n    CompactionItem,\n    ToolApprovalItem,\n]\n</code></pre> <p>An item generated by an agent.</p>"},{"location":"ref/items/#agents.items.RunItemBase","title":"RunItemBase  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass RunItemBase(Generic[T], abc.ABC):\n    agent: Agent[Any]\n    \"\"\"The agent whose run caused this item to be generated.\"\"\"\n\n    raw_item: T\n    \"\"\"The raw Responses item from the run. This will always be either an output item (i.e.\n    `openai.types.responses.ResponseOutputItem` or an input item\n    (i.e. `openai.types.responses.ResponseInputItemParam`).\n    \"\"\"\n\n    _agent_ref: weakref.ReferenceType[Agent[Any]] | None = field(\n        init=False,\n        repr=False,\n        default=None,\n    )\n\n    def __post_init__(self) -&gt; None:\n        # Store a weak reference so we can release the strong reference later if desired.\n        self._agent_ref = weakref.ref(self.agent)\n\n    def __getattribute__(self, name: str) -&gt; Any:\n        if name == \"agent\":\n            return self._get_agent_via_weakref(\"agent\", \"_agent_ref\")\n        return super().__getattribute__(name)\n\n    def release_agent(self) -&gt; None:\n        \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n        if \"agent\" not in self.__dict__:\n            return\n        agent = self.__dict__[\"agent\"]\n        if agent is None:\n            return\n        self._agent_ref = weakref.ref(agent) if agent is not None else None\n        # Set to None instead of deleting so dataclass repr/asdict keep working.\n        self.__dict__[\"agent\"] = None\n\n    def _get_agent_via_weakref(self, attr_name: str, ref_name: str) -&gt; Any:\n        # Preserve the dataclass field so repr/asdict still read it, but lazily resolve the weakref\n        # when the stored value is None (meaning release_agent already dropped the strong ref).\n        # If the attribute was never overridden we fall back to the default descriptor chain.\n        data = object.__getattribute__(self, \"__dict__\")\n        value = data.get(attr_name, _MISSING_ATTR_SENTINEL)\n        if value is _MISSING_ATTR_SENTINEL:\n            return object.__getattribute__(self, attr_name)\n        if value is not None:\n            return value\n        ref = object.__getattribute__(self, ref_name)\n        if ref is not None:\n            agent = ref()\n            if agent is not None:\n                return agent\n        return None\n\n    def to_input_item(self) -&gt; TResponseInputItem:\n        \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n        if isinstance(self.raw_item, dict):\n            # We know that input items are dicts, so we can ignore the type error\n            return self.raw_item  # type: ignore\n        elif isinstance(self.raw_item, BaseModel):\n            # All output items are Pydantic models that can be converted to input items.\n            return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n        else:\n            raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.RunItemBase.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.RunItemBase.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: T\n</code></pre> <p>The raw Responses item from the run. This will always be either an output item (i.e. <code>openai.types.responses.ResponseOutputItem</code> or an input item (i.e. <code>openai.types.responses.ResponseInputItemParam</code>).</p>"},{"location":"ref/items/#agents.items.RunItemBase.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.RunItemBase.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MessageOutputItem","title":"MessageOutputItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[ResponseOutputMessage]</code></p> <p>Represents a message from the LLM.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MessageOutputItem(RunItemBase[ResponseOutputMessage]):\n    \"\"\"Represents a message from the LLM.\"\"\"\n\n    raw_item: ResponseOutputMessage\n    \"\"\"The raw response output message.\"\"\"\n\n    type: Literal[\"message_output_item\"] = \"message_output_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MessageOutputItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ResponseOutputMessage\n</code></pre> <p>The raw response output message.</p>"},{"location":"ref/items/#agents.items.MessageOutputItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MessageOutputItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.MessageOutputItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffCallItem","title":"HandoffCallItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[ResponseFunctionToolCall]</code></p> <p>Represents a tool call for a handoff from one agent to another.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass HandoffCallItem(RunItemBase[ResponseFunctionToolCall]):\n    \"\"\"Represents a tool call for a handoff from one agent to another.\"\"\"\n\n    raw_item: ResponseFunctionToolCall\n    \"\"\"The raw response function tool call that represents the handoff.\"\"\"\n\n    type: Literal[\"handoff_call_item\"] = \"handoff_call_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffCallItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ResponseFunctionToolCall\n</code></pre> <p>The raw response function tool call that represents the handoff.</p>"},{"location":"ref/items/#agents.items.HandoffCallItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.HandoffCallItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffCallItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffOutputItem","title":"HandoffOutputItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[TResponseInputItem]</code></p> <p>Represents the output of a handoff.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass HandoffOutputItem(RunItemBase[TResponseInputItem]):\n    \"\"\"Represents the output of a handoff.\"\"\"\n\n    raw_item: TResponseInputItem\n    \"\"\"The raw input item that represents the handoff taking place.\"\"\"\n\n    source_agent: Agent[Any]\n    \"\"\"The agent that made the handoff.\"\"\"\n\n    target_agent: Agent[Any]\n    \"\"\"The agent that is being handed off to.\"\"\"\n\n    type: Literal[\"handoff_output_item\"] = \"handoff_output_item\"\n\n    _source_agent_ref: weakref.ReferenceType[Agent[Any]] | None = field(\n        init=False,\n        repr=False,\n        default=None,\n    )\n    _target_agent_ref: weakref.ReferenceType[Agent[Any]] | None = field(\n        init=False,\n        repr=False,\n        default=None,\n    )\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        # Maintain weak references so downstream code can release the strong references when safe.\n        self._source_agent_ref = weakref.ref(self.source_agent)\n        self._target_agent_ref = weakref.ref(self.target_agent)\n\n    def __getattribute__(self, name: str) -&gt; Any:\n        if name == \"source_agent\":\n            # Provide lazy weakref access like the base `agent` field so HandoffOutputItem\n            # callers keep seeing the original agent until GC occurs.\n            return self._get_agent_via_weakref(\"source_agent\", \"_source_agent_ref\")\n        if name == \"target_agent\":\n            # Same as above but for the target of the handoff.\n            return self._get_agent_via_weakref(\"target_agent\", \"_target_agent_ref\")\n        return super().__getattribute__(name)\n\n    def release_agent(self) -&gt; None:\n        super().release_agent()\n        if \"source_agent\" in self.__dict__:\n            source_agent = self.__dict__[\"source_agent\"]\n            if source_agent is not None:\n                self._source_agent_ref = weakref.ref(source_agent)\n            # Preserve dataclass fields for repr/asdict while dropping strong refs.\n            self.__dict__[\"source_agent\"] = None\n        if \"target_agent\" in self.__dict__:\n            target_agent = self.__dict__[\"target_agent\"]\n            if target_agent is not None:\n                self._target_agent_ref = weakref.ref(target_agent)\n            # Preserve dataclass fields for repr/asdict while dropping strong refs.\n            self.__dict__[\"target_agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffOutputItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: TResponseInputItem\n</code></pre> <p>The raw input item that represents the handoff taking place.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.source_agent","title":"source_agent  <code>instance-attribute</code>","text":"<pre><code>source_agent: Agent[Any]\n</code></pre> <p>The agent that made the handoff.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.target_agent","title":"target_agent  <code>instance-attribute</code>","text":"<pre><code>target_agent: Agent[Any]\n</code></pre> <p>The agent that is being handed off to.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallItem","title":"ToolCallItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[Any]</code></p> <p>Represents a tool call e.g. a function call or computer action call.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass ToolCallItem(RunItemBase[Any]):\n    \"\"\"Represents a tool call e.g. a function call or computer action call.\"\"\"\n\n    raw_item: ToolCallItemTypes\n    \"\"\"The raw tool call item.\"\"\"\n\n    type: Literal[\"tool_call_item\"] = \"tool_call_item\"\n\n    description: str | None = None\n    \"\"\"Optional tool description if known at item creation time.\"\"\"\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ToolCallItemTypes\n</code></pre> <p>The raw tool call item.</p>"},{"location":"ref/items/#agents.items.ToolCallItem.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre> <p>Optional tool description if known at item creation time.</p>"},{"location":"ref/items/#agents.items.ToolCallItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.ToolCallItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallOutputItem","title":"ToolCallOutputItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[Any]</code></p> <p>Represents the output of a tool call.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass ToolCallOutputItem(RunItemBase[Any]):\n    \"\"\"Represents the output of a tool call.\"\"\"\n\n    raw_item: ToolCallOutputTypes\n    \"\"\"The raw item from the model.\"\"\"\n\n    output: Any\n    \"\"\"The output of the tool call. This is whatever the tool call returned; the `raw_item`\n    contains a string representation of the output.\n    \"\"\"\n\n    type: Literal[\"tool_call_output_item\"] = \"tool_call_output_item\"\n\n    def to_input_item(self) -&gt; TResponseInputItem:\n        \"\"\"Converts the tool output into an input item for the next model turn.\n\n        Hosted tool outputs (e.g. shell/apply_patch) carry a `status` field for the SDK's\n        book-keeping, but the Responses API does not yet accept that parameter. Strip it from the\n        payload we send back to the model while keeping the original raw item intact.\n        \"\"\"\n\n        if isinstance(self.raw_item, dict):\n            payload = dict(self.raw_item)\n            payload_type = payload.get(\"type\")\n            if payload_type == \"shell_call_output\":\n                payload = dict(payload)\n                payload.pop(\"status\", None)\n                payload.pop(\"shell_output\", None)\n                payload.pop(\"provider_data\", None)\n                outputs = payload.get(\"output\")\n                if isinstance(outputs, list):\n                    for entry in outputs:\n                        if not isinstance(entry, dict):\n                            continue\n                        outcome = entry.get(\"outcome\")\n                        if isinstance(outcome, dict):\n                            if outcome.get(\"type\") == \"exit\":\n                                entry[\"outcome\"] = outcome\n            return cast(TResponseInputItem, payload)\n\n        return super().to_input_item()\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ToolCallOutputTypes\n</code></pre> <p>The raw item from the model.</p>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: Any\n</code></pre> <p>The output of the tool call. This is whatever the tool call returned; the <code>raw_item</code> contains a string representation of the output.</p>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts the tool output into an input item for the next model turn.</p> <p>Hosted tool outputs (e.g. shell/apply_patch) carry a <code>status</code> field for the SDK's book-keeping, but the Responses API does not yet accept that parameter. Strip it from the payload we send back to the model while keeping the original raw item intact.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts the tool output into an input item for the next model turn.\n\n    Hosted tool outputs (e.g. shell/apply_patch) carry a `status` field for the SDK's\n    book-keeping, but the Responses API does not yet accept that parameter. Strip it from the\n    payload we send back to the model while keeping the original raw item intact.\n    \"\"\"\n\n    if isinstance(self.raw_item, dict):\n        payload = dict(self.raw_item)\n        payload_type = payload.get(\"type\")\n        if payload_type == \"shell_call_output\":\n            payload = dict(payload)\n            payload.pop(\"status\", None)\n            payload.pop(\"shell_output\", None)\n            payload.pop(\"provider_data\", None)\n            outputs = payload.get(\"output\")\n            if isinstance(outputs, list):\n                for entry in outputs:\n                    if not isinstance(entry, dict):\n                        continue\n                    outcome = entry.get(\"outcome\")\n                    if isinstance(outcome, dict):\n                        if outcome.get(\"type\") == \"exit\":\n                            entry[\"outcome\"] = outcome\n        return cast(TResponseInputItem, payload)\n\n    return super().to_input_item()\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.ReasoningItem","title":"ReasoningItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[ResponseReasoningItem]</code></p> <p>Represents a reasoning item.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass ReasoningItem(RunItemBase[ResponseReasoningItem]):\n    \"\"\"Represents a reasoning item.\"\"\"\n\n    raw_item: ResponseReasoningItem\n    \"\"\"The raw reasoning item.\"\"\"\n\n    type: Literal[\"reasoning_item\"] = \"reasoning_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.ReasoningItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ResponseReasoningItem\n</code></pre> <p>The raw reasoning item.</p>"},{"location":"ref/items/#agents.items.ReasoningItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.ReasoningItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.ReasoningItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MCPListToolsItem","title":"MCPListToolsItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[McpListTools]</code></p> <p>Represents a call to an MCP server to list tools.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MCPListToolsItem(RunItemBase[McpListTools]):\n    \"\"\"Represents a call to an MCP server to list tools.\"\"\"\n\n    raw_item: McpListTools\n    \"\"\"The raw MCP list tools call.\"\"\"\n\n    type: Literal[\"mcp_list_tools_item\"] = \"mcp_list_tools_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MCPListToolsItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: McpListTools\n</code></pre> <p>The raw MCP list tools call.</p>"},{"location":"ref/items/#agents.items.MCPListToolsItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MCPListToolsItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.MCPListToolsItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem","title":"MCPApprovalRequestItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[McpApprovalRequest]</code></p> <p>Represents a request for MCP approval.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MCPApprovalRequestItem(RunItemBase[McpApprovalRequest]):\n    \"\"\"Represents a request for MCP approval.\"\"\"\n\n    raw_item: McpApprovalRequest\n    \"\"\"The raw MCP approval request.\"\"\"\n\n    type: Literal[\"mcp_approval_request_item\"] = \"mcp_approval_request_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: McpApprovalRequest\n</code></pre> <p>The raw MCP approval request.</p>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem","title":"MCPApprovalResponseItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[McpApprovalResponse]</code></p> <p>Represents a response to an MCP approval request.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MCPApprovalResponseItem(RunItemBase[McpApprovalResponse]):\n    \"\"\"Represents a response to an MCP approval request.\"\"\"\n\n    raw_item: McpApprovalResponse\n    \"\"\"The raw MCP approval response.\"\"\"\n\n    type: Literal[\"mcp_approval_response_item\"] = \"mcp_approval_response_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: McpApprovalResponse\n</code></pre> <p>The raw MCP approval response.</p>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.CompactionItem","title":"CompactionItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[TResponseInputItem]</code></p> <p>Represents a compaction item from responses.compact.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass CompactionItem(RunItemBase[TResponseInputItem]):\n    \"\"\"Represents a compaction item from responses.compact.\"\"\"\n\n    type: Literal[\"compaction_item\"] = \"compaction_item\"\n\n    def to_input_item(self) -&gt; TResponseInputItem:\n        \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n        return self.raw_item\n</code></pre>"},{"location":"ref/items/#agents.items.CompactionItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.CompactionItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: T\n</code></pre> <p>The raw Responses item from the run. This will always be either an output item (i.e. <code>openai.types.responses.ResponseOutputItem</code> or an input item (i.e. <code>openai.types.responses.ResponseInputItemParam</code>).</p>"},{"location":"ref/items/#agents.items.CompactionItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    return self.raw_item\n</code></pre>"},{"location":"ref/items/#agents.items.CompactionItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.ToolApprovalItem","title":"ToolApprovalItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[Any]</code></p> <p>Tool call that requires approval before execution.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass ToolApprovalItem(RunItemBase[Any]):\n    \"\"\"Tool call that requires approval before execution.\"\"\"\n\n    raw_item: ToolApprovalRawItem\n    \"\"\"Raw tool call awaiting approval (function, hosted, shell, etc.).\"\"\"\n\n    tool_name: str | None = None\n    \"\"\"Tool name for approval tracking; falls back to raw_item.name when absent.\"\"\"\n\n    type: Literal[\"tool_approval_item\"] = \"tool_approval_item\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Populate tool_name from the raw item if not provided.\"\"\"\n        if self.tool_name is None:\n            # Extract name from raw_item - handle different types\n            if isinstance(self.raw_item, dict):\n                self.tool_name = self.raw_item.get(\"name\")\n            elif hasattr(self.raw_item, \"name\"):\n                self.tool_name = self.raw_item.name\n            else:\n                self.tool_name = None\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Hash by object identity to keep distinct approvals separate.\"\"\"\n        return object.__hash__(self)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Equality is based on object identity.\"\"\"\n        return self is other\n\n    @property\n    def name(self) -&gt; str | None:\n        \"\"\"Return the tool name from tool_name or raw_item (backwards compatible).\"\"\"\n        if self.tool_name:\n            return self.tool_name\n        if isinstance(self.raw_item, dict):\n            candidate = self.raw_item.get(\"name\") or self.raw_item.get(\"tool_name\")\n        else:\n            candidate = getattr(self.raw_item, \"name\", None) or getattr(\n                self.raw_item, \"tool_name\", None\n            )\n        return str(candidate) if candidate is not None else None\n\n    @property\n    def arguments(self) -&gt; str | None:\n        \"\"\"Return tool call arguments if present on the raw item.\"\"\"\n        candidate: Any | None = None\n        if isinstance(self.raw_item, dict):\n            candidate = self.raw_item.get(\"arguments\")\n            if candidate is None:\n                candidate = self.raw_item.get(\"params\") or self.raw_item.get(\"input\")\n        elif hasattr(self.raw_item, \"arguments\"):\n            candidate = self.raw_item.arguments\n        elif hasattr(self.raw_item, \"params\") or hasattr(self.raw_item, \"input\"):\n            candidate = getattr(self.raw_item, \"params\", None) or getattr(\n                self.raw_item, \"input\", None\n            )\n        if candidate is None:\n            return None\n        if isinstance(candidate, str):\n            return candidate\n        try:\n            return json.dumps(candidate)\n        except (TypeError, ValueError):\n            return str(candidate)\n\n    def _extract_call_id(self) -&gt; str | None:\n        \"\"\"Return call identifier from the raw item.\"\"\"\n        if isinstance(self.raw_item, dict):\n            return self.raw_item.get(\"call_id\") or self.raw_item.get(\"id\")\n        return getattr(self.raw_item, \"call_id\", None) or getattr(self.raw_item, \"id\", None)\n\n    @property\n    def call_id(self) -&gt; str | None:\n        \"\"\"Return call identifier from the raw item.\"\"\"\n        return self._extract_call_id()\n\n    def to_input_item(self) -&gt; TResponseInputItem:\n        \"\"\"ToolApprovalItem should never be sent as input; raise to surface misuse.\"\"\"\n        raise AgentsException(\n            \"ToolApprovalItem cannot be converted to an input item. \"\n            \"These items should be filtered out before preparing input for the API.\"\n        )\n</code></pre>"},{"location":"ref/items/#agents.items.ToolApprovalItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ToolApprovalRawItem\n</code></pre> <p>Raw tool call awaiting approval (function, hosted, shell, etc.).</p>"},{"location":"ref/items/#agents.items.ToolApprovalItem.tool_name","title":"tool_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_name: str | None = None\n</code></pre> <p>Tool name for approval tracking; falls back to raw_item.name when absent.</p>"},{"location":"ref/items/#agents.items.ToolApprovalItem.name","title":"name  <code>property</code>","text":"<pre><code>name: str | None\n</code></pre> <p>Return the tool name from tool_name or raw_item (backwards compatible).</p>"},{"location":"ref/items/#agents.items.ToolApprovalItem.arguments","title":"arguments  <code>property</code>","text":"<pre><code>arguments: str | None\n</code></pre> <p>Return tool call arguments if present on the raw item.</p>"},{"location":"ref/items/#agents.items.ToolApprovalItem.call_id","title":"call_id  <code>property</code>","text":"<pre><code>call_id: str | None\n</code></pre> <p>Return call identifier from the raw item.</p>"},{"location":"ref/items/#agents.items.ToolApprovalItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.ToolApprovalItem.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Populate tool_name from the raw item if not provided.</p> Source code in <code>src/agents/items.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Populate tool_name from the raw item if not provided.\"\"\"\n    if self.tool_name is None:\n        # Extract name from raw_item - handle different types\n        if isinstance(self.raw_item, dict):\n            self.tool_name = self.raw_item.get(\"name\")\n        elif hasattr(self.raw_item, \"name\"):\n            self.tool_name = self.raw_item.name\n        else:\n            self.tool_name = None\n</code></pre>"},{"location":"ref/items/#agents.items.ToolApprovalItem.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Hash by object identity to keep distinct approvals separate.</p> Source code in <code>src/agents/items.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash by object identity to keep distinct approvals separate.\"\"\"\n    return object.__hash__(self)\n</code></pre>"},{"location":"ref/items/#agents.items.ToolApprovalItem.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: object) -&gt; bool\n</code></pre> <p>Equality is based on object identity.</p> Source code in <code>src/agents/items.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Equality is based on object identity.\"\"\"\n    return self is other\n</code></pre>"},{"location":"ref/items/#agents.items.ToolApprovalItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>ToolApprovalItem should never be sent as input; raise to surface misuse.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"ToolApprovalItem should never be sent as input; raise to surface misuse.\"\"\"\n    raise AgentsException(\n        \"ToolApprovalItem cannot be converted to an input item. \"\n        \"These items should be filtered out before preparing input for the API.\"\n    )\n</code></pre>"},{"location":"ref/items/#agents.items.ToolApprovalItem.release_agent","title":"release_agent","text":"<pre><code>release_agent() -&gt; None\n</code></pre> <p>Release the strong reference to the agent while keeping a weak reference.</p> Source code in <code>src/agents/items.py</code> <pre><code>def release_agent(self) -&gt; None:\n    \"\"\"Release the strong reference to the agent while keeping a weak reference.\"\"\"\n    if \"agent\" not in self.__dict__:\n        return\n    agent = self.__dict__[\"agent\"]\n    if agent is None:\n        return\n    self._agent_ref = weakref.ref(agent) if agent is not None else None\n    # Set to None instead of deleting so dataclass repr/asdict keep working.\n    self.__dict__[\"agent\"] = None\n</code></pre>"},{"location":"ref/items/#agents.items.ModelResponse","title":"ModelResponse","text":"Source code in <code>src/agents/items.py</code> <pre><code>@pydantic.dataclasses.dataclass\nclass ModelResponse:\n    output: list[TResponseOutputItem]\n    \"\"\"A list of outputs (messages, tool calls, etc) generated by the model\"\"\"\n\n    usage: Usage\n    \"\"\"The usage information for the response.\"\"\"\n\n    response_id: str | None\n    \"\"\"An ID for the response which can be used to refer to the response in subsequent calls to the\n    model. Not supported by all model providers.\n    If using OpenAI models via the Responses API, this is the `response_id` parameter, and it can\n    be passed to `Runner.run`.\n    \"\"\"\n\n    def to_input_items(self) -&gt; list[TResponseInputItem]:\n        \"\"\"Convert the output into a list of input items suitable for passing to the model.\"\"\"\n        # We happen to know that the shape of the Pydantic output items are the same as the\n        # equivalent TypedDict input items, so we can just convert each one.\n        # This is also tested via unit tests.\n        return [it.model_dump(exclude_unset=True) for it in self.output]  # type: ignore\n</code></pre>"},{"location":"ref/items/#agents.items.ModelResponse.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: list[TResponseOutputItem]\n</code></pre> <p>A list of outputs (messages, tool calls, etc) generated by the model</p>"},{"location":"ref/items/#agents.items.ModelResponse.usage","title":"usage  <code>instance-attribute</code>","text":"<pre><code>usage: Usage\n</code></pre> <p>The usage information for the response.</p>"},{"location":"ref/items/#agents.items.ModelResponse.response_id","title":"response_id  <code>instance-attribute</code>","text":"<pre><code>response_id: str | None\n</code></pre> <p>An ID for the response which can be used to refer to the response in subsequent calls to the model. Not supported by all model providers. If using OpenAI models via the Responses API, this is the <code>response_id</code> parameter, and it can be passed to <code>Runner.run</code>.</p>"},{"location":"ref/items/#agents.items.ModelResponse.to_input_items","title":"to_input_items","text":"<pre><code>to_input_items() -&gt; list[TResponseInputItem]\n</code></pre> <p>Convert the output into a list of input items suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_items(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Convert the output into a list of input items suitable for passing to the model.\"\"\"\n    # We happen to know that the shape of the Pydantic output items are the same as the\n    # equivalent TypedDict input items, so we can just convert each one.\n    # This is also tested via unit tests.\n    return [it.model_dump(exclude_unset=True) for it in self.output]  # type: ignore\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers","title":"ItemHelpers","text":"Source code in <code>src/agents/items.py</code> <pre><code>class ItemHelpers:\n    @classmethod\n    def extract_last_content(cls, message: TResponseOutputItem) -&gt; str:\n        \"\"\"Extracts the last text content or refusal from a message.\"\"\"\n        if not isinstance(message, ResponseOutputMessage):\n            return \"\"\n\n        if not message.content:\n            return \"\"\n        last_content = message.content[-1]\n        if isinstance(last_content, ResponseOutputText):\n            return last_content.text\n        elif isinstance(last_content, ResponseOutputRefusal):\n            return last_content.refusal\n        else:\n            raise ModelBehaviorError(f\"Unexpected content type: {type(last_content)}\")\n\n    @classmethod\n    def extract_last_text(cls, message: TResponseOutputItem) -&gt; str | None:\n        \"\"\"Extracts the last text content from a message, if any. Ignores refusals.\"\"\"\n        if isinstance(message, ResponseOutputMessage):\n            if not message.content:\n                return None\n            last_content = message.content[-1]\n            if isinstance(last_content, ResponseOutputText):\n                return last_content.text\n\n        return None\n\n    @classmethod\n    def input_to_new_input_list(\n        cls, input: str | list[TResponseInputItem]\n    ) -&gt; list[TResponseInputItem]:\n        \"\"\"Converts a string or list of input items into a list of input items.\"\"\"\n        if isinstance(input, str):\n            return [\n                {\n                    \"content\": input,\n                    \"role\": \"user\",\n                }\n            ]\n        return input.copy()\n\n    @classmethod\n    def text_message_outputs(cls, items: list[RunItem]) -&gt; str:\n        \"\"\"Concatenates all the text content from a list of message output items.\"\"\"\n        text = \"\"\n        for item in items:\n            if isinstance(item, MessageOutputItem):\n                text += cls.text_message_output(item)\n        return text\n\n    @classmethod\n    def text_message_output(cls, message: MessageOutputItem) -&gt; str:\n        \"\"\"Extracts all the text content from a single message output item.\"\"\"\n        text = \"\"\n        for item in message.raw_item.content:\n            if isinstance(item, ResponseOutputText):\n                text += item.text\n        return text\n\n    @classmethod\n    def tool_call_output_item(\n        cls, tool_call: ResponseFunctionToolCall, output: Any\n    ) -&gt; FunctionCallOutput:\n        \"\"\"Creates a tool call output item from a tool call and its output.\n\n        Accepts either plain values (stringified) or structured outputs using\n        input_text/input_image/input_file shapes. Structured outputs may be\n        provided as Pydantic models or dicts, or an iterable of such items.\n        \"\"\"\n\n        converted_output = cls._convert_tool_output(output)\n\n        return {\n            \"call_id\": tool_call.call_id,\n            \"output\": converted_output,\n            \"type\": \"function_call_output\",\n        }\n\n    @classmethod\n    def _convert_tool_output(cls, output: Any) -&gt; str | ResponseFunctionCallOutputItemListParam:\n        \"\"\"Converts a tool return value into an output acceptable by the Responses API.\"\"\"\n\n        # If the output is either a single or list of the known structured output types, convert to\n        # ResponseFunctionCallOutputItemListParam. Else, just stringify.\n        if isinstance(output, (list, tuple)):\n            maybe_converted_output_list = [\n                cls._maybe_get_output_as_structured_function_output(item) for item in output\n            ]\n            if all(maybe_converted_output_list):\n                return [\n                    cls._convert_single_tool_output_pydantic_model(item)\n                    for item in maybe_converted_output_list\n                    if item is not None\n                ]\n            else:\n                return str(output)\n        else:\n            maybe_converted_output = cls._maybe_get_output_as_structured_function_output(output)\n            if maybe_converted_output:\n                return [cls._convert_single_tool_output_pydantic_model(maybe_converted_output)]\n            else:\n                return str(output)\n\n    @classmethod\n    def _maybe_get_output_as_structured_function_output(\n        cls, output: Any\n    ) -&gt; ValidToolOutputPydanticModels | None:\n        if isinstance(output, (ToolOutputText, ToolOutputImage, ToolOutputFileContent)):\n            return output\n        elif isinstance(output, dict):\n            # Require explicit 'type' field in dict to be considered a structured output\n            if \"type\" not in output:\n                return None\n            try:\n                return ValidToolOutputPydanticModelsTypeAdapter.validate_python(output)\n            except pydantic.ValidationError:\n                logger.debug(\"dict was not a valid tool output pydantic model\")\n                return None\n\n        return None\n\n    @classmethod\n    def _convert_single_tool_output_pydantic_model(\n        cls, output: ValidToolOutputPydanticModels\n    ) -&gt; ResponseFunctionCallOutputItemParam:\n        if isinstance(output, ToolOutputText):\n            return {\"type\": \"input_text\", \"text\": output.text}\n        elif isinstance(output, ToolOutputImage):\n            # Forward all provided optional fields so the Responses API receives\n            # the correct identifiers and settings for the image resource.\n            result: ResponseInputImageContentParam = {\"type\": \"input_image\"}\n            if output.image_url is not None:\n                result[\"image_url\"] = output.image_url\n            if output.file_id is not None:\n                result[\"file_id\"] = output.file_id\n            if output.detail is not None:\n                result[\"detail\"] = output.detail\n            return result\n        elif isinstance(output, ToolOutputFileContent):\n            # Forward all provided optional fields so the Responses API receives\n            # the correct identifiers and metadata for the file resource.\n            result_file: ResponseInputFileContentParam = {\"type\": \"input_file\"}\n            if output.file_data is not None:\n                result_file[\"file_data\"] = output.file_data\n            if output.file_url is not None:\n                result_file[\"file_url\"] = output.file_url\n            if output.file_id is not None:\n                result_file[\"file_id\"] = output.file_id\n            if output.filename is not None:\n                result_file[\"filename\"] = output.filename\n            return result_file\n        else:\n            assert_never(output)\n            raise ValueError(f\"Unexpected tool output type: {output}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.extract_last_content","title":"extract_last_content  <code>classmethod</code>","text":"<pre><code>extract_last_content(message: TResponseOutputItem) -&gt; str\n</code></pre> <p>Extracts the last text content or refusal from a message.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef extract_last_content(cls, message: TResponseOutputItem) -&gt; str:\n    \"\"\"Extracts the last text content or refusal from a message.\"\"\"\n    if not isinstance(message, ResponseOutputMessage):\n        return \"\"\n\n    if not message.content:\n        return \"\"\n    last_content = message.content[-1]\n    if isinstance(last_content, ResponseOutputText):\n        return last_content.text\n    elif isinstance(last_content, ResponseOutputRefusal):\n        return last_content.refusal\n    else:\n        raise ModelBehaviorError(f\"Unexpected content type: {type(last_content)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.extract_last_text","title":"extract_last_text  <code>classmethod</code>","text":"<pre><code>extract_last_text(\n    message: TResponseOutputItem,\n) -&gt; str | None\n</code></pre> <p>Extracts the last text content from a message, if any. Ignores refusals.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef extract_last_text(cls, message: TResponseOutputItem) -&gt; str | None:\n    \"\"\"Extracts the last text content from a message, if any. Ignores refusals.\"\"\"\n    if isinstance(message, ResponseOutputMessage):\n        if not message.content:\n            return None\n        last_content = message.content[-1]\n        if isinstance(last_content, ResponseOutputText):\n            return last_content.text\n\n    return None\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.input_to_new_input_list","title":"input_to_new_input_list  <code>classmethod</code>","text":"<pre><code>input_to_new_input_list(\n    input: str | list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Converts a string or list of input items into a list of input items.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef input_to_new_input_list(\n    cls, input: str | list[TResponseInputItem]\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Converts a string or list of input items into a list of input items.\"\"\"\n    if isinstance(input, str):\n        return [\n            {\n                \"content\": input,\n                \"role\": \"user\",\n            }\n        ]\n    return input.copy()\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.text_message_outputs","title":"text_message_outputs  <code>classmethod</code>","text":"<pre><code>text_message_outputs(items: list[RunItem]) -&gt; str\n</code></pre> <p>Concatenates all the text content from a list of message output items.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef text_message_outputs(cls, items: list[RunItem]) -&gt; str:\n    \"\"\"Concatenates all the text content from a list of message output items.\"\"\"\n    text = \"\"\n    for item in items:\n        if isinstance(item, MessageOutputItem):\n            text += cls.text_message_output(item)\n    return text\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.text_message_output","title":"text_message_output  <code>classmethod</code>","text":"<pre><code>text_message_output(message: MessageOutputItem) -&gt; str\n</code></pre> <p>Extracts all the text content from a single message output item.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef text_message_output(cls, message: MessageOutputItem) -&gt; str:\n    \"\"\"Extracts all the text content from a single message output item.\"\"\"\n    text = \"\"\n    for item in message.raw_item.content:\n        if isinstance(item, ResponseOutputText):\n            text += item.text\n    return text\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.tool_call_output_item","title":"tool_call_output_item  <code>classmethod</code>","text":"<pre><code>tool_call_output_item(\n    tool_call: ResponseFunctionToolCall, output: Any\n) -&gt; FunctionCallOutput\n</code></pre> <p>Creates a tool call output item from a tool call and its output.</p> <p>Accepts either plain values (stringified) or structured outputs using input_text/input_image/input_file shapes. Structured outputs may be provided as Pydantic models or dicts, or an iterable of such items.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef tool_call_output_item(\n    cls, tool_call: ResponseFunctionToolCall, output: Any\n) -&gt; FunctionCallOutput:\n    \"\"\"Creates a tool call output item from a tool call and its output.\n\n    Accepts either plain values (stringified) or structured outputs using\n    input_text/input_image/input_file shapes. Structured outputs may be\n    provided as Pydantic models or dicts, or an iterable of such items.\n    \"\"\"\n\n    converted_output = cls._convert_tool_output(output)\n\n    return {\n        \"call_id\": tool_call.call_id,\n        \"output\": converted_output,\n        \"type\": \"function_call_output\",\n    }\n</code></pre>"},{"location":"ref/lifecycle/","title":"<code>Lifecycle</code>","text":""},{"location":"ref/lifecycle/#agents.lifecycle.RunHooks","title":"RunHooks  <code>module-attribute</code>","text":"<pre><code>RunHooks = RunHooksBase[TContext, Agent]\n</code></pre> <p>Run hooks when using <code>Agent</code>.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooks","title":"AgentHooks  <code>module-attribute</code>","text":"<pre><code>AgentHooks = AgentHooksBase[TContext, Agent]\n</code></pre> <p>Agent hooks for <code>Agent</code>s.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase","title":"RunHooksBase","text":"<p>               Bases: <code>Generic[TContext, TAgent]</code></p> <p>A class that receives callbacks on various lifecycle events in an agent run. Subclass and override the methods you need.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_llm_start","title":"on_llm_start  <code>async</code>","text":"<pre><code>on_llm_start(\n    context: RunContextWrapper[TContext],\n    agent: Agent[TContext],\n    system_prompt: Optional[str],\n    input_items: list[TResponseInputItem],\n) -&gt; None\n</code></pre> <p>Called just before invoking the LLM for this agent.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_llm_end","title":"on_llm_end  <code>async</code>","text":"<pre><code>on_llm_end(\n    context: RunContextWrapper[TContext],\n    agent: Agent[TContext],\n    response: ModelResponse,\n) -&gt; None\n</code></pre> <p>Called immediately after the LLM call returns for this agent.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_agent_start","title":"on_agent_start  <code>async</code>","text":"<pre><code>on_agent_start(\n    context: AgentHookContext[TContext], agent: TAgent\n) -&gt; None\n</code></pre> <p>Called before the agent is invoked. Called each time the current agent changes.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>AgentHookContext[TContext]</code> <p>The agent hook context.</p> required <code>agent</code> <code>TAgent</code> <p>The agent that is about to be invoked.</p> required"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_agent_end","title":"on_agent_end  <code>async</code>","text":"<pre><code>on_agent_end(\n    context: AgentHookContext[TContext],\n    agent: TAgent,\n    output: Any,\n) -&gt; None\n</code></pre> <p>Called when the agent produces a final output.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>AgentHookContext[TContext]</code> <p>The agent hook context.</p> required <code>agent</code> <code>TAgent</code> <p>The agent that produced the output.</p> required <code>output</code> <code>Any</code> <p>The final output produced by the agent.</p> required"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_handoff","title":"on_handoff  <code>async</code>","text":"<pre><code>on_handoff(\n    context: RunContextWrapper[TContext],\n    from_agent: TAgent,\n    to_agent: TAgent,\n) -&gt; None\n</code></pre> <p>Called when a handoff occurs.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_tool_start","title":"on_tool_start  <code>async</code>","text":"<pre><code>on_tool_start(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n) -&gt; None\n</code></pre> <p>Called immediately before a local tool is invoked.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_tool_end","title":"on_tool_end  <code>async</code>","text":"<pre><code>on_tool_end(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n    result: str,\n) -&gt; None\n</code></pre> <p>Called immediately after a local tool is invoked.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase","title":"AgentHooksBase","text":"<p>               Bases: <code>Generic[TContext, TAgent]</code></p> <p>A class that receives callbacks on various lifecycle events for a specific agent. You can set this on <code>agent.hooks</code> to receive events for that specific agent.</p> <p>Subclass and override the methods you need.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_start","title":"on_start  <code>async</code>","text":"<pre><code>on_start(\n    context: AgentHookContext[TContext], agent: TAgent\n) -&gt; None\n</code></pre> <p>Called before the agent is invoked. Called each time the running agent is changed to this agent.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>AgentHookContext[TContext]</code> <p>The agent hook context.</p> required <code>agent</code> <code>TAgent</code> <p>This agent instance.</p> required"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_end","title":"on_end  <code>async</code>","text":"<pre><code>on_end(\n    context: AgentHookContext[TContext],\n    agent: TAgent,\n    output: Any,\n) -&gt; None\n</code></pre> <p>Called when the agent produces a final output.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>AgentHookContext[TContext]</code> <p>The agent hook context.</p> required <code>agent</code> <code>TAgent</code> <p>This agent instance.</p> required <code>output</code> <code>Any</code> <p>The final output produced by the agent.</p> required"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_handoff","title":"on_handoff  <code>async</code>","text":"<pre><code>on_handoff(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    source: TAgent,\n) -&gt; None\n</code></pre> <p>Called when the agent is being handed off to. The <code>source</code> is the agent that is handing off to this agent.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_tool_start","title":"on_tool_start  <code>async</code>","text":"<pre><code>on_tool_start(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n) -&gt; None\n</code></pre> <p>Called immediately before a local tool is invoked.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_tool_end","title":"on_tool_end  <code>async</code>","text":"<pre><code>on_tool_end(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n    result: str,\n) -&gt; None\n</code></pre> <p>Called immediately after a local tool is invoked.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_llm_start","title":"on_llm_start  <code>async</code>","text":"<pre><code>on_llm_start(\n    context: RunContextWrapper[TContext],\n    agent: Agent[TContext],\n    system_prompt: Optional[str],\n    input_items: list[TResponseInputItem],\n) -&gt; None\n</code></pre> <p>Called immediately before the agent issues an LLM call.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_llm_end","title":"on_llm_end  <code>async</code>","text":"<pre><code>on_llm_end(\n    context: RunContextWrapper[TContext],\n    agent: Agent[TContext],\n    response: ModelResponse,\n) -&gt; None\n</code></pre> <p>Called immediately after the agent receives the LLM response.</p>"},{"location":"ref/logger/","title":"<code>Logger</code>","text":""},{"location":"ref/memory/","title":"Memory","text":""},{"location":"ref/memory/#agents.memory.Session","title":"Session","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for session implementations.</p> <p>Session stores conversation history for a specific session, allowing agents to maintain context without requiring explicit manual memory management.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>@runtime_checkable\nclass Session(Protocol):\n    \"\"\"Protocol for session implementations.\n\n    Session stores conversation history for a specific session, allowing\n    agents to maintain context without requiring explicit manual memory management.\n    \"\"\"\n\n    session_id: str\n    session_settings: SessionSettings | None = None\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, retrieves all items.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        ...\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        ...\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        ...\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, retrieves all items.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, retrieves all items.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession","title":"SQLiteSession","text":"<p>               Bases: <code>SessionABC</code></p> <p>SQLite-based implementation of session storage.</p> <p>This implementation stores conversation history in a SQLite database. By default, uses an in-memory database that is lost when the process ends. For persistent storage, provide a file path.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>class SQLiteSession(SessionABC):\n    \"\"\"SQLite-based implementation of session storage.\n\n    This implementation stores conversation history in a SQLite database.\n    By default, uses an in-memory database that is lost when the process ends.\n    For persistent storage, provide a file path.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        db_path: str | Path = \":memory:\",\n        sessions_table: str = \"agent_sessions\",\n        messages_table: str = \"agent_messages\",\n        session_settings: SessionSettings | None = None,\n    ):\n        \"\"\"Initialize the SQLite session.\n\n        Args:\n            session_id: Unique identifier for the conversation session\n            db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n            sessions_table: Name of the table to store session metadata. Defaults to\n                'agent_sessions'\n            messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n            session_settings: Session configuration settings including default limit for\n                retrieving items. If None, uses default SessionSettings().\n        \"\"\"\n        self.session_id = session_id\n        self.session_settings = session_settings or SessionSettings()\n        self.db_path = db_path\n        self.sessions_table = sessions_table\n        self.messages_table = messages_table\n        self._local = threading.local()\n        self._lock = threading.Lock()\n\n        # For in-memory databases, we need a shared connection to avoid thread isolation\n        # For file databases, we use thread-local connections for better concurrency\n        self._is_memory_db = str(db_path) == \":memory:\"\n        if self._is_memory_db:\n            self._shared_connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n            self._shared_connection.execute(\"PRAGMA journal_mode=WAL\")\n            self._init_db_for_connection(self._shared_connection)\n        else:\n            # For file databases, initialize the schema once since it persists\n            init_conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n            init_conn.execute(\"PRAGMA journal_mode=WAL\")\n            self._init_db_for_connection(init_conn)\n            init_conn.close()\n\n    def _get_connection(self) -&gt; sqlite3.Connection:\n        \"\"\"Get a database connection.\"\"\"\n        if self._is_memory_db:\n            # Use shared connection for in-memory database to avoid thread isolation\n            return self._shared_connection\n        else:\n            # Use thread-local connections for file databases\n            if not hasattr(self._local, \"connection\"):\n                self._local.connection = sqlite3.connect(\n                    str(self.db_path),\n                    check_same_thread=False,\n                )\n                self._local.connection.execute(\"PRAGMA journal_mode=WAL\")\n            assert isinstance(self._local.connection, sqlite3.Connection), (\n                f\"Expected sqlite3.Connection, got {type(self._local.connection)}\"\n            )\n            return self._local.connection\n\n    def _init_db_for_connection(self, conn: sqlite3.Connection) -&gt; None:\n        \"\"\"Initialize the database schema for a specific connection.\"\"\"\n        conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.sessions_table} (\n                session_id TEXT PRIMARY KEY,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\"\n        )\n\n        conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.messages_table} (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL,\n                message_data TEXT NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (session_id) REFERENCES {self.sessions_table} (session_id)\n                    ON DELETE CASCADE\n            )\n        \"\"\"\n        )\n\n        conn.execute(\n            f\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_{self.messages_table}_session_id\n            ON {self.messages_table} (session_id, id)\n        \"\"\"\n        )\n\n        conn.commit()\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        def _get_items_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                if session_limit is None:\n                    # Fetch all items in chronological order\n                    cursor = conn.execute(\n                        f\"\"\"\n                        SELECT message_data FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY id ASC\n                    \"\"\",\n                        (self.session_id,),\n                    )\n                else:\n                    # Fetch the latest N items in chronological order\n                    cursor = conn.execute(\n                        f\"\"\"\n                        SELECT message_data FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY id DESC\n                        LIMIT ?\n                        \"\"\",\n                        (self.session_id, session_limit),\n                    )\n\n                rows = cursor.fetchall()\n\n                # Reverse to get chronological order when using DESC\n                if session_limit is not None:\n                    rows = list(reversed(rows))\n\n                items = []\n                for (message_data,) in rows:\n                    try:\n                        item = json.loads(message_data)\n                        items.append(item)\n                    except json.JSONDecodeError:\n                        # Skip invalid JSON entries\n                        continue\n\n                return items\n\n        return await asyncio.to_thread(_get_items_sync)\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        if not items:\n            return\n\n        def _add_items_sync():\n            conn = self._get_connection()\n\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Ensure session exists\n                conn.execute(\n                    f\"\"\"\n                    INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n                \"\"\",\n                    (self.session_id,),\n                )\n\n                # Add items\n                message_data = [(self.session_id, json.dumps(item)) for item in items]\n                conn.executemany(\n                    f\"\"\"\n                    INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n                \"\"\",\n                    message_data,\n                )\n\n                # Update session timestamp\n                conn.execute(\n                    f\"\"\"\n                    UPDATE {self.sessions_table}\n                    SET updated_at = CURRENT_TIMESTAMP\n                    WHERE session_id = ?\n                \"\"\",\n                    (self.session_id,),\n                )\n\n                conn.commit()\n\n        await asyncio.to_thread(_add_items_sync)\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n\n        def _pop_item_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Use DELETE with RETURNING to atomically delete and return the most recent item\n                cursor = conn.execute(\n                    f\"\"\"\n                    DELETE FROM {self.messages_table}\n                    WHERE id = (\n                        SELECT id FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY id DESC\n                        LIMIT 1\n                    )\n                    RETURNING message_data\n                    \"\"\",\n                    (self.session_id,),\n                )\n\n                result = cursor.fetchone()\n                conn.commit()\n\n                if result:\n                    message_data = result[0]\n                    try:\n                        item = json.loads(message_data)\n                        return item\n                    except json.JSONDecodeError:\n                        # Return None for corrupted JSON entries (already deleted)\n                        return None\n\n                return None\n\n        return await asyncio.to_thread(_pop_item_sync)\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n\n        def _clear_session_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                conn.execute(\n                    f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                    (self.session_id,),\n                )\n                conn.execute(\n                    f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                    (self.session_id,),\n                )\n                conn.commit()\n\n        await asyncio.to_thread(_clear_session_sync)\n\n    def close(self) -&gt; None:\n        \"\"\"Close the database connection.\"\"\"\n        if self._is_memory_db:\n            if hasattr(self, \"_shared_connection\"):\n                self._shared_connection.close()\n        else:\n            if hasattr(self._local, \"connection\"):\n                self._local.connection.close()\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n    session_settings: SessionSettings | None = None,\n)\n</code></pre> <p>Initialize the SQLite session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Unique identifier for the conversation session</p> required <code>db_path</code> <code>str | Path</code> <p>Path to the SQLite database file. Defaults to ':memory:' (in-memory database)</p> <code>':memory:'</code> <code>sessions_table</code> <code>str</code> <p>Name of the table to store session metadata. Defaults to 'agent_sessions'</p> <code>'agent_sessions'</code> <code>messages_table</code> <code>str</code> <p>Name of the table to store message data. Defaults to 'agent_messages'</p> <code>'agent_messages'</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings including default limit for retrieving items. If None, uses default SessionSettings().</p> <code>None</code> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n    session_settings: SessionSettings | None = None,\n):\n    \"\"\"Initialize the SQLite session.\n\n    Args:\n        session_id: Unique identifier for the conversation session\n        db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n        sessions_table: Name of the table to store session metadata. Defaults to\n            'agent_sessions'\n        messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n        session_settings: Session configuration settings including default limit for\n            retrieving items. If None, uses default SessionSettings().\n    \"\"\"\n    self.session_id = session_id\n    self.session_settings = session_settings or SessionSettings()\n    self.db_path = db_path\n    self.sessions_table = sessions_table\n    self.messages_table = messages_table\n    self._local = threading.local()\n    self._lock = threading.Lock()\n\n    # For in-memory databases, we need a shared connection to avoid thread isolation\n    # For file databases, we use thread-local connections for better concurrency\n    self._is_memory_db = str(db_path) == \":memory:\"\n    if self._is_memory_db:\n        self._shared_connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n        self._shared_connection.execute(\"PRAGMA journal_mode=WAL\")\n        self._init_db_for_connection(self._shared_connection)\n    else:\n        # For file databases, initialize the schema once since it persists\n        init_conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n        init_conn.execute(\"PRAGMA journal_mode=WAL\")\n        self._init_db_for_connection(init_conn)\n        init_conn.close()\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, uses session_settings.limit.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    session_limit = resolve_session_limit(limit, self.session_settings)\n\n    def _get_items_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            if session_limit is None:\n                # Fetch all items in chronological order\n                cursor = conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id ASC\n                \"\"\",\n                    (self.session_id,),\n                )\n            else:\n                # Fetch the latest N items in chronological order\n                cursor = conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id DESC\n                    LIMIT ?\n                    \"\"\",\n                    (self.session_id, session_limit),\n                )\n\n            rows = cursor.fetchall()\n\n            # Reverse to get chronological order when using DESC\n            if session_limit is not None:\n                rows = list(reversed(rows))\n\n            items = []\n            for (message_data,) in rows:\n                try:\n                    item = json.loads(message_data)\n                    items.append(item)\n                except json.JSONDecodeError:\n                    # Skip invalid JSON entries\n                    continue\n\n            return items\n\n    return await asyncio.to_thread(_get_items_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    if not items:\n        return\n\n    def _add_items_sync():\n        conn = self._get_connection()\n\n        with self._lock if self._is_memory_db else threading.Lock():\n            # Ensure session exists\n            conn.execute(\n                f\"\"\"\n                INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n            \"\"\",\n                (self.session_id,),\n            )\n\n            # Add items\n            message_data = [(self.session_id, json.dumps(item)) for item in items]\n            conn.executemany(\n                f\"\"\"\n                INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n            \"\"\",\n                message_data,\n            )\n\n            # Update session timestamp\n            conn.execute(\n                f\"\"\"\n                UPDATE {self.sessions_table}\n                SET updated_at = CURRENT_TIMESTAMP\n                WHERE session_id = ?\n            \"\"\",\n                (self.session_id,),\n            )\n\n            conn.commit()\n\n    await asyncio.to_thread(_add_items_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n\n    def _pop_item_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            # Use DELETE with RETURNING to atomically delete and return the most recent item\n            cursor = conn.execute(\n                f\"\"\"\n                DELETE FROM {self.messages_table}\n                WHERE id = (\n                    SELECT id FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id DESC\n                    LIMIT 1\n                )\n                RETURNING message_data\n                \"\"\",\n                (self.session_id,),\n            )\n\n            result = cursor.fetchone()\n            conn.commit()\n\n            if result:\n                message_data = result[0]\n                try:\n                    item = json.loads(message_data)\n                    return item\n                except json.JSONDecodeError:\n                    # Return None for corrupted JSON entries (already deleted)\n                    return None\n\n            return None\n\n    return await asyncio.to_thread(_pop_item_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n\n    def _clear_session_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            conn.execute(\n                f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.execute(\n                f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.commit()\n\n    await asyncio.to_thread(_clear_session_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._is_memory_db:\n        if hasattr(self, \"_shared_connection\"):\n            self._shared_connection.close()\n    else:\n        if hasattr(self._local, \"connection\"):\n            self._local.connection.close()\n</code></pre>"},{"location":"ref/memory/#agents.memory.OpenAIConversationsSession","title":"OpenAIConversationsSession","text":"<p>               Bases: <code>SessionABC</code></p> Source code in <code>src/agents/memory/openai_conversations_session.py</code> <pre><code>class OpenAIConversationsSession(SessionABC):\n    def __init__(\n        self,\n        *,\n        conversation_id: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        session_settings: SessionSettings | None = None,\n    ):\n        self._session_id: str | None = conversation_id\n        self.session_settings = session_settings or SessionSettings()\n        _openai_client = openai_client\n        if _openai_client is None:\n            _openai_client = get_default_openai_client() or AsyncOpenAI()\n        # this never be None here\n        self._openai_client: AsyncOpenAI = _openai_client\n\n    @property\n    def session_id(self) -&gt; str:\n        \"\"\"Get the session ID (conversation ID).\n\n        Returns:\n            The conversation ID for this session.\n\n        Raises:\n            ValueError: If the session has not been initialized yet.\n                Call any session method (get_items, add_items, etc.) first\n                to trigger lazy initialization.\n        \"\"\"\n        if self._session_id is None:\n            raise ValueError(\n                \"Session ID not yet available. The session is lazily initialized \"\n                \"on first API call. Call get_items(), add_items(), or similar first.\"\n            )\n        return self._session_id\n\n    @session_id.setter\n    def session_id(self, value: str) -&gt; None:\n        \"\"\"Set the session ID (conversation ID).\"\"\"\n        self._session_id = value\n\n    async def _get_session_id(self) -&gt; str:\n        if self._session_id is None:\n            self._session_id = await start_openai_conversations_session(self._openai_client)\n        return self._session_id\n\n    async def _clear_session_id(self) -&gt; None:\n        self._session_id = None\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        session_id = await self._get_session_id()\n\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        all_items = []\n        if session_limit is None:\n            async for item in self._openai_client.conversations.items.list(\n                conversation_id=session_id,\n                order=\"asc\",\n            ):\n                # calling model_dump() to make this serializable\n                all_items.append(item.model_dump(exclude_unset=True))\n        else:\n            async for item in self._openai_client.conversations.items.list(\n                conversation_id=session_id,\n                limit=session_limit,\n                order=\"desc\",\n            ):\n                # calling model_dump() to make this serializable\n                all_items.append(item.model_dump(exclude_unset=True))\n                if session_limit is not None and len(all_items) &gt;= session_limit:\n                    break\n            all_items.reverse()\n\n        return all_items  # type: ignore\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        session_id = await self._get_session_id()\n        if not items:\n            return\n\n        await self._openai_client.conversations.items.create(\n            conversation_id=session_id,\n            items=items,\n        )\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        session_id = await self._get_session_id()\n        items = await self.get_items(limit=1)\n        if not items:\n            return None\n        item_id: str = str(items[0][\"id\"])  # type: ignore [typeddict-item]\n        await self._openai_client.conversations.items.delete(\n            conversation_id=session_id, item_id=item_id\n        )\n        return items[0]\n\n    async def clear_session(self) -&gt; None:\n        session_id = await self._get_session_id()\n        await self._openai_client.conversations.delete(\n            conversation_id=session_id,\n        )\n        await self._clear_session_id()\n</code></pre>"},{"location":"ref/memory/#agents.memory.OpenAIConversationsSession.session_id","title":"session_id  <code>property</code> <code>writable</code>","text":"<pre><code>session_id: str\n</code></pre> <p>Get the session ID (conversation ID).</p> <p>Returns:</p> Type Description <code>str</code> <p>The conversation ID for this session.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the session has not been initialized yet. Call any session method (get_items, add_items, etc.) first to trigger lazy initialization.</p>"},{"location":"ref/model_settings/","title":"<code>Model settings</code>","text":""},{"location":"ref/model_settings/#agents.model_settings.ModelSettings","title":"ModelSettings","text":"<p>Settings to use when calling an LLM.</p> <p>This class holds optional model configuration parameters (e.g. temperature, top_p, penalties, truncation, etc.).</p> <p>Not all models/providers support all of these parameters, so please check the API documentation for the specific model and provider you are using.</p> Source code in <code>src/agents/model_settings.py</code> <pre><code>@dataclass\nclass ModelSettings:\n    \"\"\"Settings to use when calling an LLM.\n\n    This class holds optional model configuration parameters (e.g. temperature,\n    top_p, penalties, truncation, etc.).\n\n    Not all models/providers support all of these parameters, so please check the API documentation\n    for the specific model and provider you are using.\n    \"\"\"\n\n    temperature: float | None = None\n    \"\"\"The temperature to use when calling the model.\"\"\"\n\n    top_p: float | None = None\n    \"\"\"The top_p to use when calling the model.\"\"\"\n\n    frequency_penalty: float | None = None\n    \"\"\"The frequency penalty to use when calling the model.\"\"\"\n\n    presence_penalty: float | None = None\n    \"\"\"The presence penalty to use when calling the model.\"\"\"\n\n    tool_choice: ToolChoice | None = None\n    \"\"\"The tool choice to use when calling the model.\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Controls whether the model can make multiple parallel tool calls in a single turn.\n    If not provided (i.e., set to None), this behavior defers to the underlying\n    model provider's default. For most current providers (e.g., OpenAI), this typically\n    means parallel tool calls are enabled (True).\n    Set to True to explicitly enable parallel tool calls, or False to restrict the\n    model to at most one tool call per turn.\n    \"\"\"\n\n    truncation: Literal[\"auto\", \"disabled\"] | None = None\n    \"\"\"The truncation strategy to use when calling the model.\n    See [Responses API documentation](https://platform.openai.com/docs/api-reference/responses/create#responses_create-truncation)\n    for more details.\n    \"\"\"\n\n    max_tokens: int | None = None\n    \"\"\"The maximum number of output tokens to generate.\"\"\"\n\n    reasoning: Reasoning | None = None\n    \"\"\"Configuration options for\n    [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n    \"\"\"\n\n    verbosity: Literal[\"low\", \"medium\", \"high\"] | None = None\n    \"\"\"Constrains the verbosity of the model's response.\n    \"\"\"\n\n    metadata: dict[str, str] | None = None\n    \"\"\"Metadata to include with the model response call.\"\"\"\n\n    store: bool | None = None\n    \"\"\"Whether to store the generated model response for later retrieval.\n    For Responses API: automatically enabled when not specified.\n    For Chat Completions API: disabled when not specified.\"\"\"\n\n    prompt_cache_retention: Literal[\"in_memory\", \"24h\"] | None = None\n    \"\"\"The retention policy for the prompt cache. Set to `24h` to enable extended\n    prompt caching, which keeps cached prefixes active for longer, up to a maximum\n    of 24 hours.\n    [Learn more](https://platform.openai.com/docs/guides/prompt-caching#prompt-cache-retention).\"\"\"\n\n    include_usage: bool | None = None\n    \"\"\"Whether to include usage chunk.\n    Only available for Chat Completions API.\"\"\"\n\n    # TODO: revisit ResponseIncludable | str if ResponseIncludable covers more cases\n    # We've added str to support missing ones like\n    # \"web_search_call.action.sources\" etc.\n    response_include: list[ResponseIncludable | str] | None = None\n    \"\"\"Additional output data to include in the model response.\n    [include parameter](https://platform.openai.com/docs/api-reference/responses/create#responses-create-include)\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top tokens to return logprobs for. Setting this will\n    automatically include ``\"message.output_text.logprobs\"`` in the response.\"\"\"\n\n    extra_query: Query | None = None\n    \"\"\"Additional query fields to provide with the request.\n    Defaults to None if not provided.\"\"\"\n\n    extra_body: Body | None = None\n    \"\"\"Additional body fields to provide with the request.\n    Defaults to None if not provided.\"\"\"\n\n    extra_headers: Headers | None = None\n    \"\"\"Additional headers to provide with the request.\n    Defaults to None if not provided.\"\"\"\n\n    extra_args: dict[str, Any] | None = None\n    \"\"\"Arbitrary keyword arguments to pass to the model API call.\n    These will be passed directly to the underlying model provider's API.\n    Use with caution as not all models support all parameters.\"\"\"\n\n    def resolve(self, override: ModelSettings | None) -&gt; ModelSettings:\n        \"\"\"Produce a new ModelSettings by overlaying any non-None values from the\n        override on top of this instance.\"\"\"\n        if override is None:\n            return self\n\n        changes = {\n            field.name: getattr(override, field.name)\n            for field in fields(self)\n            if getattr(override, field.name) is not None\n        }\n\n        # Handle extra_args merging specially - merge dictionaries instead of replacing\n        if self.extra_args is not None or override.extra_args is not None:\n            merged_args = {}\n            if self.extra_args:\n                merged_args.update(self.extra_args)\n            if override.extra_args:\n                merged_args.update(override.extra_args)\n            changes[\"extra_args\"] = merged_args if merged_args else None\n\n        return replace(self, **changes)\n\n    def to_json_dict(self) -&gt; dict[str, Any]:\n        dataclass_dict = dataclasses.asdict(self)\n\n        json_dict: dict[str, Any] = {}\n\n        for field_name, value in dataclass_dict.items():\n            if isinstance(value, BaseModel):\n                json_dict[field_name] = value.model_dump(mode=\"json\")\n            else:\n                json_dict[field_name] = value\n\n        return json_dict\n</code></pre>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float | None = None\n</code></pre> <p>The temperature to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p: float | None = None\n</code></pre> <p>The top_p to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.frequency_penalty","title":"frequency_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frequency_penalty: float | None = None\n</code></pre> <p>The frequency penalty to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.presence_penalty","title":"presence_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>presence_penalty: float | None = None\n</code></pre> <p>The presence penalty to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.tool_choice","title":"tool_choice  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_choice: ToolChoice | None = None\n</code></pre> <p>The tool choice to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.parallel_tool_calls","title":"parallel_tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parallel_tool_calls: bool | None = None\n</code></pre> <p>Controls whether the model can make multiple parallel tool calls in a single turn. If not provided (i.e., set to None), this behavior defers to the underlying model provider's default. For most current providers (e.g., OpenAI), this typically means parallel tool calls are enabled (True). Set to True to explicitly enable parallel tool calls, or False to restrict the model to at most one tool call per turn.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.truncation","title":"truncation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>truncation: Literal['auto', 'disabled'] | None = None\n</code></pre> <p>The truncation strategy to use when calling the model. See Responses API documentation for more details.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int | None = None\n</code></pre> <p>The maximum number of output tokens to generate.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.reasoning","title":"reasoning  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reasoning: Reasoning | None = None\n</code></pre> <p>Configuration options for reasoning models.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.verbosity","title":"verbosity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>verbosity: Literal['low', 'medium', 'high'] | None = None\n</code></pre> <p>Constrains the verbosity of the model's response.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: dict[str, str] | None = None\n</code></pre> <p>Metadata to include with the model response call.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.store","title":"store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>store: bool | None = None\n</code></pre> <p>Whether to store the generated model response for later retrieval. For Responses API: automatically enabled when not specified. For Chat Completions API: disabled when not specified.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.prompt_cache_retention","title":"prompt_cache_retention  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt_cache_retention: (\n    Literal[\"in_memory\", \"24h\"] | None\n) = None\n</code></pre> <p>The retention policy for the prompt cache. Set to <code>24h</code> to enable extended prompt caching, which keeps cached prefixes active for longer, up to a maximum of 24 hours. Learn more.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.include_usage","title":"include_usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_usage: bool | None = None\n</code></pre> <p>Whether to include usage chunk. Only available for Chat Completions API.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.response_include","title":"response_include  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>response_include: list[ResponseIncludable | str] | None = (\n    None\n)\n</code></pre> <p>Additional output data to include in the model response. include parameter</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.top_logprobs","title":"top_logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_logprobs: int | None = None\n</code></pre> <p>Number of top tokens to return logprobs for. Setting this will automatically include <code>\"message.output_text.logprobs\"</code> in the response.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_query","title":"extra_query  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_query: Query | None = None\n</code></pre> <p>Additional query fields to provide with the request. Defaults to None if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_body","title":"extra_body  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_body: Body | None = None\n</code></pre> <p>Additional body fields to provide with the request. Defaults to None if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_headers","title":"extra_headers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_headers: Headers | None = None\n</code></pre> <p>Additional headers to provide with the request. Defaults to None if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_args","title":"extra_args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_args: dict[str, Any] | None = None\n</code></pre> <p>Arbitrary keyword arguments to pass to the model API call. These will be passed directly to the underlying model provider's API. Use with caution as not all models support all parameters.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.resolve","title":"resolve","text":"<pre><code>resolve(override: ModelSettings | None) -&gt; ModelSettings\n</code></pre> <p>Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance.</p> Source code in <code>src/agents/model_settings.py</code> <pre><code>def resolve(self, override: ModelSettings | None) -&gt; ModelSettings:\n    \"\"\"Produce a new ModelSettings by overlaying any non-None values from the\n    override on top of this instance.\"\"\"\n    if override is None:\n        return self\n\n    changes = {\n        field.name: getattr(override, field.name)\n        for field in fields(self)\n        if getattr(override, field.name) is not None\n    }\n\n    # Handle extra_args merging specially - merge dictionaries instead of replacing\n    if self.extra_args is not None or override.extra_args is not None:\n        merged_args = {}\n        if self.extra_args:\n            merged_args.update(self.extra_args)\n        if override.extra_args:\n            merged_args.update(override.extra_args)\n        changes[\"extra_args\"] = merged_args if merged_args else None\n\n    return replace(self, **changes)\n</code></pre>"},{"location":"ref/prompts/","title":"<code>Prompts</code>","text":""},{"location":"ref/prompts/#agents.prompts.DynamicPromptFunction","title":"DynamicPromptFunction  <code>module-attribute</code>","text":"<pre><code>DynamicPromptFunction = Callable[\n    [GenerateDynamicPromptData], MaybeAwaitable[Prompt]\n]\n</code></pre> <p>A function that dynamically generates a prompt.</p>"},{"location":"ref/prompts/#agents.prompts.Prompt","title":"Prompt","text":"<p>               Bases: <code>TypedDict</code></p> <p>Prompt configuration to use for interacting with an OpenAI model.</p> Source code in <code>src/agents/prompts.py</code> <pre><code>class Prompt(TypedDict):\n    \"\"\"Prompt configuration to use for interacting with an OpenAI model.\"\"\"\n\n    id: str\n    \"\"\"The unique ID of the prompt.\"\"\"\n\n    version: NotRequired[str]\n    \"\"\"Optional version of the prompt.\"\"\"\n\n    variables: NotRequired[dict[str, ResponsesPromptVariables]]\n    \"\"\"Optional variables to substitute into the prompt.\"\"\"\n</code></pre>"},{"location":"ref/prompts/#agents.prompts.Prompt.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The unique ID of the prompt.</p>"},{"location":"ref/prompts/#agents.prompts.Prompt.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: NotRequired[str]\n</code></pre> <p>Optional version of the prompt.</p>"},{"location":"ref/prompts/#agents.prompts.Prompt.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: NotRequired[dict[str, Variables]]\n</code></pre> <p>Optional variables to substitute into the prompt.</p>"},{"location":"ref/prompts/#agents.prompts.GenerateDynamicPromptData","title":"GenerateDynamicPromptData  <code>dataclass</code>","text":"<p>Inputs to a function that allows you to dynamically generate a prompt.</p> Source code in <code>src/agents/prompts.py</code> <pre><code>@dataclass\nclass GenerateDynamicPromptData:\n    \"\"\"Inputs to a function that allows you to dynamically generate a prompt.\"\"\"\n\n    context: RunContextWrapper[Any]\n    \"\"\"The run context.\"\"\"\n\n    agent: Agent[Any]\n    \"\"\"The agent for which the prompt is being generated.\"\"\"\n</code></pre>"},{"location":"ref/prompts/#agents.prompts.GenerateDynamicPromptData.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: RunContextWrapper[Any]\n</code></pre> <p>The run context.</p>"},{"location":"ref/prompts/#agents.prompts.GenerateDynamicPromptData.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent for which the prompt is being generated.</p>"},{"location":"ref/repl/","title":"<code>repl</code>","text":""},{"location":"ref/repl/#agents.repl.run_demo_loop","title":"run_demo_loop  <code>async</code>","text":"<pre><code>run_demo_loop(\n    agent: Agent[Any],\n    *,\n    stream: bool = True,\n    context: TContext | None = None,\n) -&gt; None\n</code></pre> <p>Run a simple REPL loop with the given agent.</p> <p>This utility allows quick manual testing and debugging of an agent from the command line. Conversation state is preserved across turns. Enter <code>exit</code> or <code>quit</code> to stop the loop.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent[Any]</code> <p>The starting agent to run.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the agent output.</p> <code>True</code> <code>context</code> <code>TContext | None</code> <p>Additional context information to pass to the runner.</p> <code>None</code> Source code in <code>src/agents/repl.py</code> <pre><code>async def run_demo_loop(\n    agent: Agent[Any], *, stream: bool = True, context: TContext | None = None\n) -&gt; None:\n    \"\"\"Run a simple REPL loop with the given agent.\n\n    This utility allows quick manual testing and debugging of an agent from the\n    command line. Conversation state is preserved across turns. Enter ``exit``\n    or ``quit`` to stop the loop.\n\n    Args:\n        agent: The starting agent to run.\n        stream: Whether to stream the agent output.\n        context: Additional context information to pass to the runner.\n    \"\"\"\n\n    current_agent = agent\n    input_items: list[TResponseInputItem] = []\n    while True:\n        try:\n            user_input = input(\" &gt; \")\n        except (EOFError, KeyboardInterrupt):\n            print()\n            break\n        if user_input.strip().lower() in {\"exit\", \"quit\"}:\n            break\n        if not user_input:\n            continue\n\n        input_items.append({\"role\": \"user\", \"content\": user_input})\n\n        result: RunResultBase\n        if stream:\n            result = Runner.run_streamed(current_agent, input=input_items, context=context)\n            async for event in result.stream_events():\n                if isinstance(event, RawResponsesStreamEvent):\n                    if isinstance(event.data, ResponseTextDeltaEvent):\n                        print(event.data.delta, end=\"\", flush=True)\n                elif isinstance(event, RunItemStreamEvent):\n                    if event.item.type == \"tool_call_item\":\n                        print(\"\\n[tool called]\", flush=True)\n                    elif event.item.type == \"tool_call_output_item\":\n                        print(f\"\\n[tool output: {event.item.output}]\", flush=True)\n                elif isinstance(event, AgentUpdatedStreamEvent):\n                    print(f\"\\n[Agent updated: {event.new_agent.name}]\", flush=True)\n            print()\n        else:\n            result = await Runner.run(current_agent, input_items, context=context)\n            if result.final_output is not None:\n                print(result.final_output)\n\n        current_agent = result.last_agent\n        input_items = result.to_input_list()\n</code></pre>"},{"location":"ref/result/","title":"<code>Results</code>","text":""},{"location":"ref/result/#agents.result.RunResultBase","title":"RunResultBase  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/agents/result.py</code> <pre><code>@dataclass\nclass RunResultBase(abc.ABC):\n    input: str | list[TResponseInputItem]\n    \"\"\"The original input items i.e. the items before run() was called. This may be a mutated\n    version of the input, if there are handoff input filters that mutate the input.\n    \"\"\"\n\n    new_items: list[RunItem]\n    \"\"\"The new items generated during the agent run. These include things like new messages, tool\n    calls and their outputs, etc.\n    \"\"\"\n\n    raw_responses: list[ModelResponse]\n    \"\"\"The raw LLM responses generated by the model during the agent run.\"\"\"\n\n    final_output: Any\n    \"\"\"The output of the last agent.\"\"\"\n\n    input_guardrail_results: list[InputGuardrailResult]\n    \"\"\"Guardrail results for the input messages.\"\"\"\n\n    output_guardrail_results: list[OutputGuardrailResult]\n    \"\"\"Guardrail results for the final output of the agent.\"\"\"\n\n    tool_input_guardrail_results: list[ToolInputGuardrailResult]\n    \"\"\"Tool input guardrail results from all tools executed during the run.\"\"\"\n\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult]\n    \"\"\"Tool output guardrail results from all tools executed during the run.\"\"\"\n\n    context_wrapper: RunContextWrapper[Any]\n    \"\"\"The context wrapper for the agent run.\"\"\"\n\n    _trace_state: TraceState | None = field(default=None, init=False, repr=False)\n    \"\"\"Serialized trace metadata captured during the run.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def last_agent(self) -&gt; Agent[Any]:\n        \"\"\"The last agent that was run.\"\"\"\n\n    def release_agents(self, *, release_new_items: bool = True) -&gt; None:\n        \"\"\"\n        Release strong references to agents held by this result. After calling this method,\n        accessing `item.agent` or `last_agent` may return `None` if the agent has been garbage\n        collected. Callers can use this when they are done inspecting the result and want to\n        eagerly drop any associated agent graph.\n        \"\"\"\n        if release_new_items:\n            for item in self.new_items:\n                release = getattr(item, \"release_agent\", None)\n                if callable(release):\n                    release()\n        self._release_last_agent_reference()\n\n    def __del__(self) -&gt; None:\n        try:\n            # Fall back to releasing agents automatically in case the caller never invoked\n            # `release_agents()` explicitly so GC of the RunResult drops the last strong reference.\n            # We pass `release_new_items=False` so RunItems that the user intentionally keeps\n            # continue exposing their originating agent until that agent itself is collected.\n            self.release_agents(release_new_items=False)\n        except Exception:\n            # Avoid raising from __del__.\n            pass\n\n    @abc.abstractmethod\n    def _release_last_agent_reference(self) -&gt; None:\n        \"\"\"Release stored agent reference specific to the concrete result type.\"\"\"\n\n    def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n        \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n        is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n        TypeError if the final output is not of the given type.\n\n        Args:\n            cls: The type to cast the final output to.\n            raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n                the given type.\n\n        Returns:\n            The final output casted to the given type.\n        \"\"\"\n        if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n            raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n        return cast(T, self.final_output)\n\n    def to_input_list(self) -&gt; list[TResponseInputItem]:\n        \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n        original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n        new_items: list[TResponseInputItem] = []\n        for item in self.new_items:\n            if isinstance(item, ToolApprovalItem):\n                continue\n            new_items.append(item.to_input_item())\n\n        return original_items + new_items\n\n    @property\n    def last_response_id(self) -&gt; str | None:\n        \"\"\"Convenience method to get the response ID of the last model response.\"\"\"\n        if not self.raw_responses:\n            return None\n\n        return self.raw_responses[-1].response_id\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultBase.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: str | list[TResponseInputItem]\n</code></pre> <p>The original input items i.e. the items before run() was called. This may be a mutated version of the input, if there are handoff input filters that mutate the input.</p>"},{"location":"ref/result/#agents.result.RunResultBase.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: list[RunItem]\n</code></pre> <p>The new items generated during the agent run. These include things like new messages, tool calls and their outputs, etc.</p>"},{"location":"ref/result/#agents.result.RunResultBase.raw_responses","title":"raw_responses  <code>instance-attribute</code>","text":"<pre><code>raw_responses: list[ModelResponse]\n</code></pre> <p>The raw LLM responses generated by the model during the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.final_output","title":"final_output  <code>instance-attribute</code>","text":"<pre><code>final_output: Any\n</code></pre> <p>The output of the last agent.</p>"},{"location":"ref/result/#agents.result.RunResultBase.input_guardrail_results","title":"input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>input_guardrail_results: list[InputGuardrailResult]\n</code></pre> <p>Guardrail results for the input messages.</p>"},{"location":"ref/result/#agents.result.RunResultBase.output_guardrail_results","title":"output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>Guardrail results for the final output of the agent.</p>"},{"location":"ref/result/#agents.result.RunResultBase.tool_input_guardrail_results","title":"tool_input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_input_guardrail_results: list[ToolInputGuardrailResult]\n</code></pre> <p>Tool input guardrail results from all tools executed during the run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.tool_output_guardrail_results","title":"tool_output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_output_guardrail_results: list[\n    ToolOutputGuardrailResult\n]\n</code></pre> <p>Tool output guardrail results from all tools executed during the run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.context_wrapper","title":"context_wrapper  <code>instance-attribute</code>","text":"<pre><code>context_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The context wrapper for the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.last_agent","title":"last_agent  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>last_agent: Agent[Any]\n</code></pre> <p>The last agent that was run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.last_response_id","title":"last_response_id  <code>property</code>","text":"<pre><code>last_response_id: str | None\n</code></pre> <p>Convenience method to get the response ID of the last model response.</p>"},{"location":"ref/result/#agents.result.RunResultBase.release_agents","title":"release_agents","text":"<pre><code>release_agents(*, release_new_items: bool = True) -&gt; None\n</code></pre> <p>Release strong references to agents held by this result. After calling this method, accessing <code>item.agent</code> or <code>last_agent</code> may return <code>None</code> if the agent has been garbage collected. Callers can use this when they are done inspecting the result and want to eagerly drop any associated agent graph.</p> Source code in <code>src/agents/result.py</code> <pre><code>def release_agents(self, *, release_new_items: bool = True) -&gt; None:\n    \"\"\"\n    Release strong references to agents held by this result. After calling this method,\n    accessing `item.agent` or `last_agent` may return `None` if the agent has been garbage\n    collected. Callers can use this when they are done inspecting the result and want to\n    eagerly drop any associated agent graph.\n    \"\"\"\n    if release_new_items:\n        for item in self.new_items:\n            release = getattr(item, \"release_agent\", None)\n            if callable(release):\n                release()\n    self._release_last_agent_reference()\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultBase.final_output_as","title":"final_output_as","text":"<pre><code>final_output_as(\n    cls: type[T], raise_if_incorrect_type: bool = False\n) -&gt; T\n</code></pre> <p>A convenience method to cast the final output to a specific type. By default, the cast is only for the typechecker. If you set <code>raise_if_incorrect_type</code> to True, we'll raise a TypeError if the final output is not of the given type.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[T]</code> <p>The type to cast the final output to.</p> required <code>raise_if_incorrect_type</code> <code>bool</code> <p>If True, we'll raise a TypeError if the final output is not of the given type.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>The final output casted to the given type.</p> Source code in <code>src/agents/result.py</code> <pre><code>def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n    \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n    is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n    TypeError if the final output is not of the given type.\n\n    Args:\n        cls: The type to cast the final output to.\n        raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n            the given type.\n\n    Returns:\n        The final output casted to the given type.\n    \"\"\"\n    if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n        raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n    return cast(T, self.final_output)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultBase.to_input_list","title":"to_input_list","text":"<pre><code>to_input_list() -&gt; list[TResponseInputItem]\n</code></pre> <p>Creates a new input list, merging the original input with all the new items generated.</p> Source code in <code>src/agents/result.py</code> <pre><code>def to_input_list(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n    original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n    new_items: list[TResponseInputItem] = []\n    for item in self.new_items:\n        if isinstance(item, ToolApprovalItem):\n            continue\n        new_items.append(item.to_input_item())\n\n    return original_items + new_items\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult","title":"RunResult  <code>dataclass</code>","text":"<p>               Bases: <code>RunResultBase</code></p> Source code in <code>src/agents/result.py</code> <pre><code>@dataclass\nclass RunResult(RunResultBase):\n    _last_agent: Agent[Any]\n    _last_agent_ref: weakref.ReferenceType[Agent[Any]] | None = field(\n        init=False,\n        repr=False,\n        default=None,\n    )\n    _last_processed_response: ProcessedResponse | None = field(default=None, repr=False)\n    \"\"\"The last processed model response. This is needed for resuming from interruptions.\"\"\"\n    _tool_use_tracker_snapshot: dict[str, list[str]] = field(default_factory=dict, repr=False)\n    _current_turn_persisted_item_count: int = 0\n    \"\"\"Number of items from new_items already persisted to session for the\n    current turn.\"\"\"\n    _current_turn: int = 0\n    \"\"\"The current turn number. This is preserved when converting to RunState.\"\"\"\n    _model_input_items: list[RunItem] = field(default_factory=list, repr=False)\n    \"\"\"Filtered items used to build model input when resuming runs.\"\"\"\n    _original_input: str | list[TResponseInputItem] | None = field(default=None, repr=False)\n    \"\"\"The original input for the current run segment.\n    This is updated when handoffs or resume logic replace the input history, and used by to_state()\n    to preserve the correct originalInput when serializing state.\"\"\"\n    _conversation_id: str | None = field(default=None, repr=False)\n    \"\"\"Conversation identifier for server-managed runs.\"\"\"\n    _previous_response_id: str | None = field(default=None, repr=False)\n    \"\"\"Response identifier returned by the server for the last turn.\"\"\"\n    _auto_previous_response_id: bool = field(default=False, repr=False)\n    \"\"\"Whether automatic previous response tracking was enabled.\"\"\"\n    max_turns: int = 10\n    \"\"\"The maximum number of turns allowed for this run.\"\"\"\n    interruptions: list[ToolApprovalItem] = field(default_factory=list)\n    \"\"\"Pending tool approval requests (interruptions) for this run.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        self._last_agent_ref = weakref.ref(self._last_agent)\n\n    @property\n    def last_agent(self) -&gt; Agent[Any]:\n        \"\"\"The last agent that was run.\"\"\"\n        agent = cast(\"Agent[Any] | None\", self.__dict__.get(\"_last_agent\"))\n        if agent is not None:\n            return agent\n        if self._last_agent_ref:\n            agent = self._last_agent_ref()\n            if agent is not None:\n                return agent\n        raise AgentsException(\"Last agent reference is no longer available.\")\n\n    def _release_last_agent_reference(self) -&gt; None:\n        agent = cast(\"Agent[Any] | None\", self.__dict__.get(\"_last_agent\"))\n        if agent is None:\n            return\n        self._last_agent_ref = weakref.ref(agent)\n        # Preserve dataclass field so repr/asdict continue to succeed.\n        self.__dict__[\"_last_agent\"] = None\n\n    def to_state(self) -&gt; RunState[Any]:\n        \"\"\"Create a RunState from this result to resume execution.\n\n        This is useful when the run was interrupted (e.g., for tool approval). You can\n        approve or reject the tool calls on the returned state, then pass it back to\n        `Runner.run()` to continue execution.\n\n        Returns:\n            A RunState that can be used to resume the run.\n\n        Example:\n            ```python\n            # Run agent until it needs approval\n            result = await Runner.run(agent, \"Use the delete_file tool\")\n\n            if result.interruptions:\n                # Approve the tool call\n                state = result.to_state()\n                state.approve(result.interruptions[0])\n\n                # Resume the run\n                result = await Runner.run(agent, state)\n            ```\n        \"\"\"\n        # Create a RunState from the current result\n        original_input_for_state = getattr(self, \"_original_input\", None)\n        state = RunState(\n            context=self.context_wrapper,\n            original_input=original_input_for_state\n            if original_input_for_state is not None\n            else self.input,\n            starting_agent=self.last_agent,\n            max_turns=self.max_turns,\n        )\n\n        return _populate_state_from_result(\n            state,\n            self,\n            current_turn=self._current_turn,\n            last_processed_response=self._last_processed_response,\n            current_turn_persisted_item_count=self._current_turn_persisted_item_count,\n            tool_use_tracker_snapshot=self._tool_use_tracker_snapshot,\n            conversation_id=self._conversation_id,\n            previous_response_id=self._previous_response_id,\n            auto_previous_response_id=self._auto_previous_response_id,\n        )\n\n    def __str__(self) -&gt; str:\n        return pretty_print_result(self)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult.max_turns","title":"max_turns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_turns: int = 10\n</code></pre> <p>The maximum number of turns allowed for this run.</p>"},{"location":"ref/result/#agents.result.RunResult.interruptions","title":"interruptions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interruptions: list[ToolApprovalItem] = field(\n    default_factory=list\n)\n</code></pre> <p>Pending tool approval requests (interruptions) for this run.</p>"},{"location":"ref/result/#agents.result.RunResult.last_agent","title":"last_agent  <code>property</code>","text":"<pre><code>last_agent: Agent[Any]\n</code></pre> <p>The last agent that was run.</p>"},{"location":"ref/result/#agents.result.RunResult.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: str | list[TResponseInputItem]\n</code></pre> <p>The original input items i.e. the items before run() was called. This may be a mutated version of the input, if there are handoff input filters that mutate the input.</p>"},{"location":"ref/result/#agents.result.RunResult.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: list[RunItem]\n</code></pre> <p>The new items generated during the agent run. These include things like new messages, tool calls and their outputs, etc.</p>"},{"location":"ref/result/#agents.result.RunResult.raw_responses","title":"raw_responses  <code>instance-attribute</code>","text":"<pre><code>raw_responses: list[ModelResponse]\n</code></pre> <p>The raw LLM responses generated by the model during the agent run.</p>"},{"location":"ref/result/#agents.result.RunResult.final_output","title":"final_output  <code>instance-attribute</code>","text":"<pre><code>final_output: Any\n</code></pre> <p>The output of the last agent.</p>"},{"location":"ref/result/#agents.result.RunResult.input_guardrail_results","title":"input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>input_guardrail_results: list[InputGuardrailResult]\n</code></pre> <p>Guardrail results for the input messages.</p>"},{"location":"ref/result/#agents.result.RunResult.output_guardrail_results","title":"output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>Guardrail results for the final output of the agent.</p>"},{"location":"ref/result/#agents.result.RunResult.tool_input_guardrail_results","title":"tool_input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_input_guardrail_results: list[ToolInputGuardrailResult]\n</code></pre> <p>Tool input guardrail results from all tools executed during the run.</p>"},{"location":"ref/result/#agents.result.RunResult.tool_output_guardrail_results","title":"tool_output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_output_guardrail_results: list[\n    ToolOutputGuardrailResult\n]\n</code></pre> <p>Tool output guardrail results from all tools executed during the run.</p>"},{"location":"ref/result/#agents.result.RunResult.context_wrapper","title":"context_wrapper  <code>instance-attribute</code>","text":"<pre><code>context_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The context wrapper for the agent run.</p>"},{"location":"ref/result/#agents.result.RunResult.last_response_id","title":"last_response_id  <code>property</code>","text":"<pre><code>last_response_id: str | None\n</code></pre> <p>Convenience method to get the response ID of the last model response.</p>"},{"location":"ref/result/#agents.result.RunResult.to_state","title":"to_state","text":"<pre><code>to_state() -&gt; RunState[Any]\n</code></pre> <p>Create a RunState from this result to resume execution.</p> <p>This is useful when the run was interrupted (e.g., for tool approval). You can approve or reject the tool calls on the returned state, then pass it back to <code>Runner.run()</code> to continue execution.</p> <p>Returns:</p> Type Description <code>RunState[Any]</code> <p>A RunState that can be used to resume the run.</p> Example <pre><code># Run agent until it needs approval\nresult = await Runner.run(agent, \"Use the delete_file tool\")\n\nif result.interruptions:\n    # Approve the tool call\n    state = result.to_state()\n    state.approve(result.interruptions[0])\n\n    # Resume the run\n    result = await Runner.run(agent, state)\n</code></pre> Source code in <code>src/agents/result.py</code> <pre><code>def to_state(self) -&gt; RunState[Any]:\n    \"\"\"Create a RunState from this result to resume execution.\n\n    This is useful when the run was interrupted (e.g., for tool approval). You can\n    approve or reject the tool calls on the returned state, then pass it back to\n    `Runner.run()` to continue execution.\n\n    Returns:\n        A RunState that can be used to resume the run.\n\n    Example:\n        ```python\n        # Run agent until it needs approval\n        result = await Runner.run(agent, \"Use the delete_file tool\")\n\n        if result.interruptions:\n            # Approve the tool call\n            state = result.to_state()\n            state.approve(result.interruptions[0])\n\n            # Resume the run\n            result = await Runner.run(agent, state)\n        ```\n    \"\"\"\n    # Create a RunState from the current result\n    original_input_for_state = getattr(self, \"_original_input\", None)\n    state = RunState(\n        context=self.context_wrapper,\n        original_input=original_input_for_state\n        if original_input_for_state is not None\n        else self.input,\n        starting_agent=self.last_agent,\n        max_turns=self.max_turns,\n    )\n\n    return _populate_state_from_result(\n        state,\n        self,\n        current_turn=self._current_turn,\n        last_processed_response=self._last_processed_response,\n        current_turn_persisted_item_count=self._current_turn_persisted_item_count,\n        tool_use_tracker_snapshot=self._tool_use_tracker_snapshot,\n        conversation_id=self._conversation_id,\n        previous_response_id=self._previous_response_id,\n        auto_previous_response_id=self._auto_previous_response_id,\n    )\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult.release_agents","title":"release_agents","text":"<pre><code>release_agents(*, release_new_items: bool = True) -&gt; None\n</code></pre> <p>Release strong references to agents held by this result. After calling this method, accessing <code>item.agent</code> or <code>last_agent</code> may return <code>None</code> if the agent has been garbage collected. Callers can use this when they are done inspecting the result and want to eagerly drop any associated agent graph.</p> Source code in <code>src/agents/result.py</code> <pre><code>def release_agents(self, *, release_new_items: bool = True) -&gt; None:\n    \"\"\"\n    Release strong references to agents held by this result. After calling this method,\n    accessing `item.agent` or `last_agent` may return `None` if the agent has been garbage\n    collected. Callers can use this when they are done inspecting the result and want to\n    eagerly drop any associated agent graph.\n    \"\"\"\n    if release_new_items:\n        for item in self.new_items:\n            release = getattr(item, \"release_agent\", None)\n            if callable(release):\n                release()\n    self._release_last_agent_reference()\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult.final_output_as","title":"final_output_as","text":"<pre><code>final_output_as(\n    cls: type[T], raise_if_incorrect_type: bool = False\n) -&gt; T\n</code></pre> <p>A convenience method to cast the final output to a specific type. By default, the cast is only for the typechecker. If you set <code>raise_if_incorrect_type</code> to True, we'll raise a TypeError if the final output is not of the given type.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[T]</code> <p>The type to cast the final output to.</p> required <code>raise_if_incorrect_type</code> <code>bool</code> <p>If True, we'll raise a TypeError if the final output is not of the given type.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>The final output casted to the given type.</p> Source code in <code>src/agents/result.py</code> <pre><code>def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n    \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n    is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n    TypeError if the final output is not of the given type.\n\n    Args:\n        cls: The type to cast the final output to.\n        raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n            the given type.\n\n    Returns:\n        The final output casted to the given type.\n    \"\"\"\n    if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n        raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n    return cast(T, self.final_output)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult.to_input_list","title":"to_input_list","text":"<pre><code>to_input_list() -&gt; list[TResponseInputItem]\n</code></pre> <p>Creates a new input list, merging the original input with all the new items generated.</p> Source code in <code>src/agents/result.py</code> <pre><code>def to_input_list(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n    original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n    new_items: list[TResponseInputItem] = []\n    for item in self.new_items:\n        if isinstance(item, ToolApprovalItem):\n            continue\n        new_items.append(item.to_input_item())\n\n    return original_items + new_items\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming","title":"RunResultStreaming  <code>dataclass</code>","text":"<p>               Bases: <code>RunResultBase</code></p> <p>The result of an agent run in streaming mode. You can use the <code>stream_events</code> method to receive semantic events as they are generated.</p> <p>The streaming method will raise: - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit. - A GuardrailTripwireTriggered exception if a guardrail is tripped.</p> Source code in <code>src/agents/result.py</code> <pre><code>@dataclass\nclass RunResultStreaming(RunResultBase):\n    \"\"\"The result of an agent run in streaming mode. You can use the `stream_events` method to\n    receive semantic events as they are generated.\n\n    The streaming method will raise:\n    - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n    - A GuardrailTripwireTriggered exception if a guardrail is tripped.\n    \"\"\"\n\n    current_agent: Agent[Any]\n    \"\"\"The current agent that is running.\"\"\"\n\n    current_turn: int\n    \"\"\"The current turn number.\"\"\"\n\n    max_turns: int\n    \"\"\"The maximum number of turns the agent can run for.\"\"\"\n\n    final_output: Any\n    \"\"\"The final output of the agent. This is None until the agent has finished running.\"\"\"\n\n    _current_agent_output_schema: AgentOutputSchemaBase | None = field(repr=False)\n\n    trace: Trace | None = field(repr=False)\n\n    is_complete: bool = False\n    \"\"\"Whether the agent has finished running.\"\"\"\n\n    _current_agent_ref: weakref.ReferenceType[Agent[Any]] | None = field(\n        init=False,\n        repr=False,\n        default=None,\n    )\n\n    _model_input_items: list[RunItem] = field(default_factory=list, repr=False)\n    \"\"\"Filtered items used to build model input between streaming turns.\"\"\"\n\n    # Queues that the background run_loop writes to\n    _event_queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel] = field(\n        default_factory=asyncio.Queue, repr=False\n    )\n    _input_guardrail_queue: asyncio.Queue[InputGuardrailResult] = field(\n        default_factory=asyncio.Queue, repr=False\n    )\n\n    # Store the asyncio tasks that we're waiting on\n    run_loop_task: asyncio.Task[Any] | None = field(default=None, repr=False)\n    _input_guardrails_task: asyncio.Task[Any] | None = field(default=None, repr=False)\n    _output_guardrails_task: asyncio.Task[Any] | None = field(default=None, repr=False)\n    _stored_exception: Exception | None = field(default=None, repr=False)\n    _cancel_mode: Literal[\"none\", \"immediate\", \"after_turn\"] = field(default=\"none\", repr=False)\n    _last_processed_response: ProcessedResponse | None = field(default=None, repr=False)\n    \"\"\"The last processed model response. This is needed for resuming from interruptions.\"\"\"\n    interruptions: list[ToolApprovalItem] = field(default_factory=list)\n    \"\"\"Pending tool approval requests (interruptions) for this run.\"\"\"\n    _waiting_on_event_queue: bool = field(default=False, repr=False)\n\n    _current_turn_persisted_item_count: int = 0\n    \"\"\"Number of items from new_items already persisted to session for the\n    current turn.\"\"\"\n\n    _stream_input_persisted: bool = False\n    \"\"\"Whether the input has been persisted to the session. Prevents double-saving.\"\"\"\n\n    _original_input_for_persistence: list[TResponseInputItem] = field(default_factory=list)\n    \"\"\"Original turn input before session history was merged, used for\n    persistence (matches JS sessionInputOriginalSnapshot).\"\"\"\n\n    _max_turns_handled: bool = field(default=False, repr=False)\n\n    _original_input: str | list[TResponseInputItem] | None = field(default=None, repr=False)\n    \"\"\"The original input from the first turn. Unlike `input`, this is never updated during the run.\n    Used by to_state() to preserve the correct originalInput when serializing state.\"\"\"\n    _tool_use_tracker_snapshot: dict[str, list[str]] = field(default_factory=dict, repr=False)\n    _state: Any = field(default=None, repr=False)\n    \"\"\"Internal reference to the RunState for streaming results.\"\"\"\n    _conversation_id: str | None = field(default=None, repr=False)\n    \"\"\"Conversation identifier for server-managed runs.\"\"\"\n    _previous_response_id: str | None = field(default=None, repr=False)\n    \"\"\"Response identifier returned by the server for the last turn.\"\"\"\n    _auto_previous_response_id: bool = field(default=False, repr=False)\n    \"\"\"Whether automatic previous response tracking was enabled.\"\"\"\n    _run_impl_task: InitVar[asyncio.Task[Any] | None] = None\n\n    def __post_init__(self, _run_impl_task: asyncio.Task[Any] | None) -&gt; None:\n        self._current_agent_ref = weakref.ref(self.current_agent)\n        # Store the original input at creation time (it will be set via input field)\n        if self._original_input is None:\n            self._original_input = self.input\n        # Compatibility shim: accept legacy `_run_impl_task` constructor keyword.\n        if self.run_loop_task is None and _run_impl_task is not None:\n            self.run_loop_task = _run_impl_task\n\n    @property\n    def last_agent(self) -&gt; Agent[Any]:\n        \"\"\"The last agent that was run. Updates as the agent run progresses, so the true last agent\n        is only available after the agent run is complete.\n        \"\"\"\n        agent = cast(\"Agent[Any] | None\", self.__dict__.get(\"current_agent\"))\n        if agent is not None:\n            return agent\n        if self._current_agent_ref:\n            agent = self._current_agent_ref()\n            if agent is not None:\n                return agent\n        raise AgentsException(\"Last agent reference is no longer available.\")\n\n    def _release_last_agent_reference(self) -&gt; None:\n        agent = cast(\"Agent[Any] | None\", self.__dict__.get(\"current_agent\"))\n        if agent is None:\n            return\n        self._current_agent_ref = weakref.ref(agent)\n        # Preserve dataclass field so repr/asdict continue to succeed.\n        self.__dict__[\"current_agent\"] = None\n\n    def cancel(self, mode: Literal[\"immediate\", \"after_turn\"] = \"immediate\") -&gt; None:\n        \"\"\"Cancel the streaming run.\n\n        Args:\n            mode: Cancellation strategy:\n                - \"immediate\": Stop immediately, cancel all tasks, clear queues (default)\n                - \"after_turn\": Complete current turn gracefully before stopping\n                    * Allows LLM response to finish\n                    * Executes pending tool calls\n                    * Saves session state properly\n                    * Tracks usage accurately\n                    * Stops before next turn begins\n\n        Example:\n            ```python\n            result = Runner.run_streamed(agent, \"Task\", session=session)\n\n            async for event in result.stream_events():\n                if user_interrupted():\n                    result.cancel(mode=\"after_turn\")  # Graceful\n                    # result.cancel()  # Immediate (default)\n            ```\n\n        Note: After calling cancel(), you should continue consuming stream_events()\n        to allow the cancellation to complete properly.\n        \"\"\"\n        # Store the cancel mode for the background task to check\n        self._cancel_mode = mode\n\n        if mode == \"immediate\":\n            # Existing behavior - immediate shutdown\n            self._cleanup_tasks()  # Cancel all running tasks\n            self.is_complete = True  # Mark the run as complete to stop event streaming\n\n            while not self._input_guardrail_queue.empty():\n                self._input_guardrail_queue.get_nowait()\n\n            # Unblock any streamers waiting on the event queue.\n            self._event_queue.put_nowait(QueueCompleteSentinel())\n            if not self._waiting_on_event_queue:\n                self._drain_event_queue()\n\n        elif mode == \"after_turn\":\n            # Soft cancel - just set the flag\n            # The streaming loop will check this and stop gracefully\n            # Don't call _cleanup_tasks() or clear queues yet\n            pass\n\n    async def stream_events(self) -&gt; AsyncIterator[StreamEvent]:\n        \"\"\"Stream deltas for new items as they are generated. We're using the types from the\n        OpenAI Responses API, so these are semantic events: each event has a `type` field that\n        describes the type of the event, along with the data for that event.\n\n        This will raise:\n        - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n        - A GuardrailTripwireTriggered exception if a guardrail is tripped.\n        \"\"\"\n        cancelled = False\n        try:\n            while True:\n                self._check_errors()\n                if self._stored_exception:\n                    logger.debug(\"Breaking due to stored exception\")\n                    self.is_complete = True\n                    break\n\n                if self.is_complete and self._event_queue.empty():\n                    break\n\n                try:\n                    self._waiting_on_event_queue = True\n                    item = await self._event_queue.get()\n                except asyncio.CancelledError:\n                    cancelled = True\n                    self.cancel()\n                    raise\n                finally:\n                    self._waiting_on_event_queue = False\n\n                if isinstance(item, QueueCompleteSentinel):\n                    # Await input guardrails if they are still running, so late\n                    # exceptions are captured.\n                    await self._await_task_safely(self._input_guardrails_task)\n\n                    self._event_queue.task_done()\n\n                    # Check for errors, in case the queue was completed\n                    # due to an exception\n                    self._check_errors()\n                    break\n\n                yield item\n                self._event_queue.task_done()\n        finally:\n            if cancelled:\n                # Cancellation should return promptly, so avoid waiting on long-running tasks.\n                # Tasks have already been cancelled above.\n                self._cleanup_tasks()\n            else:\n                # Ensure main execution completes before cleanup to avoid race conditions\n                # with session operations\n                await self._await_task_safely(self.run_loop_task)\n                # Safely terminate all background tasks after main execution has finished\n                self._cleanup_tasks()\n\n            # Allow any pending callbacks (e.g., cancellation handlers) to enqueue their\n            # completion sentinels before we clear the queues for observability.\n            await asyncio.sleep(0)\n\n            # Drain queues so callers observing internal state see them empty after completion.\n            self._drain_event_queue()\n            self._drain_input_guardrail_queue()\n\n        if self._stored_exception:\n            raise self._stored_exception\n\n    def _create_error_details(self) -&gt; RunErrorDetails:\n        \"\"\"Return a `RunErrorDetails` object considering the current attributes of the class.\"\"\"\n        return RunErrorDetails(\n            input=self.input,\n            new_items=self.new_items,\n            raw_responses=self.raw_responses,\n            last_agent=self.current_agent,\n            context_wrapper=self.context_wrapper,\n            input_guardrail_results=self.input_guardrail_results,\n            output_guardrail_results=self.output_guardrail_results,\n        )\n\n    def _check_errors(self):\n        if self.current_turn &gt; self.max_turns and not self._max_turns_handled:\n            max_turns_exc = MaxTurnsExceeded(f\"Max turns ({self.max_turns}) exceeded\")\n            max_turns_exc.run_data = self._create_error_details()\n            self._stored_exception = max_turns_exc\n\n        # Fetch all the completed guardrail results from the queue and raise if needed\n        while not self._input_guardrail_queue.empty():\n            guardrail_result = self._input_guardrail_queue.get_nowait()\n            if guardrail_result.output.tripwire_triggered:\n                tripwire_exc = InputGuardrailTripwireTriggered(guardrail_result)\n                tripwire_exc.run_data = self._create_error_details()\n                self._stored_exception = tripwire_exc\n\n        # Check the tasks for any exceptions\n        if self.run_loop_task and self.run_loop_task.done():\n            if not self.run_loop_task.cancelled():\n                run_impl_exc = self.run_loop_task.exception()\n                if run_impl_exc and isinstance(run_impl_exc, Exception):\n                    if isinstance(run_impl_exc, AgentsException) and run_impl_exc.run_data is None:\n                        run_impl_exc.run_data = self._create_error_details()\n                    self._stored_exception = run_impl_exc\n\n        if self._input_guardrails_task and self._input_guardrails_task.done():\n            if not self._input_guardrails_task.cancelled():\n                in_guard_exc = self._input_guardrails_task.exception()\n                if in_guard_exc and isinstance(in_guard_exc, Exception):\n                    if isinstance(in_guard_exc, AgentsException) and in_guard_exc.run_data is None:\n                        in_guard_exc.run_data = self._create_error_details()\n                    self._stored_exception = in_guard_exc\n\n        if self._output_guardrails_task and self._output_guardrails_task.done():\n            if not self._output_guardrails_task.cancelled():\n                out_guard_exc = self._output_guardrails_task.exception()\n                if out_guard_exc and isinstance(out_guard_exc, Exception):\n                    if (\n                        isinstance(out_guard_exc, AgentsException)\n                        and out_guard_exc.run_data is None\n                    ):\n                        out_guard_exc.run_data = self._create_error_details()\n                    self._stored_exception = out_guard_exc\n\n    def _cleanup_tasks(self):\n        if self.run_loop_task and not self.run_loop_task.done():\n            self.run_loop_task.cancel()\n\n        if self._input_guardrails_task and not self._input_guardrails_task.done():\n            self._input_guardrails_task.cancel()\n\n        if self._output_guardrails_task and not self._output_guardrails_task.done():\n            self._output_guardrails_task.cancel()\n\n    def __str__(self) -&gt; str:\n        return pretty_print_run_result_streaming(self)\n\n    async def _await_task_safely(self, task: asyncio.Task[Any] | None) -&gt; None:\n        \"\"\"Await a task if present, ignoring cancellation and storing exceptions elsewhere.\n\n        This ensures we do not lose late guardrail exceptions while not surfacing\n        CancelledError to callers of stream_events.\n        \"\"\"\n        if task and not task.done():\n            try:\n                await task\n            except asyncio.CancelledError:\n                # Task was cancelled (e.g., due to result.cancel()). Nothing to do here.\n                pass\n            except Exception:\n                # The exception will be surfaced via _check_errors() if needed.\n                pass\n\n    def _drain_event_queue(self) -&gt; None:\n        \"\"\"Remove any pending items from the event queue and mark them done.\"\"\"\n        while not self._event_queue.empty():\n            try:\n                self._event_queue.get_nowait()\n                self._event_queue.task_done()\n            except asyncio.QueueEmpty:\n                break\n            except ValueError:\n                # task_done called too many times; nothing more to drain.\n                break\n\n    def _drain_input_guardrail_queue(self) -&gt; None:\n        \"\"\"Remove any pending items from the input guardrail queue.\"\"\"\n        while not self._input_guardrail_queue.empty():\n            try:\n                self._input_guardrail_queue.get_nowait()\n            except asyncio.QueueEmpty:\n                break\n\n    def to_state(self) -&gt; RunState[Any]:\n        \"\"\"Create a RunState from this streaming result to resume execution.\n\n        This is useful when the run was interrupted (e.g., for tool approval). You can\n        approve or reject the tool calls on the returned state, then pass it back to\n        `Runner.run_streamed()` to continue execution.\n\n        Returns:\n            A RunState that can be used to resume the run.\n\n        Example:\n            ```python\n            # Run agent until it needs approval\n            result = Runner.run_streamed(agent, \"Use the delete_file tool\")\n            async for event in result.stream_events():\n                pass\n\n            if result.interruptions:\n                # Approve the tool call\n                state = result.to_state()\n                state.approve(result.interruptions[0])\n\n                # Resume the run\n                result = Runner.run_streamed(agent, state)\n                async for event in result.stream_events():\n                    pass\n            ```\n        \"\"\"\n        # Create a RunState from the current result\n        # Use _original_input (updated on handoffs/resume when input history changes).\n        # This avoids serializing a mutated view of input history.\n        state = RunState(\n            context=self.context_wrapper,\n            original_input=self._original_input if self._original_input is not None else self.input,\n            starting_agent=self.last_agent,\n            max_turns=self.max_turns,\n        )\n\n        return _populate_state_from_result(\n            state,\n            self,\n            current_turn=self.current_turn,\n            last_processed_response=self._last_processed_response,\n            current_turn_persisted_item_count=self._current_turn_persisted_item_count,\n            tool_use_tracker_snapshot=self._tool_use_tracker_snapshot,\n            conversation_id=self._conversation_id,\n            previous_response_id=self._previous_response_id,\n            auto_previous_response_id=self._auto_previous_response_id,\n        )\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.current_agent","title":"current_agent  <code>instance-attribute</code>","text":"<pre><code>current_agent: Agent[Any]\n</code></pre> <p>The current agent that is running.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.current_turn","title":"current_turn  <code>instance-attribute</code>","text":"<pre><code>current_turn: int\n</code></pre> <p>The current turn number.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.max_turns","title":"max_turns  <code>instance-attribute</code>","text":"<pre><code>max_turns: int\n</code></pre> <p>The maximum number of turns the agent can run for.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.final_output","title":"final_output  <code>instance-attribute</code>","text":"<pre><code>final_output: Any\n</code></pre> <p>The final output of the agent. This is None until the agent has finished running.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.is_complete","title":"is_complete  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_complete: bool = False\n</code></pre> <p>Whether the agent has finished running.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.interruptions","title":"interruptions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interruptions: list[ToolApprovalItem] = field(\n    default_factory=list\n)\n</code></pre> <p>Pending tool approval requests (interruptions) for this run.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.last_agent","title":"last_agent  <code>property</code>","text":"<pre><code>last_agent: Agent[Any]\n</code></pre> <p>The last agent that was run. Updates as the agent run progresses, so the true last agent is only available after the agent run is complete.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: str | list[TResponseInputItem]\n</code></pre> <p>The original input items i.e. the items before run() was called. This may be a mutated version of the input, if there are handoff input filters that mutate the input.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: list[RunItem]\n</code></pre> <p>The new items generated during the agent run. These include things like new messages, tool calls and their outputs, etc.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.raw_responses","title":"raw_responses  <code>instance-attribute</code>","text":"<pre><code>raw_responses: list[ModelResponse]\n</code></pre> <p>The raw LLM responses generated by the model during the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.input_guardrail_results","title":"input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>input_guardrail_results: list[InputGuardrailResult]\n</code></pre> <p>Guardrail results for the input messages.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.output_guardrail_results","title":"output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>Guardrail results for the final output of the agent.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.tool_input_guardrail_results","title":"tool_input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_input_guardrail_results: list[ToolInputGuardrailResult]\n</code></pre> <p>Tool input guardrail results from all tools executed during the run.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.tool_output_guardrail_results","title":"tool_output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_output_guardrail_results: list[\n    ToolOutputGuardrailResult\n]\n</code></pre> <p>Tool output guardrail results from all tools executed during the run.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.context_wrapper","title":"context_wrapper  <code>instance-attribute</code>","text":"<pre><code>context_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The context wrapper for the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.last_response_id","title":"last_response_id  <code>property</code>","text":"<pre><code>last_response_id: str | None\n</code></pre> <p>Convenience method to get the response ID of the last model response.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.cancel","title":"cancel","text":"<pre><code>cancel(\n    mode: Literal[\"immediate\", \"after_turn\"] = \"immediate\",\n) -&gt; None\n</code></pre> <p>Cancel the streaming run.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['immediate', 'after_turn']</code> <p>Cancellation strategy: - \"immediate\": Stop immediately, cancel all tasks, clear queues (default) - \"after_turn\": Complete current turn gracefully before stopping     * Allows LLM response to finish     * Executes pending tool calls     * Saves session state properly     * Tracks usage accurately     * Stops before next turn begins</p> <code>'immediate'</code> Example <pre><code>result = Runner.run_streamed(agent, \"Task\", session=session)\n\nasync for event in result.stream_events():\n    if user_interrupted():\n        result.cancel(mode=\"after_turn\")  # Graceful\n        # result.cancel()  # Immediate (default)\n</code></pre> <p>Note: After calling cancel(), you should continue consuming stream_events() to allow the cancellation to complete properly.</p> Source code in <code>src/agents/result.py</code> <pre><code>def cancel(self, mode: Literal[\"immediate\", \"after_turn\"] = \"immediate\") -&gt; None:\n    \"\"\"Cancel the streaming run.\n\n    Args:\n        mode: Cancellation strategy:\n            - \"immediate\": Stop immediately, cancel all tasks, clear queues (default)\n            - \"after_turn\": Complete current turn gracefully before stopping\n                * Allows LLM response to finish\n                * Executes pending tool calls\n                * Saves session state properly\n                * Tracks usage accurately\n                * Stops before next turn begins\n\n    Example:\n        ```python\n        result = Runner.run_streamed(agent, \"Task\", session=session)\n\n        async for event in result.stream_events():\n            if user_interrupted():\n                result.cancel(mode=\"after_turn\")  # Graceful\n                # result.cancel()  # Immediate (default)\n        ```\n\n    Note: After calling cancel(), you should continue consuming stream_events()\n    to allow the cancellation to complete properly.\n    \"\"\"\n    # Store the cancel mode for the background task to check\n    self._cancel_mode = mode\n\n    if mode == \"immediate\":\n        # Existing behavior - immediate shutdown\n        self._cleanup_tasks()  # Cancel all running tasks\n        self.is_complete = True  # Mark the run as complete to stop event streaming\n\n        while not self._input_guardrail_queue.empty():\n            self._input_guardrail_queue.get_nowait()\n\n        # Unblock any streamers waiting on the event queue.\n        self._event_queue.put_nowait(QueueCompleteSentinel())\n        if not self._waiting_on_event_queue:\n            self._drain_event_queue()\n\n    elif mode == \"after_turn\":\n        # Soft cancel - just set the flag\n        # The streaming loop will check this and stop gracefully\n        # Don't call _cleanup_tasks() or clear queues yet\n        pass\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.stream_events","title":"stream_events  <code>async</code>","text":"<pre><code>stream_events() -&gt; AsyncIterator[StreamEvent]\n</code></pre> <p>Stream deltas for new items as they are generated. We're using the types from the OpenAI Responses API, so these are semantic events: each event has a <code>type</code> field that describes the type of the event, along with the data for that event.</p> <p>This will raise: - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit. - A GuardrailTripwireTriggered exception if a guardrail is tripped.</p> Source code in <code>src/agents/result.py</code> <pre><code>async def stream_events(self) -&gt; AsyncIterator[StreamEvent]:\n    \"\"\"Stream deltas for new items as they are generated. We're using the types from the\n    OpenAI Responses API, so these are semantic events: each event has a `type` field that\n    describes the type of the event, along with the data for that event.\n\n    This will raise:\n    - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n    - A GuardrailTripwireTriggered exception if a guardrail is tripped.\n    \"\"\"\n    cancelled = False\n    try:\n        while True:\n            self._check_errors()\n            if self._stored_exception:\n                logger.debug(\"Breaking due to stored exception\")\n                self.is_complete = True\n                break\n\n            if self.is_complete and self._event_queue.empty():\n                break\n\n            try:\n                self._waiting_on_event_queue = True\n                item = await self._event_queue.get()\n            except asyncio.CancelledError:\n                cancelled = True\n                self.cancel()\n                raise\n            finally:\n                self._waiting_on_event_queue = False\n\n            if isinstance(item, QueueCompleteSentinel):\n                # Await input guardrails if they are still running, so late\n                # exceptions are captured.\n                await self._await_task_safely(self._input_guardrails_task)\n\n                self._event_queue.task_done()\n\n                # Check for errors, in case the queue was completed\n                # due to an exception\n                self._check_errors()\n                break\n\n            yield item\n            self._event_queue.task_done()\n    finally:\n        if cancelled:\n            # Cancellation should return promptly, so avoid waiting on long-running tasks.\n            # Tasks have already been cancelled above.\n            self._cleanup_tasks()\n        else:\n            # Ensure main execution completes before cleanup to avoid race conditions\n            # with session operations\n            await self._await_task_safely(self.run_loop_task)\n            # Safely terminate all background tasks after main execution has finished\n            self._cleanup_tasks()\n\n        # Allow any pending callbacks (e.g., cancellation handlers) to enqueue their\n        # completion sentinels before we clear the queues for observability.\n        await asyncio.sleep(0)\n\n        # Drain queues so callers observing internal state see them empty after completion.\n        self._drain_event_queue()\n        self._drain_input_guardrail_queue()\n\n    if self._stored_exception:\n        raise self._stored_exception\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.to_state","title":"to_state","text":"<pre><code>to_state() -&gt; RunState[Any]\n</code></pre> <p>Create a RunState from this streaming result to resume execution.</p> <p>This is useful when the run was interrupted (e.g., for tool approval). You can approve or reject the tool calls on the returned state, then pass it back to <code>Runner.run_streamed()</code> to continue execution.</p> <p>Returns:</p> Type Description <code>RunState[Any]</code> <p>A RunState that can be used to resume the run.</p> Example <pre><code># Run agent until it needs approval\nresult = Runner.run_streamed(agent, \"Use the delete_file tool\")\nasync for event in result.stream_events():\n    pass\n\nif result.interruptions:\n    # Approve the tool call\n    state = result.to_state()\n    state.approve(result.interruptions[0])\n\n    # Resume the run\n    result = Runner.run_streamed(agent, state)\n    async for event in result.stream_events():\n        pass\n</code></pre> Source code in <code>src/agents/result.py</code> <pre><code>def to_state(self) -&gt; RunState[Any]:\n    \"\"\"Create a RunState from this streaming result to resume execution.\n\n    This is useful when the run was interrupted (e.g., for tool approval). You can\n    approve or reject the tool calls on the returned state, then pass it back to\n    `Runner.run_streamed()` to continue execution.\n\n    Returns:\n        A RunState that can be used to resume the run.\n\n    Example:\n        ```python\n        # Run agent until it needs approval\n        result = Runner.run_streamed(agent, \"Use the delete_file tool\")\n        async for event in result.stream_events():\n            pass\n\n        if result.interruptions:\n            # Approve the tool call\n            state = result.to_state()\n            state.approve(result.interruptions[0])\n\n            # Resume the run\n            result = Runner.run_streamed(agent, state)\n            async for event in result.stream_events():\n                pass\n        ```\n    \"\"\"\n    # Create a RunState from the current result\n    # Use _original_input (updated on handoffs/resume when input history changes).\n    # This avoids serializing a mutated view of input history.\n    state = RunState(\n        context=self.context_wrapper,\n        original_input=self._original_input if self._original_input is not None else self.input,\n        starting_agent=self.last_agent,\n        max_turns=self.max_turns,\n    )\n\n    return _populate_state_from_result(\n        state,\n        self,\n        current_turn=self.current_turn,\n        last_processed_response=self._last_processed_response,\n        current_turn_persisted_item_count=self._current_turn_persisted_item_count,\n        tool_use_tracker_snapshot=self._tool_use_tracker_snapshot,\n        conversation_id=self._conversation_id,\n        previous_response_id=self._previous_response_id,\n        auto_previous_response_id=self._auto_previous_response_id,\n    )\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.release_agents","title":"release_agents","text":"<pre><code>release_agents(*, release_new_items: bool = True) -&gt; None\n</code></pre> <p>Release strong references to agents held by this result. After calling this method, accessing <code>item.agent</code> or <code>last_agent</code> may return <code>None</code> if the agent has been garbage collected. Callers can use this when they are done inspecting the result and want to eagerly drop any associated agent graph.</p> Source code in <code>src/agents/result.py</code> <pre><code>def release_agents(self, *, release_new_items: bool = True) -&gt; None:\n    \"\"\"\n    Release strong references to agents held by this result. After calling this method,\n    accessing `item.agent` or `last_agent` may return `None` if the agent has been garbage\n    collected. Callers can use this when they are done inspecting the result and want to\n    eagerly drop any associated agent graph.\n    \"\"\"\n    if release_new_items:\n        for item in self.new_items:\n            release = getattr(item, \"release_agent\", None)\n            if callable(release):\n                release()\n    self._release_last_agent_reference()\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.final_output_as","title":"final_output_as","text":"<pre><code>final_output_as(\n    cls: type[T], raise_if_incorrect_type: bool = False\n) -&gt; T\n</code></pre> <p>A convenience method to cast the final output to a specific type. By default, the cast is only for the typechecker. If you set <code>raise_if_incorrect_type</code> to True, we'll raise a TypeError if the final output is not of the given type.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[T]</code> <p>The type to cast the final output to.</p> required <code>raise_if_incorrect_type</code> <code>bool</code> <p>If True, we'll raise a TypeError if the final output is not of the given type.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>The final output casted to the given type.</p> Source code in <code>src/agents/result.py</code> <pre><code>def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n    \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n    is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n    TypeError if the final output is not of the given type.\n\n    Args:\n        cls: The type to cast the final output to.\n        raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n            the given type.\n\n    Returns:\n        The final output casted to the given type.\n    \"\"\"\n    if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n        raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n    return cast(T, self.final_output)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.to_input_list","title":"to_input_list","text":"<pre><code>to_input_list() -&gt; list[TResponseInputItem]\n</code></pre> <p>Creates a new input list, merging the original input with all the new items generated.</p> Source code in <code>src/agents/result.py</code> <pre><code>def to_input_list(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n    original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n    new_items: list[TResponseInputItem] = []\n    for item in self.new_items:\n        if isinstance(item, ToolApprovalItem):\n            continue\n        new_items.append(item.to_input_item())\n\n    return original_items + new_items\n</code></pre>"},{"location":"ref/run/","title":"<code>Runner</code>","text":""},{"location":"ref/run/#agents.run.Runner","title":"Runner","text":"Source code in <code>src/agents/run.py</code> <pre><code>class Runner:\n    @classmethod\n    async def run(\n        cls,\n        starting_agent: Agent[TContext],\n        input: str | list[TResponseInputItem] | RunState[TContext],\n        *,\n        context: TContext | None = None,\n        max_turns: int = DEFAULT_MAX_TURNS,\n        hooks: RunHooks[TContext] | None = None,\n        run_config: RunConfig | None = None,\n        error_handlers: RunErrorHandlers[TContext] | None = None,\n        previous_response_id: str | None = None,\n        auto_previous_response_id: bool = False,\n        conversation_id: str | None = None,\n        session: Session | None = None,\n    ) -&gt; RunResult:\n        \"\"\"\n        Run a workflow starting at the given agent.\n\n        The agent will run in a loop until a final output is generated. The loop runs like so:\n\n          1. The agent is invoked with the given input.\n          2. If there is a final output (i.e. the agent produces something of type\n             `agent.output_type`), the loop terminates.\n          3. If there's a handoff, we run the loop again, with the new agent.\n          4. Else, we run tool calls (if any), and re-run the loop.\n\n        In two cases, the agent may raise an exception:\n\n          1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.\n          2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered\n             exception is raised.\n\n        Note:\n            Only the first agent's input guardrails are run.\n\n        Args:\n            starting_agent: The starting agent to run.\n            input: The initial input to the agent. You can pass a single string for a\n                user message, or a list of input items.\n            context: The context to run the agent with.\n            max_turns: The maximum number of turns to run the agent for. A turn is\n                defined as one AI invocation (including any tool calls that might occur).\n            hooks: An object that receives callbacks on various lifecycle events.\n            run_config: Global settings for the entire agent run.\n            error_handlers: Error handlers keyed by error kind. Currently supports max_turns.\n            previous_response_id: The ID of the previous response. If using OpenAI\n                models via the Responses API, this allows you to skip passing in input\n                from the previous turn.\n            conversation_id: The conversation ID\n                (https://platform.openai.com/docs/guides/conversation-state?api-mode=responses).\n                If provided, the conversation will be used to read and write items.\n                Every agent will have access to the conversation history so far,\n                and its output items will be written to the conversation.\n                We recommend only using this if you are exclusively using OpenAI models;\n                other model providers don't write to the Conversation object,\n                so you'll end up having partial conversations stored.\n            session: A session for automatic conversation history management.\n\n        Returns:\n            A run result containing all the inputs, guardrail results and the output of\n            the last agent. Agents may perform handoffs, so we don't know the specific\n            type of the output.\n        \"\"\"\n\n        runner = DEFAULT_AGENT_RUNNER\n        return await runner.run(\n            starting_agent,\n            input,\n            context=context,\n            max_turns=max_turns,\n            hooks=hooks,\n            run_config=run_config,\n            error_handlers=error_handlers,\n            previous_response_id=previous_response_id,\n            auto_previous_response_id=auto_previous_response_id,\n            conversation_id=conversation_id,\n            session=session,\n        )\n\n    @classmethod\n    def run_sync(\n        cls,\n        starting_agent: Agent[TContext],\n        input: str | list[TResponseInputItem] | RunState[TContext],\n        *,\n        context: TContext | None = None,\n        max_turns: int = DEFAULT_MAX_TURNS,\n        hooks: RunHooks[TContext] | None = None,\n        run_config: RunConfig | None = None,\n        error_handlers: RunErrorHandlers[TContext] | None = None,\n        previous_response_id: str | None = None,\n        auto_previous_response_id: bool = False,\n        conversation_id: str | None = None,\n        session: Session | None = None,\n    ) -&gt; RunResult:\n        \"\"\"\n        Run a workflow synchronously, starting at the given agent.\n\n        Note:\n            This just wraps the `run` method, so it will not work if there's already an\n            event loop (e.g. inside an async function, or in a Jupyter notebook or async\n            context like FastAPI). For those cases, use the `run` method instead.\n\n        The agent will run in a loop until a final output is generated. The loop runs:\n\n          1. The agent is invoked with the given input.\n          2. If there is a final output (i.e. the agent produces something of type\n             `agent.output_type`), the loop terminates.\n          3. If there's a handoff, we run the loop again, with the new agent.\n          4. Else, we run tool calls (if any), and re-run the loop.\n\n        In two cases, the agent may raise an exception:\n\n          1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.\n          2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered\n             exception is raised.\n\n        Note:\n            Only the first agent's input guardrails are run.\n\n        Args:\n            starting_agent: The starting agent to run.\n            input: The initial input to the agent. You can pass a single string for a\n                user message, or a list of input items.\n            context: The context to run the agent with.\n            max_turns: The maximum number of turns to run the agent for. A turn is\n                defined as one AI invocation (including any tool calls that might occur).\n            hooks: An object that receives callbacks on various lifecycle events.\n            run_config: Global settings for the entire agent run.\n            error_handlers: Error handlers keyed by error kind. Currently supports max_turns.\n            previous_response_id: The ID of the previous response, if using OpenAI\n                models via the Responses API, this allows you to skip passing in input\n                from the previous turn.\n            conversation_id: The ID of the stored conversation, if any.\n            session: A session for automatic conversation history management.\n\n        Returns:\n            A run result containing all the inputs, guardrail results and the output of\n            the last agent. Agents may perform handoffs, so we don't know the specific\n            type of the output.\n        \"\"\"\n\n        runner = DEFAULT_AGENT_RUNNER\n        return runner.run_sync(\n            starting_agent,\n            input,\n            context=context,\n            max_turns=max_turns,\n            hooks=hooks,\n            run_config=run_config,\n            error_handlers=error_handlers,\n            previous_response_id=previous_response_id,\n            conversation_id=conversation_id,\n            session=session,\n            auto_previous_response_id=auto_previous_response_id,\n        )\n\n    @classmethod\n    def run_streamed(\n        cls,\n        starting_agent: Agent[TContext],\n        input: str | list[TResponseInputItem] | RunState[TContext],\n        context: TContext | None = None,\n        max_turns: int = DEFAULT_MAX_TURNS,\n        hooks: RunHooks[TContext] | None = None,\n        run_config: RunConfig | None = None,\n        previous_response_id: str | None = None,\n        auto_previous_response_id: bool = False,\n        conversation_id: str | None = None,\n        session: Session | None = None,\n        *,\n        error_handlers: RunErrorHandlers[TContext] | None = None,\n    ) -&gt; RunResultStreaming:\n        \"\"\"\n        Run a workflow starting at the given agent in streaming mode.\n\n        The returned result object contains a method you can use to stream semantic\n        events as they are generated.\n\n        The agent will run in a loop until a final output is generated. The loop runs like so:\n\n          1. The agent is invoked with the given input.\n          2. If there is a final output (i.e. the agent produces something of type\n             `agent.output_type`), the loop terminates.\n          3. If there's a handoff, we run the loop again, with the new agent.\n          4. Else, we run tool calls (if any), and re-run the loop.\n\n        In two cases, the agent may raise an exception:\n\n          1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.\n          2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered\n             exception is raised.\n\n        Note:\n            Only the first agent's input guardrails are run.\n\n        Args:\n            starting_agent: The starting agent to run.\n            input: The initial input to the agent. You can pass a single string for a\n                user message, or a list of input items.\n            context: The context to run the agent with.\n            max_turns: The maximum number of turns to run the agent for. A turn is\n                defined as one AI invocation (including any tool calls that might occur).\n            hooks: An object that receives callbacks on various lifecycle events.\n            run_config: Global settings for the entire agent run.\n            error_handlers: Error handlers keyed by error kind. Currently supports max_turns.\n            previous_response_id: The ID of the previous response, if using OpenAI\n                models via the Responses API, this allows you to skip passing in input\n                from the previous turn.\n            conversation_id: The ID of the stored conversation, if any.\n            session: A session for automatic conversation history management.\n\n        Returns:\n            A result object that contains data about the run, as well as a method to\n            stream events.\n        \"\"\"\n\n        runner = DEFAULT_AGENT_RUNNER\n        return runner.run_streamed(\n            starting_agent,\n            input,\n            context=context,\n            max_turns=max_turns,\n            hooks=hooks,\n            run_config=run_config,\n            error_handlers=error_handlers,\n            previous_response_id=previous_response_id,\n            auto_previous_response_id=auto_previous_response_id,\n            conversation_id=conversation_id,\n            session=session,\n        )\n</code></pre>"},{"location":"ref/run/#agents.run.Runner.run","title":"run  <code>async</code> <code>classmethod</code>","text":"<pre><code>run(\n    starting_agent: Agent[TContext],\n    input: str\n    | list[TResponseInputItem]\n    | RunState[TContext],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    error_handlers: RunErrorHandlers[TContext]\n    | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult\n</code></pre> <p>Run a workflow starting at the given agent.</p> <p>The agent will run in a loop until a final output is generated. The loop runs like so:</p> <ol> <li>The agent is invoked with the given input.</li> <li>If there is a final output (i.e. the agent produces something of type      <code>agent.output_type</code>), the loop terminates.</li> <li>If there's a handoff, we run the loop again, with the new agent.</li> <li>Else, we run tool calls (if any), and re-run the loop.</li> </ol> <p>In two cases, the agent may raise an exception:</p> <ol> <li>If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.</li> <li>If a guardrail tripwire is triggered, a GuardrailTripwireTriggered      exception is raised.</li> </ol> Note <p>Only the first agent's input guardrails are run.</p> <p>Parameters:</p> Name Type Description Default <code>starting_agent</code> <code>Agent[TContext]</code> <p>The starting agent to run.</p> required <code>input</code> <code>str | list[TResponseInputItem] | RunState[TContext]</code> <p>The initial input to the agent. You can pass a single string for a user message, or a list of input items.</p> required <code>context</code> <code>TContext | None</code> <p>The context to run the agent with.</p> <code>None</code> <code>max_turns</code> <code>int</code> <p>The maximum number of turns to run the agent for. A turn is defined as one AI invocation (including any tool calls that might occur).</p> <code>DEFAULT_MAX_TURNS</code> <code>hooks</code> <code>RunHooks[TContext] | None</code> <p>An object that receives callbacks on various lifecycle events.</p> <code>None</code> <code>run_config</code> <code>RunConfig | None</code> <p>Global settings for the entire agent run.</p> <code>None</code> <code>error_handlers</code> <code>RunErrorHandlers[TContext] | None</code> <p>Error handlers keyed by error kind. Currently supports max_turns.</p> <code>None</code> <code>previous_response_id</code> <code>str | None</code> <p>The ID of the previous response. If using OpenAI models via the Responses API, this allows you to skip passing in input from the previous turn.</p> <code>None</code> <code>conversation_id</code> <code>str | None</code> <p>The conversation ID (https://platform.openai.com/docs/guides/conversation-state?api-mode=responses). If provided, the conversation will be used to read and write items. Every agent will have access to the conversation history so far, and its output items will be written to the conversation. We recommend only using this if you are exclusively using OpenAI models; other model providers don't write to the Conversation object, so you'll end up having partial conversations stored.</p> <code>None</code> <code>session</code> <code>Session | None</code> <p>A session for automatic conversation history management.</p> <code>None</code> <p>Returns:</p> Type Description <code>RunResult</code> <p>A run result containing all the inputs, guardrail results and the output of</p> <code>RunResult</code> <p>the last agent. Agents may perform handoffs, so we don't know the specific</p> <code>RunResult</code> <p>type of the output.</p> Source code in <code>src/agents/run.py</code> <pre><code>@classmethod\nasync def run(\n    cls,\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem] | RunState[TContext],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    error_handlers: RunErrorHandlers[TContext] | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult:\n    \"\"\"\n    Run a workflow starting at the given agent.\n\n    The agent will run in a loop until a final output is generated. The loop runs like so:\n\n      1. The agent is invoked with the given input.\n      2. If there is a final output (i.e. the agent produces something of type\n         `agent.output_type`), the loop terminates.\n      3. If there's a handoff, we run the loop again, with the new agent.\n      4. Else, we run tool calls (if any), and re-run the loop.\n\n    In two cases, the agent may raise an exception:\n\n      1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.\n      2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered\n         exception is raised.\n\n    Note:\n        Only the first agent's input guardrails are run.\n\n    Args:\n        starting_agent: The starting agent to run.\n        input: The initial input to the agent. You can pass a single string for a\n            user message, or a list of input items.\n        context: The context to run the agent with.\n        max_turns: The maximum number of turns to run the agent for. A turn is\n            defined as one AI invocation (including any tool calls that might occur).\n        hooks: An object that receives callbacks on various lifecycle events.\n        run_config: Global settings for the entire agent run.\n        error_handlers: Error handlers keyed by error kind. Currently supports max_turns.\n        previous_response_id: The ID of the previous response. If using OpenAI\n            models via the Responses API, this allows you to skip passing in input\n            from the previous turn.\n        conversation_id: The conversation ID\n            (https://platform.openai.com/docs/guides/conversation-state?api-mode=responses).\n            If provided, the conversation will be used to read and write items.\n            Every agent will have access to the conversation history so far,\n            and its output items will be written to the conversation.\n            We recommend only using this if you are exclusively using OpenAI models;\n            other model providers don't write to the Conversation object,\n            so you'll end up having partial conversations stored.\n        session: A session for automatic conversation history management.\n\n    Returns:\n        A run result containing all the inputs, guardrail results and the output of\n        the last agent. Agents may perform handoffs, so we don't know the specific\n        type of the output.\n    \"\"\"\n\n    runner = DEFAULT_AGENT_RUNNER\n    return await runner.run(\n        starting_agent,\n        input,\n        context=context,\n        max_turns=max_turns,\n        hooks=hooks,\n        run_config=run_config,\n        error_handlers=error_handlers,\n        previous_response_id=previous_response_id,\n        auto_previous_response_id=auto_previous_response_id,\n        conversation_id=conversation_id,\n        session=session,\n    )\n</code></pre>"},{"location":"ref/run/#agents.run.Runner.run_sync","title":"run_sync  <code>classmethod</code>","text":"<pre><code>run_sync(\n    starting_agent: Agent[TContext],\n    input: str\n    | list[TResponseInputItem]\n    | RunState[TContext],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    error_handlers: RunErrorHandlers[TContext]\n    | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult\n</code></pre> <p>Run a workflow synchronously, starting at the given agent.</p> Note <p>This just wraps the <code>run</code> method, so it will not work if there's already an event loop (e.g. inside an async function, or in a Jupyter notebook or async context like FastAPI). For those cases, use the <code>run</code> method instead.</p> <p>The agent will run in a loop until a final output is generated. The loop runs:</p> <ol> <li>The agent is invoked with the given input.</li> <li>If there is a final output (i.e. the agent produces something of type      <code>agent.output_type</code>), the loop terminates.</li> <li>If there's a handoff, we run the loop again, with the new agent.</li> <li>Else, we run tool calls (if any), and re-run the loop.</li> </ol> <p>In two cases, the agent may raise an exception:</p> <ol> <li>If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.</li> <li>If a guardrail tripwire is triggered, a GuardrailTripwireTriggered      exception is raised.</li> </ol> Note <p>Only the first agent's input guardrails are run.</p> <p>Parameters:</p> Name Type Description Default <code>starting_agent</code> <code>Agent[TContext]</code> <p>The starting agent to run.</p> required <code>input</code> <code>str | list[TResponseInputItem] | RunState[TContext]</code> <p>The initial input to the agent. You can pass a single string for a user message, or a list of input items.</p> required <code>context</code> <code>TContext | None</code> <p>The context to run the agent with.</p> <code>None</code> <code>max_turns</code> <code>int</code> <p>The maximum number of turns to run the agent for. A turn is defined as one AI invocation (including any tool calls that might occur).</p> <code>DEFAULT_MAX_TURNS</code> <code>hooks</code> <code>RunHooks[TContext] | None</code> <p>An object that receives callbacks on various lifecycle events.</p> <code>None</code> <code>run_config</code> <code>RunConfig | None</code> <p>Global settings for the entire agent run.</p> <code>None</code> <code>error_handlers</code> <code>RunErrorHandlers[TContext] | None</code> <p>Error handlers keyed by error kind. Currently supports max_turns.</p> <code>None</code> <code>previous_response_id</code> <code>str | None</code> <p>The ID of the previous response, if using OpenAI models via the Responses API, this allows you to skip passing in input from the previous turn.</p> <code>None</code> <code>conversation_id</code> <code>str | None</code> <p>The ID of the stored conversation, if any.</p> <code>None</code> <code>session</code> <code>Session | None</code> <p>A session for automatic conversation history management.</p> <code>None</code> <p>Returns:</p> Type Description <code>RunResult</code> <p>A run result containing all the inputs, guardrail results and the output of</p> <code>RunResult</code> <p>the last agent. Agents may perform handoffs, so we don't know the specific</p> <code>RunResult</code> <p>type of the output.</p> Source code in <code>src/agents/run.py</code> <pre><code>@classmethod\ndef run_sync(\n    cls,\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem] | RunState[TContext],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    error_handlers: RunErrorHandlers[TContext] | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult:\n    \"\"\"\n    Run a workflow synchronously, starting at the given agent.\n\n    Note:\n        This just wraps the `run` method, so it will not work if there's already an\n        event loop (e.g. inside an async function, or in a Jupyter notebook or async\n        context like FastAPI). For those cases, use the `run` method instead.\n\n    The agent will run in a loop until a final output is generated. The loop runs:\n\n      1. The agent is invoked with the given input.\n      2. If there is a final output (i.e. the agent produces something of type\n         `agent.output_type`), the loop terminates.\n      3. If there's a handoff, we run the loop again, with the new agent.\n      4. Else, we run tool calls (if any), and re-run the loop.\n\n    In two cases, the agent may raise an exception:\n\n      1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.\n      2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered\n         exception is raised.\n\n    Note:\n        Only the first agent's input guardrails are run.\n\n    Args:\n        starting_agent: The starting agent to run.\n        input: The initial input to the agent. You can pass a single string for a\n            user message, or a list of input items.\n        context: The context to run the agent with.\n        max_turns: The maximum number of turns to run the agent for. A turn is\n            defined as one AI invocation (including any tool calls that might occur).\n        hooks: An object that receives callbacks on various lifecycle events.\n        run_config: Global settings for the entire agent run.\n        error_handlers: Error handlers keyed by error kind. Currently supports max_turns.\n        previous_response_id: The ID of the previous response, if using OpenAI\n            models via the Responses API, this allows you to skip passing in input\n            from the previous turn.\n        conversation_id: The ID of the stored conversation, if any.\n        session: A session for automatic conversation history management.\n\n    Returns:\n        A run result containing all the inputs, guardrail results and the output of\n        the last agent. Agents may perform handoffs, so we don't know the specific\n        type of the output.\n    \"\"\"\n\n    runner = DEFAULT_AGENT_RUNNER\n    return runner.run_sync(\n        starting_agent,\n        input,\n        context=context,\n        max_turns=max_turns,\n        hooks=hooks,\n        run_config=run_config,\n        error_handlers=error_handlers,\n        previous_response_id=previous_response_id,\n        conversation_id=conversation_id,\n        session=session,\n        auto_previous_response_id=auto_previous_response_id,\n    )\n</code></pre>"},{"location":"ref/run/#agents.run.Runner.run_streamed","title":"run_streamed  <code>classmethod</code>","text":"<pre><code>run_streamed(\n    starting_agent: Agent[TContext],\n    input: str\n    | list[TResponseInputItem]\n    | RunState[TContext],\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n    *,\n    error_handlers: RunErrorHandlers[TContext]\n    | None = None,\n) -&gt; RunResultStreaming\n</code></pre> <p>Run a workflow starting at the given agent in streaming mode.</p> <p>The returned result object contains a method you can use to stream semantic events as they are generated.</p> <p>The agent will run in a loop until a final output is generated. The loop runs like so:</p> <ol> <li>The agent is invoked with the given input.</li> <li>If there is a final output (i.e. the agent produces something of type      <code>agent.output_type</code>), the loop terminates.</li> <li>If there's a handoff, we run the loop again, with the new agent.</li> <li>Else, we run tool calls (if any), and re-run the loop.</li> </ol> <p>In two cases, the agent may raise an exception:</p> <ol> <li>If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.</li> <li>If a guardrail tripwire is triggered, a GuardrailTripwireTriggered      exception is raised.</li> </ol> Note <p>Only the first agent's input guardrails are run.</p> <p>Parameters:</p> Name Type Description Default <code>starting_agent</code> <code>Agent[TContext]</code> <p>The starting agent to run.</p> required <code>input</code> <code>str | list[TResponseInputItem] | RunState[TContext]</code> <p>The initial input to the agent. You can pass a single string for a user message, or a list of input items.</p> required <code>context</code> <code>TContext | None</code> <p>The context to run the agent with.</p> <code>None</code> <code>max_turns</code> <code>int</code> <p>The maximum number of turns to run the agent for. A turn is defined as one AI invocation (including any tool calls that might occur).</p> <code>DEFAULT_MAX_TURNS</code> <code>hooks</code> <code>RunHooks[TContext] | None</code> <p>An object that receives callbacks on various lifecycle events.</p> <code>None</code> <code>run_config</code> <code>RunConfig | None</code> <p>Global settings for the entire agent run.</p> <code>None</code> <code>error_handlers</code> <code>RunErrorHandlers[TContext] | None</code> <p>Error handlers keyed by error kind. Currently supports max_turns.</p> <code>None</code> <code>previous_response_id</code> <code>str | None</code> <p>The ID of the previous response, if using OpenAI models via the Responses API, this allows you to skip passing in input from the previous turn.</p> <code>None</code> <code>conversation_id</code> <code>str | None</code> <p>The ID of the stored conversation, if any.</p> <code>None</code> <code>session</code> <code>Session | None</code> <p>A session for automatic conversation history management.</p> <code>None</code> <p>Returns:</p> Type Description <code>RunResultStreaming</code> <p>A result object that contains data about the run, as well as a method to</p> <code>RunResultStreaming</code> <p>stream events.</p> Source code in <code>src/agents/run.py</code> <pre><code>@classmethod\ndef run_streamed(\n    cls,\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem] | RunState[TContext],\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n    conversation_id: str | None = None,\n    session: Session | None = None,\n    *,\n    error_handlers: RunErrorHandlers[TContext] | None = None,\n) -&gt; RunResultStreaming:\n    \"\"\"\n    Run a workflow starting at the given agent in streaming mode.\n\n    The returned result object contains a method you can use to stream semantic\n    events as they are generated.\n\n    The agent will run in a loop until a final output is generated. The loop runs like so:\n\n      1. The agent is invoked with the given input.\n      2. If there is a final output (i.e. the agent produces something of type\n         `agent.output_type`), the loop terminates.\n      3. If there's a handoff, we run the loop again, with the new agent.\n      4. Else, we run tool calls (if any), and re-run the loop.\n\n    In two cases, the agent may raise an exception:\n\n      1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised unless handled.\n      2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered\n         exception is raised.\n\n    Note:\n        Only the first agent's input guardrails are run.\n\n    Args:\n        starting_agent: The starting agent to run.\n        input: The initial input to the agent. You can pass a single string for a\n            user message, or a list of input items.\n        context: The context to run the agent with.\n        max_turns: The maximum number of turns to run the agent for. A turn is\n            defined as one AI invocation (including any tool calls that might occur).\n        hooks: An object that receives callbacks on various lifecycle events.\n        run_config: Global settings for the entire agent run.\n        error_handlers: Error handlers keyed by error kind. Currently supports max_turns.\n        previous_response_id: The ID of the previous response, if using OpenAI\n            models via the Responses API, this allows you to skip passing in input\n            from the previous turn.\n        conversation_id: The ID of the stored conversation, if any.\n        session: A session for automatic conversation history management.\n\n    Returns:\n        A result object that contains data about the run, as well as a method to\n        stream events.\n    \"\"\"\n\n    runner = DEFAULT_AGENT_RUNNER\n    return runner.run_streamed(\n        starting_agent,\n        input,\n        context=context,\n        max_turns=max_turns,\n        hooks=hooks,\n        run_config=run_config,\n        error_handlers=error_handlers,\n        previous_response_id=previous_response_id,\n        auto_previous_response_id=auto_previous_response_id,\n        conversation_id=conversation_id,\n        session=session,\n    )\n</code></pre>"},{"location":"ref/run/#agents.run.RunConfig","title":"RunConfig  <code>dataclass</code>","text":"<p>Configures settings for the entire agent run.</p> Source code in <code>src/agents/run_config.py</code> <pre><code>@dataclass\nclass RunConfig:\n    \"\"\"Configures settings for the entire agent run.\"\"\"\n\n    model: str | Model | None = None\n    \"\"\"The model to use for the entire agent run. If set, will override the model set on every\n    agent. The model_provider passed in below must be able to resolve this model name.\n    \"\"\"\n\n    model_provider: ModelProvider = field(default_factory=MultiProvider)\n    \"\"\"The model provider to use when looking up string model names. Defaults to OpenAI.\"\"\"\n\n    model_settings: ModelSettings | None = None\n    \"\"\"Configure global model settings. Any non-null values will override the agent-specific model\n    settings.\n    \"\"\"\n\n    handoff_input_filter: HandoffInputFilter | None = None\n    \"\"\"A global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that\n    will take precedence. The input filter allows you to edit the inputs that are sent to the new\n    agent. See the documentation in `Handoff.input_filter` for more details.\n    \"\"\"\n\n    nest_handoff_history: bool = False\n    \"\"\"Opt-in beta: wrap prior run history in a single assistant message before handing off when no\n    custom input filter is set. This is disabled by default while we stabilize nested handoffs; set\n    to True to enable the collapsed transcript behavior.\n    \"\"\"\n\n    handoff_history_mapper: HandoffHistoryMapper | None = None\n    \"\"\"Optional function that receives the normalized transcript (history + handoff items) and\n    returns the input history that should be passed to the next agent. When left as `None`, the\n    runner collapses the transcript into a single assistant message. This function only runs when\n    `nest_handoff_history` is True.\n    \"\"\"\n\n    input_guardrails: list[InputGuardrail[Any]] | None = None\n    \"\"\"A list of input guardrails to run on the initial run input.\"\"\"\n\n    output_guardrails: list[OutputGuardrail[Any]] | None = None\n    \"\"\"A list of output guardrails to run on the final output of the run.\"\"\"\n\n    tracing_disabled: bool = False\n    \"\"\"Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.\n    \"\"\"\n\n    tracing: TracingConfig | None = None\n    \"\"\"Tracing configuration for this run.\"\"\"\n\n    trace_include_sensitive_data: bool = field(\n        default_factory=_default_trace_include_sensitive_data\n    )\n    \"\"\"Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or\n    LLM generations) in traces. If False, we'll still create spans for these events, but the\n    sensitive data will not be included.\n    \"\"\"\n\n    workflow_name: str = \"Agent workflow\"\n    \"\"\"The name of the run, used for tracing. Should be a logical name for the run, like\n    \"Code generation workflow\" or \"Customer support agent\".\n    \"\"\"\n\n    trace_id: str | None = None\n    \"\"\"A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.\"\"\"\n\n    group_id: str | None = None\n    \"\"\"\n    A grouping identifier to use for tracing, to link multiple traces from the same conversation\n    or process. For example, you might use a chat thread ID.\n    \"\"\"\n\n    trace_metadata: dict[str, Any] | None = None\n    \"\"\"\n    An optional dictionary of additional metadata to include with the trace.\n    \"\"\"\n\n    session_input_callback: SessionInputCallback | None = None\n    \"\"\"Defines how to handle session history when new input is provided.\n    - `None` (default): The new input is appended to the session history.\n    - `SessionInputCallback`: A custom function that receives the history and new input, and\n      returns the desired combined list of items.\n    \"\"\"\n\n    call_model_input_filter: CallModelInputFilter | None = None\n    \"\"\"\n    Optional callback that is invoked immediately before calling the model. It receives the current\n    agent, context and the model input (instructions and input items), and must return a possibly\n    modified `ModelInputData` to use for the model call.\n\n    This allows you to edit the input sent to the model e.g. to stay within a token limit.\n    For example, you can use this to add a system prompt to the input.\n    \"\"\"\n\n    tool_error_formatter: ToolErrorFormatter | None = None\n    \"\"\"Optional callback that formats tool error messages returned to the model.\n\n    Returning ``None`` falls back to the SDK default message.\n    \"\"\"\n\n    session_settings: SessionSettings | None = None\n    \"\"\"Configure session settings. Any non-null values will override the session's default\n    settings. Used to control session behavior like the number of items to retrieve.\n    \"\"\"\n</code></pre>"},{"location":"ref/run/#agents.run.RunConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str | Model | None = None\n</code></pre> <p>The model to use for the entire agent run. If set, will override the model set on every agent. The model_provider passed in below must be able to resolve this model name.</p>"},{"location":"ref/run/#agents.run.RunConfig.model_provider","title":"model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_provider: ModelProvider = field(\n    default_factory=MultiProvider\n)\n</code></pre> <p>The model provider to use when looking up string model names. Defaults to OpenAI.</p>"},{"location":"ref/run/#agents.run.RunConfig.model_settings","title":"model_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_settings: ModelSettings | None = None\n</code></pre> <p>Configure global model settings. Any non-null values will override the agent-specific model settings.</p>"},{"location":"ref/run/#agents.run.RunConfig.handoff_input_filter","title":"handoff_input_filter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_input_filter: HandoffInputFilter | None = None\n</code></pre> <p>A global input filter to apply to all handoffs. If <code>Handoff.input_filter</code> is set, then that will take precedence. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in <code>Handoff.input_filter</code> for more details.</p>"},{"location":"ref/run/#agents.run.RunConfig.nest_handoff_history","title":"nest_handoff_history  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nest_handoff_history: bool = False\n</code></pre> <p>Opt-in beta: wrap prior run history in a single assistant message before handing off when no custom input filter is set. This is disabled by default while we stabilize nested handoffs; set to True to enable the collapsed transcript behavior.</p>"},{"location":"ref/run/#agents.run.RunConfig.handoff_history_mapper","title":"handoff_history_mapper  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_history_mapper: HandoffHistoryMapper | None = None\n</code></pre> <p>Optional function that receives the normalized transcript (history + handoff items) and returns the input history that should be passed to the next agent. When left as <code>None</code>, the runner collapses the transcript into a single assistant message. This function only runs when <code>nest_handoff_history</code> is True.</p>"},{"location":"ref/run/#agents.run.RunConfig.input_guardrails","title":"input_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_guardrails: list[InputGuardrail[Any]] | None = None\n</code></pre> <p>A list of input guardrails to run on the initial run input.</p>"},{"location":"ref/run/#agents.run.RunConfig.output_guardrails","title":"output_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrails: list[OutputGuardrail[Any]] | None = None\n</code></pre> <p>A list of output guardrails to run on the final output of the run.</p>"},{"location":"ref/run/#agents.run.RunConfig.tracing_disabled","title":"tracing_disabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing_disabled: bool = False\n</code></pre> <p>Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.</p>"},{"location":"ref/run/#agents.run.RunConfig.tracing","title":"tracing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing: TracingConfig | None = None\n</code></pre> <p>Tracing configuration for this run.</p>"},{"location":"ref/run/#agents.run.RunConfig.trace_include_sensitive_data","title":"trace_include_sensitive_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_include_sensitive_data: bool = field(\n    default_factory=_default_trace_include_sensitive_data\n)\n</code></pre> <p>Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or LLM generations) in traces. If False, we'll still create spans for these events, but the sensitive data will not be included.</p>"},{"location":"ref/run/#agents.run.RunConfig.workflow_name","title":"workflow_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workflow_name: str = 'Agent workflow'\n</code></pre> <p>The name of the run, used for tracing. Should be a logical name for the run, like \"Code generation workflow\" or \"Customer support agent\".</p>"},{"location":"ref/run/#agents.run.RunConfig.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: str | None = None\n</code></pre> <p>A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.</p>"},{"location":"ref/run/#agents.run.RunConfig.group_id","title":"group_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>group_id: str | None = None\n</code></pre> <p>A grouping identifier to use for tracing, to link multiple traces from the same conversation or process. For example, you might use a chat thread ID.</p>"},{"location":"ref/run/#agents.run.RunConfig.trace_metadata","title":"trace_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_metadata: dict[str, Any] | None = None\n</code></pre> <p>An optional dictionary of additional metadata to include with the trace.</p>"},{"location":"ref/run/#agents.run.RunConfig.session_input_callback","title":"session_input_callback  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_input_callback: SessionInputCallback | None = None\n</code></pre> <p>Defines how to handle session history when new input is provided. - <code>None</code> (default): The new input is appended to the session history. - <code>SessionInputCallback</code>: A custom function that receives the history and new input, and   returns the desired combined list of items.</p>"},{"location":"ref/run/#agents.run.RunConfig.call_model_input_filter","title":"call_model_input_filter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>call_model_input_filter: CallModelInputFilter | None = None\n</code></pre> <p>Optional callback that is invoked immediately before calling the model. It receives the current agent, context and the model input (instructions and input items), and must return a possibly modified <code>ModelInputData</code> to use for the model call.</p> <p>This allows you to edit the input sent to the model e.g. to stay within a token limit. For example, you can use this to add a system prompt to the input.</p>"},{"location":"ref/run/#agents.run.RunConfig.tool_error_formatter","title":"tool_error_formatter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_error_formatter: ToolErrorFormatter | None = None\n</code></pre> <p>Optional callback that formats tool error messages returned to the model.</p> <p>Returning <code>None</code> falls back to the SDK default message.</p>"},{"location":"ref/run/#agents.run.RunConfig.session_settings","title":"session_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_settings: SessionSettings | None = None\n</code></pre> <p>Configure session settings. Any non-null values will override the session's default settings. Used to control session behavior like the number of items to retrieve.</p>"},{"location":"ref/run_config/","title":"<code>Run Config</code>","text":""},{"location":"ref/run_config/#agents.run_config.ModelInputData","title":"ModelInputData  <code>dataclass</code>","text":"<p>Container for the data that will be sent to the model.</p> Source code in <code>src/agents/run_config.py</code> <pre><code>@dataclass\nclass ModelInputData:\n    \"\"\"Container for the data that will be sent to the model.\"\"\"\n\n    input: list[TResponseInputItem]\n    instructions: str | None\n</code></pre>"},{"location":"ref/run_config/#agents.run_config.CallModelData","title":"CallModelData  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Data passed to <code>RunConfig.call_model_input_filter</code> prior to model call.</p> Source code in <code>src/agents/run_config.py</code> <pre><code>@dataclass\nclass CallModelData(Generic[TContext]):\n    \"\"\"Data passed to `RunConfig.call_model_input_filter` prior to model call.\"\"\"\n\n    model_data: ModelInputData\n    agent: Agent[TContext]\n    context: TContext | None\n</code></pre>"},{"location":"ref/run_config/#agents.run_config.ToolErrorFormatterArgs","title":"ToolErrorFormatterArgs  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Data passed to <code>RunConfig.tool_error_formatter</code> callbacks.</p> Source code in <code>src/agents/run_config.py</code> <pre><code>@dataclass\nclass ToolErrorFormatterArgs(Generic[TContext]):\n    \"\"\"Data passed to ``RunConfig.tool_error_formatter`` callbacks.\"\"\"\n\n    kind: Literal[\"approval_rejected\"]\n    \"\"\"The category of tool error being formatted.\"\"\"\n\n    tool_type: Literal[\"function\", \"computer\", \"shell\", \"apply_patch\"]\n    \"\"\"The tool runtime that produced the error.\"\"\"\n\n    tool_name: str\n    \"\"\"The name of the tool that produced the error.\"\"\"\n\n    call_id: str\n    \"\"\"The unique tool call identifier.\"\"\"\n\n    default_message: str\n    \"\"\"The SDK default message for this error kind.\"\"\"\n\n    run_context: RunContextWrapper[TContext]\n    \"\"\"The active run context for the current execution.\"\"\"\n</code></pre>"},{"location":"ref/run_config/#agents.run_config.ToolErrorFormatterArgs.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['approval_rejected']\n</code></pre> <p>The category of tool error being formatted.</p>"},{"location":"ref/run_config/#agents.run_config.ToolErrorFormatterArgs.tool_type","title":"tool_type  <code>instance-attribute</code>","text":"<pre><code>tool_type: Literal[\n    \"function\", \"computer\", \"shell\", \"apply_patch\"\n]\n</code></pre> <p>The tool runtime that produced the error.</p>"},{"location":"ref/run_config/#agents.run_config.ToolErrorFormatterArgs.tool_name","title":"tool_name  <code>instance-attribute</code>","text":"<pre><code>tool_name: str\n</code></pre> <p>The name of the tool that produced the error.</p>"},{"location":"ref/run_config/#agents.run_config.ToolErrorFormatterArgs.call_id","title":"call_id  <code>instance-attribute</code>","text":"<pre><code>call_id: str\n</code></pre> <p>The unique tool call identifier.</p>"},{"location":"ref/run_config/#agents.run_config.ToolErrorFormatterArgs.default_message","title":"default_message  <code>instance-attribute</code>","text":"<pre><code>default_message: str\n</code></pre> <p>The SDK default message for this error kind.</p>"},{"location":"ref/run_config/#agents.run_config.ToolErrorFormatterArgs.run_context","title":"run_context  <code>instance-attribute</code>","text":"<pre><code>run_context: RunContextWrapper[TContext]\n</code></pre> <p>The active run context for the current execution.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig","title":"RunConfig  <code>dataclass</code>","text":"<p>Configures settings for the entire agent run.</p> Source code in <code>src/agents/run_config.py</code> <pre><code>@dataclass\nclass RunConfig:\n    \"\"\"Configures settings for the entire agent run.\"\"\"\n\n    model: str | Model | None = None\n    \"\"\"The model to use for the entire agent run. If set, will override the model set on every\n    agent. The model_provider passed in below must be able to resolve this model name.\n    \"\"\"\n\n    model_provider: ModelProvider = field(default_factory=MultiProvider)\n    \"\"\"The model provider to use when looking up string model names. Defaults to OpenAI.\"\"\"\n\n    model_settings: ModelSettings | None = None\n    \"\"\"Configure global model settings. Any non-null values will override the agent-specific model\n    settings.\n    \"\"\"\n\n    handoff_input_filter: HandoffInputFilter | None = None\n    \"\"\"A global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that\n    will take precedence. The input filter allows you to edit the inputs that are sent to the new\n    agent. See the documentation in `Handoff.input_filter` for more details.\n    \"\"\"\n\n    nest_handoff_history: bool = False\n    \"\"\"Opt-in beta: wrap prior run history in a single assistant message before handing off when no\n    custom input filter is set. This is disabled by default while we stabilize nested handoffs; set\n    to True to enable the collapsed transcript behavior.\n    \"\"\"\n\n    handoff_history_mapper: HandoffHistoryMapper | None = None\n    \"\"\"Optional function that receives the normalized transcript (history + handoff items) and\n    returns the input history that should be passed to the next agent. When left as `None`, the\n    runner collapses the transcript into a single assistant message. This function only runs when\n    `nest_handoff_history` is True.\n    \"\"\"\n\n    input_guardrails: list[InputGuardrail[Any]] | None = None\n    \"\"\"A list of input guardrails to run on the initial run input.\"\"\"\n\n    output_guardrails: list[OutputGuardrail[Any]] | None = None\n    \"\"\"A list of output guardrails to run on the final output of the run.\"\"\"\n\n    tracing_disabled: bool = False\n    \"\"\"Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.\n    \"\"\"\n\n    tracing: TracingConfig | None = None\n    \"\"\"Tracing configuration for this run.\"\"\"\n\n    trace_include_sensitive_data: bool = field(\n        default_factory=_default_trace_include_sensitive_data\n    )\n    \"\"\"Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or\n    LLM generations) in traces. If False, we'll still create spans for these events, but the\n    sensitive data will not be included.\n    \"\"\"\n\n    workflow_name: str = \"Agent workflow\"\n    \"\"\"The name of the run, used for tracing. Should be a logical name for the run, like\n    \"Code generation workflow\" or \"Customer support agent\".\n    \"\"\"\n\n    trace_id: str | None = None\n    \"\"\"A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.\"\"\"\n\n    group_id: str | None = None\n    \"\"\"\n    A grouping identifier to use for tracing, to link multiple traces from the same conversation\n    or process. For example, you might use a chat thread ID.\n    \"\"\"\n\n    trace_metadata: dict[str, Any] | None = None\n    \"\"\"\n    An optional dictionary of additional metadata to include with the trace.\n    \"\"\"\n\n    session_input_callback: SessionInputCallback | None = None\n    \"\"\"Defines how to handle session history when new input is provided.\n    - `None` (default): The new input is appended to the session history.\n    - `SessionInputCallback`: A custom function that receives the history and new input, and\n      returns the desired combined list of items.\n    \"\"\"\n\n    call_model_input_filter: CallModelInputFilter | None = None\n    \"\"\"\n    Optional callback that is invoked immediately before calling the model. It receives the current\n    agent, context and the model input (instructions and input items), and must return a possibly\n    modified `ModelInputData` to use for the model call.\n\n    This allows you to edit the input sent to the model e.g. to stay within a token limit.\n    For example, you can use this to add a system prompt to the input.\n    \"\"\"\n\n    tool_error_formatter: ToolErrorFormatter | None = None\n    \"\"\"Optional callback that formats tool error messages returned to the model.\n\n    Returning ``None`` falls back to the SDK default message.\n    \"\"\"\n\n    session_settings: SessionSettings | None = None\n    \"\"\"Configure session settings. Any non-null values will override the session's default\n    settings. Used to control session behavior like the number of items to retrieve.\n    \"\"\"\n</code></pre>"},{"location":"ref/run_config/#agents.run_config.RunConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str | Model | None = None\n</code></pre> <p>The model to use for the entire agent run. If set, will override the model set on every agent. The model_provider passed in below must be able to resolve this model name.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.model_provider","title":"model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_provider: ModelProvider = field(\n    default_factory=MultiProvider\n)\n</code></pre> <p>The model provider to use when looking up string model names. Defaults to OpenAI.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.model_settings","title":"model_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_settings: ModelSettings | None = None\n</code></pre> <p>Configure global model settings. Any non-null values will override the agent-specific model settings.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.handoff_input_filter","title":"handoff_input_filter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_input_filter: HandoffInputFilter | None = None\n</code></pre> <p>A global input filter to apply to all handoffs. If <code>Handoff.input_filter</code> is set, then that will take precedence. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in <code>Handoff.input_filter</code> for more details.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.nest_handoff_history","title":"nest_handoff_history  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nest_handoff_history: bool = False\n</code></pre> <p>Opt-in beta: wrap prior run history in a single assistant message before handing off when no custom input filter is set. This is disabled by default while we stabilize nested handoffs; set to True to enable the collapsed transcript behavior.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.handoff_history_mapper","title":"handoff_history_mapper  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_history_mapper: HandoffHistoryMapper | None = None\n</code></pre> <p>Optional function that receives the normalized transcript (history + handoff items) and returns the input history that should be passed to the next agent. When left as <code>None</code>, the runner collapses the transcript into a single assistant message. This function only runs when <code>nest_handoff_history</code> is True.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.input_guardrails","title":"input_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_guardrails: list[InputGuardrail[Any]] | None = None\n</code></pre> <p>A list of input guardrails to run on the initial run input.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.output_guardrails","title":"output_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrails: list[OutputGuardrail[Any]] | None = None\n</code></pre> <p>A list of output guardrails to run on the final output of the run.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.tracing_disabled","title":"tracing_disabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing_disabled: bool = False\n</code></pre> <p>Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.tracing","title":"tracing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing: TracingConfig | None = None\n</code></pre> <p>Tracing configuration for this run.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.trace_include_sensitive_data","title":"trace_include_sensitive_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_include_sensitive_data: bool = field(\n    default_factory=_default_trace_include_sensitive_data\n)\n</code></pre> <p>Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or LLM generations) in traces. If False, we'll still create spans for these events, but the sensitive data will not be included.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.workflow_name","title":"workflow_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workflow_name: str = 'Agent workflow'\n</code></pre> <p>The name of the run, used for tracing. Should be a logical name for the run, like \"Code generation workflow\" or \"Customer support agent\".</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: str | None = None\n</code></pre> <p>A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.group_id","title":"group_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>group_id: str | None = None\n</code></pre> <p>A grouping identifier to use for tracing, to link multiple traces from the same conversation or process. For example, you might use a chat thread ID.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.trace_metadata","title":"trace_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_metadata: dict[str, Any] | None = None\n</code></pre> <p>An optional dictionary of additional metadata to include with the trace.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.session_input_callback","title":"session_input_callback  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_input_callback: SessionInputCallback | None = None\n</code></pre> <p>Defines how to handle session history when new input is provided. - <code>None</code> (default): The new input is appended to the session history. - <code>SessionInputCallback</code>: A custom function that receives the history and new input, and   returns the desired combined list of items.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.call_model_input_filter","title":"call_model_input_filter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>call_model_input_filter: CallModelInputFilter | None = None\n</code></pre> <p>Optional callback that is invoked immediately before calling the model. It receives the current agent, context and the model input (instructions and input items), and must return a possibly modified <code>ModelInputData</code> to use for the model call.</p> <p>This allows you to edit the input sent to the model e.g. to stay within a token limit. For example, you can use this to add a system prompt to the input.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.tool_error_formatter","title":"tool_error_formatter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_error_formatter: ToolErrorFormatter | None = None\n</code></pre> <p>Optional callback that formats tool error messages returned to the model.</p> <p>Returning <code>None</code> falls back to the SDK default message.</p>"},{"location":"ref/run_config/#agents.run_config.RunConfig.session_settings","title":"session_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_settings: SessionSettings | None = None\n</code></pre> <p>Configure session settings. Any non-null values will override the session's default settings. Used to control session behavior like the number of items to retrieve.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions","title":"RunOptions","text":"<p>               Bases: <code>TypedDict</code>, <code>Generic[TContext]</code></p> <p>Arguments for <code>AgentRunner</code> methods.</p> Source code in <code>src/agents/run_config.py</code> <pre><code>class RunOptions(TypedDict, Generic[TContext]):\n    \"\"\"Arguments for ``AgentRunner`` methods.\"\"\"\n\n    context: NotRequired[TContext | None]\n    \"\"\"The context for the run.\"\"\"\n\n    max_turns: NotRequired[int]\n    \"\"\"The maximum number of turns to run for.\"\"\"\n\n    hooks: NotRequired[RunHooks[TContext] | None]\n    \"\"\"Lifecycle hooks for the run.\"\"\"\n\n    run_config: NotRequired[RunConfig | None]\n    \"\"\"Run configuration.\"\"\"\n\n    previous_response_id: NotRequired[str | None]\n    \"\"\"The ID of the previous response, if any.\"\"\"\n\n    auto_previous_response_id: NotRequired[bool]\n    \"\"\"Enable automatic response chaining for the first turn.\"\"\"\n\n    conversation_id: NotRequired[str | None]\n    \"\"\"The ID of the stored conversation, if any.\"\"\"\n\n    session: NotRequired[Session | None]\n    \"\"\"The session for the run.\"\"\"\n\n    error_handlers: NotRequired[RunErrorHandlers[TContext] | None]\n    \"\"\"Error handlers keyed by error kind. Currently supports max_turns.\"\"\"\n</code></pre>"},{"location":"ref/run_config/#agents.run_config.RunOptions.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: NotRequired[TContext | None]\n</code></pre> <p>The context for the run.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.max_turns","title":"max_turns  <code>instance-attribute</code>","text":"<pre><code>max_turns: NotRequired[int]\n</code></pre> <p>The maximum number of turns to run for.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.hooks","title":"hooks  <code>instance-attribute</code>","text":"<pre><code>hooks: NotRequired[RunHooks[TContext] | None]\n</code></pre> <p>Lifecycle hooks for the run.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.run_config","title":"run_config  <code>instance-attribute</code>","text":"<pre><code>run_config: NotRequired[RunConfig | None]\n</code></pre> <p>Run configuration.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.previous_response_id","title":"previous_response_id  <code>instance-attribute</code>","text":"<pre><code>previous_response_id: NotRequired[str | None]\n</code></pre> <p>The ID of the previous response, if any.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.auto_previous_response_id","title":"auto_previous_response_id  <code>instance-attribute</code>","text":"<pre><code>auto_previous_response_id: NotRequired[bool]\n</code></pre> <p>Enable automatic response chaining for the first turn.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.conversation_id","title":"conversation_id  <code>instance-attribute</code>","text":"<pre><code>conversation_id: NotRequired[str | None]\n</code></pre> <p>The ID of the stored conversation, if any.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.session","title":"session  <code>instance-attribute</code>","text":"<pre><code>session: NotRequired[Session | None]\n</code></pre> <p>The session for the run.</p>"},{"location":"ref/run_config/#agents.run_config.RunOptions.error_handlers","title":"error_handlers  <code>instance-attribute</code>","text":"<pre><code>error_handlers: NotRequired[\n    RunErrorHandlers[TContext] | None\n]\n</code></pre> <p>Error handlers keyed by error kind. Currently supports max_turns.</p>"},{"location":"ref/run_context/","title":"<code>Run context</code>","text":""},{"location":"ref/run_context/#agents.run_context.RunContextWrapper","title":"RunContextWrapper  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>This wraps the context object that you passed to <code>Runner.run()</code>. It also contains information about the usage of the agent run so far.</p> <p>NOTE: Contexts are not passed to the LLM. They're a way to pass dependencies and data to code you implement, like tool functions, callbacks, hooks, etc.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>@dataclass(eq=False)\nclass RunContextWrapper(Generic[TContext]):\n    \"\"\"This wraps the context object that you passed to `Runner.run()`. It also contains\n    information about the usage of the agent run so far.\n\n    NOTE: Contexts are not passed to the LLM. They're a way to pass dependencies and data to code\n    you implement, like tool functions, callbacks, hooks, etc.\n    \"\"\"\n\n    context: TContext\n    \"\"\"The context object (or None), passed by you to `Runner.run()`\"\"\"\n\n    usage: Usage = field(default_factory=Usage)\n    \"\"\"The usage of the agent run so far. For streamed responses, the usage will be stale until the\n    last chunk of the stream is processed.\n    \"\"\"\n\n    turn_input: list[TResponseInputItem] = field(default_factory=list)\n    _approvals: dict[str, _ApprovalRecord] = field(default_factory=dict)\n    tool_input: Any | None = None\n    \"\"\"Structured input for the current agent tool run, when available.\"\"\"\n\n    @staticmethod\n    def _to_str_or_none(value: Any) -&gt; str | None:\n        if isinstance(value, str):\n            return value\n        if value is not None:\n            try:\n                return str(value)\n            except Exception:\n                return None\n        return None\n\n    @staticmethod\n    def _resolve_tool_name(approval_item: ToolApprovalItem) -&gt; str:\n        raw = approval_item.raw_item\n        if approval_item.tool_name:\n            return approval_item.tool_name\n        candidate: Any | None\n        if isinstance(raw, dict):\n            candidate = raw.get(\"name\") or raw.get(\"type\")\n        else:\n            candidate = getattr(raw, \"name\", None) or getattr(raw, \"type\", None)\n        return RunContextWrapper._to_str_or_none(candidate) or \"unknown_tool\"\n\n    @staticmethod\n    def _resolve_call_id(approval_item: ToolApprovalItem) -&gt; str | None:\n        raw = approval_item.raw_item\n        if isinstance(raw, dict):\n            provider_data = raw.get(\"provider_data\")\n            if (\n                isinstance(provider_data, dict)\n                and provider_data.get(\"type\") == \"mcp_approval_request\"\n            ):\n                candidate = provider_data.get(\"id\")\n                if isinstance(candidate, str):\n                    return candidate\n            candidate = raw.get(\"call_id\") or raw.get(\"id\")\n        else:\n            provider_data = getattr(raw, \"provider_data\", None)\n            if (\n                isinstance(provider_data, dict)\n                and provider_data.get(\"type\") == \"mcp_approval_request\"\n            ):\n                candidate = provider_data.get(\"id\")\n                if isinstance(candidate, str):\n                    return candidate\n            candidate = getattr(raw, \"call_id\", None) or getattr(raw, \"id\", None)\n        return RunContextWrapper._to_str_or_none(candidate)\n\n    def _get_or_create_approval_entry(self, tool_name: str) -&gt; _ApprovalRecord:\n        approval_entry = self._approvals.get(tool_name)\n        if approval_entry is None:\n            approval_entry = _ApprovalRecord()\n            self._approvals[tool_name] = approval_entry\n        return approval_entry\n\n    def is_tool_approved(self, tool_name: str, call_id: str) -&gt; bool | None:\n        \"\"\"Return True/False/None for the given tool call.\"\"\"\n        approval_entry = self._approvals.get(tool_name)\n        if not approval_entry:\n            return None\n\n        # Check for permanent approval/rejection\n        if approval_entry.approved is True and approval_entry.rejected is True:\n            # Approval takes precedence\n            return True\n\n        if approval_entry.approved is True:\n            return True\n\n        if approval_entry.rejected is True:\n            return False\n\n        approved_ids = (\n            set(approval_entry.approved) if isinstance(approval_entry.approved, list) else set()\n        )\n        rejected_ids = (\n            set(approval_entry.rejected) if isinstance(approval_entry.rejected, list) else set()\n        )\n\n        if call_id in approved_ids:\n            return True\n        if call_id in rejected_ids:\n            return False\n        # Per-call approvals are scoped to the exact call ID, so other calls require a new decision.\n        return None\n\n    def _apply_approval_decision(\n        self, approval_item: ToolApprovalItem, *, always: bool, approve: bool\n    ) -&gt; None:\n        \"\"\"Record an approval or rejection decision.\"\"\"\n        tool_name = self._resolve_tool_name(approval_item)\n        call_id = self._resolve_call_id(approval_item)\n\n        approval_entry = self._get_or_create_approval_entry(tool_name)\n        if always or call_id is None:\n            approval_entry.approved = approve\n            approval_entry.rejected = [] if approve else True\n            if not approve:\n                approval_entry.approved = False\n            return\n\n        opposite = approval_entry.rejected if approve else approval_entry.approved\n        if isinstance(opposite, list) and call_id in opposite:\n            opposite.remove(call_id)\n\n        target = approval_entry.approved if approve else approval_entry.rejected\n        if isinstance(target, list) and call_id not in target:\n            target.append(call_id)\n\n    def approve_tool(self, approval_item: ToolApprovalItem, always_approve: bool = False) -&gt; None:\n        \"\"\"Approve a tool call, optionally for all future calls.\"\"\"\n        self._apply_approval_decision(\n            approval_item,\n            always=always_approve,\n            approve=True,\n        )\n\n    def reject_tool(self, approval_item: ToolApprovalItem, always_reject: bool = False) -&gt; None:\n        \"\"\"Reject a tool call, optionally for all future calls.\"\"\"\n        self._apply_approval_decision(\n            approval_item,\n            always=always_reject,\n            approve=False,\n        )\n\n    def get_approval_status(\n        self, tool_name: str, call_id: str, *, existing_pending: ToolApprovalItem | None = None\n    ) -&gt; bool | None:\n        \"\"\"Return approval status, retrying with pending item's tool name if necessary.\"\"\"\n        status = self.is_tool_approved(tool_name, call_id)\n        if status is None and existing_pending:\n            fallback_tool_name = self._resolve_tool_name(existing_pending)\n            status = self.is_tool_approved(fallback_tool_name, call_id)\n        return status\n\n    def _rebuild_approvals(self, approvals: dict[str, dict[str, Any]]) -&gt; None:\n        \"\"\"Restore approvals from serialized state.\"\"\"\n        self._approvals = {}\n        for tool_name, record_dict in approvals.items():\n            record = _ApprovalRecord()\n            record.approved = record_dict.get(\"approved\", [])\n            record.rejected = record_dict.get(\"rejected\", [])\n            self._approvals[tool_name] = record\n\n    def _fork_with_tool_input(self, tool_input: Any) -&gt; RunContextWrapper[TContext]:\n        \"\"\"Create a child context that shares approvals and usage with tool input set.\"\"\"\n        fork = RunContextWrapper(context=self.context)\n        fork.usage = self.usage\n        fork._approvals = self._approvals\n        fork.turn_input = self.turn_input\n        fork.tool_input = tool_input\n        return fork\n\n    def _fork_without_tool_input(self) -&gt; RunContextWrapper[TContext]:\n        \"\"\"Create a child context that shares approvals and usage without tool input.\"\"\"\n        fork = RunContextWrapper(context=self.context)\n        fork.usage = self.usage\n        fork._approvals = self._approvals\n        fork.turn_input = self.turn_input\n        return fork\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: TContext\n</code></pre> <p>The context object (or None), passed by you to <code>Runner.run()</code></p>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Usage = field(default_factory=Usage)\n</code></pre> <p>The usage of the agent run so far. For streamed responses, the usage will be stale until the last chunk of the stream is processed.</p>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.tool_input","title":"tool_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_input: Any | None = None\n</code></pre> <p>Structured input for the current agent tool run, when available.</p>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.is_tool_approved","title":"is_tool_approved","text":"<pre><code>is_tool_approved(\n    tool_name: str, call_id: str\n) -&gt; bool | None\n</code></pre> <p>Return True/False/None for the given tool call.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def is_tool_approved(self, tool_name: str, call_id: str) -&gt; bool | None:\n    \"\"\"Return True/False/None for the given tool call.\"\"\"\n    approval_entry = self._approvals.get(tool_name)\n    if not approval_entry:\n        return None\n\n    # Check for permanent approval/rejection\n    if approval_entry.approved is True and approval_entry.rejected is True:\n        # Approval takes precedence\n        return True\n\n    if approval_entry.approved is True:\n        return True\n\n    if approval_entry.rejected is True:\n        return False\n\n    approved_ids = (\n        set(approval_entry.approved) if isinstance(approval_entry.approved, list) else set()\n    )\n    rejected_ids = (\n        set(approval_entry.rejected) if isinstance(approval_entry.rejected, list) else set()\n    )\n\n    if call_id in approved_ids:\n        return True\n    if call_id in rejected_ids:\n        return False\n    # Per-call approvals are scoped to the exact call ID, so other calls require a new decision.\n    return None\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.approve_tool","title":"approve_tool","text":"<pre><code>approve_tool(\n    approval_item: ToolApprovalItem,\n    always_approve: bool = False,\n) -&gt; None\n</code></pre> <p>Approve a tool call, optionally for all future calls.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def approve_tool(self, approval_item: ToolApprovalItem, always_approve: bool = False) -&gt; None:\n    \"\"\"Approve a tool call, optionally for all future calls.\"\"\"\n    self._apply_approval_decision(\n        approval_item,\n        always=always_approve,\n        approve=True,\n    )\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.reject_tool","title":"reject_tool","text":"<pre><code>reject_tool(\n    approval_item: ToolApprovalItem,\n    always_reject: bool = False,\n) -&gt; None\n</code></pre> <p>Reject a tool call, optionally for all future calls.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def reject_tool(self, approval_item: ToolApprovalItem, always_reject: bool = False) -&gt; None:\n    \"\"\"Reject a tool call, optionally for all future calls.\"\"\"\n    self._apply_approval_decision(\n        approval_item,\n        always=always_reject,\n        approve=False,\n    )\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.get_approval_status","title":"get_approval_status","text":"<pre><code>get_approval_status(\n    tool_name: str,\n    call_id: str,\n    *,\n    existing_pending: ToolApprovalItem | None = None,\n) -&gt; bool | None\n</code></pre> <p>Return approval status, retrying with pending item's tool name if necessary.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def get_approval_status(\n    self, tool_name: str, call_id: str, *, existing_pending: ToolApprovalItem | None = None\n) -&gt; bool | None:\n    \"\"\"Return approval status, retrying with pending item's tool name if necessary.\"\"\"\n    status = self.is_tool_approved(tool_name, call_id)\n    if status is None and existing_pending:\n        fallback_tool_name = self._resolve_tool_name(existing_pending)\n        status = self.is_tool_approved(fallback_tool_name, call_id)\n    return status\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext","title":"AgentHookContext  <code>dataclass</code>","text":"<p>               Bases: <code>RunContextWrapper[TContext]</code></p> <p>Context passed to agent hooks (on_start, on_end).</p> Source code in <code>src/agents/run_context.py</code> <pre><code>@dataclass(eq=False)\nclass AgentHookContext(RunContextWrapper[TContext]):\n    \"\"\"Context passed to agent hooks (on_start, on_end).\"\"\"\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: TContext\n</code></pre> <p>The context object (or None), passed by you to <code>Runner.run()</code></p>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Usage = field(default_factory=Usage)\n</code></pre> <p>The usage of the agent run so far. For streamed responses, the usage will be stale until the last chunk of the stream is processed.</p>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext.tool_input","title":"tool_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_input: Any | None = None\n</code></pre> <p>Structured input for the current agent tool run, when available.</p>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext.is_tool_approved","title":"is_tool_approved","text":"<pre><code>is_tool_approved(\n    tool_name: str, call_id: str\n) -&gt; bool | None\n</code></pre> <p>Return True/False/None for the given tool call.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def is_tool_approved(self, tool_name: str, call_id: str) -&gt; bool | None:\n    \"\"\"Return True/False/None for the given tool call.\"\"\"\n    approval_entry = self._approvals.get(tool_name)\n    if not approval_entry:\n        return None\n\n    # Check for permanent approval/rejection\n    if approval_entry.approved is True and approval_entry.rejected is True:\n        # Approval takes precedence\n        return True\n\n    if approval_entry.approved is True:\n        return True\n\n    if approval_entry.rejected is True:\n        return False\n\n    approved_ids = (\n        set(approval_entry.approved) if isinstance(approval_entry.approved, list) else set()\n    )\n    rejected_ids = (\n        set(approval_entry.rejected) if isinstance(approval_entry.rejected, list) else set()\n    )\n\n    if call_id in approved_ids:\n        return True\n    if call_id in rejected_ids:\n        return False\n    # Per-call approvals are scoped to the exact call ID, so other calls require a new decision.\n    return None\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext.approve_tool","title":"approve_tool","text":"<pre><code>approve_tool(\n    approval_item: ToolApprovalItem,\n    always_approve: bool = False,\n) -&gt; None\n</code></pre> <p>Approve a tool call, optionally for all future calls.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def approve_tool(self, approval_item: ToolApprovalItem, always_approve: bool = False) -&gt; None:\n    \"\"\"Approve a tool call, optionally for all future calls.\"\"\"\n    self._apply_approval_decision(\n        approval_item,\n        always=always_approve,\n        approve=True,\n    )\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext.reject_tool","title":"reject_tool","text":"<pre><code>reject_tool(\n    approval_item: ToolApprovalItem,\n    always_reject: bool = False,\n) -&gt; None\n</code></pre> <p>Reject a tool call, optionally for all future calls.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def reject_tool(self, approval_item: ToolApprovalItem, always_reject: bool = False) -&gt; None:\n    \"\"\"Reject a tool call, optionally for all future calls.\"\"\"\n    self._apply_approval_decision(\n        approval_item,\n        always=always_reject,\n        approve=False,\n    )\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.AgentHookContext.get_approval_status","title":"get_approval_status","text":"<pre><code>get_approval_status(\n    tool_name: str,\n    call_id: str,\n    *,\n    existing_pending: ToolApprovalItem | None = None,\n) -&gt; bool | None\n</code></pre> <p>Return approval status, retrying with pending item's tool name if necessary.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def get_approval_status(\n    self, tool_name: str, call_id: str, *, existing_pending: ToolApprovalItem | None = None\n) -&gt; bool | None:\n    \"\"\"Return approval status, retrying with pending item's tool name if necessary.\"\"\"\n    status = self.is_tool_approved(tool_name, call_id)\n    if status is None and existing_pending:\n        fallback_tool_name = self._resolve_tool_name(existing_pending)\n        status = self.is_tool_approved(fallback_tool_name, call_id)\n    return status\n</code></pre>"},{"location":"ref/run_error_handlers/","title":"<code>Run Error Handlers</code>","text":""},{"location":"ref/run_error_handlers/#agents.run_error_handlers.RunErrorData","title":"RunErrorData  <code>dataclass</code>","text":"<p>Snapshot of run data passed to error handlers.</p> Source code in <code>src/agents/run_error_handlers.py</code> <pre><code>@dataclass\nclass RunErrorData:\n    \"\"\"Snapshot of run data passed to error handlers.\"\"\"\n\n    input: str | list[TResponseInputItem]\n    new_items: list[RunItem]\n    history: list[TResponseInputItem]\n    output: list[TResponseInputItem]\n    raw_responses: list[ModelResponse]\n    last_agent: Agent[Any]\n</code></pre>"},{"location":"ref/run_error_handlers/#agents.run_error_handlers.RunErrorHandlerResult","title":"RunErrorHandlerResult  <code>dataclass</code>","text":"<p>Result returned by an error handler.</p> Source code in <code>src/agents/run_error_handlers.py</code> <pre><code>@dataclass\nclass RunErrorHandlerResult:\n    \"\"\"Result returned by an error handler.\"\"\"\n\n    final_output: Any\n    include_in_history: bool = True\n</code></pre>"},{"location":"ref/run_error_handlers/#agents.run_error_handlers.RunErrorHandlers","title":"RunErrorHandlers","text":"<p>               Bases: <code>TypedDict</code>, <code>Generic[TContext]</code></p> <p>Error handlers keyed by error kind.</p> Source code in <code>src/agents/run_error_handlers.py</code> <pre><code>class RunErrorHandlers(TypedDict, Generic[TContext], total=False):\n    \"\"\"Error handlers keyed by error kind.\"\"\"\n\n    max_turns: RunErrorHandler[TContext]\n</code></pre>"},{"location":"ref/run_state/","title":"<code>Run State</code>","text":"<p>RunState class for serializing and resuming agent runs with human-in-the-loop support.</p>"},{"location":"ref/run_state/#agents.run_state.RunState","title":"RunState  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext, TAgent]</code></p> <p>Serializable snapshot of an agent run, including context, usage, and interruptions.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>@dataclass\nclass RunState(Generic[TContext, TAgent]):\n    \"\"\"Serializable snapshot of an agent run, including context, usage, and interruptions.\"\"\"\n\n    _current_turn: int = 0\n    \"\"\"Current turn number in the conversation.\"\"\"\n\n    _current_agent: TAgent | None = None\n    \"\"\"The agent currently handling the conversation.\"\"\"\n\n    _original_input: str | list[Any] = field(default_factory=list)\n    \"\"\"Original user input prior to any processing.\"\"\"\n\n    _model_responses: list[ModelResponse] = field(default_factory=list)\n    \"\"\"Responses from the model so far.\"\"\"\n\n    _context: RunContextWrapper[TContext] | None = None\n    \"\"\"Run context tracking approvals, usage, and other metadata.\"\"\"\n\n    _generated_items: list[RunItem] = field(default_factory=list)\n    \"\"\"Items used to build model input when resuming; may be filtered by handoffs.\"\"\"\n\n    _session_items: list[RunItem] = field(default_factory=list)\n    \"\"\"Full, unfiltered run items for session history.\"\"\"\n\n    _max_turns: int = 10\n    \"\"\"Maximum allowed turns before forcing termination.\"\"\"\n\n    _conversation_id: str | None = None\n    \"\"\"Conversation identifier for server-managed conversation tracking.\"\"\"\n\n    _previous_response_id: str | None = None\n    \"\"\"Response identifier of the last server-managed response.\"\"\"\n\n    _auto_previous_response_id: bool = False\n    \"\"\"Whether the previous response id should be automatically tracked.\"\"\"\n\n    _input_guardrail_results: list[InputGuardrailResult] = field(default_factory=list)\n    \"\"\"Results from input guardrails applied to the run.\"\"\"\n\n    _output_guardrail_results: list[OutputGuardrailResult] = field(default_factory=list)\n    \"\"\"Results from output guardrails applied to the run.\"\"\"\n\n    _tool_input_guardrail_results: list[ToolInputGuardrailResult] = field(default_factory=list)\n    \"\"\"Results from tool input guardrails applied during the run.\"\"\"\n\n    _tool_output_guardrail_results: list[ToolOutputGuardrailResult] = field(default_factory=list)\n    \"\"\"Results from tool output guardrails applied during the run.\"\"\"\n\n    _current_step: NextStepInterruption | None = None\n    \"\"\"Current step if the run is interrupted (e.g., for tool approval).\"\"\"\n\n    _last_processed_response: ProcessedResponse | None = None\n    \"\"\"The last processed model response. This is needed for resuming from interruptions.\"\"\"\n\n    _current_turn_persisted_item_count: int = 0\n    \"\"\"Tracks how many items from this turn were already written to the session.\"\"\"\n\n    _tool_use_tracker_snapshot: dict[str, list[str]] = field(default_factory=dict)\n    \"\"\"Serialized snapshot of the AgentToolUseTracker (agent name -&gt; tools used).\"\"\"\n\n    _trace_state: TraceState | None = field(default=None, repr=False)\n    \"\"\"Serialized trace metadata for resuming tracing context.\"\"\"\n\n    def __init__(\n        self,\n        context: RunContextWrapper[TContext],\n        original_input: str | list[Any],\n        starting_agent: TAgent,\n        max_turns: int = 10,\n        *,\n        conversation_id: str | None = None,\n        previous_response_id: str | None = None,\n        auto_previous_response_id: bool = False,\n    ):\n        \"\"\"Initialize a new RunState.\"\"\"\n        self._context = context\n        self._original_input = _clone_original_input(original_input)\n        self._current_agent = starting_agent\n        self._max_turns = max_turns\n        self._conversation_id = conversation_id\n        self._previous_response_id = previous_response_id\n        self._auto_previous_response_id = auto_previous_response_id\n        self._model_responses = []\n        self._generated_items = []\n        self._session_items = []\n        self._input_guardrail_results = []\n        self._output_guardrail_results = []\n        self._tool_input_guardrail_results = []\n        self._tool_output_guardrail_results = []\n        self._current_step = None\n        self._current_turn = 0\n        self._last_processed_response = None\n        self._current_turn_persisted_item_count = 0\n        self._tool_use_tracker_snapshot = {}\n        self._trace_state = None\n\n    def get_interruptions(self) -&gt; list[ToolApprovalItem]:\n        \"\"\"Return pending interruptions if the current step is an interruption.\"\"\"\n        # Import at runtime to avoid circular import\n        from .run_internal.run_steps import NextStepInterruption\n\n        if self._current_step is None or not isinstance(self._current_step, NextStepInterruption):\n            return []\n        return self._current_step.interruptions\n\n    def approve(self, approval_item: ToolApprovalItem, always_approve: bool = False) -&gt; None:\n        \"\"\"Approve a tool call and rerun with this state to continue.\"\"\"\n        if self._context is None:\n            raise UserError(\"Cannot approve tool: RunState has no context\")\n        self._context.approve_tool(approval_item, always_approve=always_approve)\n\n    def reject(self, approval_item: ToolApprovalItem, always_reject: bool = False) -&gt; None:\n        \"\"\"Reject a tool call and rerun with this state to continue.\"\"\"\n        if self._context is None:\n            raise UserError(\"Cannot reject tool: RunState has no context\")\n        self._context.reject_tool(approval_item, always_reject=always_reject)\n\n    def _serialize_approvals(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Serialize approval records into a JSON-friendly mapping.\"\"\"\n        if self._context is None:\n            return {}\n        approvals_dict: dict[str, dict[str, Any]] = {}\n        for tool_name, record in self._context._approvals.items():\n            approvals_dict[tool_name] = {\n                \"approved\": record.approved\n                if isinstance(record.approved, bool)\n                else list(record.approved),\n                \"rejected\": record.rejected\n                if isinstance(record.rejected, bool)\n                else list(record.rejected),\n            }\n        return approvals_dict\n\n    def _serialize_model_responses(self) -&gt; list[dict[str, Any]]:\n        \"\"\"Serialize model responses.\"\"\"\n        return [\n            {\n                \"usage\": serialize_usage(resp.usage),\n                \"output\": [_serialize_raw_item_value(item) for item in resp.output],\n                \"response_id\": resp.response_id,\n            }\n            for resp in self._model_responses\n        ]\n\n    def _serialize_original_input(self) -&gt; str | list[Any]:\n        \"\"\"Normalize original input into the shape expected by Responses API.\"\"\"\n        if not isinstance(self._original_input, list):\n            return self._original_input\n\n        normalized_items = []\n        for item in self._original_input:\n            normalized_item = _serialize_raw_item_value(item)\n            if isinstance(normalized_item, dict):\n                normalized_item = dict(normalized_item)\n                role = normalized_item.get(\"role\")\n                if role == \"assistant\":\n                    content = normalized_item.get(\"content\")\n                    if isinstance(content, str):\n                        normalized_item[\"content\"] = [{\"type\": \"output_text\", \"text\": content}]\n                    if \"status\" not in normalized_item:\n                        normalized_item[\"status\"] = \"completed\"\n            normalized_items.append(normalized_item)\n        return normalized_items\n\n    def _serialize_context_payload(\n        self,\n        *,\n        context_serializer: ContextSerializer | None = None,\n        strict_context: bool = False,\n    ) -&gt; tuple[dict[str, Any] | None, dict[str, Any]]:\n        \"\"\"Validate and serialize the stored run context.\"\"\"\n        if self._context is None:\n            return None, _build_context_meta(\n                None,\n                serialized_via=\"none\",\n                requires_deserializer=False,\n                omitted=False,\n            )\n\n        raw_context_payload = self._context.context\n        if raw_context_payload is None:\n            return None, _build_context_meta(\n                raw_context_payload,\n                serialized_via=\"none\",\n                requires_deserializer=False,\n                omitted=False,\n            )\n\n        if isinstance(raw_context_payload, Mapping):\n            return (\n                dict(raw_context_payload),\n                _build_context_meta(\n                    raw_context_payload,\n                    serialized_via=\"mapping\",\n                    requires_deserializer=False,\n                    omitted=False,\n                ),\n            )\n\n        if strict_context and context_serializer is None:\n            # Avoid silently dropping non-mapping context data when strict mode is requested.\n            raise UserError(\n                \"RunState serialization requires context to be a mapping when strict_context \"\n                \"is True. Provide context_serializer to serialize custom contexts.\"\n            )\n\n        if context_serializer is not None:\n            try:\n                serialized = context_serializer(raw_context_payload)\n            except Exception as exc:\n                raise UserError(\n                    \"Context serializer failed while serializing RunState context.\"\n                ) from exc\n            if not isinstance(serialized, Mapping):\n                raise UserError(\"Context serializer must return a mapping.\")\n            return (\n                dict(serialized),\n                _build_context_meta(\n                    raw_context_payload,\n                    serialized_via=\"context_serializer\",\n                    requires_deserializer=True,\n                    omitted=False,\n                ),\n            )\n\n        if hasattr(raw_context_payload, \"model_dump\"):\n            try:\n                serialized = raw_context_payload.model_dump(exclude_unset=True)\n            except TypeError:\n                serialized = raw_context_payload.model_dump()\n            if not isinstance(serialized, Mapping):\n                raise UserError(\"RunState context model_dump must return a mapping.\")\n            # We can persist the data, but the original type is lost unless the caller rebuilds it.\n            logger.warning(\n                \"RunState context was serialized from a Pydantic model. \"\n                \"Provide context_deserializer or context_override to restore the original type.\"\n            )\n            return (\n                dict(serialized),\n                _build_context_meta(\n                    raw_context_payload,\n                    serialized_via=\"model_dump\",\n                    requires_deserializer=True,\n                    omitted=False,\n                ),\n            )\n\n        if dataclasses.is_dataclass(raw_context_payload):\n            serialized = dataclasses.asdict(cast(Any, raw_context_payload))\n            if not isinstance(serialized, Mapping):\n                raise UserError(\"RunState dataclass context must serialize to a mapping.\")\n            # Dataclass instances serialize to dicts, so reconstruction requires a deserializer.\n            logger.warning(\n                \"RunState context was serialized from a dataclass. \"\n                \"Provide context_deserializer or context_override to restore the original type.\"\n            )\n            return (\n                dict(serialized),\n                _build_context_meta(\n                    raw_context_payload,\n                    serialized_via=\"asdict\",\n                    requires_deserializer=True,\n                    omitted=False,\n                ),\n            )\n\n        # Fall back to an empty dict so the run state remains serializable, but\n        # explicitly warn because the original context will be unavailable on restore.\n        logger.warning(\n            \"RunState context of type %s is not serializable; storing empty context. \"\n            \"Provide context_serializer to preserve it.\",\n            type(raw_context_payload).__name__,\n        )\n        return (\n            {},\n            _build_context_meta(\n                raw_context_payload,\n                serialized_via=\"omitted\",\n                requires_deserializer=True,\n                omitted=True,\n            ),\n        )\n\n    def _serialize_tool_input(self, tool_input: Any) -&gt; Any:\n        \"\"\"Normalize tool input for JSON serialization.\"\"\"\n        if tool_input is None:\n            return None\n\n        if dataclasses.is_dataclass(tool_input):\n            return dataclasses.asdict(cast(Any, tool_input))\n\n        if hasattr(tool_input, \"model_dump\"):\n            try:\n                serialized = tool_input.model_dump(exclude_unset=True)\n            except TypeError:\n                serialized = tool_input.model_dump()\n            return _to_dump_compatible(serialized)\n\n        return _to_dump_compatible(tool_input)\n\n    def _merge_generated_items_with_processed(self) -&gt; list[RunItem]:\n        \"\"\"Merge persisted and newly processed items without duplication.\"\"\"\n        generated_items = list(self._generated_items)\n        if not (self._last_processed_response and self._last_processed_response.new_items):\n            return generated_items\n\n        seen_id_types: set[tuple[str, str]] = set()\n        seen_call_ids: set[str] = set()\n        seen_call_id_types: set[tuple[str, str]] = set()\n\n        def _id_type_call(item: Any) -&gt; tuple[str | None, str | None, str | None]:\n            item_id = None\n            item_type = None\n            call_id = None\n            if hasattr(item, \"raw_item\"):\n                raw = item.raw_item\n                if isinstance(raw, dict):\n                    item_id = raw.get(\"id\")\n                    item_type = raw.get(\"type\")\n                    call_id = raw.get(\"call_id\")\n                else:\n                    item_id = _get_attr(raw, \"id\")\n                    item_type = _get_attr(raw, \"type\")\n                    call_id = _get_attr(raw, \"call_id\")\n            if item_id is None and hasattr(item, \"id\"):\n                item_id = _get_attr(item, \"id\")\n            if item_type is None and hasattr(item, \"type\"):\n                item_type = _get_attr(item, \"type\")\n            return item_id, item_type, call_id\n\n        for existing in generated_items:\n            item_id, item_type, call_id = _id_type_call(existing)\n            if item_id and item_type:\n                seen_id_types.add((item_id, item_type))\n            if call_id and item_type:\n                seen_call_id_types.add((call_id, item_type))\n            elif call_id:\n                seen_call_ids.add(call_id)\n\n        for new_item in self._last_processed_response.new_items:\n            item_id, item_type, call_id = _id_type_call(new_item)\n            if call_id and item_type:\n                if (call_id, item_type) in seen_call_id_types:\n                    continue\n            elif call_id and call_id in seen_call_ids:\n                continue\n            if item_id and item_type and (item_id, item_type) in seen_id_types:\n                continue\n            if item_id and item_type:\n                seen_id_types.add((item_id, item_type))\n            if call_id and item_type:\n                seen_call_id_types.add((call_id, item_type))\n            elif call_id:\n                seen_call_ids.add(call_id)\n            generated_items.append(new_item)\n        return generated_items\n\n    def to_json(\n        self,\n        *,\n        context_serializer: ContextSerializer | None = None,\n        strict_context: bool = False,\n        include_tracing_api_key: bool = False,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Serializes the run state to a JSON-compatible dictionary.\n\n        This method is used to serialize the run state to a dictionary that can be used to\n        resume the run later.\n\n        Args:\n            context_serializer: Optional function to serialize non-mapping context values.\n            strict_context: When True, require mapping contexts or a context_serializer.\n            include_tracing_api_key: When True, include the tracing API key in the trace payload.\n\n        Returns:\n            A dictionary representation of the run state.\n\n        Raises:\n            UserError: If required state (agent, context) is missing.\n        \"\"\"\n        if self._current_agent is None:\n            raise UserError(\"Cannot serialize RunState: No current agent\")\n        if self._context is None:\n            raise UserError(\"Cannot serialize RunState: No context\")\n\n        approvals_dict = self._serialize_approvals()\n        model_responses = self._serialize_model_responses()\n        original_input_serialized = self._serialize_original_input()\n        context_payload, context_meta = self._serialize_context_payload(\n            context_serializer=context_serializer,\n            strict_context=strict_context,\n        )\n\n        context_entry: dict[str, Any] = {\n            \"usage\": serialize_usage(self._context.usage),\n            \"approvals\": approvals_dict,\n            \"context\": context_payload,\n            # Preserve metadata so deserialization can warn when context types were erased.\n            \"context_meta\": context_meta,\n        }\n        tool_input = self._serialize_tool_input(self._context.tool_input)\n        if tool_input is not None:\n            context_entry[\"tool_input\"] = tool_input\n\n        result = {\n            \"$schemaVersion\": CURRENT_SCHEMA_VERSION,\n            \"current_turn\": self._current_turn,\n            \"current_agent\": {\"name\": self._current_agent.name},\n            \"original_input\": original_input_serialized,\n            \"model_responses\": model_responses,\n            \"context\": context_entry,\n            \"tool_use_tracker\": copy.deepcopy(self._tool_use_tracker_snapshot),\n            \"max_turns\": self._max_turns,\n            \"no_active_agent_run\": True,\n            \"input_guardrail_results\": _serialize_guardrail_results(self._input_guardrail_results),\n            \"output_guardrail_results\": _serialize_guardrail_results(\n                self._output_guardrail_results\n            ),\n            \"tool_input_guardrail_results\": _serialize_tool_guardrail_results(\n                self._tool_input_guardrail_results, type_label=\"tool_input\"\n            ),\n            \"tool_output_guardrail_results\": _serialize_tool_guardrail_results(\n                self._tool_output_guardrail_results, type_label=\"tool_output\"\n            ),\n            \"conversation_id\": self._conversation_id,\n            \"previous_response_id\": self._previous_response_id,\n            \"auto_previous_response_id\": self._auto_previous_response_id,\n        }\n\n        generated_items = self._merge_generated_items_with_processed()\n        result[\"generated_items\"] = [self._serialize_item(item) for item in generated_items]\n        result[\"session_items\"] = [self._serialize_item(item) for item in list(self._session_items)]\n        result[\"current_step\"] = self._serialize_current_step()\n        result[\"last_model_response\"] = _serialize_last_model_response(model_responses)\n        result[\"last_processed_response\"] = (\n            self._serialize_processed_response(self._last_processed_response)\n            if self._last_processed_response\n            else None\n        )\n        result[\"current_turn_persisted_item_count\"] = self._current_turn_persisted_item_count\n        result[\"trace\"] = self._serialize_trace_data(\n            include_tracing_api_key=include_tracing_api_key\n        )\n\n        return result\n\n    def _serialize_processed_response(\n        self, processed_response: ProcessedResponse\n    ) -&gt; dict[str, Any]:\n        \"\"\"Serialize a ProcessedResponse to JSON format.\n\n        Args:\n            processed_response: The ProcessedResponse to serialize.\n\n        Returns:\n            A dictionary representation of the ProcessedResponse.\n        \"\"\"\n\n        action_groups = _serialize_tool_action_groups(processed_response)\n\n        interruptions_data = [\n            _serialize_tool_approval_interruption(interruption, include_tool_name=True)\n            for interruption in processed_response.interruptions\n            if isinstance(interruption, ToolApprovalItem)\n        ]\n\n        return {\n            \"new_items\": [self._serialize_item(item) for item in processed_response.new_items],\n            \"tools_used\": processed_response.tools_used,\n            **action_groups,\n            \"interruptions\": interruptions_data,\n        }\n\n    def _serialize_current_step(self) -&gt; dict[str, Any] | None:\n        \"\"\"Serialize the current step if it's an interruption.\"\"\"\n        # Import at runtime to avoid circular import\n        from .run_internal.run_steps import NextStepInterruption\n\n        if self._current_step is None or not isinstance(self._current_step, NextStepInterruption):\n            return None\n\n        interruptions_data = [\n            _serialize_tool_approval_interruption(\n                item, include_tool_name=item.tool_name is not None\n            )\n            for item in self._current_step.interruptions\n            if isinstance(item, ToolApprovalItem)\n        ]\n\n        return {\n            \"type\": \"next_step_interruption\",\n            \"data\": {\n                \"interruptions\": interruptions_data,\n            },\n        }\n\n    def _serialize_item(self, item: RunItem) -&gt; dict[str, Any]:\n        \"\"\"Serialize a run item to JSON-compatible dict.\"\"\"\n        raw_item_dict: Any = _serialize_raw_item_value(item.raw_item)\n\n        result: dict[str, Any] = {\n            \"type\": item.type,\n            \"raw_item\": raw_item_dict,\n            \"agent\": {\"name\": item.agent.name},\n        }\n\n        # Add additional fields based on item type\n        if hasattr(item, \"output\"):\n            serialized_output = item.output\n            try:\n                if hasattr(serialized_output, \"model_dump\"):\n                    serialized_output = serialized_output.model_dump(exclude_unset=True)\n                elif dataclasses.is_dataclass(serialized_output):\n                    serialized_output = dataclasses.asdict(serialized_output)  # type: ignore[arg-type]\n                serialized_output = _ensure_json_compatible(serialized_output)\n            except Exception:\n                serialized_output = str(item.output)\n            result[\"output\"] = serialized_output\n        if hasattr(item, \"source_agent\"):\n            result[\"source_agent\"] = {\"name\": item.source_agent.name}\n        if hasattr(item, \"target_agent\"):\n            result[\"target_agent\"] = {\"name\": item.target_agent.name}\n        if hasattr(item, \"tool_name\") and item.tool_name is not None:\n            result[\"tool_name\"] = item.tool_name\n        if hasattr(item, \"description\") and item.description is not None:\n            result[\"description\"] = item.description\n\n        return result\n\n    def _lookup_function_name(self, call_id: str) -&gt; str:\n        \"\"\"Attempt to find the function name for the provided call_id.\"\"\"\n        if not call_id:\n            return \"\"\n\n        def _extract_name(raw: Any) -&gt; str | None:\n            if isinstance(raw, dict):\n                candidate_call_id = cast(Optional[str], raw.get(\"call_id\"))\n                if candidate_call_id == call_id:\n                    name_value = raw.get(\"name\", \"\")\n                    return str(name_value) if name_value else \"\"\n            else:\n                candidate_call_id = cast(Optional[str], _get_attr(raw, \"call_id\"))\n                if candidate_call_id == call_id:\n                    name_value = _get_attr(raw, \"name\", \"\")\n                    return str(name_value) if name_value else \"\"\n            return None\n\n        # Search generated items first\n        for run_item in self._generated_items:\n            if run_item.type != \"tool_call_item\":\n                continue\n            name = _extract_name(run_item.raw_item)\n            if name is not None:\n                return name\n\n        # Inspect last processed response\n        if self._last_processed_response is not None:\n            for run_item in self._last_processed_response.new_items:\n                if run_item.type != \"tool_call_item\":\n                    continue\n                name = _extract_name(run_item.raw_item)\n                if name is not None:\n                    return name\n\n        # Finally, inspect the original input list where the function call originated\n        if isinstance(self._original_input, list):\n            for input_item in self._original_input:\n                if not isinstance(input_item, dict):\n                    continue\n                if input_item.get(\"type\") != \"function_call\":\n                    continue\n                item_call_id = cast(Optional[str], input_item.get(\"call_id\"))\n                if item_call_id == call_id:\n                    name_value = input_item.get(\"name\", \"\")\n                    return str(name_value) if name_value else \"\"\n\n        return \"\"\n\n    def to_string(\n        self,\n        *,\n        context_serializer: ContextSerializer | None = None,\n        strict_context: bool = False,\n        include_tracing_api_key: bool = False,\n    ) -&gt; str:\n        \"\"\"Serializes the run state to a JSON string.\n\n        Args:\n            include_tracing_api_key: When True, include the tracing API key in the trace payload.\n\n        Returns:\n            JSON string representation of the run state.\n        \"\"\"\n        return json.dumps(\n            self.to_json(\n                context_serializer=context_serializer,\n                strict_context=strict_context,\n                include_tracing_api_key=include_tracing_api_key,\n            ),\n            indent=2,\n        )\n\n    def set_trace(self, trace: Trace | None) -&gt; None:\n        \"\"\"Capture trace metadata for serialization/resumption.\"\"\"\n        self._trace_state = TraceState.from_trace(trace)\n\n    def _serialize_trace_data(self, *, include_tracing_api_key: bool) -&gt; dict[str, Any] | None:\n        if not self._trace_state:\n            return None\n        return self._trace_state.to_json(include_tracing_api_key=include_tracing_api_key)\n\n    def set_tool_use_tracker_snapshot(self, snapshot: Mapping[str, Sequence[str]] | None) -&gt; None:\n        \"\"\"Store a copy of the serialized tool-use tracker data.\"\"\"\n        if not snapshot:\n            self._tool_use_tracker_snapshot = {}\n            return\n\n        normalized: dict[str, list[str]] = {}\n        for agent_name, tools in snapshot.items():\n            if not isinstance(agent_name, str):\n                continue\n            normalized[agent_name] = [tool for tool in tools if isinstance(tool, str)]\n        self._tool_use_tracker_snapshot = normalized\n\n    def get_tool_use_tracker_snapshot(self) -&gt; dict[str, list[str]]:\n        \"\"\"Return a defensive copy of the tool-use tracker snapshot.\"\"\"\n        return {\n            agent_name: list(tool_names)\n            for agent_name, tool_names in self._tool_use_tracker_snapshot.items()\n        }\n\n    @staticmethod\n    async def from_string(\n        initial_agent: Agent[Any],\n        state_string: str,\n        *,\n        context_override: ContextOverride | None = None,\n        context_deserializer: ContextDeserializer | None = None,\n        strict_context: bool = False,\n    ) -&gt; RunState[Any, Agent[Any]]:\n        \"\"\"Deserializes a run state from a JSON string.\n\n        This method is used to deserialize a run state from a string that was serialized using\n        the `to_string()` method.\n\n        Args:\n            initial_agent: The initial agent (used to build agent map for resolution).\n            state_string: The JSON string to deserialize.\n            context_override: Optional context mapping or RunContextWrapper to use instead of the\n                serialized context.\n            context_deserializer: Optional function to rebuild non-mapping context values.\n            strict_context: When True, require a deserializer or override for non-mapping contexts.\n\n        Returns:\n            A reconstructed RunState instance.\n\n        Raises:\n            UserError: If the string is invalid JSON or has incompatible schema version.\n        \"\"\"\n        try:\n            state_json = json.loads(state_string)\n        except json.JSONDecodeError as e:\n            raise UserError(f\"Failed to parse run state JSON: {e}\") from e\n\n        return await RunState.from_json(\n            initial_agent=initial_agent,\n            state_json=state_json,\n            context_override=context_override,\n            context_deserializer=context_deserializer,\n            strict_context=strict_context,\n        )\n\n    @staticmethod\n    async def from_json(\n        initial_agent: Agent[Any],\n        state_json: dict[str, Any],\n        *,\n        context_override: ContextOverride | None = None,\n        context_deserializer: ContextDeserializer | None = None,\n        strict_context: bool = False,\n    ) -&gt; RunState[Any, Agent[Any]]:\n        \"\"\"Deserializes a run state from a JSON dictionary.\n\n        This method is used to deserialize a run state from a dict that was created using\n        the `to_json()` method.\n\n        Args:\n            initial_agent: The initial agent (used to build agent map for resolution).\n            state_json: The JSON dictionary to deserialize.\n            context_override: Optional context mapping or RunContextWrapper to use instead of the\n                serialized context.\n            context_deserializer: Optional function to rebuild non-mapping context values.\n            strict_context: When True, require a deserializer or override for non-mapping contexts.\n\n        Returns:\n            A reconstructed RunState instance.\n\n        Raises:\n            UserError: If the dict has incompatible schema version.\n        \"\"\"\n        return await _build_run_state_from_json(\n            initial_agent=initial_agent,\n            state_json=state_json,\n            context_override=context_override,\n            context_deserializer=context_deserializer,\n            strict_context=strict_context,\n        )\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.__init__","title":"__init__","text":"<pre><code>__init__(\n    context: RunContextWrapper[TContext],\n    original_input: str | list[Any],\n    starting_agent: TAgent,\n    max_turns: int = 10,\n    *,\n    conversation_id: str | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n)\n</code></pre> <p>Initialize a new RunState.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def __init__(\n    self,\n    context: RunContextWrapper[TContext],\n    original_input: str | list[Any],\n    starting_agent: TAgent,\n    max_turns: int = 10,\n    *,\n    conversation_id: str | None = None,\n    previous_response_id: str | None = None,\n    auto_previous_response_id: bool = False,\n):\n    \"\"\"Initialize a new RunState.\"\"\"\n    self._context = context\n    self._original_input = _clone_original_input(original_input)\n    self._current_agent = starting_agent\n    self._max_turns = max_turns\n    self._conversation_id = conversation_id\n    self._previous_response_id = previous_response_id\n    self._auto_previous_response_id = auto_previous_response_id\n    self._model_responses = []\n    self._generated_items = []\n    self._session_items = []\n    self._input_guardrail_results = []\n    self._output_guardrail_results = []\n    self._tool_input_guardrail_results = []\n    self._tool_output_guardrail_results = []\n    self._current_step = None\n    self._current_turn = 0\n    self._last_processed_response = None\n    self._current_turn_persisted_item_count = 0\n    self._tool_use_tracker_snapshot = {}\n    self._trace_state = None\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.get_interruptions","title":"get_interruptions","text":"<pre><code>get_interruptions() -&gt; list[ToolApprovalItem]\n</code></pre> <p>Return pending interruptions if the current step is an interruption.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def get_interruptions(self) -&gt; list[ToolApprovalItem]:\n    \"\"\"Return pending interruptions if the current step is an interruption.\"\"\"\n    # Import at runtime to avoid circular import\n    from .run_internal.run_steps import NextStepInterruption\n\n    if self._current_step is None or not isinstance(self._current_step, NextStepInterruption):\n        return []\n    return self._current_step.interruptions\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.approve","title":"approve","text":"<pre><code>approve(\n    approval_item: ToolApprovalItem,\n    always_approve: bool = False,\n) -&gt; None\n</code></pre> <p>Approve a tool call and rerun with this state to continue.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def approve(self, approval_item: ToolApprovalItem, always_approve: bool = False) -&gt; None:\n    \"\"\"Approve a tool call and rerun with this state to continue.\"\"\"\n    if self._context is None:\n        raise UserError(\"Cannot approve tool: RunState has no context\")\n    self._context.approve_tool(approval_item, always_approve=always_approve)\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.reject","title":"reject","text":"<pre><code>reject(\n    approval_item: ToolApprovalItem,\n    always_reject: bool = False,\n) -&gt; None\n</code></pre> <p>Reject a tool call and rerun with this state to continue.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def reject(self, approval_item: ToolApprovalItem, always_reject: bool = False) -&gt; None:\n    \"\"\"Reject a tool call and rerun with this state to continue.\"\"\"\n    if self._context is None:\n        raise UserError(\"Cannot reject tool: RunState has no context\")\n    self._context.reject_tool(approval_item, always_reject=always_reject)\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.to_json","title":"to_json","text":"<pre><code>to_json(\n    *,\n    context_serializer: ContextSerializer | None = None,\n    strict_context: bool = False,\n    include_tracing_api_key: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Serializes the run state to a JSON-compatible dictionary.</p> <p>This method is used to serialize the run state to a dictionary that can be used to resume the run later.</p> <p>Parameters:</p> Name Type Description Default <code>context_serializer</code> <code>ContextSerializer | None</code> <p>Optional function to serialize non-mapping context values.</p> <code>None</code> <code>strict_context</code> <code>bool</code> <p>When True, require mapping contexts or a context_serializer.</p> <code>False</code> <code>include_tracing_api_key</code> <code>bool</code> <p>When True, include the tracing API key in the trace payload.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary representation of the run state.</p> <p>Raises:</p> Type Description <code>UserError</code> <p>If required state (agent, context) is missing.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def to_json(\n    self,\n    *,\n    context_serializer: ContextSerializer | None = None,\n    strict_context: bool = False,\n    include_tracing_api_key: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Serializes the run state to a JSON-compatible dictionary.\n\n    This method is used to serialize the run state to a dictionary that can be used to\n    resume the run later.\n\n    Args:\n        context_serializer: Optional function to serialize non-mapping context values.\n        strict_context: When True, require mapping contexts or a context_serializer.\n        include_tracing_api_key: When True, include the tracing API key in the trace payload.\n\n    Returns:\n        A dictionary representation of the run state.\n\n    Raises:\n        UserError: If required state (agent, context) is missing.\n    \"\"\"\n    if self._current_agent is None:\n        raise UserError(\"Cannot serialize RunState: No current agent\")\n    if self._context is None:\n        raise UserError(\"Cannot serialize RunState: No context\")\n\n    approvals_dict = self._serialize_approvals()\n    model_responses = self._serialize_model_responses()\n    original_input_serialized = self._serialize_original_input()\n    context_payload, context_meta = self._serialize_context_payload(\n        context_serializer=context_serializer,\n        strict_context=strict_context,\n    )\n\n    context_entry: dict[str, Any] = {\n        \"usage\": serialize_usage(self._context.usage),\n        \"approvals\": approvals_dict,\n        \"context\": context_payload,\n        # Preserve metadata so deserialization can warn when context types were erased.\n        \"context_meta\": context_meta,\n    }\n    tool_input = self._serialize_tool_input(self._context.tool_input)\n    if tool_input is not None:\n        context_entry[\"tool_input\"] = tool_input\n\n    result = {\n        \"$schemaVersion\": CURRENT_SCHEMA_VERSION,\n        \"current_turn\": self._current_turn,\n        \"current_agent\": {\"name\": self._current_agent.name},\n        \"original_input\": original_input_serialized,\n        \"model_responses\": model_responses,\n        \"context\": context_entry,\n        \"tool_use_tracker\": copy.deepcopy(self._tool_use_tracker_snapshot),\n        \"max_turns\": self._max_turns,\n        \"no_active_agent_run\": True,\n        \"input_guardrail_results\": _serialize_guardrail_results(self._input_guardrail_results),\n        \"output_guardrail_results\": _serialize_guardrail_results(\n            self._output_guardrail_results\n        ),\n        \"tool_input_guardrail_results\": _serialize_tool_guardrail_results(\n            self._tool_input_guardrail_results, type_label=\"tool_input\"\n        ),\n        \"tool_output_guardrail_results\": _serialize_tool_guardrail_results(\n            self._tool_output_guardrail_results, type_label=\"tool_output\"\n        ),\n        \"conversation_id\": self._conversation_id,\n        \"previous_response_id\": self._previous_response_id,\n        \"auto_previous_response_id\": self._auto_previous_response_id,\n    }\n\n    generated_items = self._merge_generated_items_with_processed()\n    result[\"generated_items\"] = [self._serialize_item(item) for item in generated_items]\n    result[\"session_items\"] = [self._serialize_item(item) for item in list(self._session_items)]\n    result[\"current_step\"] = self._serialize_current_step()\n    result[\"last_model_response\"] = _serialize_last_model_response(model_responses)\n    result[\"last_processed_response\"] = (\n        self._serialize_processed_response(self._last_processed_response)\n        if self._last_processed_response\n        else None\n    )\n    result[\"current_turn_persisted_item_count\"] = self._current_turn_persisted_item_count\n    result[\"trace\"] = self._serialize_trace_data(\n        include_tracing_api_key=include_tracing_api_key\n    )\n\n    return result\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.to_string","title":"to_string","text":"<pre><code>to_string(\n    *,\n    context_serializer: ContextSerializer | None = None,\n    strict_context: bool = False,\n    include_tracing_api_key: bool = False,\n) -&gt; str\n</code></pre> <p>Serializes the run state to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>include_tracing_api_key</code> <code>bool</code> <p>When True, include the tracing API key in the trace payload.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation of the run state.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def to_string(\n    self,\n    *,\n    context_serializer: ContextSerializer | None = None,\n    strict_context: bool = False,\n    include_tracing_api_key: bool = False,\n) -&gt; str:\n    \"\"\"Serializes the run state to a JSON string.\n\n    Args:\n        include_tracing_api_key: When True, include the tracing API key in the trace payload.\n\n    Returns:\n        JSON string representation of the run state.\n    \"\"\"\n    return json.dumps(\n        self.to_json(\n            context_serializer=context_serializer,\n            strict_context=strict_context,\n            include_tracing_api_key=include_tracing_api_key,\n        ),\n        indent=2,\n    )\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.set_trace","title":"set_trace","text":"<pre><code>set_trace(trace: Trace | None) -&gt; None\n</code></pre> <p>Capture trace metadata for serialization/resumption.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def set_trace(self, trace: Trace | None) -&gt; None:\n    \"\"\"Capture trace metadata for serialization/resumption.\"\"\"\n    self._trace_state = TraceState.from_trace(trace)\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.set_tool_use_tracker_snapshot","title":"set_tool_use_tracker_snapshot","text":"<pre><code>set_tool_use_tracker_snapshot(\n    snapshot: Mapping[str, Sequence[str]] | None,\n) -&gt; None\n</code></pre> <p>Store a copy of the serialized tool-use tracker data.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def set_tool_use_tracker_snapshot(self, snapshot: Mapping[str, Sequence[str]] | None) -&gt; None:\n    \"\"\"Store a copy of the serialized tool-use tracker data.\"\"\"\n    if not snapshot:\n        self._tool_use_tracker_snapshot = {}\n        return\n\n    normalized: dict[str, list[str]] = {}\n    for agent_name, tools in snapshot.items():\n        if not isinstance(agent_name, str):\n            continue\n        normalized[agent_name] = [tool for tool in tools if isinstance(tool, str)]\n    self._tool_use_tracker_snapshot = normalized\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.get_tool_use_tracker_snapshot","title":"get_tool_use_tracker_snapshot","text":"<pre><code>get_tool_use_tracker_snapshot() -&gt; dict[str, list[str]]\n</code></pre> <p>Return a defensive copy of the tool-use tracker snapshot.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>def get_tool_use_tracker_snapshot(self) -&gt; dict[str, list[str]]:\n    \"\"\"Return a defensive copy of the tool-use tracker snapshot.\"\"\"\n    return {\n        agent_name: list(tool_names)\n        for agent_name, tool_names in self._tool_use_tracker_snapshot.items()\n    }\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.from_string","title":"from_string  <code>async</code> <code>staticmethod</code>","text":"<pre><code>from_string(\n    initial_agent: Agent[Any],\n    state_string: str,\n    *,\n    context_override: ContextOverride | None = None,\n    context_deserializer: ContextDeserializer | None = None,\n    strict_context: bool = False,\n) -&gt; RunState[Any, Agent[Any]]\n</code></pre> <p>Deserializes a run state from a JSON string.</p> <p>This method is used to deserialize a run state from a string that was serialized using the <code>to_string()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>initial_agent</code> <code>Agent[Any]</code> <p>The initial agent (used to build agent map for resolution).</p> required <code>state_string</code> <code>str</code> <p>The JSON string to deserialize.</p> required <code>context_override</code> <code>ContextOverride | None</code> <p>Optional context mapping or RunContextWrapper to use instead of the serialized context.</p> <code>None</code> <code>context_deserializer</code> <code>ContextDeserializer | None</code> <p>Optional function to rebuild non-mapping context values.</p> <code>None</code> <code>strict_context</code> <code>bool</code> <p>When True, require a deserializer or override for non-mapping contexts.</p> <code>False</code> <p>Returns:</p> Type Description <code>RunState[Any, Agent[Any]]</code> <p>A reconstructed RunState instance.</p> <p>Raises:</p> Type Description <code>UserError</code> <p>If the string is invalid JSON or has incompatible schema version.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>@staticmethod\nasync def from_string(\n    initial_agent: Agent[Any],\n    state_string: str,\n    *,\n    context_override: ContextOverride | None = None,\n    context_deserializer: ContextDeserializer | None = None,\n    strict_context: bool = False,\n) -&gt; RunState[Any, Agent[Any]]:\n    \"\"\"Deserializes a run state from a JSON string.\n\n    This method is used to deserialize a run state from a string that was serialized using\n    the `to_string()` method.\n\n    Args:\n        initial_agent: The initial agent (used to build agent map for resolution).\n        state_string: The JSON string to deserialize.\n        context_override: Optional context mapping or RunContextWrapper to use instead of the\n            serialized context.\n        context_deserializer: Optional function to rebuild non-mapping context values.\n        strict_context: When True, require a deserializer or override for non-mapping contexts.\n\n    Returns:\n        A reconstructed RunState instance.\n\n    Raises:\n        UserError: If the string is invalid JSON or has incompatible schema version.\n    \"\"\"\n    try:\n        state_json = json.loads(state_string)\n    except json.JSONDecodeError as e:\n        raise UserError(f\"Failed to parse run state JSON: {e}\") from e\n\n    return await RunState.from_json(\n        initial_agent=initial_agent,\n        state_json=state_json,\n        context_override=context_override,\n        context_deserializer=context_deserializer,\n        strict_context=strict_context,\n    )\n</code></pre>"},{"location":"ref/run_state/#agents.run_state.RunState.from_json","title":"from_json  <code>async</code> <code>staticmethod</code>","text":"<pre><code>from_json(\n    initial_agent: Agent[Any],\n    state_json: dict[str, Any],\n    *,\n    context_override: ContextOverride | None = None,\n    context_deserializer: ContextDeserializer | None = None,\n    strict_context: bool = False,\n) -&gt; RunState[Any, Agent[Any]]\n</code></pre> <p>Deserializes a run state from a JSON dictionary.</p> <p>This method is used to deserialize a run state from a dict that was created using the <code>to_json()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>initial_agent</code> <code>Agent[Any]</code> <p>The initial agent (used to build agent map for resolution).</p> required <code>state_json</code> <code>dict[str, Any]</code> <p>The JSON dictionary to deserialize.</p> required <code>context_override</code> <code>ContextOverride | None</code> <p>Optional context mapping or RunContextWrapper to use instead of the serialized context.</p> <code>None</code> <code>context_deserializer</code> <code>ContextDeserializer | None</code> <p>Optional function to rebuild non-mapping context values.</p> <code>None</code> <code>strict_context</code> <code>bool</code> <p>When True, require a deserializer or override for non-mapping contexts.</p> <code>False</code> <p>Returns:</p> Type Description <code>RunState[Any, Agent[Any]]</code> <p>A reconstructed RunState instance.</p> <p>Raises:</p> Type Description <code>UserError</code> <p>If the dict has incompatible schema version.</p> Source code in <code>src/agents/run_state.py</code> <pre><code>@staticmethod\nasync def from_json(\n    initial_agent: Agent[Any],\n    state_json: dict[str, Any],\n    *,\n    context_override: ContextOverride | None = None,\n    context_deserializer: ContextDeserializer | None = None,\n    strict_context: bool = False,\n) -&gt; RunState[Any, Agent[Any]]:\n    \"\"\"Deserializes a run state from a JSON dictionary.\n\n    This method is used to deserialize a run state from a dict that was created using\n    the `to_json()` method.\n\n    Args:\n        initial_agent: The initial agent (used to build agent map for resolution).\n        state_json: The JSON dictionary to deserialize.\n        context_override: Optional context mapping or RunContextWrapper to use instead of the\n            serialized context.\n        context_deserializer: Optional function to rebuild non-mapping context values.\n        strict_context: When True, require a deserializer or override for non-mapping contexts.\n\n    Returns:\n        A reconstructed RunState instance.\n\n    Raises:\n        UserError: If the dict has incompatible schema version.\n    \"\"\"\n    return await _build_run_state_from_json(\n        initial_agent=initial_agent,\n        state_json=state_json,\n        context_override=context_override,\n        context_deserializer=context_deserializer,\n        strict_context=strict_context,\n    )\n</code></pre>"},{"location":"ref/stream_events/","title":"<code>Streaming events</code>","text":""},{"location":"ref/stream_events/#agents.stream_events.StreamEvent","title":"StreamEvent  <code>module-attribute</code>","text":"<pre><code>StreamEvent: TypeAlias = Union[\n    RawResponsesStreamEvent,\n    RunItemStreamEvent,\n    AgentUpdatedStreamEvent,\n]\n</code></pre> <p>A streaming event from an agent.</p>"},{"location":"ref/stream_events/#agents.stream_events.RawResponsesStreamEvent","title":"RawResponsesStreamEvent  <code>dataclass</code>","text":"<p>Streaming event from the LLM. These are 'raw' events, i.e. they are directly passed through from the LLM.</p> Source code in <code>src/agents/stream_events.py</code> <pre><code>@dataclass\nclass RawResponsesStreamEvent:\n    \"\"\"Streaming event from the LLM. These are 'raw' events, i.e. they are directly passed through\n    from the LLM.\n    \"\"\"\n\n    data: TResponseStreamEvent\n    \"\"\"The raw responses streaming event from the LLM.\"\"\"\n\n    type: Literal[\"raw_response_event\"] = \"raw_response_event\"\n    \"\"\"The type of the event.\"\"\"\n</code></pre>"},{"location":"ref/stream_events/#agents.stream_events.RawResponsesStreamEvent.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: TResponseStreamEvent\n</code></pre> <p>The raw responses streaming event from the LLM.</p>"},{"location":"ref/stream_events/#agents.stream_events.RawResponsesStreamEvent.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['raw_response_event'] = 'raw_response_event'\n</code></pre> <p>The type of the event.</p>"},{"location":"ref/stream_events/#agents.stream_events.RunItemStreamEvent","title":"RunItemStreamEvent  <code>dataclass</code>","text":"<p>Streaming events that wrap a <code>RunItem</code>. As the agent processes the LLM response, it will generate these events for new messages, tool calls, tool outputs, handoffs, etc.</p> Source code in <code>src/agents/stream_events.py</code> <pre><code>@dataclass\nclass RunItemStreamEvent:\n    \"\"\"Streaming events that wrap a `RunItem`. As the agent processes the LLM response, it will\n    generate these events for new messages, tool calls, tool outputs, handoffs, etc.\n    \"\"\"\n\n    name: Literal[\n        \"message_output_created\",\n        \"handoff_requested\",\n        # This is misspelled, but we can't change it because that would be a breaking change\n        \"handoff_occured\",\n        \"tool_called\",\n        \"tool_output\",\n        \"reasoning_item_created\",\n        \"mcp_approval_requested\",\n        \"mcp_approval_response\",\n        \"mcp_list_tools\",\n    ]\n    \"\"\"The name of the event.\"\"\"\n\n    item: RunItem\n    \"\"\"The item that was created.\"\"\"\n\n    type: Literal[\"run_item_stream_event\"] = \"run_item_stream_event\"\n</code></pre>"},{"location":"ref/stream_events/#agents.stream_events.RunItemStreamEvent.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Literal[\n    \"message_output_created\",\n    \"handoff_requested\",\n    \"handoff_occured\",\n    \"tool_called\",\n    \"tool_output\",\n    \"reasoning_item_created\",\n    \"mcp_approval_requested\",\n    \"mcp_approval_response\",\n    \"mcp_list_tools\",\n]\n</code></pre> <p>The name of the event.</p>"},{"location":"ref/stream_events/#agents.stream_events.RunItemStreamEvent.item","title":"item  <code>instance-attribute</code>","text":"<pre><code>item: RunItem\n</code></pre> <p>The item that was created.</p>"},{"location":"ref/stream_events/#agents.stream_events.AgentUpdatedStreamEvent","title":"AgentUpdatedStreamEvent  <code>dataclass</code>","text":"<p>Event that notifies that there is a new agent running.</p> Source code in <code>src/agents/stream_events.py</code> <pre><code>@dataclass\nclass AgentUpdatedStreamEvent:\n    \"\"\"Event that notifies that there is a new agent running.\"\"\"\n\n    new_agent: Agent[Any]\n    \"\"\"The new agent.\"\"\"\n\n    type: Literal[\"agent_updated_stream_event\"] = \"agent_updated_stream_event\"\n</code></pre>"},{"location":"ref/stream_events/#agents.stream_events.AgentUpdatedStreamEvent.new_agent","title":"new_agent  <code>instance-attribute</code>","text":"<pre><code>new_agent: Agent[Any]\n</code></pre> <p>The new agent.</p>"},{"location":"ref/strict_schema/","title":"<code>Strict Schema</code>","text":""},{"location":"ref/strict_schema/#agents.strict_schema.ensure_strict_json_schema","title":"ensure_strict_json_schema","text":"<pre><code>ensure_strict_json_schema(\n    schema: dict[str, Any],\n) -&gt; dict[str, Any]\n</code></pre> <p>Mutates the given JSON schema to ensure it conforms to the <code>strict</code> standard that the OpenAI API expects.</p> Source code in <code>src/agents/strict_schema.py</code> <pre><code>def ensure_strict_json_schema(\n    schema: dict[str, Any],\n) -&gt; dict[str, Any]:\n    \"\"\"Mutates the given JSON schema to ensure it conforms to the `strict` standard\n    that the OpenAI API expects.\n    \"\"\"\n    if schema == {}:\n        return _EMPTY_SCHEMA\n    return _ensure_strict_json_schema(schema, path=(), root=schema)\n</code></pre>"},{"location":"ref/tool/","title":"<code>Tools</code>","text":""},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunction","title":"MCPToolApprovalFunction  <code>module-attribute</code>","text":"<pre><code>MCPToolApprovalFunction = Callable[\n    [MCPToolApprovalRequest],\n    MaybeAwaitable[MCPToolApprovalFunctionResult],\n]\n</code></pre> <p>A function that approves or rejects a tool call.</p>"},{"location":"ref/tool/#agents.tool.ShellApprovalFunction","title":"ShellApprovalFunction  <code>module-attribute</code>","text":"<pre><code>ShellApprovalFunction = Callable[\n    [RunContextWrapper[Any], \"ShellActionRequest\", str],\n    MaybeAwaitable[bool],\n]\n</code></pre> <p>A function that determines whether a shell action requires approval. Takes (run_context, action, call_id) and returns whether approval is needed.</p>"},{"location":"ref/tool/#agents.tool.ShellOnApprovalFunction","title":"ShellOnApprovalFunction  <code>module-attribute</code>","text":"<pre><code>ShellOnApprovalFunction = Callable[\n    [RunContextWrapper[Any], \"ToolApprovalItem\"],\n    MaybeAwaitable[ShellOnApprovalFunctionResult],\n]\n</code></pre> <p>A function that auto-approves or rejects a shell tool call when approval is needed. Takes (run_context, approval_item) and returns approval decision.</p>"},{"location":"ref/tool/#agents.tool.ApplyPatchApprovalFunction","title":"ApplyPatchApprovalFunction  <code>module-attribute</code>","text":"<pre><code>ApplyPatchApprovalFunction = Callable[\n    [RunContextWrapper[Any], ApplyPatchOperation, str],\n    MaybeAwaitable[bool],\n]\n</code></pre> <p>A function that determines whether an apply_patch operation requires approval. Takes (run_context, operation, call_id) and returns whether approval is needed.</p>"},{"location":"ref/tool/#agents.tool.ApplyPatchOnApprovalFunction","title":"ApplyPatchOnApprovalFunction  <code>module-attribute</code>","text":"<pre><code>ApplyPatchOnApprovalFunction = Callable[\n    [RunContextWrapper[Any], \"ToolApprovalItem\"],\n    MaybeAwaitable[ApplyPatchOnApprovalFunctionResult],\n]\n</code></pre> <p>A function that auto-approves or rejects an apply_patch tool call when approval is needed. Takes (run_context, approval_item) and returns approval decision.</p>"},{"location":"ref/tool/#agents.tool.LocalShellExecutor","title":"LocalShellExecutor  <code>module-attribute</code>","text":"<pre><code>LocalShellExecutor = Callable[\n    [LocalShellCommandRequest], MaybeAwaitable[str]\n]\n</code></pre> <p>A function that executes a command on a shell.</p>"},{"location":"ref/tool/#agents.tool.ShellExecutor","title":"ShellExecutor  <code>module-attribute</code>","text":"<pre><code>ShellExecutor = Callable[\n    [ShellCommandRequest],\n    MaybeAwaitable[Union[str, ShellResult]],\n]\n</code></pre> <p>Executes a shell command sequence and returns either text or structured output.</p>"},{"location":"ref/tool/#agents.tool.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    FunctionTool,\n    FileSearchTool,\n    WebSearchTool,\n    ComputerTool[Any],\n    HostedMCPTool,\n    ShellTool,\n    ApplyPatchTool,\n    LocalShellTool,\n    ImageGenerationTool,\n    CodeInterpreterTool,\n]\n</code></pre> <p>A tool that can be used in an agent.</p>"},{"location":"ref/tool/#agents.tool.ToolOutputText","title":"ToolOutputText","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool output that should be sent to the model as text.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ToolOutputText(BaseModel):\n    \"\"\"Represents a tool output that should be sent to the model as text.\"\"\"\n\n    type: Literal[\"text\"] = \"text\"\n    text: str\n</code></pre>"},{"location":"ref/tool/#agents.tool.ToolOutputTextDict","title":"ToolOutputTextDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict variant for text tool outputs.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ToolOutputTextDict(TypedDict, total=False):\n    \"\"\"TypedDict variant for text tool outputs.\"\"\"\n\n    type: Literal[\"text\"]\n    text: str\n</code></pre>"},{"location":"ref/tool/#agents.tool.ToolOutputImage","title":"ToolOutputImage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool output that should be sent to the model as an image.</p> <p>You can provide either an <code>image_url</code> (URL or data URL) or a <code>file_id</code> for previously uploaded content. The optional <code>detail</code> can control vision detail.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ToolOutputImage(BaseModel):\n    \"\"\"Represents a tool output that should be sent to the model as an image.\n\n    You can provide either an `image_url` (URL or data URL) or a `file_id` for previously uploaded\n    content. The optional `detail` can control vision detail.\n    \"\"\"\n\n    type: Literal[\"image\"] = \"image\"\n    image_url: str | None = None\n    file_id: str | None = None\n    detail: Literal[\"low\", \"high\", \"auto\"] | None = None\n\n    @model_validator(mode=\"after\")\n    def check_at_least_one_required_field(self) -&gt; ToolOutputImage:\n        \"\"\"Validate that at least one of image_url or file_id is provided.\"\"\"\n        if self.image_url is None and self.file_id is None:\n            raise ValueError(\"At least one of image_url or file_id must be provided\")\n        return self\n</code></pre>"},{"location":"ref/tool/#agents.tool.ToolOutputImage.check_at_least_one_required_field","title":"check_at_least_one_required_field","text":"<pre><code>check_at_least_one_required_field() -&gt; ToolOutputImage\n</code></pre> <p>Validate that at least one of image_url or file_id is provided.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_at_least_one_required_field(self) -&gt; ToolOutputImage:\n    \"\"\"Validate that at least one of image_url or file_id is provided.\"\"\"\n    if self.image_url is None and self.file_id is None:\n        raise ValueError(\"At least one of image_url or file_id must be provided\")\n    return self\n</code></pre>"},{"location":"ref/tool/#agents.tool.ToolOutputImageDict","title":"ToolOutputImageDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict variant for image tool outputs.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ToolOutputImageDict(TypedDict, total=False):\n    \"\"\"TypedDict variant for image tool outputs.\"\"\"\n\n    type: Literal[\"image\"]\n    image_url: NotRequired[str]\n    file_id: NotRequired[str]\n    detail: NotRequired[Literal[\"low\", \"high\", \"auto\"]]\n</code></pre>"},{"location":"ref/tool/#agents.tool.ToolOutputFileContent","title":"ToolOutputFileContent","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool output that should be sent to the model as a file.</p> <p>Provide one of <code>file_data</code> (base64), <code>file_url</code>, or <code>file_id</code>. You may also provide an optional <code>filename</code> when using <code>file_data</code> to hint file name.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ToolOutputFileContent(BaseModel):\n    \"\"\"Represents a tool output that should be sent to the model as a file.\n\n    Provide one of `file_data` (base64), `file_url`, or `file_id`. You may also\n    provide an optional `filename` when using `file_data` to hint file name.\n    \"\"\"\n\n    type: Literal[\"file\"] = \"file\"\n    file_data: str | None = None\n    file_url: str | None = None\n    file_id: str | None = None\n    filename: str | None = None\n\n    @model_validator(mode=\"after\")\n    def check_at_least_one_required_field(self) -&gt; ToolOutputFileContent:\n        \"\"\"Validate that at least one of file_data, file_url, or file_id is provided.\"\"\"\n        if self.file_data is None and self.file_url is None and self.file_id is None:\n            raise ValueError(\"At least one of file_data, file_url, or file_id must be provided\")\n        return self\n</code></pre>"},{"location":"ref/tool/#agents.tool.ToolOutputFileContent.check_at_least_one_required_field","title":"check_at_least_one_required_field","text":"<pre><code>check_at_least_one_required_field() -&gt; (\n    ToolOutputFileContent\n)\n</code></pre> <p>Validate that at least one of file_data, file_url, or file_id is provided.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_at_least_one_required_field(self) -&gt; ToolOutputFileContent:\n    \"\"\"Validate that at least one of file_data, file_url, or file_id is provided.\"\"\"\n    if self.file_data is None and self.file_url is None and self.file_id is None:\n        raise ValueError(\"At least one of file_data, file_url, or file_id must be provided\")\n    return self\n</code></pre>"},{"location":"ref/tool/#agents.tool.ToolOutputFileContentDict","title":"ToolOutputFileContentDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict variant for file content tool outputs.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ToolOutputFileContentDict(TypedDict, total=False):\n    \"\"\"TypedDict variant for file content tool outputs.\"\"\"\n\n    type: Literal[\"file\"]\n    file_data: NotRequired[str]\n    file_url: NotRequired[str]\n    file_id: NotRequired[str]\n    filename: NotRequired[str]\n</code></pre>"},{"location":"ref/tool/#agents.tool.ComputerCreate","title":"ComputerCreate","text":"<p>               Bases: <code>Protocol[ComputerT_co]</code></p> <p>Initializes a computer for the current run context.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ComputerCreate(Protocol[ComputerT_co]):\n    \"\"\"Initializes a computer for the current run context.\"\"\"\n\n    def __call__(self, *, run_context: RunContextWrapper[Any]) -&gt; MaybeAwaitable[ComputerT_co]: ...\n</code></pre>"},{"location":"ref/tool/#agents.tool.ComputerDispose","title":"ComputerDispose","text":"<p>               Bases: <code>Protocol[ComputerT_contra]</code></p> <p>Cleans up a computer initialized for a run context.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ComputerDispose(Protocol[ComputerT_contra]):\n    \"\"\"Cleans up a computer initialized for a run context.\"\"\"\n\n    def __call__(\n        self,\n        *,\n        run_context: RunContextWrapper[Any],\n        computer: ComputerT_contra,\n    ) -&gt; MaybeAwaitable[None]: ...\n</code></pre>"},{"location":"ref/tool/#agents.tool.ComputerProvider","title":"ComputerProvider  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[ComputerT]</code></p> <p>Configures create/dispose hooks for per-run computer lifecycle management.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ComputerProvider(Generic[ComputerT]):\n    \"\"\"Configures create/dispose hooks for per-run computer lifecycle management.\"\"\"\n\n    create: ComputerCreate[ComputerT]\n    dispose: ComputerDispose[ComputerT] | None = None\n</code></pre>"},{"location":"ref/tool/#agents.tool.FunctionToolResult","title":"FunctionToolResult  <code>dataclass</code>","text":"Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass FunctionToolResult:\n    tool: FunctionTool\n    \"\"\"The tool that was run.\"\"\"\n\n    output: Any\n    \"\"\"The output of the tool.\"\"\"\n\n    run_item: RunItem | None\n    \"\"\"The run item that was produced as a result of the tool call.\n\n    This can be None when the tool run is interrupted and no output item should be emitted yet.\n    \"\"\"\n\n    interruptions: list[ToolApprovalItem] = field(default_factory=list)\n    \"\"\"Interruptions from nested agent runs (for agent-as-tool).\"\"\"\n\n    agent_run_result: Any = None  # RunResult | None, but avoid circular import\n    \"\"\"Nested agent run result (for agent-as-tool).\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.tool","title":"tool  <code>instance-attribute</code>","text":"<pre><code>tool: FunctionTool\n</code></pre> <p>The tool that was run.</p>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: Any\n</code></pre> <p>The output of the tool.</p>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.run_item","title":"run_item  <code>instance-attribute</code>","text":"<pre><code>run_item: RunItem | None\n</code></pre> <p>The run item that was produced as a result of the tool call.</p> <p>This can be None when the tool run is interrupted and no output item should be emitted yet.</p>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.interruptions","title":"interruptions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interruptions: list[ToolApprovalItem] = field(\n    default_factory=list\n)\n</code></pre> <p>Interruptions from nested agent runs (for agent-as-tool).</p>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.agent_run_result","title":"agent_run_result  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent_run_result: Any = None\n</code></pre> <p>Nested agent run result (for agent-as-tool).</p>"},{"location":"ref/tool/#agents.tool.FunctionTool","title":"FunctionTool  <code>dataclass</code>","text":"<p>A tool that wraps a function. In most cases, you should use  the <code>function_tool</code> helpers to create a FunctionTool, as they let you easily wrap a Python function.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass FunctionTool:\n    \"\"\"A tool that wraps a function. In most cases, you should use  the `function_tool` helpers to\n    create a FunctionTool, as they let you easily wrap a Python function.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the tool, as shown to the LLM. Generally the name of the function.\"\"\"\n\n    description: str\n    \"\"\"A description of the tool, as shown to the LLM.\"\"\"\n\n    params_json_schema: dict[str, Any]\n    \"\"\"The JSON schema for the tool's parameters.\"\"\"\n\n    on_invoke_tool: Callable[[ToolContext[Any], str], Awaitable[Any]]\n    \"\"\"A function that invokes the tool with the given context and parameters. The params passed\n    are:\n    1. The tool run context.\n    2. The arguments from the LLM, as a JSON string.\n\n    You must return a one of the structured tool output types (e.g. ToolOutputText, ToolOutputImage,\n    ToolOutputFileContent) or a string representation of the tool output, or a list of them,\n    or something we can call `str()` on.\n    In case of errors, you can either raise an Exception (which will cause the run to fail) or\n    return a string error message (which will be sent back to the LLM).\n    \"\"\"\n\n    strict_json_schema: bool = True\n    \"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\n    as it increases the likelihood of correct JSON input.\"\"\"\n\n    is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]] = True\n    \"\"\"Whether the tool is enabled. Either a bool or a Callable that takes the run context and agent\n    and returns whether the tool is enabled. You can use this to dynamically enable/disable a tool\n    based on your context/state.\"\"\"\n\n    # Keep guardrail fields before needs_approval to preserve v0.7.0 positional\n    # constructor compatibility for public FunctionTool callers.\n    # Tool-specific guardrails.\n    tool_input_guardrails: list[ToolInputGuardrail[Any]] | None = None\n    \"\"\"Optional list of input guardrails to run before invoking this tool.\"\"\"\n\n    tool_output_guardrails: list[ToolOutputGuardrail[Any]] | None = None\n    \"\"\"Optional list of output guardrails to run after invoking this tool.\"\"\"\n\n    needs_approval: (\n        bool | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]]\n    ) = False\n    \"\"\"Whether the tool needs approval before execution. If True, the run will be interrupted\n    and the tool call will need to be approved using RunState.approve() or rejected using\n    RunState.reject() before continuing. Can be a bool (always/never needs approval) or a\n    function that takes (run_context, tool_parameters, call_id) and returns whether this\n    specific call needs approval.\"\"\"\n\n    _is_agent_tool: bool = field(default=False, init=False, repr=False)\n    \"\"\"Internal flag indicating if this tool is an agent-as-tool.\"\"\"\n\n    _agent_instance: Any = field(default=None, init=False, repr=False)\n    \"\"\"Internal reference to the agent instance if this is an agent-as-tool.\"\"\"\n\n    def __post_init__(self):\n        if self.strict_json_schema:\n            self.params_json_schema = ensure_strict_json_schema(self.params_json_schema)\n</code></pre>"},{"location":"ref/tool/#agents.tool.FunctionTool.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the tool, as shown to the LLM. Generally the name of the function.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre> <p>A description of the tool, as shown to the LLM.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.params_json_schema","title":"params_json_schema  <code>instance-attribute</code>","text":"<pre><code>params_json_schema: dict[str, Any]\n</code></pre> <p>The JSON schema for the tool's parameters.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.on_invoke_tool","title":"on_invoke_tool  <code>instance-attribute</code>","text":"<pre><code>on_invoke_tool: Callable[\n    [ToolContext[Any], str], Awaitable[Any]\n]\n</code></pre> <p>A function that invokes the tool with the given context and parameters. The params passed are: 1. The tool run context. 2. The arguments from the LLM, as a JSON string.</p> <p>You must return a one of the structured tool output types (e.g. ToolOutputText, ToolOutputImage, ToolOutputFileContent) or a string representation of the tool output, or a list of them, or something we can call <code>str()</code> on. In case of errors, you can either raise an Exception (which will cause the run to fail) or return a string error message (which will be sent back to the LLM).</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.strict_json_schema","title":"strict_json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strict_json_schema: bool = True\n</code></pre> <p>Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.is_enabled","title":"is_enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_enabled: (\n    bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ]\n) = True\n</code></pre> <p>Whether the tool is enabled. Either a bool or a Callable that takes the run context and agent and returns whether the tool is enabled. You can use this to dynamically enable/disable a tool based on your context/state.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.tool_input_guardrails","title":"tool_input_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_input_guardrails: (\n    list[ToolInputGuardrail[Any]] | None\n) = None\n</code></pre> <p>Optional list of input guardrails to run before invoking this tool.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.tool_output_guardrails","title":"tool_output_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_output_guardrails: (\n    list[ToolOutputGuardrail[Any]] | None\n) = None\n</code></pre> <p>Optional list of output guardrails to run after invoking this tool.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.needs_approval","title":"needs_approval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>needs_approval: (\n    bool\n    | Callable[\n        [RunContextWrapper[Any], dict[str, Any], str],\n        Awaitable[bool],\n    ]\n) = False\n</code></pre> <p>Whether the tool needs approval before execution. If True, the run will be interrupted and the tool call will need to be approved using RunState.approve() or rejected using RunState.reject() before continuing. Can be a bool (always/never needs approval) or a function that takes (run_context, tool_parameters, call_id) and returns whether this specific call needs approval.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool","title":"FileSearchTool  <code>dataclass</code>","text":"<p>A hosted tool that lets the LLM search through a vector store. Currently only supported with OpenAI models, using the Responses API.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass FileSearchTool:\n    \"\"\"A hosted tool that lets the LLM search through a vector store. Currently only supported with\n    OpenAI models, using the Responses API.\n    \"\"\"\n\n    vector_store_ids: list[str]\n    \"\"\"The IDs of the vector stores to search.\"\"\"\n\n    max_num_results: int | None = None\n    \"\"\"The maximum number of results to return.\"\"\"\n\n    include_search_results: bool = False\n    \"\"\"Whether to include the search results in the output produced by the LLM.\"\"\"\n\n    ranking_options: RankingOptions | None = None\n    \"\"\"Ranking options for search.\"\"\"\n\n    filters: Filters | None = None\n    \"\"\"A filter to apply based on file attributes.\"\"\"\n\n    @property\n    def name(self):\n        return \"file_search\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.FileSearchTool.vector_store_ids","title":"vector_store_ids  <code>instance-attribute</code>","text":"<pre><code>vector_store_ids: list[str]\n</code></pre> <p>The IDs of the vector stores to search.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.max_num_results","title":"max_num_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_num_results: int | None = None\n</code></pre> <p>The maximum number of results to return.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.include_search_results","title":"include_search_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_search_results: bool = False\n</code></pre> <p>Whether to include the search results in the output produced by the LLM.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.ranking_options","title":"ranking_options  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ranking_options: RankingOptions | None = None\n</code></pre> <p>Ranking options for search.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.filters","title":"filters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filters: Filters | None = None\n</code></pre> <p>A filter to apply based on file attributes.</p>"},{"location":"ref/tool/#agents.tool.WebSearchTool","title":"WebSearchTool  <code>dataclass</code>","text":"<p>A hosted tool that lets the LLM search the web. Currently only supported with OpenAI models, using the Responses API.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass WebSearchTool:\n    \"\"\"A hosted tool that lets the LLM search the web. Currently only supported with OpenAI models,\n    using the Responses API.\n    \"\"\"\n\n    user_location: UserLocation | None = None\n    \"\"\"Optional location for the search. Lets you customize results to be relevant to a location.\"\"\"\n\n    filters: WebSearchToolFilters | None = None\n    \"\"\"A filter to apply based on file attributes.\"\"\"\n\n    search_context_size: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n    \"\"\"The amount of context to use for the search.\"\"\"\n\n    @property\n    def name(self):\n        return \"web_search\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.WebSearchTool.user_location","title":"user_location  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_location: UserLocation | None = None\n</code></pre> <p>Optional location for the search. Lets you customize results to be relevant to a location.</p>"},{"location":"ref/tool/#agents.tool.WebSearchTool.filters","title":"filters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filters: Filters | None = None\n</code></pre> <p>A filter to apply based on file attributes.</p>"},{"location":"ref/tool/#agents.tool.WebSearchTool.search_context_size","title":"search_context_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>search_context_size: Literal[\"low\", \"medium\", \"high\"] = (\n    \"medium\"\n)\n</code></pre> <p>The amount of context to use for the search.</p>"},{"location":"ref/tool/#agents.tool.ComputerTool","title":"ComputerTool  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[ComputerT]</code></p> <p>A hosted tool that lets the LLM control a computer.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass(eq=False)\nclass ComputerTool(Generic[ComputerT]):\n    \"\"\"A hosted tool that lets the LLM control a computer.\"\"\"\n\n    computer: ComputerConfig[ComputerT]\n    \"\"\"The computer implementation, or a factory that produces a computer per run.\"\"\"\n\n    on_safety_check: Callable[[ComputerToolSafetyCheckData], MaybeAwaitable[bool]] | None = None\n    \"\"\"Optional callback to acknowledge computer tool safety checks.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        _store_computer_initializer(self)\n\n    @property\n    def name(self):\n        return \"computer_use_preview\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ComputerTool.computer","title":"computer  <code>instance-attribute</code>","text":"<pre><code>computer: ComputerConfig[ComputerT]\n</code></pre> <p>The computer implementation, or a factory that produces a computer per run.</p>"},{"location":"ref/tool/#agents.tool.ComputerTool.on_safety_check","title":"on_safety_check  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>on_safety_check: (\n    Callable[\n        [ComputerToolSafetyCheckData], MaybeAwaitable[bool]\n    ]\n    | None\n) = None\n</code></pre> <p>Optional callback to acknowledge computer tool safety checks.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData","title":"ComputerToolSafetyCheckData  <code>dataclass</code>","text":"<p>Information about a computer tool safety check.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ComputerToolSafetyCheckData:\n    \"\"\"Information about a computer tool safety check.\"\"\"\n\n    ctx_wrapper: RunContextWrapper[Any]\n    \"\"\"The run context.\"\"\"\n\n    agent: Agent[Any]\n    \"\"\"The agent performing the computer action.\"\"\"\n\n    tool_call: ResponseComputerToolCall\n    \"\"\"The computer tool call.\"\"\"\n\n    safety_check: PendingSafetyCheck\n    \"\"\"The pending safety check to acknowledge.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.ctx_wrapper","title":"ctx_wrapper  <code>instance-attribute</code>","text":"<pre><code>ctx_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The run context.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent performing the computer action.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.tool_call","title":"tool_call  <code>instance-attribute</code>","text":"<pre><code>tool_call: ResponseComputerToolCall\n</code></pre> <p>The computer tool call.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.safety_check","title":"safety_check  <code>instance-attribute</code>","text":"<pre><code>safety_check: PendingSafetyCheck\n</code></pre> <p>The pending safety check to acknowledge.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalRequest","title":"MCPToolApprovalRequest  <code>dataclass</code>","text":"<p>A request to approve a tool call.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass MCPToolApprovalRequest:\n    \"\"\"A request to approve a tool call.\"\"\"\n\n    ctx_wrapper: RunContextWrapper[Any]\n    \"\"\"The run context.\"\"\"\n\n    data: McpApprovalRequest\n    \"\"\"The data from the MCP tool approval request.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalRequest.ctx_wrapper","title":"ctx_wrapper  <code>instance-attribute</code>","text":"<pre><code>ctx_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The run context.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalRequest.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: McpApprovalRequest\n</code></pre> <p>The data from the MCP tool approval request.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunctionResult","title":"MCPToolApprovalFunctionResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>The result of an MCP tool approval function.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class MCPToolApprovalFunctionResult(TypedDict):\n    \"\"\"The result of an MCP tool approval function.\"\"\"\n\n    approve: bool\n    \"\"\"Whether to approve the tool call.\"\"\"\n\n    reason: NotRequired[str]\n    \"\"\"An optional reason, if rejected.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunctionResult.approve","title":"approve  <code>instance-attribute</code>","text":"<pre><code>approve: bool\n</code></pre> <p>Whether to approve the tool call.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunctionResult.reason","title":"reason  <code>instance-attribute</code>","text":"<pre><code>reason: NotRequired[str]\n</code></pre> <p>An optional reason, if rejected.</p>"},{"location":"ref/tool/#agents.tool.ShellOnApprovalFunctionResult","title":"ShellOnApprovalFunctionResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>The result of a shell tool on_approval callback.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ShellOnApprovalFunctionResult(TypedDict):\n    \"\"\"The result of a shell tool on_approval callback.\"\"\"\n\n    approve: bool\n    \"\"\"Whether to approve the tool call.\"\"\"\n\n    reason: NotRequired[str]\n    \"\"\"An optional reason, if rejected.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellOnApprovalFunctionResult.approve","title":"approve  <code>instance-attribute</code>","text":"<pre><code>approve: bool\n</code></pre> <p>Whether to approve the tool call.</p>"},{"location":"ref/tool/#agents.tool.ShellOnApprovalFunctionResult.reason","title":"reason  <code>instance-attribute</code>","text":"<pre><code>reason: NotRequired[str]\n</code></pre> <p>An optional reason, if rejected.</p>"},{"location":"ref/tool/#agents.tool.ApplyPatchOnApprovalFunctionResult","title":"ApplyPatchOnApprovalFunctionResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>The result of an apply_patch tool on_approval callback.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class ApplyPatchOnApprovalFunctionResult(TypedDict):\n    \"\"\"The result of an apply_patch tool on_approval callback.\"\"\"\n\n    approve: bool\n    \"\"\"Whether to approve the tool call.\"\"\"\n\n    reason: NotRequired[str]\n    \"\"\"An optional reason, if rejected.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ApplyPatchOnApprovalFunctionResult.approve","title":"approve  <code>instance-attribute</code>","text":"<pre><code>approve: bool\n</code></pre> <p>Whether to approve the tool call.</p>"},{"location":"ref/tool/#agents.tool.ApplyPatchOnApprovalFunctionResult.reason","title":"reason  <code>instance-attribute</code>","text":"<pre><code>reason: NotRequired[str]\n</code></pre> <p>An optional reason, if rejected.</p>"},{"location":"ref/tool/#agents.tool.HostedMCPTool","title":"HostedMCPTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to use a remote MCP server. The LLM will automatically list and call tools, without requiring a round trip back to your code. If you want to run MCP servers locally via stdio, in a VPC or other non-publicly-accessible environment, or you just prefer to run tool calls locally, then you can instead use the servers in <code>agents.mcp</code> and pass <code>Agent(mcp_servers=[...])</code> to the agent.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass HostedMCPTool:\n    \"\"\"A tool that allows the LLM to use a remote MCP server. The LLM will automatically list and\n    call tools, without requiring a round trip back to your code.\n    If you want to run MCP servers locally via stdio, in a VPC or other non-publicly-accessible\n    environment, or you just prefer to run tool calls locally, then you can instead use the servers\n    in `agents.mcp` and pass `Agent(mcp_servers=[...])` to the agent.\"\"\"\n\n    tool_config: Mcp\n    \"\"\"The MCP tool config, which includes the server URL and other settings.\"\"\"\n\n    on_approval_request: MCPToolApprovalFunction | None = None\n    \"\"\"An optional function that will be called if approval is requested for an MCP tool. If not\n    provided, you will need to manually add approvals/rejections to the input and call\n    `Runner.run(...)` again.\"\"\"\n\n    @property\n    def name(self):\n        return \"hosted_mcp\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.HostedMCPTool.tool_config","title":"tool_config  <code>instance-attribute</code>","text":"<pre><code>tool_config: Mcp\n</code></pre> <p>The MCP tool config, which includes the server URL and other settings.</p>"},{"location":"ref/tool/#agents.tool.HostedMCPTool.on_approval_request","title":"on_approval_request  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>on_approval_request: MCPToolApprovalFunction | None = None\n</code></pre> <p>An optional function that will be called if approval is requested for an MCP tool. If not provided, you will need to manually add approvals/rejections to the input and call <code>Runner.run(...)</code> again.</p>"},{"location":"ref/tool/#agents.tool.CodeInterpreterTool","title":"CodeInterpreterTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to execute code in a sandboxed environment.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass CodeInterpreterTool:\n    \"\"\"A tool that allows the LLM to execute code in a sandboxed environment.\"\"\"\n\n    tool_config: CodeInterpreter\n    \"\"\"The tool config, which includes the container and other settings.\"\"\"\n\n    @property\n    def name(self):\n        return \"code_interpreter\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.CodeInterpreterTool.tool_config","title":"tool_config  <code>instance-attribute</code>","text":"<pre><code>tool_config: CodeInterpreter\n</code></pre> <p>The tool config, which includes the container and other settings.</p>"},{"location":"ref/tool/#agents.tool.ImageGenerationTool","title":"ImageGenerationTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to generate images.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ImageGenerationTool:\n    \"\"\"A tool that allows the LLM to generate images.\"\"\"\n\n    tool_config: ImageGeneration\n    \"\"\"The tool config, which image generation settings.\"\"\"\n\n    @property\n    def name(self):\n        return \"image_generation\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ImageGenerationTool.tool_config","title":"tool_config  <code>instance-attribute</code>","text":"<pre><code>tool_config: ImageGeneration\n</code></pre> <p>The tool config, which image generation settings.</p>"},{"location":"ref/tool/#agents.tool.LocalShellCommandRequest","title":"LocalShellCommandRequest  <code>dataclass</code>","text":"<p>A request to execute a command on a shell.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass LocalShellCommandRequest:\n    \"\"\"A request to execute a command on a shell.\"\"\"\n\n    ctx_wrapper: RunContextWrapper[Any]\n    \"\"\"The run context.\"\"\"\n\n    data: LocalShellCall\n    \"\"\"The data from the local shell tool call.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.LocalShellCommandRequest.ctx_wrapper","title":"ctx_wrapper  <code>instance-attribute</code>","text":"<pre><code>ctx_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The run context.</p>"},{"location":"ref/tool/#agents.tool.LocalShellCommandRequest.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: LocalShellCall\n</code></pre> <p>The data from the local shell tool call.</p>"},{"location":"ref/tool/#agents.tool.LocalShellTool","title":"LocalShellTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to execute commands on a shell.</p> <p>For more details, see: https://platform.openai.com/docs/guides/tools-local-shell</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass LocalShellTool:\n    \"\"\"A tool that allows the LLM to execute commands on a shell.\n\n    For more details, see:\n    https://platform.openai.com/docs/guides/tools-local-shell\n    \"\"\"\n\n    executor: LocalShellExecutor\n    \"\"\"A function that executes a command on a shell.\"\"\"\n\n    @property\n    def name(self):\n        return \"local_shell\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.LocalShellTool.executor","title":"executor  <code>instance-attribute</code>","text":"<pre><code>executor: LocalShellExecutor\n</code></pre> <p>A function that executes a command on a shell.</p>"},{"location":"ref/tool/#agents.tool.ShellCallOutcome","title":"ShellCallOutcome  <code>dataclass</code>","text":"<p>Describes the terminal condition of a shell command.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ShellCallOutcome:\n    \"\"\"Describes the terminal condition of a shell command.\"\"\"\n\n    type: Literal[\"exit\", \"timeout\"]\n    exit_code: int | None = None\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellCommandOutput","title":"ShellCommandOutput  <code>dataclass</code>","text":"<p>Structured output for a single shell command execution.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ShellCommandOutput:\n    \"\"\"Structured output for a single shell command execution.\"\"\"\n\n    stdout: str = \"\"\n    stderr: str = \"\"\n    outcome: ShellCallOutcome = field(default_factory=lambda: ShellCallOutcome(type=\"exit\"))\n    command: str | None = None\n    provider_data: dict[str, Any] | None = None\n\n    @property\n    def exit_code(self) -&gt; int | None:\n        return self.outcome.exit_code\n\n    @property\n    def status(self) -&gt; Literal[\"completed\", \"timeout\"]:\n        return \"timeout\" if self.outcome.type == \"timeout\" else \"completed\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellResult","title":"ShellResult  <code>dataclass</code>","text":"<p>Result returned by a shell executor.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ShellResult:\n    \"\"\"Result returned by a shell executor.\"\"\"\n\n    output: list[ShellCommandOutput]\n    max_output_length: int | None = None\n    provider_data: dict[str, Any] | None = None\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellActionRequest","title":"ShellActionRequest  <code>dataclass</code>","text":"<p>Action payload for a next-generation shell call.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ShellActionRequest:\n    \"\"\"Action payload for a next-generation shell call.\"\"\"\n\n    commands: list[str]\n    timeout_ms: int | None = None\n    max_output_length: int | None = None\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellCallData","title":"ShellCallData  <code>dataclass</code>","text":"<p>Normalized shell call data provided to shell executors.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ShellCallData:\n    \"\"\"Normalized shell call data provided to shell executors.\"\"\"\n\n    call_id: str\n    action: ShellActionRequest\n    status: Literal[\"in_progress\", \"completed\"] | None = None\n    raw: Any | None = None\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellCommandRequest","title":"ShellCommandRequest  <code>dataclass</code>","text":"<p>A request to execute a modern shell call.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ShellCommandRequest:\n    \"\"\"A request to execute a modern shell call.\"\"\"\n\n    ctx_wrapper: RunContextWrapper[Any]\n    data: ShellCallData\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellTool","title":"ShellTool  <code>dataclass</code>","text":"<p>Next-generation shell tool. LocalShellTool will be deprecated in favor of this.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ShellTool:\n    \"\"\"Next-generation shell tool. LocalShellTool will be deprecated in favor of this.\"\"\"\n\n    executor: ShellExecutor\n    name: str = \"shell\"\n    needs_approval: bool | ShellApprovalFunction = False\n    \"\"\"Whether the shell tool needs approval before execution. If True, the run will be interrupted\n    and the tool call will need to be approved using RunState.approve() or rejected using\n    RunState.reject() before continuing. Can be a bool (always/never needs approval) or a\n    function that takes (run_context, action, call_id) and returns whether this specific call\n    needs approval.\n    \"\"\"\n    on_approval: ShellOnApprovalFunction | None = None\n    \"\"\"Optional handler to auto-approve or reject when approval is required.\n    If provided, it will be invoked immediately when an approval is needed.\n    \"\"\"\n\n    @property\n    def type(self) -&gt; str:\n        return \"shell\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ShellTool.needs_approval","title":"needs_approval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>needs_approval: bool | ShellApprovalFunction = False\n</code></pre> <p>Whether the shell tool needs approval before execution. If True, the run will be interrupted and the tool call will need to be approved using RunState.approve() or rejected using RunState.reject() before continuing. Can be a bool (always/never needs approval) or a function that takes (run_context, action, call_id) and returns whether this specific call needs approval.</p>"},{"location":"ref/tool/#agents.tool.ShellTool.on_approval","title":"on_approval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>on_approval: ShellOnApprovalFunction | None = None\n</code></pre> <p>Optional handler to auto-approve or reject when approval is required. If provided, it will be invoked immediately when an approval is needed.</p>"},{"location":"ref/tool/#agents.tool.ApplyPatchTool","title":"ApplyPatchTool  <code>dataclass</code>","text":"<p>Hosted apply_patch tool. Lets the model request file mutations via unified diffs.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ApplyPatchTool:\n    \"\"\"Hosted apply_patch tool. Lets the model request file mutations via unified diffs.\"\"\"\n\n    editor: ApplyPatchEditor\n    name: str = \"apply_patch\"\n    needs_approval: bool | ApplyPatchApprovalFunction = False\n    \"\"\"Whether the apply_patch tool needs approval before execution. If True, the run will be\n    interrupted and the tool call will need to be approved using RunState.approve() or rejected\n    using RunState.reject() before continuing. Can be a bool (always/never needs approval) or a\n    function that takes (run_context, operation, call_id) and returns whether this specific call\n    needs approval.\n    \"\"\"\n    on_approval: ApplyPatchOnApprovalFunction | None = None\n    \"\"\"Optional handler to auto-approve or reject when approval is required.\n    If provided, it will be invoked immediately when an approval is needed.\n    \"\"\"\n\n    @property\n    def type(self) -&gt; str:\n        return \"apply_patch\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ApplyPatchTool.needs_approval","title":"needs_approval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>needs_approval: bool | ApplyPatchApprovalFunction = False\n</code></pre> <p>Whether the apply_patch tool needs approval before execution. If True, the run will be interrupted and the tool call will need to be approved using RunState.approve() or rejected using RunState.reject() before continuing. Can be a bool (always/never needs approval) or a function that takes (run_context, operation, call_id) and returns whether this specific call needs approval.</p>"},{"location":"ref/tool/#agents.tool.ApplyPatchTool.on_approval","title":"on_approval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>on_approval: ApplyPatchOnApprovalFunction | None = None\n</code></pre> <p>Optional handler to auto-approve or reject when approval is required. If provided, it will be invoked immediately when an approval is needed.</p>"},{"location":"ref/tool/#agents.tool.resolve_computer","title":"resolve_computer  <code>async</code>","text":"<pre><code>resolve_computer(\n    *,\n    tool: ComputerTool[Any],\n    run_context: RunContextWrapper[Any],\n) -&gt; ComputerLike\n</code></pre> <p>Resolve a computer for a given run context, initializing it if needed.</p> Source code in <code>src/agents/tool.py</code> <pre><code>async def resolve_computer(\n    *, tool: ComputerTool[Any], run_context: RunContextWrapper[Any]\n) -&gt; ComputerLike:\n    \"\"\"Resolve a computer for a given run context, initializing it if needed.\"\"\"\n    per_context = _computer_cache.get(tool)\n    if per_context is None:\n        per_context = weakref.WeakKeyDictionary()\n        _computer_cache[tool] = per_context\n\n    cached = per_context.get(run_context)\n    if cached is not None:\n        _track_resolved_computer(tool=tool, run_context=run_context, resolved=cached)\n        return cached.computer\n\n    initializer_config = _get_computer_initializer(tool)\n    lifecycle: ComputerProvider[Any] | None = (\n        cast(ComputerProvider[Any], initializer_config)\n        if _is_computer_provider(initializer_config)\n        else None\n    )\n    initializer: ComputerCreate[Any] | None = None\n    disposer: ComputerDispose[Any] | None = lifecycle.dispose if lifecycle else None\n\n    if lifecycle is not None:\n        initializer = lifecycle.create\n    elif callable(initializer_config):\n        initializer = initializer_config\n    elif _is_computer_provider(tool.computer):\n        lifecycle_provider = cast(ComputerProvider[Any], tool.computer)\n        initializer = lifecycle_provider.create\n        disposer = lifecycle_provider.dispose\n\n    if initializer:\n        computer_candidate = initializer(run_context=run_context)\n        computer = (\n            await computer_candidate\n            if inspect.isawaitable(computer_candidate)\n            else computer_candidate\n        )\n    else:\n        computer = cast(ComputerLike, tool.computer)\n\n    if not isinstance(computer, (Computer, AsyncComputer)):\n        raise UserError(\"The computer tool did not provide a computer instance.\")\n\n    resolved = _ResolvedComputer(computer=computer, dispose=disposer)\n    per_context[run_context] = resolved\n    _track_resolved_computer(tool=tool, run_context=run_context, resolved=resolved)\n    tool.computer = computer\n    return computer\n</code></pre>"},{"location":"ref/tool/#agents.tool.dispose_resolved_computers","title":"dispose_resolved_computers  <code>async</code>","text":"<pre><code>dispose_resolved_computers(\n    *, run_context: RunContextWrapper[Any]\n) -&gt; None\n</code></pre> <p>Dispose any computer instances created for the provided run context.</p> Source code in <code>src/agents/tool.py</code> <pre><code>async def dispose_resolved_computers(*, run_context: RunContextWrapper[Any]) -&gt; None:\n    \"\"\"Dispose any computer instances created for the provided run context.\"\"\"\n    resolved_by_tool = _computers_by_run_context.pop(run_context, None)\n    if not resolved_by_tool:\n        return\n\n    disposers: list[tuple[ComputerDispose[ComputerLike], ComputerLike]] = []\n\n    for tool, _resolved in resolved_by_tool.items():\n        per_context = _computer_cache.get(tool)\n        if per_context is not None:\n            per_context.pop(run_context, None)\n\n        initializer = _get_computer_initializer(tool)\n        if initializer is not None:\n            tool.computer = initializer\n\n        if _resolved.dispose is not None:\n            disposers.append((_resolved.dispose, _resolved.computer))\n\n    for dispose, computer in disposers:\n        try:\n            result = dispose(run_context=run_context, computer=computer)\n            if inspect.isawaitable(result):\n                await result\n        except Exception as exc:\n            logger.warning(\"Failed to dispose computer for run context: %s\", exc)\n</code></pre>"},{"location":"ref/tool/#agents.tool.default_tool_error_function","title":"default_tool_error_function","text":"<pre><code>default_tool_error_function(\n    ctx: RunContextWrapper[Any], error: Exception\n) -&gt; str\n</code></pre> <p>The default tool error function, which just returns a generic error message.</p> Source code in <code>src/agents/tool.py</code> <pre><code>def default_tool_error_function(ctx: RunContextWrapper[Any], error: Exception) -&gt; str:\n    \"\"\"The default tool error function, which just returns a generic error message.\"\"\"\n    json_decode_error = _extract_tool_argument_json_error(error)\n    if json_decode_error is not None:\n        return (\n            \"An error occurred while parsing tool arguments. \"\n            \"Please try again with valid JSON. \"\n            f\"Error: {json_decode_error}\"\n        )\n    return f\"An error occurred while running the tool. Please try again. Error: {str(error)}\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.function_tool","title":"function_tool","text":"<pre><code>function_tool(\n    func: ToolFunction[...],\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction | None = None,\n    strict_mode: bool = True,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ] = True,\n    needs_approval: bool\n    | Callable[\n        [RunContextWrapper[Any], dict[str, Any], str],\n        Awaitable[bool],\n    ] = False,\n    tool_input_guardrails: list[ToolInputGuardrail[Any]]\n    | None = None,\n    tool_output_guardrails: list[ToolOutputGuardrail[Any]]\n    | None = None,\n) -&gt; FunctionTool\n</code></pre><pre><code>function_tool(\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction | None = None,\n    strict_mode: bool = True,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ] = True,\n    needs_approval: bool\n    | Callable[\n        [RunContextWrapper[Any], dict[str, Any], str],\n        Awaitable[bool],\n    ] = False,\n    tool_input_guardrails: list[ToolInputGuardrail[Any]]\n    | None = None,\n    tool_output_guardrails: list[ToolOutputGuardrail[Any]]\n    | None = None,\n) -&gt; Callable[[ToolFunction[...]], FunctionTool]\n</code></pre> <pre><code>function_tool(\n    func: ToolFunction[...] | None = None,\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction\n    | None = default_tool_error_function,\n    strict_mode: bool = True,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ] = True,\n    needs_approval: bool\n    | Callable[\n        [RunContextWrapper[Any], dict[str, Any], str],\n        Awaitable[bool],\n    ] = False,\n    tool_input_guardrails: list[ToolInputGuardrail[Any]]\n    | None = None,\n    tool_output_guardrails: list[ToolOutputGuardrail[Any]]\n    | None = None,\n) -&gt; (\n    FunctionTool\n    | Callable[[ToolFunction[...]], FunctionTool]\n)\n</code></pre> <p>Decorator to create a FunctionTool from a function. By default, we will: 1. Parse the function signature to create a JSON schema for the tool's parameters. 2. Use the function's docstring to populate the tool's description. 3. Use the function's docstring to populate argument descriptions. The docstring style is detected automatically, but you can override it.</p> <p>If the function takes a <code>RunContextWrapper</code> as the first argument, it must match the context type of the agent that uses the tool.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>ToolFunction[...] | None</code> <p>The function to wrap.</p> <code>None</code> <code>name_override</code> <code>str | None</code> <p>If provided, use this name for the tool instead of the function's name.</p> <code>None</code> <code>description_override</code> <code>str | None</code> <p>If provided, use this description for the tool instead of the function's docstring.</p> <code>None</code> <code>docstring_style</code> <code>DocstringStyle | None</code> <p>If provided, use this style for the tool's docstring. If not provided, we will attempt to auto-detect the style.</p> <code>None</code> <code>use_docstring_info</code> <code>bool</code> <p>If True, use the function's docstring to populate the tool's description and argument descriptions.</p> <code>True</code> <code>failure_error_function</code> <code>ToolErrorFunction | None</code> <p>If provided, use this function to generate an error message when the tool call fails. The error message is sent to the LLM. If you pass None, then no error message will be sent and instead an Exception will be raised.</p> <code>default_tool_error_function</code> <code>strict_mode</code> <code>bool</code> <p>Whether to enable strict mode for the tool's JSON schema. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input. If False, it allows non-strict JSON schemas. For example, if a parameter has a default value, it will be optional, additional properties are allowed, etc. See here for more: https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas</p> <code>True</code> <code>is_enabled</code> <code>bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]]</code> <p>Whether the tool is enabled. Can be a bool or a callable that takes the run context and agent and returns whether the tool is enabled. Disabled tools are hidden from the LLM at runtime.</p> <code>True</code> <code>needs_approval</code> <code>bool | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]]</code> <p>Whether the tool needs approval before execution. If True, the run will be interrupted and the tool call will need to be approved using RunState.approve() or rejected using RunState.reject() before continuing. Can be a bool (always/never needs approval) or a function that takes (run_context, tool_parameters, call_id) and returns whether this specific call needs approval.</p> <code>False</code> <code>tool_input_guardrails</code> <code>list[ToolInputGuardrail[Any]] | None</code> <p>Optional list of guardrails to run before invoking the tool.</p> <code>None</code> <code>tool_output_guardrails</code> <code>list[ToolOutputGuardrail[Any]] | None</code> <p>Optional list of guardrails to run after the tool returns.</p> <code>None</code> Source code in <code>src/agents/tool.py</code> <pre><code>def function_tool(\n    func: ToolFunction[...] | None = None,\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n    strict_mode: bool = True,\n    is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]] = True,\n    needs_approval: bool\n    | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]] = False,\n    tool_input_guardrails: list[ToolInputGuardrail[Any]] | None = None,\n    tool_output_guardrails: list[ToolOutputGuardrail[Any]] | None = None,\n) -&gt; FunctionTool | Callable[[ToolFunction[...]], FunctionTool]:\n    \"\"\"\n    Decorator to create a FunctionTool from a function. By default, we will:\n    1. Parse the function signature to create a JSON schema for the tool's parameters.\n    2. Use the function's docstring to populate the tool's description.\n    3. Use the function's docstring to populate argument descriptions.\n    The docstring style is detected automatically, but you can override it.\n\n    If the function takes a `RunContextWrapper` as the first argument, it *must* match the\n    context type of the agent that uses the tool.\n\n    Args:\n        func: The function to wrap.\n        name_override: If provided, use this name for the tool instead of the function's name.\n        description_override: If provided, use this description for the tool instead of the\n            function's docstring.\n        docstring_style: If provided, use this style for the tool's docstring. If not provided,\n            we will attempt to auto-detect the style.\n        use_docstring_info: If True, use the function's docstring to populate the tool's\n            description and argument descriptions.\n        failure_error_function: If provided, use this function to generate an error message when\n            the tool call fails. The error message is sent to the LLM. If you pass None, then no\n            error message will be sent and instead an Exception will be raised.\n        strict_mode: Whether to enable strict mode for the tool's JSON schema. We *strongly*\n            recommend setting this to True, as it increases the likelihood of correct JSON input.\n            If False, it allows non-strict JSON schemas. For example, if a parameter has a default\n            value, it will be optional, additional properties are allowed, etc. See here for more:\n            https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas\n        is_enabled: Whether the tool is enabled. Can be a bool or a callable that takes the run\n            context and agent and returns whether the tool is enabled. Disabled tools are hidden\n            from the LLM at runtime.\n        needs_approval: Whether the tool needs approval before execution. If True, the run will\n            be interrupted and the tool call will need to be approved using RunState.approve() or\n            rejected using RunState.reject() before continuing. Can be a bool (always/never needs\n            approval) or a function that takes (run_context, tool_parameters, call_id) and returns\n            whether this specific call needs approval.\n        tool_input_guardrails: Optional list of guardrails to run before invoking the tool.\n        tool_output_guardrails: Optional list of guardrails to run after the tool returns.\n    \"\"\"\n\n    def _create_function_tool(the_func: ToolFunction[...]) -&gt; FunctionTool:\n        schema = function_schema(\n            func=the_func,\n            name_override=name_override,\n            description_override=description_override,\n            docstring_style=docstring_style,\n            use_docstring_info=use_docstring_info,\n            strict_json_schema=strict_mode,\n        )\n\n        async def _on_invoke_tool_impl(ctx: ToolContext[Any], input: str) -&gt; Any:\n            try:\n                json_data: dict[str, Any] = json.loads(input) if input else {}\n            except Exception as e:\n                if _debug.DONT_LOG_TOOL_DATA:\n                    logger.debug(f\"Invalid JSON input for tool {schema.name}\")\n                else:\n                    logger.debug(f\"Invalid JSON input for tool {schema.name}: {input}\")\n                raise ModelBehaviorError(\n                    f\"Invalid JSON input for tool {schema.name}: {input}\"\n                ) from e\n\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Invoking tool {schema.name}\")\n            else:\n                logger.debug(f\"Invoking tool {schema.name} with input {input}\")\n\n            try:\n                parsed = (\n                    schema.params_pydantic_model(**json_data)\n                    if json_data\n                    else schema.params_pydantic_model()\n                )\n            except ValidationError as e:\n                raise ModelBehaviorError(f\"Invalid JSON input for tool {schema.name}: {e}\") from e\n\n            args, kwargs_dict = schema.to_call_args(parsed)\n\n            if not _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Tool call args: {args}, kwargs: {kwargs_dict}\")\n\n            if inspect.iscoroutinefunction(the_func):\n                if schema.takes_context:\n                    result = await the_func(ctx, *args, **kwargs_dict)\n                else:\n                    result = await the_func(*args, **kwargs_dict)\n            else:\n                if schema.takes_context:\n                    result = await asyncio.to_thread(the_func, ctx, *args, **kwargs_dict)\n                else:\n                    result = await asyncio.to_thread(the_func, *args, **kwargs_dict)\n\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Tool {schema.name} completed.\")\n            else:\n                logger.debug(f\"Tool {schema.name} returned {result}\")\n\n            return result\n\n        async def _on_invoke_tool(ctx: ToolContext[Any], input: str) -&gt; Any:\n            try:\n                return await _on_invoke_tool_impl(ctx, input)\n            except Exception as e:\n                if failure_error_function is None:\n                    raise\n\n                result = failure_error_function(ctx, e)\n                if inspect.isawaitable(result):\n                    return await result\n\n                json_decode_error = _extract_tool_argument_json_error(e)\n                if json_decode_error is not None:\n                    span_error_message = \"Error running tool\"\n                    span_error_detail = str(json_decode_error)\n                else:\n                    span_error_message = \"Error running tool (non-fatal)\"\n                    span_error_detail = str(e)\n\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=span_error_message,\n                        data={\n                            \"tool_name\": schema.name,\n                            \"error\": span_error_detail,\n                        },\n                    )\n                )\n                if _debug.DONT_LOG_TOOL_DATA:\n                    logger.debug(f\"Tool {schema.name} failed\")\n                else:\n                    logger.error(\n                        f\"Tool {schema.name} failed: {input} {e}\",\n                        exc_info=e,\n                    )\n                return result\n\n        return FunctionTool(\n            name=schema.name,\n            description=schema.description or \"\",\n            params_json_schema=schema.params_json_schema,\n            on_invoke_tool=_on_invoke_tool,\n            strict_json_schema=strict_mode,\n            is_enabled=is_enabled,\n            needs_approval=needs_approval,\n            tool_input_guardrails=tool_input_guardrails,\n            tool_output_guardrails=tool_output_guardrails,\n        )\n\n    # If func is actually a callable, we were used as @function_tool with no parentheses\n    if callable(func):\n        return _create_function_tool(func)\n\n    # Otherwise, we were used as @function_tool(...), so return a decorator\n    def decorator(real_func: ToolFunction[...]) -&gt; FunctionTool:\n        return _create_function_tool(real_func)\n\n    return decorator\n</code></pre>"},{"location":"ref/tool_context/","title":"<code>Tool Context</code>","text":""},{"location":"ref/tool_context/#agents.tool_context.ToolContext","title":"ToolContext  <code>dataclass</code>","text":"<p>               Bases: <code>RunContextWrapper[TContext]</code></p> <p>The context of a tool call.</p> Source code in <code>src/agents/tool_context.py</code> <pre><code>@dataclass\nclass ToolContext(RunContextWrapper[TContext]):\n    \"\"\"The context of a tool call.\"\"\"\n\n    tool_name: str = field(default_factory=_assert_must_pass_tool_name)\n    \"\"\"The name of the tool being invoked.\"\"\"\n\n    tool_call_id: str = field(default_factory=_assert_must_pass_tool_call_id)\n    \"\"\"The ID of the tool call.\"\"\"\n\n    tool_arguments: str = field(default_factory=_assert_must_pass_tool_arguments)\n    \"\"\"The raw arguments string of the tool call.\"\"\"\n\n    tool_call: ResponseFunctionToolCall | None = None\n    \"\"\"The tool call object associated with this invocation.\"\"\"\n\n    def __init__(\n        self,\n        context: TContext,\n        usage: Usage | object = _MISSING,\n        tool_name: str | object = _MISSING,\n        tool_call_id: str | object = _MISSING,\n        tool_arguments: str | object = _MISSING,\n        tool_call: ResponseFunctionToolCall | None = None,\n        *,\n        turn_input: list[TResponseInputItem] | None = None,\n        _approvals: dict[str, _ApprovalRecord] | None = None,\n        tool_input: Any | None = None,\n    ) -&gt; None:\n        \"\"\"Preserve the v0.7 positional constructor while accepting new context fields.\"\"\"\n        resolved_usage = Usage() if usage is _MISSING else cast(Usage, usage)\n        super().__init__(\n            context=context,\n            usage=resolved_usage,\n            turn_input=list(turn_input or []),\n            _approvals={} if _approvals is None else _approvals,\n            tool_input=tool_input,\n        )\n        self.tool_name = (\n            _assert_must_pass_tool_name() if tool_name is _MISSING else cast(str, tool_name)\n        )\n        self.tool_arguments = (\n            _assert_must_pass_tool_arguments()\n            if tool_arguments is _MISSING\n            else cast(str, tool_arguments)\n        )\n        self.tool_call_id = (\n            _assert_must_pass_tool_call_id()\n            if tool_call_id is _MISSING\n            else cast(str, tool_call_id)\n        )\n        self.tool_call = tool_call\n\n    @classmethod\n    def from_agent_context(\n        cls,\n        context: RunContextWrapper[TContext],\n        tool_call_id: str,\n        tool_call: ResponseFunctionToolCall | None = None,\n    ) -&gt; ToolContext:\n        \"\"\"\n        Create a ToolContext from a RunContextWrapper.\n        \"\"\"\n        # Grab the names of the RunContextWrapper's init=True fields\n        base_values: dict[str, Any] = {\n            f.name: getattr(context, f.name) for f in fields(RunContextWrapper) if f.init\n        }\n        tool_name = tool_call.name if tool_call is not None else _assert_must_pass_tool_name()\n        tool_args = (\n            tool_call.arguments if tool_call is not None else _assert_must_pass_tool_arguments()\n        )\n\n        tool_context = cls(\n            tool_name=tool_name,\n            tool_call_id=tool_call_id,\n            tool_arguments=tool_args,\n            tool_call=tool_call,\n            **base_values,\n        )\n        return tool_context\n</code></pre>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.tool_name","title":"tool_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_name: str = (\n    _assert_must_pass_tool_name()\n    if tool_name is _MISSING\n    else cast(str, tool_name)\n)\n</code></pre> <p>The name of the tool being invoked.</p>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.tool_arguments","title":"tool_arguments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_arguments: str = (\n    _assert_must_pass_tool_arguments()\n    if tool_arguments is _MISSING\n    else cast(str, tool_arguments)\n)\n</code></pre> <p>The raw arguments string of the tool call.</p>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.tool_call_id","title":"tool_call_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_call_id: str = (\n    _assert_must_pass_tool_call_id()\n    if tool_call_id is _MISSING\n    else cast(str, tool_call_id)\n)\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.tool_call","title":"tool_call  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_call: ResponseFunctionToolCall | None = tool_call\n</code></pre> <p>The tool call object associated with this invocation.</p>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: TContext\n</code></pre> <p>The context object (or None), passed by you to <code>Runner.run()</code></p>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Usage = field(default_factory=Usage)\n</code></pre> <p>The usage of the agent run so far. For streamed responses, the usage will be stale until the last chunk of the stream is processed.</p>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.tool_input","title":"tool_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_input: Any | None = None\n</code></pre> <p>Structured input for the current agent tool run, when available.</p>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.__init__","title":"__init__","text":"<pre><code>__init__(\n    context: TContext,\n    usage: Usage | object = _MISSING,\n    tool_name: str | object = _MISSING,\n    tool_call_id: str | object = _MISSING,\n    tool_arguments: str | object = _MISSING,\n    tool_call: ResponseFunctionToolCall | None = None,\n    *,\n    turn_input: list[TResponseInputItem] | None = None,\n    _approvals: dict[str, _ApprovalRecord] | None = None,\n    tool_input: Any | None = None,\n) -&gt; None\n</code></pre> <p>Preserve the v0.7 positional constructor while accepting new context fields.</p> Source code in <code>src/agents/tool_context.py</code> <pre><code>def __init__(\n    self,\n    context: TContext,\n    usage: Usage | object = _MISSING,\n    tool_name: str | object = _MISSING,\n    tool_call_id: str | object = _MISSING,\n    tool_arguments: str | object = _MISSING,\n    tool_call: ResponseFunctionToolCall | None = None,\n    *,\n    turn_input: list[TResponseInputItem] | None = None,\n    _approvals: dict[str, _ApprovalRecord] | None = None,\n    tool_input: Any | None = None,\n) -&gt; None:\n    \"\"\"Preserve the v0.7 positional constructor while accepting new context fields.\"\"\"\n    resolved_usage = Usage() if usage is _MISSING else cast(Usage, usage)\n    super().__init__(\n        context=context,\n        usage=resolved_usage,\n        turn_input=list(turn_input or []),\n        _approvals={} if _approvals is None else _approvals,\n        tool_input=tool_input,\n    )\n    self.tool_name = (\n        _assert_must_pass_tool_name() if tool_name is _MISSING else cast(str, tool_name)\n    )\n    self.tool_arguments = (\n        _assert_must_pass_tool_arguments()\n        if tool_arguments is _MISSING\n        else cast(str, tool_arguments)\n    )\n    self.tool_call_id = (\n        _assert_must_pass_tool_call_id()\n        if tool_call_id is _MISSING\n        else cast(str, tool_call_id)\n    )\n    self.tool_call = tool_call\n</code></pre>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.from_agent_context","title":"from_agent_context  <code>classmethod</code>","text":"<pre><code>from_agent_context(\n    context: RunContextWrapper[TContext],\n    tool_call_id: str,\n    tool_call: ResponseFunctionToolCall | None = None,\n) -&gt; ToolContext\n</code></pre> <p>Create a ToolContext from a RunContextWrapper.</p> Source code in <code>src/agents/tool_context.py</code> <pre><code>@classmethod\ndef from_agent_context(\n    cls,\n    context: RunContextWrapper[TContext],\n    tool_call_id: str,\n    tool_call: ResponseFunctionToolCall | None = None,\n) -&gt; ToolContext:\n    \"\"\"\n    Create a ToolContext from a RunContextWrapper.\n    \"\"\"\n    # Grab the names of the RunContextWrapper's init=True fields\n    base_values: dict[str, Any] = {\n        f.name: getattr(context, f.name) for f in fields(RunContextWrapper) if f.init\n    }\n    tool_name = tool_call.name if tool_call is not None else _assert_must_pass_tool_name()\n    tool_args = (\n        tool_call.arguments if tool_call is not None else _assert_must_pass_tool_arguments()\n    )\n\n    tool_context = cls(\n        tool_name=tool_name,\n        tool_call_id=tool_call_id,\n        tool_arguments=tool_args,\n        tool_call=tool_call,\n        **base_values,\n    )\n    return tool_context\n</code></pre>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.is_tool_approved","title":"is_tool_approved","text":"<pre><code>is_tool_approved(\n    tool_name: str, call_id: str\n) -&gt; bool | None\n</code></pre> <p>Return True/False/None for the given tool call.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def is_tool_approved(self, tool_name: str, call_id: str) -&gt; bool | None:\n    \"\"\"Return True/False/None for the given tool call.\"\"\"\n    approval_entry = self._approvals.get(tool_name)\n    if not approval_entry:\n        return None\n\n    # Check for permanent approval/rejection\n    if approval_entry.approved is True and approval_entry.rejected is True:\n        # Approval takes precedence\n        return True\n\n    if approval_entry.approved is True:\n        return True\n\n    if approval_entry.rejected is True:\n        return False\n\n    approved_ids = (\n        set(approval_entry.approved) if isinstance(approval_entry.approved, list) else set()\n    )\n    rejected_ids = (\n        set(approval_entry.rejected) if isinstance(approval_entry.rejected, list) else set()\n    )\n\n    if call_id in approved_ids:\n        return True\n    if call_id in rejected_ids:\n        return False\n    # Per-call approvals are scoped to the exact call ID, so other calls require a new decision.\n    return None\n</code></pre>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.approve_tool","title":"approve_tool","text":"<pre><code>approve_tool(\n    approval_item: ToolApprovalItem,\n    always_approve: bool = False,\n) -&gt; None\n</code></pre> <p>Approve a tool call, optionally for all future calls.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def approve_tool(self, approval_item: ToolApprovalItem, always_approve: bool = False) -&gt; None:\n    \"\"\"Approve a tool call, optionally for all future calls.\"\"\"\n    self._apply_approval_decision(\n        approval_item,\n        always=always_approve,\n        approve=True,\n    )\n</code></pre>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.reject_tool","title":"reject_tool","text":"<pre><code>reject_tool(\n    approval_item: ToolApprovalItem,\n    always_reject: bool = False,\n) -&gt; None\n</code></pre> <p>Reject a tool call, optionally for all future calls.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def reject_tool(self, approval_item: ToolApprovalItem, always_reject: bool = False) -&gt; None:\n    \"\"\"Reject a tool call, optionally for all future calls.\"\"\"\n    self._apply_approval_decision(\n        approval_item,\n        always=always_reject,\n        approve=False,\n    )\n</code></pre>"},{"location":"ref/tool_context/#agents.tool_context.ToolContext.get_approval_status","title":"get_approval_status","text":"<pre><code>get_approval_status(\n    tool_name: str,\n    call_id: str,\n    *,\n    existing_pending: ToolApprovalItem | None = None,\n) -&gt; bool | None\n</code></pre> <p>Return approval status, retrying with pending item's tool name if necessary.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>def get_approval_status(\n    self, tool_name: str, call_id: str, *, existing_pending: ToolApprovalItem | None = None\n) -&gt; bool | None:\n    \"\"\"Return approval status, retrying with pending item's tool name if necessary.\"\"\"\n    status = self.is_tool_approved(tool_name, call_id)\n    if status is None and existing_pending:\n        fallback_tool_name = self._resolve_tool_name(existing_pending)\n        status = self.is_tool_approved(fallback_tool_name, call_id)\n    return status\n</code></pre>"},{"location":"ref/tool_guardrails/","title":"<code>Tool Guardrails</code>","text":""},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrailResult","title":"ToolInputGuardrailResult  <code>dataclass</code>","text":"<p>The result of a tool input guardrail run.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@dataclass\nclass ToolInputGuardrailResult:\n    \"\"\"The result of a tool input guardrail run.\"\"\"\n\n    guardrail: ToolInputGuardrail[Any]\n    \"\"\"The guardrail that was run.\"\"\"\n\n    output: ToolGuardrailFunctionOutput\n    \"\"\"The output of the guardrail function.\"\"\"\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrailResult.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: ToolInputGuardrail[Any]\n</code></pre> <p>The guardrail that was run.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrailResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: ToolGuardrailFunctionOutput\n</code></pre> <p>The output of the guardrail function.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrailResult","title":"ToolOutputGuardrailResult  <code>dataclass</code>","text":"<p>The result of a tool output guardrail run.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@dataclass\nclass ToolOutputGuardrailResult:\n    \"\"\"The result of a tool output guardrail run.\"\"\"\n\n    guardrail: ToolOutputGuardrail[Any]\n    \"\"\"The guardrail that was run.\"\"\"\n\n    output: ToolGuardrailFunctionOutput\n    \"\"\"The output of the guardrail function.\"\"\"\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrailResult.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: ToolOutputGuardrail[Any]\n</code></pre> <p>The guardrail that was run.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrailResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: ToolGuardrailFunctionOutput\n</code></pre> <p>The output of the guardrail function.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.RejectContentBehavior","title":"RejectContentBehavior","text":"<p>               Bases: <code>TypedDict</code></p> <p>Rejects the tool call/output but continues execution with a message to the model.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>class RejectContentBehavior(TypedDict):\n    \"\"\"Rejects the tool call/output but continues execution with a message to the model.\"\"\"\n\n    type: Literal[\"reject_content\"]\n    message: str\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.RaiseExceptionBehavior","title":"RaiseExceptionBehavior","text":"<p>               Bases: <code>TypedDict</code></p> <p>Raises an exception to halt execution.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>class RaiseExceptionBehavior(TypedDict):\n    \"\"\"Raises an exception to halt execution.\"\"\"\n\n    type: Literal[\"raise_exception\"]\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.AllowBehavior","title":"AllowBehavior","text":"<p>               Bases: <code>TypedDict</code></p> <p>Allows normal tool execution to continue.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>class AllowBehavior(TypedDict):\n    \"\"\"Allows normal tool execution to continue.\"\"\"\n\n    type: Literal[\"allow\"]\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolGuardrailFunctionOutput","title":"ToolGuardrailFunctionOutput  <code>dataclass</code>","text":"<p>The output of a tool guardrail function.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@dataclass\nclass ToolGuardrailFunctionOutput:\n    \"\"\"The output of a tool guardrail function.\"\"\"\n\n    output_info: Any\n    \"\"\"\n    Optional data about checks performed. For example, the guardrail could include\n    information about the checks it performed and granular results.\n    \"\"\"\n\n    behavior: RejectContentBehavior | RaiseExceptionBehavior | AllowBehavior = field(\n        default_factory=lambda: AllowBehavior(type=\"allow\")\n    )\n    \"\"\"\n    Defines how the system should respond when this guardrail result is processed.\n    - allow: Allow normal tool execution to continue without interference (default)\n    - reject_content: Reject the tool call/output but continue execution with a message to the model\n    - raise_exception: Halt execution by raising a ToolGuardrailTripwireTriggered exception\n    \"\"\"\n\n    @classmethod\n    def allow(cls, output_info: Any = None) -&gt; ToolGuardrailFunctionOutput:\n        \"\"\"Create a guardrail output that allows the tool execution to continue normally.\n\n        Args:\n            output_info: Optional data about checks performed.\n\n        Returns:\n            ToolGuardrailFunctionOutput configured to allow normal execution.\n        \"\"\"\n        return cls(output_info=output_info, behavior=AllowBehavior(type=\"allow\"))\n\n    @classmethod\n    def reject_content(cls, message: str, output_info: Any = None) -&gt; ToolGuardrailFunctionOutput:\n        \"\"\"Create a guardrail output that rejects the tool call/output but continues execution.\n\n        Args:\n            message: Message to send to the model instead of the tool result.\n            output_info: Optional data about checks performed.\n\n        Returns:\n            ToolGuardrailFunctionOutput configured to reject the content.\n        \"\"\"\n        return cls(\n            output_info=output_info,\n            behavior=RejectContentBehavior(type=\"reject_content\", message=message),\n        )\n\n    @classmethod\n    def raise_exception(cls, output_info: Any = None) -&gt; ToolGuardrailFunctionOutput:\n        \"\"\"Create a guardrail output that raises an exception to halt execution.\n\n        Args:\n            output_info: Optional data about checks performed.\n\n        Returns:\n            ToolGuardrailFunctionOutput configured to raise an exception.\n        \"\"\"\n        return cls(output_info=output_info, behavior=RaiseExceptionBehavior(type=\"raise_exception\"))\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolGuardrailFunctionOutput.output_info","title":"output_info  <code>instance-attribute</code>","text":"<pre><code>output_info: Any\n</code></pre> <p>Optional data about checks performed. For example, the guardrail could include information about the checks it performed and granular results.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolGuardrailFunctionOutput.behavior","title":"behavior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>behavior: (\n    RejectContentBehavior\n    | RaiseExceptionBehavior\n    | AllowBehavior\n) = field(\n    default_factory=lambda: AllowBehavior(type=\"allow\")\n)\n</code></pre> <p>Defines how the system should respond when this guardrail result is processed. - allow: Allow normal tool execution to continue without interference (default) - reject_content: Reject the tool call/output but continue execution with a message to the model - raise_exception: Halt execution by raising a ToolGuardrailTripwireTriggered exception</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolGuardrailFunctionOutput.allow","title":"allow  <code>classmethod</code>","text":"<pre><code>allow(\n    output_info: Any = None,\n) -&gt; ToolGuardrailFunctionOutput\n</code></pre> <p>Create a guardrail output that allows the tool execution to continue normally.</p> <p>Parameters:</p> Name Type Description Default <code>output_info</code> <code>Any</code> <p>Optional data about checks performed.</p> <code>None</code> <p>Returns:</p> Type Description <code>ToolGuardrailFunctionOutput</code> <p>ToolGuardrailFunctionOutput configured to allow normal execution.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@classmethod\ndef allow(cls, output_info: Any = None) -&gt; ToolGuardrailFunctionOutput:\n    \"\"\"Create a guardrail output that allows the tool execution to continue normally.\n\n    Args:\n        output_info: Optional data about checks performed.\n\n    Returns:\n        ToolGuardrailFunctionOutput configured to allow normal execution.\n    \"\"\"\n    return cls(output_info=output_info, behavior=AllowBehavior(type=\"allow\"))\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolGuardrailFunctionOutput.reject_content","title":"reject_content  <code>classmethod</code>","text":"<pre><code>reject_content(\n    message: str, output_info: Any = None\n) -&gt; ToolGuardrailFunctionOutput\n</code></pre> <p>Create a guardrail output that rejects the tool call/output but continues execution.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to send to the model instead of the tool result.</p> required <code>output_info</code> <code>Any</code> <p>Optional data about checks performed.</p> <code>None</code> <p>Returns:</p> Type Description <code>ToolGuardrailFunctionOutput</code> <p>ToolGuardrailFunctionOutput configured to reject the content.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@classmethod\ndef reject_content(cls, message: str, output_info: Any = None) -&gt; ToolGuardrailFunctionOutput:\n    \"\"\"Create a guardrail output that rejects the tool call/output but continues execution.\n\n    Args:\n        message: Message to send to the model instead of the tool result.\n        output_info: Optional data about checks performed.\n\n    Returns:\n        ToolGuardrailFunctionOutput configured to reject the content.\n    \"\"\"\n    return cls(\n        output_info=output_info,\n        behavior=RejectContentBehavior(type=\"reject_content\", message=message),\n    )\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolGuardrailFunctionOutput.raise_exception","title":"raise_exception  <code>classmethod</code>","text":"<pre><code>raise_exception(\n    output_info: Any = None,\n) -&gt; ToolGuardrailFunctionOutput\n</code></pre> <p>Create a guardrail output that raises an exception to halt execution.</p> <p>Parameters:</p> Name Type Description Default <code>output_info</code> <code>Any</code> <p>Optional data about checks performed.</p> <code>None</code> <p>Returns:</p> Type Description <code>ToolGuardrailFunctionOutput</code> <p>ToolGuardrailFunctionOutput configured to raise an exception.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@classmethod\ndef raise_exception(cls, output_info: Any = None) -&gt; ToolGuardrailFunctionOutput:\n    \"\"\"Create a guardrail output that raises an exception to halt execution.\n\n    Args:\n        output_info: Optional data about checks performed.\n\n    Returns:\n        ToolGuardrailFunctionOutput configured to raise an exception.\n    \"\"\"\n    return cls(output_info=output_info, behavior=RaiseExceptionBehavior(type=\"raise_exception\"))\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrailData","title":"ToolInputGuardrailData  <code>dataclass</code>","text":"<p>Input data passed to a tool input guardrail function.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@dataclass\nclass ToolInputGuardrailData:\n    \"\"\"Input data passed to a tool input guardrail function.\"\"\"\n\n    context: ToolContext[Any]\n    \"\"\"\n    The tool context containing information about the current tool execution.\n    \"\"\"\n\n    agent: Agent[Any]\n    \"\"\"\n    The agent that is executing the tool.\n    \"\"\"\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrailData.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: ToolContext[Any]\n</code></pre> <p>The tool context containing information about the current tool execution.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrailData.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent that is executing the tool.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrailData","title":"ToolOutputGuardrailData  <code>dataclass</code>","text":"<p>               Bases: <code>ToolInputGuardrailData</code></p> <p>Input data passed to a tool output guardrail function.</p> <p>Extends input data with the tool's output.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@dataclass\nclass ToolOutputGuardrailData(ToolInputGuardrailData):\n    \"\"\"Input data passed to a tool output guardrail function.\n\n    Extends input data with the tool's output.\n    \"\"\"\n\n    output: Any\n    \"\"\"\n    The output produced by the tool function.\n    \"\"\"\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrailData.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: Any\n</code></pre> <p>The output produced by the tool function.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrailData.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: ToolContext[Any]\n</code></pre> <p>The tool context containing information about the current tool execution.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrailData.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent that is executing the tool.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrail","title":"ToolInputGuardrail  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext_co]</code></p> <p>A guardrail that runs before a function tool is invoked.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@dataclass\nclass ToolInputGuardrail(Generic[TContext_co]):\n    \"\"\"A guardrail that runs before a function tool is invoked.\"\"\"\n\n    guardrail_function: Callable[\n        [ToolInputGuardrailData], MaybeAwaitable[ToolGuardrailFunctionOutput]\n    ]\n    \"\"\"\n    The function that implements the guardrail logic.\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"\n    Optional name for the guardrail. If not provided, uses the function name.\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return self.name or self.guardrail_function.__name__\n\n    async def run(self, data: ToolInputGuardrailData) -&gt; ToolGuardrailFunctionOutput:\n        if not callable(self.guardrail_function):\n            raise UserError(f\"Guardrail function must be callable, got {self.guardrail_function}\")\n\n        result = self.guardrail_function(data)\n        if inspect.isawaitable(result):\n            return await result\n        return result\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrail.guardrail_function","title":"guardrail_function  <code>instance-attribute</code>","text":"<pre><code>guardrail_function: Callable[\n    [ToolInputGuardrailData],\n    MaybeAwaitable[ToolGuardrailFunctionOutput],\n]\n</code></pre> <p>The function that implements the guardrail logic.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolInputGuardrail.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre> <p>Optional name for the guardrail. If not provided, uses the function name.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrail","title":"ToolOutputGuardrail  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext_co]</code></p> <p>A guardrail that runs after a function tool is invoked.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>@dataclass\nclass ToolOutputGuardrail(Generic[TContext_co]):\n    \"\"\"A guardrail that runs after a function tool is invoked.\"\"\"\n\n    guardrail_function: Callable[\n        [ToolOutputGuardrailData], MaybeAwaitable[ToolGuardrailFunctionOutput]\n    ]\n    \"\"\"\n    The function that implements the guardrail logic.\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"\n    Optional name for the guardrail. If not provided, uses the function name.\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return self.name or self.guardrail_function.__name__\n\n    async def run(self, data: ToolOutputGuardrailData) -&gt; ToolGuardrailFunctionOutput:\n        if not callable(self.guardrail_function):\n            raise UserError(f\"Guardrail function must be callable, got {self.guardrail_function}\")\n\n        result = self.guardrail_function(data)\n        if inspect.isawaitable(result):\n            return await result\n        return result\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrail.guardrail_function","title":"guardrail_function  <code>instance-attribute</code>","text":"<pre><code>guardrail_function: Callable[\n    [ToolOutputGuardrailData],\n    MaybeAwaitable[ToolGuardrailFunctionOutput],\n]\n</code></pre> <p>The function that implements the guardrail logic.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.ToolOutputGuardrail.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre> <p>Optional name for the guardrail. If not provided, uses the function name.</p>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.tool_input_guardrail","title":"tool_input_guardrail","text":"<pre><code>tool_input_guardrail(func: _ToolInputFuncSync)\n</code></pre><pre><code>tool_input_guardrail(func: _ToolInputFuncAsync)\n</code></pre><pre><code>tool_input_guardrail(\n    *, name: str | None = None\n) -&gt; Callable[\n    [_ToolInputFuncSync | _ToolInputFuncAsync],\n    ToolInputGuardrail[Any],\n]\n</code></pre> <pre><code>tool_input_guardrail(\n    func: _ToolInputFuncSync\n    | _ToolInputFuncAsync\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    ToolInputGuardrail[Any]\n    | Callable[\n        [_ToolInputFuncSync | _ToolInputFuncAsync],\n        ToolInputGuardrail[Any],\n    ]\n)\n</code></pre> <p>Decorator to create a ToolInputGuardrail from a function.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>def tool_input_guardrail(\n    func: _ToolInputFuncSync | _ToolInputFuncAsync | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    ToolInputGuardrail[Any]\n    | Callable[[_ToolInputFuncSync | _ToolInputFuncAsync], ToolInputGuardrail[Any]]\n):\n    \"\"\"Decorator to create a ToolInputGuardrail from a function.\"\"\"\n\n    def decorator(f: _ToolInputFuncSync | _ToolInputFuncAsync) -&gt; ToolInputGuardrail[Any]:\n        return ToolInputGuardrail(guardrail_function=f, name=name or f.__name__)\n\n    if func is not None:\n        return decorator(func)\n    return decorator\n</code></pre>"},{"location":"ref/tool_guardrails/#agents.tool_guardrails.tool_output_guardrail","title":"tool_output_guardrail","text":"<pre><code>tool_output_guardrail(func: _ToolOutputFuncSync)\n</code></pre><pre><code>tool_output_guardrail(func: _ToolOutputFuncAsync)\n</code></pre><pre><code>tool_output_guardrail(\n    *, name: str | None = None\n) -&gt; Callable[\n    [_ToolOutputFuncSync | _ToolOutputFuncAsync],\n    ToolOutputGuardrail[Any],\n]\n</code></pre> <pre><code>tool_output_guardrail(\n    func: _ToolOutputFuncSync\n    | _ToolOutputFuncAsync\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    ToolOutputGuardrail[Any]\n    | Callable[\n        [_ToolOutputFuncSync | _ToolOutputFuncAsync],\n        ToolOutputGuardrail[Any],\n    ]\n)\n</code></pre> <p>Decorator to create a ToolOutputGuardrail from a function.</p> Source code in <code>src/agents/tool_guardrails.py</code> <pre><code>def tool_output_guardrail(\n    func: _ToolOutputFuncSync | _ToolOutputFuncAsync | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    ToolOutputGuardrail[Any]\n    | Callable[[_ToolOutputFuncSync | _ToolOutputFuncAsync], ToolOutputGuardrail[Any]]\n):\n    \"\"\"Decorator to create a ToolOutputGuardrail from a function.\"\"\"\n\n    def decorator(f: _ToolOutputFuncSync | _ToolOutputFuncAsync) -&gt; ToolOutputGuardrail[Any]:\n        return ToolOutputGuardrail(guardrail_function=f, name=name or f.__name__)\n\n    if func is not None:\n        return decorator(func)\n    return decorator\n</code></pre>"},{"location":"ref/usage/","title":"<code>Usage</code>","text":""},{"location":"ref/usage/#agents.usage.RequestUsage","title":"RequestUsage","text":"<p>Usage details for a single API request.</p> Source code in <code>src/agents/usage.py</code> <pre><code>@dataclass\nclass RequestUsage:\n    \"\"\"Usage details for a single API request.\"\"\"\n\n    input_tokens: int\n    \"\"\"Input tokens for this individual request.\"\"\"\n\n    output_tokens: int\n    \"\"\"Output tokens for this individual request.\"\"\"\n\n    total_tokens: int\n    \"\"\"Total tokens (input + output) for this individual request.\"\"\"\n\n    input_tokens_details: InputTokensDetails\n    \"\"\"Details about the input tokens for this individual request.\"\"\"\n\n    output_tokens_details: OutputTokensDetails\n    \"\"\"Details about the output tokens for this individual request.\"\"\"\n</code></pre>"},{"location":"ref/usage/#agents.usage.RequestUsage.input_tokens","title":"input_tokens  <code>instance-attribute</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Input tokens for this individual request.</p>"},{"location":"ref/usage/#agents.usage.RequestUsage.output_tokens","title":"output_tokens  <code>instance-attribute</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Output tokens for this individual request.</p>"},{"location":"ref/usage/#agents.usage.RequestUsage.total_tokens","title":"total_tokens  <code>instance-attribute</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Total tokens (input + output) for this individual request.</p>"},{"location":"ref/usage/#agents.usage.RequestUsage.input_tokens_details","title":"input_tokens_details  <code>instance-attribute</code>","text":"<pre><code>input_tokens_details: InputTokensDetails\n</code></pre> <p>Details about the input tokens for this individual request.</p>"},{"location":"ref/usage/#agents.usage.RequestUsage.output_tokens_details","title":"output_tokens_details  <code>instance-attribute</code>","text":"<pre><code>output_tokens_details: OutputTokensDetails\n</code></pre> <p>Details about the output tokens for this individual request.</p>"},{"location":"ref/usage/#agents.usage.Usage","title":"Usage","text":"Source code in <code>src/agents/usage.py</code> <pre><code>@dataclass\nclass Usage:\n    requests: int = 0\n    \"\"\"Total requests made to the LLM API.\"\"\"\n\n    input_tokens: int = 0\n    \"\"\"Total input tokens sent, across all requests.\"\"\"\n\n    input_tokens_details: Annotated[\n        InputTokensDetails, BeforeValidator(_normalize_input_tokens_details)\n    ] = field(default_factory=lambda: InputTokensDetails(cached_tokens=0))\n    \"\"\"Details about the input tokens, matching responses API usage details.\"\"\"\n    output_tokens: int = 0\n    \"\"\"Total output tokens received, across all requests.\"\"\"\n\n    output_tokens_details: Annotated[\n        OutputTokensDetails, BeforeValidator(_normalize_output_tokens_details)\n    ] = field(default_factory=lambda: OutputTokensDetails(reasoning_tokens=0))\n    \"\"\"Details about the output tokens, matching responses API usage details.\"\"\"\n\n    total_tokens: int = 0\n    \"\"\"Total tokens sent and received, across all requests.\"\"\"\n\n    request_usage_entries: list[RequestUsage] = field(default_factory=list)\n    \"\"\"List of RequestUsage entries for accurate per-request cost calculation.\n\n    Each call to `add()` automatically creates an entry in this list if the added usage\n    represents a new request (i.e., has non-zero tokens).\n\n    Example:\n        For a run that makes 3 API calls with 100K, 150K, and 80K input tokens each,\n        the aggregated `input_tokens` would be 330K, but `request_usage_entries` would\n        preserve the [100K, 150K, 80K] breakdown, which could be helpful for detailed\n        cost calculation or context window management.\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        # Some providers don't populate optional token detail fields\n        # (cached_tokens, reasoning_tokens), and the OpenAI SDK's generated\n        # code can bypass Pydantic validation (e.g., via model_construct),\n        # allowing None values. We normalize these to 0 to prevent TypeErrors.\n        input_details_none = self.input_tokens_details is None\n        input_cached_none = (\n            not input_details_none and self.input_tokens_details.cached_tokens is None\n        )\n        if input_details_none or input_cached_none:\n            self.input_tokens_details = InputTokensDetails(cached_tokens=0)\n\n        output_details_none = self.output_tokens_details is None\n        output_reasoning_none = (\n            not output_details_none and self.output_tokens_details.reasoning_tokens is None\n        )\n        if output_details_none or output_reasoning_none:\n            self.output_tokens_details = OutputTokensDetails(reasoning_tokens=0)\n\n    def add(self, other: Usage) -&gt; None:\n        \"\"\"Add another Usage object to this one, aggregating all fields.\n\n        This method automatically preserves request_usage_entries.\n\n        Args:\n            other: The Usage object to add to this one.\n        \"\"\"\n        self.requests += other.requests if other.requests else 0\n        self.input_tokens += other.input_tokens if other.input_tokens else 0\n        self.output_tokens += other.output_tokens if other.output_tokens else 0\n        self.total_tokens += other.total_tokens if other.total_tokens else 0\n\n        # Null guards for nested token details (other may bypass validation via model_construct)\n        other_cached = (\n            other.input_tokens_details.cached_tokens\n            if other.input_tokens_details and other.input_tokens_details.cached_tokens\n            else 0\n        )\n        other_reasoning = (\n            other.output_tokens_details.reasoning_tokens\n            if other.output_tokens_details and other.output_tokens_details.reasoning_tokens\n            else 0\n        )\n        self_cached = (\n            self.input_tokens_details.cached_tokens\n            if self.input_tokens_details and self.input_tokens_details.cached_tokens\n            else 0\n        )\n        self_reasoning = (\n            self.output_tokens_details.reasoning_tokens\n            if self.output_tokens_details and self.output_tokens_details.reasoning_tokens\n            else 0\n        )\n\n        self.input_tokens_details = InputTokensDetails(cached_tokens=self_cached + other_cached)\n\n        self.output_tokens_details = OutputTokensDetails(\n            reasoning_tokens=self_reasoning + other_reasoning\n        )\n\n        # Automatically preserve request_usage_entries.\n        # If the other Usage represents a single request with tokens, record it.\n        if other.requests == 1 and other.total_tokens &gt; 0:\n            input_details = other.input_tokens_details or InputTokensDetails(cached_tokens=0)\n            output_details = other.output_tokens_details or OutputTokensDetails(reasoning_tokens=0)\n            request_usage = RequestUsage(\n                input_tokens=other.input_tokens,\n                output_tokens=other.output_tokens,\n                total_tokens=other.total_tokens,\n                input_tokens_details=input_details,\n                output_tokens_details=output_details,\n            )\n            self.request_usage_entries.append(request_usage)\n        elif other.request_usage_entries:\n            # If the other Usage already has individual request breakdowns, merge them.\n            self.request_usage_entries.extend(other.request_usage_entries)\n</code></pre>"},{"location":"ref/usage/#agents.usage.Usage.requests","title":"requests  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requests: int = 0\n</code></pre> <p>Total requests made to the LLM API.</p>"},{"location":"ref/usage/#agents.usage.Usage.input_tokens","title":"input_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_tokens: int = 0\n</code></pre> <p>Total input tokens sent, across all requests.</p>"},{"location":"ref/usage/#agents.usage.Usage.input_tokens_details","title":"input_tokens_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_tokens_details: Annotated[\n    InputTokensDetails,\n    BeforeValidator(_normalize_input_tokens_details),\n] = field(\n    default_factory=lambda: InputTokensDetails(\n        cached_tokens=0\n    )\n)\n</code></pre> <p>Details about the input tokens, matching responses API usage details.</p>"},{"location":"ref/usage/#agents.usage.Usage.output_tokens","title":"output_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_tokens: int = 0\n</code></pre> <p>Total output tokens received, across all requests.</p>"},{"location":"ref/usage/#agents.usage.Usage.output_tokens_details","title":"output_tokens_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_tokens_details: Annotated[\n    OutputTokensDetails,\n    BeforeValidator(_normalize_output_tokens_details),\n] = field(\n    default_factory=lambda: OutputTokensDetails(\n        reasoning_tokens=0\n    )\n)\n</code></pre> <p>Details about the output tokens, matching responses API usage details.</p>"},{"location":"ref/usage/#agents.usage.Usage.total_tokens","title":"total_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_tokens: int = 0\n</code></pre> <p>Total tokens sent and received, across all requests.</p>"},{"location":"ref/usage/#agents.usage.Usage.request_usage_entries","title":"request_usage_entries  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>request_usage_entries: list[RequestUsage] = field(\n    default_factory=list\n)\n</code></pre> <p>List of RequestUsage entries for accurate per-request cost calculation.</p> <p>Each call to <code>add()</code> automatically creates an entry in this list if the added usage represents a new request (i.e., has non-zero tokens).</p> Example <p>For a run that makes 3 API calls with 100K, 150K, and 80K input tokens each, the aggregated <code>input_tokens</code> would be 330K, but <code>request_usage_entries</code> would preserve the [100K, 150K, 80K] breakdown, which could be helpful for detailed cost calculation or context window management.</p>"},{"location":"ref/usage/#agents.usage.Usage.add","title":"add","text":"<pre><code>add(other: Usage) -&gt; None\n</code></pre> <p>Add another Usage object to this one, aggregating all fields.</p> <p>This method automatically preserves request_usage_entries.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Usage</code> <p>The Usage object to add to this one.</p> required Source code in <code>src/agents/usage.py</code> <pre><code>def add(self, other: Usage) -&gt; None:\n    \"\"\"Add another Usage object to this one, aggregating all fields.\n\n    This method automatically preserves request_usage_entries.\n\n    Args:\n        other: The Usage object to add to this one.\n    \"\"\"\n    self.requests += other.requests if other.requests else 0\n    self.input_tokens += other.input_tokens if other.input_tokens else 0\n    self.output_tokens += other.output_tokens if other.output_tokens else 0\n    self.total_tokens += other.total_tokens if other.total_tokens else 0\n\n    # Null guards for nested token details (other may bypass validation via model_construct)\n    other_cached = (\n        other.input_tokens_details.cached_tokens\n        if other.input_tokens_details and other.input_tokens_details.cached_tokens\n        else 0\n    )\n    other_reasoning = (\n        other.output_tokens_details.reasoning_tokens\n        if other.output_tokens_details and other.output_tokens_details.reasoning_tokens\n        else 0\n    )\n    self_cached = (\n        self.input_tokens_details.cached_tokens\n        if self.input_tokens_details and self.input_tokens_details.cached_tokens\n        else 0\n    )\n    self_reasoning = (\n        self.output_tokens_details.reasoning_tokens\n        if self.output_tokens_details and self.output_tokens_details.reasoning_tokens\n        else 0\n    )\n\n    self.input_tokens_details = InputTokensDetails(cached_tokens=self_cached + other_cached)\n\n    self.output_tokens_details = OutputTokensDetails(\n        reasoning_tokens=self_reasoning + other_reasoning\n    )\n\n    # Automatically preserve request_usage_entries.\n    # If the other Usage represents a single request with tokens, record it.\n    if other.requests == 1 and other.total_tokens &gt; 0:\n        input_details = other.input_tokens_details or InputTokensDetails(cached_tokens=0)\n        output_details = other.output_tokens_details or OutputTokensDetails(reasoning_tokens=0)\n        request_usage = RequestUsage(\n            input_tokens=other.input_tokens,\n            output_tokens=other.output_tokens,\n            total_tokens=other.total_tokens,\n            input_tokens_details=input_details,\n            output_tokens_details=output_details,\n        )\n        self.request_usage_entries.append(request_usage)\n    elif other.request_usage_entries:\n        # If the other Usage already has individual request breakdowns, merge them.\n        self.request_usage_entries.extend(other.request_usage_entries)\n</code></pre>"},{"location":"ref/usage/#agents.usage.deserialize_usage","title":"deserialize_usage","text":"<pre><code>deserialize_usage(usage_data: Mapping[str, Any]) -&gt; Usage\n</code></pre> <p>Rebuild a Usage object from serialized JSON data.</p> Source code in <code>src/agents/usage.py</code> <pre><code>def deserialize_usage(usage_data: Mapping[str, Any]) -&gt; Usage:\n    \"\"\"Rebuild a Usage object from serialized JSON data.\"\"\"\n    input_tokens_details_raw = usage_data.get(\"input_tokens_details\")\n    output_tokens_details_raw = usage_data.get(\"output_tokens_details\")\n    input_details = _coerce_token_details(\n        TypeAdapter(InputTokensDetails),\n        input_tokens_details_raw or {\"cached_tokens\": 0},\n        InputTokensDetails(cached_tokens=0),\n    )\n    output_details = _coerce_token_details(\n        TypeAdapter(OutputTokensDetails),\n        output_tokens_details_raw or {\"reasoning_tokens\": 0},\n        OutputTokensDetails(reasoning_tokens=0),\n    )\n\n    request_entries: list[RequestUsage] = []\n    request_entries_raw = usage_data.get(\"request_usage_entries\") or []\n    for entry in request_entries_raw:\n        request_entries.append(\n            RequestUsage(\n                input_tokens=entry.get(\"input_tokens\", 0),\n                output_tokens=entry.get(\"output_tokens\", 0),\n                total_tokens=entry.get(\"total_tokens\", 0),\n                input_tokens_details=_coerce_token_details(\n                    TypeAdapter(InputTokensDetails),\n                    entry.get(\"input_tokens_details\") or {\"cached_tokens\": 0},\n                    InputTokensDetails(cached_tokens=0),\n                ),\n                output_tokens_details=_coerce_token_details(\n                    TypeAdapter(OutputTokensDetails),\n                    entry.get(\"output_tokens_details\") or {\"reasoning_tokens\": 0},\n                    OutputTokensDetails(reasoning_tokens=0),\n                ),\n            )\n        )\n\n    return Usage(\n        requests=usage_data.get(\"requests\", 0),\n        input_tokens=usage_data.get(\"input_tokens\", 0),\n        output_tokens=usage_data.get(\"output_tokens\", 0),\n        total_tokens=usage_data.get(\"total_tokens\", 0),\n        input_tokens_details=input_details,\n        output_tokens_details=output_details,\n        request_usage_entries=request_entries,\n    )\n</code></pre>"},{"location":"ref/usage/#agents.usage.serialize_usage","title":"serialize_usage","text":"<pre><code>serialize_usage(usage: Usage) -&gt; dict[str, Any]\n</code></pre> <p>Serialize a Usage object into a JSON-friendly dictionary.</p> Source code in <code>src/agents/usage.py</code> <pre><code>def serialize_usage(usage: Usage) -&gt; dict[str, Any]:\n    \"\"\"Serialize a Usage object into a JSON-friendly dictionary.\"\"\"\n    input_details = _serialize_usage_details(usage.input_tokens_details, {\"cached_tokens\": 0})\n    output_details = _serialize_usage_details(usage.output_tokens_details, {\"reasoning_tokens\": 0})\n\n    def _serialize_request_entry(entry: RequestUsage) -&gt; dict[str, Any]:\n        return {\n            \"input_tokens\": entry.input_tokens,\n            \"output_tokens\": entry.output_tokens,\n            \"total_tokens\": entry.total_tokens,\n            \"input_tokens_details\": _serialize_usage_details(\n                entry.input_tokens_details, {\"cached_tokens\": 0}\n            ),\n            \"output_tokens_details\": _serialize_usage_details(\n                entry.output_tokens_details, {\"reasoning_tokens\": 0}\n            ),\n        }\n\n    return {\n        \"requests\": usage.requests,\n        \"input_tokens\": usage.input_tokens,\n        \"input_tokens_details\": [input_details],\n        \"output_tokens\": usage.output_tokens,\n        \"output_tokens_details\": [output_details],\n        \"total_tokens\": usage.total_tokens,\n        \"request_usage_entries\": [\n            _serialize_request_entry(entry) for entry in usage.request_usage_entries\n        ],\n    }\n</code></pre>"},{"location":"ref/version/","title":"<code>Version</code>","text":""},{"location":"ref/extensions/handoff_filters/","title":"<code>Handoff filters</code>","text":""},{"location":"ref/extensions/handoff_filters/#agents.extensions.handoff_filters.default_handoff_history_mapper","title":"default_handoff_history_mapper","text":"<pre><code>default_handoff_history_mapper(\n    transcript: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Return a single assistant message summarizing the transcript.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def default_handoff_history_mapper(\n    transcript: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Return a single assistant message summarizing the transcript.\"\"\"\n\n    summary_message = _build_summary_message(transcript)\n    return [summary_message]\n</code></pre>"},{"location":"ref/extensions/handoff_filters/#agents.extensions.handoff_filters.nest_handoff_history","title":"nest_handoff_history","text":"<pre><code>nest_handoff_history(\n    handoff_input_data: HandoffInputData,\n    *,\n    history_mapper: HandoffHistoryMapper | None = None,\n) -&gt; HandoffInputData\n</code></pre> <p>Summarize the previous transcript for the next agent.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def nest_handoff_history(\n    handoff_input_data: HandoffInputData,\n    *,\n    history_mapper: HandoffHistoryMapper | None = None,\n) -&gt; HandoffInputData:\n    \"\"\"Summarize the previous transcript for the next agent.\"\"\"\n\n    normalized_history = _normalize_input_history(handoff_input_data.input_history)\n    flattened_history = _flatten_nested_history_messages(normalized_history)\n\n    # Convert items to plain inputs for the transcript summary.\n    pre_items_as_inputs: list[TResponseInputItem] = []\n    filtered_pre_items: list[RunItem] = []\n    for run_item in handoff_input_data.pre_handoff_items:\n        if isinstance(run_item, ToolApprovalItem):\n            continue\n        plain_input = _run_item_to_plain_input(run_item)\n        pre_items_as_inputs.append(plain_input)\n        if _should_forward_pre_item(plain_input):\n            filtered_pre_items.append(run_item)\n\n    new_items_as_inputs: list[TResponseInputItem] = []\n    filtered_input_items: list[RunItem] = []\n    for run_item in handoff_input_data.new_items:\n        if isinstance(run_item, ToolApprovalItem):\n            continue\n        plain_input = _run_item_to_plain_input(run_item)\n        new_items_as_inputs.append(plain_input)\n        if _should_forward_new_item(plain_input):\n            filtered_input_items.append(run_item)\n\n    transcript = flattened_history + pre_items_as_inputs + new_items_as_inputs\n\n    mapper = history_mapper or default_handoff_history_mapper\n    history_items = mapper(transcript)\n\n    return handoff_input_data.clone(\n        input_history=tuple(deepcopy(item) for item in history_items),\n        pre_handoff_items=tuple(filtered_pre_items),\n        # new_items stays unchanged for session history.\n        input_items=tuple(filtered_input_items),\n    )\n</code></pre>"},{"location":"ref/extensions/handoff_filters/#agents.extensions.handoff_filters.remove_all_tools","title":"remove_all_tools","text":"<pre><code>remove_all_tools(\n    handoff_input_data: HandoffInputData,\n) -&gt; HandoffInputData\n</code></pre> <p>Filters out all tool items: file search, web search and function calls+output.</p> Source code in <code>src/agents/extensions/handoff_filters.py</code> <pre><code>def remove_all_tools(handoff_input_data: HandoffInputData) -&gt; HandoffInputData:\n    \"\"\"Filters out all tool items: file search, web search and function calls+output.\"\"\"\n\n    history = handoff_input_data.input_history\n    new_items = handoff_input_data.new_items\n\n    filtered_history = (\n        _remove_tool_types_from_input(history) if isinstance(history, tuple) else history\n    )\n    filtered_pre_handoff_items = _remove_tools_from_items(handoff_input_data.pre_handoff_items)\n    filtered_new_items = _remove_tools_from_items(new_items)\n\n    return HandoffInputData(\n        input_history=filtered_history,\n        pre_handoff_items=filtered_pre_handoff_items,\n        new_items=filtered_new_items,\n        run_context=handoff_input_data.run_context,\n    )\n</code></pre>"},{"location":"ref/extensions/handoff_prompt/","title":"<code>Handoff prompt</code>","text":""},{"location":"ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX","title":"RECOMMENDED_PROMPT_PREFIX  <code>module-attribute</code>","text":"<pre><code>RECOMMENDED_PROMPT_PREFIX = \"# System context\\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_&lt;agent_name&gt;`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\\n\"\n</code></pre>"},{"location":"ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.prompt_with_handoff_instructions","title":"prompt_with_handoff_instructions","text":"<pre><code>prompt_with_handoff_instructions(prompt: str) -&gt; str\n</code></pre> <p>Add recommended instructions to the prompt for agents that use handoffs.</p> Source code in <code>src/agents/extensions/handoff_prompt.py</code> <pre><code>def prompt_with_handoff_instructions(prompt: str) -&gt; str:\n    \"\"\"\n    Add recommended instructions to the prompt for agents that use handoffs.\n    \"\"\"\n    return f\"{RECOMMENDED_PROMPT_PREFIX}\\n\\n{prompt}\"\n</code></pre>"},{"location":"ref/extensions/litellm/","title":"<code>LiteLLM Models</code>","text":""},{"location":"ref/extensions/litellm/#agents.extensions.models.litellm_model.InternalChatCompletionMessage","title":"InternalChatCompletionMessage","text":"<p>               Bases: <code>ChatCompletionMessage</code></p> <p>An internal subclass to carry reasoning_content and thinking_blocks without modifying the original model.</p> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class InternalChatCompletionMessage(ChatCompletionMessage):\n    \"\"\"\n    An internal subclass to carry reasoning_content and thinking_blocks without modifying the original model.\n    \"\"\"  # noqa: E501\n\n    reasoning_content: str\n    thinking_blocks: list[dict[str, Any]] | None = None\n</code></pre>"},{"location":"ref/extensions/litellm/#agents.extensions.models.litellm_model.InternalToolCall","title":"InternalToolCall","text":"<p>               Bases: <code>ChatCompletionMessageFunctionToolCall</code></p> <p>An internal subclass to carry provider-specific metadata (e.g., Gemini thought signatures) without modifying the original model.</p> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class InternalToolCall(ChatCompletionMessageFunctionToolCall):\n    \"\"\"\n    An internal subclass to carry provider-specific metadata (e.g., Gemini thought signatures)\n    without modifying the original model.\n    \"\"\"\n\n    extra_content: dict[str, Any] | None = None\n</code></pre>"},{"location":"ref/extensions/litellm/#agents.extensions.models.litellm_model.LitellmModel","title":"LitellmModel","text":"<p>               Bases: <code>Model</code></p> <p>This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI, Anthropic, Gemini, Mistral, and many other models. See supported models here: litellm models.</p> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class LitellmModel(Model):\n    \"\"\"This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI,\n    Anthropic, Gemini, Mistral, and many other models.\n    See supported models here: [litellm models](https://docs.litellm.ai/docs/providers).\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        base_url: str | None = None,\n        api_key: str | None = None,\n    ):\n        self.model = model\n        self.base_url = base_url\n        self.api_key = api_key\n\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,  # unused\n        conversation_id: str | None = None,  # unused\n        prompt: Any | None = None,\n    ) -&gt; ModelResponse:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict()\n            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=False,\n                prompt=prompt,\n            )\n\n            message: litellm.types.utils.Message | None = None\n            first_choice: litellm.types.utils.Choices | None = None\n            if response.choices and len(response.choices) &gt; 0:\n                choice = response.choices[0]\n                if isinstance(choice, litellm.types.utils.Choices):\n                    first_choice = choice\n                    message = first_choice.message\n\n            if _debug.DONT_LOG_MODEL_DATA:\n                logger.debug(\"Received model response\")\n            else:\n                if message is not None:\n                    logger.debug(\n                        f\"\"\"LLM resp:\\n{\n                            json.dumps(message.model_dump(), indent=2, ensure_ascii=False)\n                        }\\n\"\"\"\n                    )\n                else:\n                    finish_reason = first_choice.finish_reason if first_choice else \"-\"\n                    logger.debug(f\"LLM resp had no message. finish_reason: {finish_reason}\")\n\n            if hasattr(response, \"usage\"):\n                response_usage = response.usage\n                usage = (\n                    Usage(\n                        requests=1,\n                        input_tokens=response_usage.prompt_tokens,\n                        output_tokens=response_usage.completion_tokens,\n                        total_tokens=response_usage.total_tokens,\n                        input_tokens_details=InputTokensDetails(\n                            cached_tokens=getattr(\n                                response_usage.prompt_tokens_details, \"cached_tokens\", 0\n                            )\n                            or 0\n                        ),\n                        output_tokens_details=OutputTokensDetails(\n                            reasoning_tokens=getattr(\n                                response_usage.completion_tokens_details, \"reasoning_tokens\", 0\n                            )\n                            or 0\n                        ),\n                    )\n                    if response.usage\n                    else Usage()\n                )\n            else:\n                usage = Usage()\n                logger.warning(\"No usage information returned from Litellm\")\n\n            if tracing.include_data():\n                span_generation.span_data.output = (\n                    [message.model_dump()] if message is not None else []\n                )\n            span_generation.span_data.usage = {\n                \"requests\": usage.requests,\n                \"input_tokens\": usage.input_tokens,\n                \"output_tokens\": usage.output_tokens,\n                \"total_tokens\": usage.total_tokens,\n                \"input_tokens_details\": usage.input_tokens_details.model_dump(),\n                \"output_tokens_details\": usage.output_tokens_details.model_dump(),\n            }\n\n            # Build provider_data for provider specific fields\n            provider_data: dict[str, Any] = {\"model\": self.model}\n            if message is not None and hasattr(response, \"id\"):\n                provider_data[\"response_id\"] = response.id\n\n            items = (\n                Converter.message_to_output_items(\n                    LitellmConverter.convert_message_to_openai(message, model=self.model),\n                    provider_data=provider_data,\n                )\n                if message is not None\n                else []\n            )\n\n            return ModelResponse(\n                output=items,\n                usage=usage,\n                response_id=None,\n            )\n\n    async def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,  # unused\n        conversation_id: str | None = None,  # unused\n        prompt: Any | None = None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict()\n            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response, stream = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=True,\n                prompt=prompt,\n            )\n\n            final_response: Response | None = None\n            async for chunk in ChatCmplStreamHandler.handle_stream(\n                response, stream, model=self.model\n            ):\n                yield chunk\n\n                if chunk.type == \"response.completed\":\n                    final_response = chunk.response\n\n            if tracing.include_data() and final_response:\n                span_generation.span_data.output = [final_response.model_dump()]\n\n            if final_response and final_response.usage:\n                span_generation.span_data.usage = {\n                    \"requests\": 1,\n                    \"input_tokens\": final_response.usage.input_tokens,\n                    \"output_tokens\": final_response.usage.output_tokens,\n                    \"total_tokens\": final_response.usage.total_tokens,\n                    \"input_tokens_details\": (\n                        final_response.usage.input_tokens_details.model_dump()\n                        if final_response.usage.input_tokens_details\n                        else {\"cached_tokens\": 0}\n                    ),\n                    \"output_tokens_details\": (\n                        final_response.usage.output_tokens_details.model_dump()\n                        if final_response.usage.output_tokens_details\n                        else {\"reasoning_tokens\": 0}\n                    ),\n                }\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[True],\n        prompt: Any | None = None,\n    ) -&gt; tuple[Response, AsyncStream[ChatCompletionChunk]]: ...\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[False],\n        prompt: Any | None = None,\n    ) -&gt; litellm.types.utils.ModelResponse: ...\n\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: bool = False,\n        prompt: Any | None = None,\n    ) -&gt; litellm.types.utils.ModelResponse | tuple[Response, AsyncStream[ChatCompletionChunk]]:\n        # Preserve reasoning messages for tool calls when reasoning is on\n        # This is needed for models like Claude 4 Sonnet/Opus which support interleaved thinking\n        preserve_thinking_blocks = (\n            model_settings.reasoning is not None and model_settings.reasoning.effort is not None\n        )\n\n        converted_messages = Converter.items_to_messages(\n            input,\n            preserve_thinking_blocks=preserve_thinking_blocks,\n            preserve_tool_output_all_content=True,\n            model=self.model,\n        )\n\n        # Fix message ordering: reorder to ensure tool_use comes before tool_result.\n        # Required for Anthropic and Vertex AI Gemini APIs which reject tool responses without preceding tool calls.  # noqa: E501\n        if any(model.lower() in self.model.lower() for model in [\"anthropic\", \"claude\", \"gemini\"]):\n            converted_messages = self._fix_tool_message_ordering(converted_messages)\n\n        # Convert Google's extra_content to litellm's provider_specific_fields format\n        if \"gemini\" in self.model.lower():\n            converted_messages = self._convert_gemini_extra_content_to_provider_specific_fields(\n                converted_messages\n            )\n\n        if system_instructions:\n            converted_messages.insert(\n                0,\n                {\n                    \"content\": system_instructions,\n                    \"role\": \"system\",\n                },\n            )\n        converted_messages = _to_dump_compatible(converted_messages)\n\n        if tracing.include_data():\n            span.span_data.input = converted_messages\n\n        parallel_tool_calls = (\n            True\n            if model_settings.parallel_tool_calls and tools and len(tools) &gt; 0\n            else False\n            if model_settings.parallel_tool_calls is False\n            else None\n        )\n        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)\n        response_format = Converter.convert_response_format(output_schema)\n\n        converted_tools = [Converter.tool_to_openai(tool) for tool in tools] if tools else []\n\n        for handoff in handoffs:\n            converted_tools.append(Converter.convert_handoff_tool(handoff))\n\n        converted_tools = _to_dump_compatible(converted_tools)\n\n        if _debug.DONT_LOG_MODEL_DATA:\n            logger.debug(\"Calling LLM\")\n        else:\n            messages_json = json.dumps(\n                converted_messages,\n                indent=2,\n                ensure_ascii=False,\n            )\n            tools_json = json.dumps(\n                converted_tools,\n                indent=2,\n                ensure_ascii=False,\n            )\n            logger.debug(\n                f\"Calling Litellm model: {self.model}\\n\"\n                f\"{messages_json}\\n\"\n                f\"Tools:\\n{tools_json}\\n\"\n                f\"Stream: {stream}\\n\"\n                f\"Tool choice: {tool_choice}\\n\"\n                f\"Response format: {response_format}\\n\"\n            )\n\n        # Build reasoning_effort - use dict only when summary is present (OpenAI feature)\n        # Otherwise pass string for backward compatibility with all providers\n        reasoning_effort: dict[str, Any] | str | None = None\n        if model_settings.reasoning:\n            if model_settings.reasoning.summary is not None:\n                # Dict format when summary is needed (OpenAI only)\n                reasoning_effort = {\n                    \"effort\": model_settings.reasoning.effort,\n                    \"summary\": model_settings.reasoning.summary,\n                }\n            elif model_settings.reasoning.effort is not None:\n                # String format for compatibility with all providers\n                reasoning_effort = model_settings.reasoning.effort\n\n        # Enable developers to pass non-OpenAI compatible reasoning_effort data like \"none\"\n        # Priority order:\n        #  1. model_settings.reasoning (effort + summary)\n        #  2. model_settings.extra_body[\"reasoning_effort\"]\n        #  3. model_settings.extra_args[\"reasoning_effort\"]\n        if (\n            reasoning_effort is None  # Unset in model_settings\n            and isinstance(model_settings.extra_body, dict)\n            and \"reasoning_effort\" in model_settings.extra_body\n        ):\n            reasoning_effort = model_settings.extra_body[\"reasoning_effort\"]\n        if (\n            reasoning_effort is None  # Unset in both model_settings and model_settings.extra_body\n            and model_settings.extra_args\n            and \"reasoning_effort\" in model_settings.extra_args\n        ):\n            reasoning_effort = model_settings.extra_args[\"reasoning_effort\"]\n\n        stream_options = None\n        if stream and model_settings.include_usage is not None:\n            stream_options = {\"include_usage\": model_settings.include_usage}\n\n        extra_kwargs = {}\n        if model_settings.extra_query:\n            extra_kwargs[\"extra_query\"] = copy(model_settings.extra_query)\n        if model_settings.metadata:\n            extra_kwargs[\"metadata\"] = copy(model_settings.metadata)\n        if model_settings.extra_body and isinstance(model_settings.extra_body, dict):\n            extra_kwargs.update(model_settings.extra_body)\n\n        # Add kwargs from model_settings.extra_args, filtering out None values\n        if model_settings.extra_args:\n            extra_kwargs.update(model_settings.extra_args)\n\n        # Prevent duplicate reasoning_effort kwargs when it was promoted to a top-level argument.\n        extra_kwargs.pop(\"reasoning_effort\", None)\n\n        ret = await litellm.acompletion(\n            model=self.model,\n            messages=converted_messages,\n            tools=converted_tools or None,\n            temperature=model_settings.temperature,\n            top_p=model_settings.top_p,\n            frequency_penalty=model_settings.frequency_penalty,\n            presence_penalty=model_settings.presence_penalty,\n            max_tokens=model_settings.max_tokens,\n            tool_choice=self._remove_not_given(tool_choice),\n            response_format=self._remove_not_given(response_format),\n            parallel_tool_calls=parallel_tool_calls,\n            stream=stream,\n            stream_options=stream_options,\n            reasoning_effort=reasoning_effort,\n            top_logprobs=model_settings.top_logprobs,\n            extra_headers=self._merge_headers(model_settings),\n            api_key=self.api_key,\n            base_url=self.base_url,\n            **extra_kwargs,\n        )\n\n        if isinstance(ret, litellm.types.utils.ModelResponse):\n            return ret\n\n        responses_tool_choice = OpenAIResponsesConverter.convert_tool_choice(\n            model_settings.tool_choice\n        )\n        if responses_tool_choice is None or responses_tool_choice is omit:\n            responses_tool_choice = \"auto\"\n\n        response = Response(\n            id=FAKE_RESPONSES_ID,\n            created_at=time.time(),\n            model=self.model,\n            object=\"response\",\n            output=[],\n            tool_choice=responses_tool_choice,  # type: ignore[arg-type]\n            top_p=model_settings.top_p,\n            temperature=model_settings.temperature,\n            tools=[],\n            parallel_tool_calls=parallel_tool_calls or False,\n            reasoning=model_settings.reasoning,\n        )\n        return response, ret\n\n    def _convert_gemini_extra_content_to_provider_specific_fields(\n        self, messages: list[ChatCompletionMessageParam]\n    ) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"\n        Convert Gemini model's extra_content format to provider_specific_fields format for litellm.\n\n        Transforms tool calls from internal format:\n            extra_content={\"google\": {\"thought_signature\": \"...\"}}\n        To litellm format:\n            provider_specific_fields={\"thought_signature\": \"...\"}\n\n        Only processes tool_calls that appear after the last user message.\n        See: https://ai.google.dev/gemini-api/docs/thought-signatures\n        \"\"\"\n\n        # Find the index of the last user message\n        last_user_index = -1\n        for i in range(len(messages) - 1, -1, -1):\n            if isinstance(messages[i], dict) and messages[i].get(\"role\") == \"user\":\n                last_user_index = i\n                break\n\n        for i, message in enumerate(messages):\n            if not isinstance(message, dict):\n                continue\n\n            # Only process assistant messages that come after the last user message\n            # If no user message found (last_user_index == -1), process all messages\n            if last_user_index != -1 and i &lt;= last_user_index:\n                continue\n\n            # Check if this is an assistant message with tool calls\n            if message.get(\"role\") == \"assistant\" and message.get(\"tool_calls\"):\n                tool_calls = message.get(\"tool_calls\", [])\n\n                for tool_call in tool_calls:  # type: ignore[attr-defined]\n                    if not isinstance(tool_call, dict):\n                        continue\n\n                    # Default to skip validator, overridden if valid thought signature exists\n                    tool_call[\"provider_specific_fields\"] = {\n                        \"thought_signature\": \"skip_thought_signature_validator\"\n                    }\n\n                    # Override with actual thought signature if extra_content exists\n                    if \"extra_content\" in tool_call:\n                        extra_content = tool_call.pop(\"extra_content\")\n                        if isinstance(extra_content, dict):\n                            # Extract google-specific fields\n                            google_fields = extra_content.get(\"google\")\n                            if google_fields and isinstance(google_fields, dict):\n                                thought_sig = google_fields.get(\"thought_signature\")\n                                if thought_sig:\n                                    tool_call[\"provider_specific_fields\"] = {\n                                        \"thought_signature\": thought_sig\n                                    }\n\n        return messages\n\n    def _fix_tool_message_ordering(\n        self, messages: list[ChatCompletionMessageParam]\n    ) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"\n        Fix the ordering of tool messages to ensure tool_use messages come before tool_result messages.\n\n        Required for Anthropic and Vertex AI Gemini APIs which require tool calls to immediately\n        precede their corresponding tool responses in conversation history.\n        \"\"\"  # noqa: E501\n        if not messages:\n            return messages\n\n        # Collect all tool calls and tool results\n        tool_call_messages = {}  # tool_id -&gt; (index, message)\n        tool_result_messages = {}  # tool_id -&gt; (index, message)\n        other_messages = []  # (index, message) for non-tool messages\n\n        for i, message in enumerate(messages):\n            if not isinstance(message, dict):\n                other_messages.append((i, message))\n                continue\n\n            role = message.get(\"role\")\n\n            if role == \"assistant\" and message.get(\"tool_calls\"):\n                # Extract tool calls from this assistant message\n                tool_calls = message.get(\"tool_calls\", [])\n                if isinstance(tool_calls, list):\n                    for tool_call in tool_calls:\n                        if isinstance(tool_call, dict):\n                            tool_id = tool_call.get(\"id\")\n                            if tool_id:\n                                # Create a separate assistant message for each tool call\n                                single_tool_msg = cast(dict[str, Any], message.copy())\n                                single_tool_msg[\"tool_calls\"] = [tool_call]\n                                tool_call_messages[tool_id] = (\n                                    i,\n                                    cast(ChatCompletionMessageParam, single_tool_msg),\n                                )\n\n            elif role == \"tool\":\n                tool_call_id = message.get(\"tool_call_id\")\n                if tool_call_id:\n                    tool_result_messages[tool_call_id] = (i, message)\n                else:\n                    other_messages.append((i, message))\n            else:\n                other_messages.append((i, message))\n\n        # First, identify which tool results will be paired to avoid duplicates\n        paired_tool_result_indices = set()\n        for tool_id in tool_call_messages:\n            if tool_id in tool_result_messages:\n                tool_result_idx, _ = tool_result_messages[tool_id]\n                paired_tool_result_indices.add(tool_result_idx)\n\n        # Create the fixed message sequence\n        fixed_messages: list[ChatCompletionMessageParam] = []\n        used_indices = set()\n\n        # Add messages in their original order, but ensure tool_use \u2192 tool_result pairing\n        for i, original_message in enumerate(messages):\n            if i in used_indices:\n                continue\n\n            if not isinstance(original_message, dict):\n                fixed_messages.append(original_message)\n                used_indices.add(i)\n                continue\n\n            role = original_message.get(\"role\")\n\n            if role == \"assistant\" and original_message.get(\"tool_calls\"):\n                # Process each tool call in this assistant message\n                tool_calls = original_message.get(\"tool_calls\", [])\n                if isinstance(tool_calls, list):\n                    for tool_call in tool_calls:\n                        if isinstance(tool_call, dict):\n                            tool_id = tool_call.get(\"id\")\n                            if (\n                                tool_id\n                                and tool_id in tool_call_messages\n                                and tool_id in tool_result_messages\n                            ):\n                                # Add tool_use \u2192 tool_result pair\n                                _, tool_call_msg = tool_call_messages[tool_id]\n                                tool_result_idx, tool_result_msg = tool_result_messages[tool_id]\n\n                                fixed_messages.append(tool_call_msg)\n                                fixed_messages.append(tool_result_msg)\n\n                                # Mark both as used\n                                used_indices.add(tool_call_messages[tool_id][0])\n                                used_indices.add(tool_result_idx)\n                            elif tool_id and tool_id in tool_call_messages:\n                                # Tool call without result - add just the tool call\n                                _, tool_call_msg = tool_call_messages[tool_id]\n                                fixed_messages.append(tool_call_msg)\n                                used_indices.add(tool_call_messages[tool_id][0])\n\n                used_indices.add(i)  # Mark original multi-tool message as used\n\n            elif role == \"tool\":\n                # Only preserve unmatched tool results to avoid duplicates\n                if i not in paired_tool_result_indices:\n                    fixed_messages.append(original_message)\n                used_indices.add(i)\n\n            else:\n                # Regular message - add it normally\n                fixed_messages.append(original_message)\n                used_indices.add(i)\n\n        return fixed_messages\n\n    def _remove_not_given(self, value: Any) -&gt; Any:\n        if value is omit or isinstance(value, NotGiven):\n            return None\n        return value\n\n    def _merge_headers(self, model_settings: ModelSettings):\n        return {**HEADERS, **(model_settings.extra_headers or {}), **(HEADERS_OVERRIDE.get() or {})}\n</code></pre>"},{"location":"ref/extensions/litellm/#agents.extensions.models.litellm_model.LitellmConverter","title":"LitellmConverter","text":"Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class LitellmConverter:\n    @classmethod\n    def convert_message_to_openai(\n        cls, message: litellm.types.utils.Message, model: str | None = None\n    ) -&gt; ChatCompletionMessage:\n        \"\"\"\n        Convert a LiteLLM message to OpenAI ChatCompletionMessage format.\n\n        Args:\n            message: The LiteLLM message to convert\n            model: The target model to convert to. Used to handle provider-specific\n                transformations.\n        \"\"\"\n        if message.role != \"assistant\":\n            raise ModelBehaviorError(f\"Unsupported role: {message.role}\")\n\n        tool_calls: (\n            list[ChatCompletionMessageFunctionToolCall | ChatCompletionMessageCustomToolCall] | None\n        ) = (\n            [\n                LitellmConverter.convert_tool_call_to_openai(tool, model=model)\n                for tool in message.tool_calls\n            ]\n            if message.tool_calls\n            else None\n        )\n\n        provider_specific_fields = message.get(\"provider_specific_fields\", None)\n        refusal = (\n            provider_specific_fields.get(\"refusal\", None) if provider_specific_fields else None\n        )\n\n        reasoning_content = \"\"\n        if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n            reasoning_content = message.reasoning_content\n\n        # Extract full thinking blocks including signatures (for Anthropic)\n        thinking_blocks: list[dict[str, Any]] | None = None\n        if hasattr(message, \"thinking_blocks\") and message.thinking_blocks:\n            # Convert thinking blocks to dict format for compatibility\n            thinking_blocks = []\n            for block in message.thinking_blocks:\n                if isinstance(block, dict):\n                    thinking_blocks.append(cast(dict[str, Any], block))\n                else:\n                    # Convert object to dict by accessing its attributes\n                    block_dict: dict[str, Any] = {}\n                    if hasattr(block, \"__dict__\"):\n                        block_dict = dict(block.__dict__.items())\n                    elif hasattr(block, \"model_dump\"):\n                        block_dict = block.model_dump()\n                    else:\n                        # Last resort: convert to string representation\n                        block_dict = {\"thinking\": str(block)}\n                    thinking_blocks.append(block_dict)\n\n        return InternalChatCompletionMessage(\n            content=message.content,\n            refusal=refusal,\n            role=\"assistant\",\n            annotations=cls.convert_annotations_to_openai(message),\n            audio=message.get(\"audio\", None),  # litellm deletes audio if not present\n            tool_calls=tool_calls,\n            reasoning_content=reasoning_content,\n            thinking_blocks=thinking_blocks,\n        )\n\n    @classmethod\n    def convert_annotations_to_openai(\n        cls, message: litellm.types.utils.Message\n    ) -&gt; list[Annotation] | None:\n        annotations: list[litellm.types.llms.openai.ChatCompletionAnnotation] | None = message.get(\n            \"annotations\", None\n        )\n        if not annotations:\n            return None\n\n        return [\n            Annotation(\n                type=\"url_citation\",\n                url_citation=AnnotationURLCitation(\n                    start_index=annotation[\"url_citation\"][\"start_index\"],\n                    end_index=annotation[\"url_citation\"][\"end_index\"],\n                    url=annotation[\"url_citation\"][\"url\"],\n                    title=annotation[\"url_citation\"][\"title\"],\n                ),\n            )\n            for annotation in annotations\n        ]\n\n    @classmethod\n    def convert_tool_call_to_openai(\n        cls, tool_call: litellm.types.utils.ChatCompletionMessageToolCall, model: str | None = None\n    ) -&gt; ChatCompletionMessageFunctionToolCall:\n        # Clean up litellm's addition of __thought__ suffix to tool_call.id for\n        # Gemini models. See: https://github.com/BerriAI/litellm/pull/16895\n        # This suffix is redundant since we can get thought_signature from\n        # provider_specific_fields, and this hack causes validation errors when\n        # cross-model passing to other models.\n        tool_call_id = tool_call.id\n        if model and \"gemini\" in model.lower() and \"__thought__\" in tool_call_id:\n            tool_call_id = tool_call_id.split(\"__thought__\")[0]\n\n        # Convert litellm's tool call format to chat completion message format\n        base_tool_call = ChatCompletionMessageFunctionToolCall(\n            id=tool_call_id,\n            type=\"function\",\n            function=Function(\n                name=tool_call.function.name or \"\",\n                arguments=tool_call.function.arguments,\n            ),\n        )\n\n        # Preserve provider-specific fields if present (e.g., Gemini thought signatures)\n        if hasattr(tool_call, \"provider_specific_fields\") and tool_call.provider_specific_fields:\n            # Convert to nested extra_content structure\n            extra_content: dict[str, Any] = {}\n            provider_fields = tool_call.provider_specific_fields\n\n            # Check for thought_signature (Gemini specific)\n            if model and \"gemini\" in model.lower():\n                if \"thought_signature\" in provider_fields:\n                    extra_content[\"google\"] = {\n                        \"thought_signature\": provider_fields[\"thought_signature\"]\n                    }\n\n            return InternalToolCall(\n                **base_tool_call.model_dump(),\n                extra_content=extra_content if extra_content else None,\n            )\n\n        return base_tool_call\n</code></pre>"},{"location":"ref/extensions/litellm/#agents.extensions.models.litellm_model.LitellmConverter.convert_message_to_openai","title":"convert_message_to_openai  <code>classmethod</code>","text":"<pre><code>convert_message_to_openai(\n    message: Message, model: str | None = None\n) -&gt; ChatCompletionMessage\n</code></pre> <p>Convert a LiteLLM message to OpenAI ChatCompletionMessage format.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The LiteLLM message to convert</p> required <code>model</code> <code>str | None</code> <p>The target model to convert to. Used to handle provider-specific transformations.</p> <code>None</code> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>@classmethod\ndef convert_message_to_openai(\n    cls, message: litellm.types.utils.Message, model: str | None = None\n) -&gt; ChatCompletionMessage:\n    \"\"\"\n    Convert a LiteLLM message to OpenAI ChatCompletionMessage format.\n\n    Args:\n        message: The LiteLLM message to convert\n        model: The target model to convert to. Used to handle provider-specific\n            transformations.\n    \"\"\"\n    if message.role != \"assistant\":\n        raise ModelBehaviorError(f\"Unsupported role: {message.role}\")\n\n    tool_calls: (\n        list[ChatCompletionMessageFunctionToolCall | ChatCompletionMessageCustomToolCall] | None\n    ) = (\n        [\n            LitellmConverter.convert_tool_call_to_openai(tool, model=model)\n            for tool in message.tool_calls\n        ]\n        if message.tool_calls\n        else None\n    )\n\n    provider_specific_fields = message.get(\"provider_specific_fields\", None)\n    refusal = (\n        provider_specific_fields.get(\"refusal\", None) if provider_specific_fields else None\n    )\n\n    reasoning_content = \"\"\n    if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n        reasoning_content = message.reasoning_content\n\n    # Extract full thinking blocks including signatures (for Anthropic)\n    thinking_blocks: list[dict[str, Any]] | None = None\n    if hasattr(message, \"thinking_blocks\") and message.thinking_blocks:\n        # Convert thinking blocks to dict format for compatibility\n        thinking_blocks = []\n        for block in message.thinking_blocks:\n            if isinstance(block, dict):\n                thinking_blocks.append(cast(dict[str, Any], block))\n            else:\n                # Convert object to dict by accessing its attributes\n                block_dict: dict[str, Any] = {}\n                if hasattr(block, \"__dict__\"):\n                    block_dict = dict(block.__dict__.items())\n                elif hasattr(block, \"model_dump\"):\n                    block_dict = block.model_dump()\n                else:\n                    # Last resort: convert to string representation\n                    block_dict = {\"thinking\": str(block)}\n                thinking_blocks.append(block_dict)\n\n    return InternalChatCompletionMessage(\n        content=message.content,\n        refusal=refusal,\n        role=\"assistant\",\n        annotations=cls.convert_annotations_to_openai(message),\n        audio=message.get(\"audio\", None),  # litellm deletes audio if not present\n        tool_calls=tool_calls,\n        reasoning_content=reasoning_content,\n        thinking_blocks=thinking_blocks,\n    )\n</code></pre>"},{"location":"ref/extensions/visualization/","title":"<code>Visualization</code>","text":""},{"location":"ref/extensions/visualization/#agents.extensions.visualization.get_main_graph","title":"get_main_graph","text":"<pre><code>get_main_graph(agent: Agent) -&gt; str\n</code></pre> <p>Generates the main graph structure in DOT format for the given agent.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent for which the graph is to be generated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The DOT format string representing the graph.</p> Source code in <code>src/agents/extensions/visualization.py</code> <pre><code>def get_main_graph(agent: Agent) -&gt; str:\n    \"\"\"\n    Generates the main graph structure in DOT format for the given agent.\n\n    Args:\n        agent (Agent): The agent for which the graph is to be generated.\n\n    Returns:\n        str: The DOT format string representing the graph.\n    \"\"\"\n    parts = [\n        \"\"\"\n    digraph G {\n        graph [splines=true];\n        node [fontname=\"Arial\"];\n        edge [penwidth=1.5];\n    \"\"\"\n    ]\n    parts.append(get_all_nodes(agent))\n    parts.append(get_all_edges(agent))\n    parts.append(\"}\")\n    return \"\".join(parts)\n</code></pre>"},{"location":"ref/extensions/visualization/#agents.extensions.visualization.get_all_nodes","title":"get_all_nodes","text":"<pre><code>get_all_nodes(\n    agent: Agent,\n    parent: Agent | None = None,\n    visited: set[str] | None = None,\n) -&gt; str\n</code></pre> <p>Recursively generates the nodes for the given agent and its handoffs in DOT format.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent for which the nodes are to be generated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The DOT format string representing the nodes.</p> Source code in <code>src/agents/extensions/visualization.py</code> <pre><code>def get_all_nodes(\n    agent: Agent, parent: Agent | None = None, visited: set[str] | None = None\n) -&gt; str:\n    \"\"\"\n    Recursively generates the nodes for the given agent and its handoffs in DOT format.\n\n    Args:\n        agent (Agent): The agent for which the nodes are to be generated.\n\n    Returns:\n        str: The DOT format string representing the nodes.\n    \"\"\"\n    if visited is None:\n        visited = set()\n    if agent.name in visited:\n        return \"\"\n    visited.add(agent.name)\n\n    parts = []\n\n    # Start and end the graph\n    if not parent:\n        parts.append(\n            '\"__start__\" [label=\"__start__\", shape=ellipse, style=filled, '\n            \"fillcolor=lightblue, width=0.5, height=0.3];\"\n            '\"__end__\" [label=\"__end__\", shape=ellipse, style=filled, '\n            \"fillcolor=lightblue, width=0.5, height=0.3];\"\n        )\n        # Ensure parent agent node is colored\n        parts.append(\n            f'\"{agent.name}\" [label=\"{agent.name}\", shape=box, style=filled, '\n            \"fillcolor=lightyellow, width=1.5, height=0.8];\"\n        )\n\n    for tool in agent.tools:\n        parts.append(\n            f'\"{tool.name}\" [label=\"{tool.name}\", shape=ellipse, style=filled, '\n            f\"fillcolor=lightgreen, width=0.5, height=0.3];\"\n        )\n\n    for mcp_server in agent.mcp_servers:\n        parts.append(\n            f'\"{mcp_server.name}\" [label=\"{mcp_server.name}\", shape=box, style=filled, '\n            f\"fillcolor=lightgrey, width=1, height=0.5];\"\n        )\n\n    for handoff in agent.handoffs:\n        if isinstance(handoff, Handoff):\n            parts.append(\n                f'\"{handoff.agent_name}\" [label=\"{handoff.agent_name}\", '\n                f\"shape=box, style=filled, style=rounded, \"\n                f\"fillcolor=lightyellow, width=1.5, height=0.8];\"\n            )\n        if isinstance(handoff, Agent):\n            if handoff.name not in visited:\n                parts.append(\n                    f'\"{handoff.name}\" [label=\"{handoff.name}\", '\n                    f\"shape=box, style=filled, style=rounded, \"\n                    f\"fillcolor=lightyellow, width=1.5, height=0.8];\"\n                )\n            parts.append(get_all_nodes(handoff, agent, visited))\n\n    return \"\".join(parts)\n</code></pre>"},{"location":"ref/extensions/visualization/#agents.extensions.visualization.get_all_edges","title":"get_all_edges","text":"<pre><code>get_all_edges(\n    agent: Agent,\n    parent: Agent | None = None,\n    visited: set[str] | None = None,\n) -&gt; str\n</code></pre> <p>Recursively generates the edges for the given agent and its handoffs in DOT format.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent for which the edges are to be generated.</p> required <code>parent</code> <code>Agent</code> <p>The parent agent. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The DOT format string representing the edges.</p> Source code in <code>src/agents/extensions/visualization.py</code> <pre><code>def get_all_edges(\n    agent: Agent, parent: Agent | None = None, visited: set[str] | None = None\n) -&gt; str:\n    \"\"\"\n    Recursively generates the edges for the given agent and its handoffs in DOT format.\n\n    Args:\n        agent (Agent): The agent for which the edges are to be generated.\n        parent (Agent, optional): The parent agent. Defaults to None.\n\n    Returns:\n        str: The DOT format string representing the edges.\n    \"\"\"\n    if visited is None:\n        visited = set()\n    if agent.name in visited:\n        return \"\"\n    visited.add(agent.name)\n\n    parts = []\n\n    if not parent:\n        parts.append(f'\"__start__\" -&gt; \"{agent.name}\";')\n\n    for tool in agent.tools:\n        parts.append(f\"\"\"\n        \"{agent.name}\" -&gt; \"{tool.name}\" [style=dotted, penwidth=1.5];\n        \"{tool.name}\" -&gt; \"{agent.name}\" [style=dotted, penwidth=1.5];\"\"\")\n\n    for mcp_server in agent.mcp_servers:\n        parts.append(f\"\"\"\n        \"{agent.name}\" -&gt; \"{mcp_server.name}\" [style=dashed, penwidth=1.5];\n        \"{mcp_server.name}\" -&gt; \"{agent.name}\" [style=dashed, penwidth=1.5];\"\"\")\n\n    for handoff in agent.handoffs:\n        if isinstance(handoff, Handoff):\n            parts.append(f\"\"\"\n            \"{agent.name}\" -&gt; \"{handoff.agent_name}\";\"\"\")\n        if isinstance(handoff, Agent):\n            parts.append(f\"\"\"\n            \"{agent.name}\" -&gt; \"{handoff.name}\";\"\"\")\n            parts.append(get_all_edges(handoff, agent, visited))\n\n    if not agent.handoffs:\n        parts.append(f'\"{agent.name}\" -&gt; \"__end__\";')\n\n    return \"\".join(parts)\n</code></pre>"},{"location":"ref/extensions/visualization/#agents.extensions.visualization.draw_graph","title":"draw_graph","text":"<pre><code>draw_graph(\n    agent: Agent, filename: str | None = None\n) -&gt; Source\n</code></pre> <p>Draws the graph for the given agent and optionally saves it as a PNG file.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent for which the graph is to be drawn.</p> required <code>filename</code> <code>str</code> <p>The name of the file to save the graph as a PNG.</p> <code>None</code> <p>Returns:</p> Type Description <code>Source</code> <p>graphviz.Source: The graphviz Source object representing the graph.</p> Source code in <code>src/agents/extensions/visualization.py</code> <pre><code>def draw_graph(agent: Agent, filename: str | None = None) -&gt; graphviz.Source:\n    \"\"\"\n    Draws the graph for the given agent and optionally saves it as a PNG file.\n\n    Args:\n        agent (Agent): The agent for which the graph is to be drawn.\n        filename (str): The name of the file to save the graph as a PNG.\n\n    Returns:\n        graphviz.Source: The graphviz Source object representing the graph.\n    \"\"\"\n    dot_code = get_main_graph(agent)\n    graph = graphviz.Source(dot_code)\n\n    if filename:\n        graph.render(filename, format=\"png\", cleanup=True)\n\n    return graph\n</code></pre>"},{"location":"ref/extensions/experimental/codex/codex/","title":"<code>Codex</code>","text":""},{"location":"ref/extensions/experimental/codex/codex_options/","title":"<code>Codex Options</code>","text":""},{"location":"ref/extensions/experimental/codex/codex_tool/","title":"<code>Codex Tool</code>","text":""},{"location":"ref/extensions/experimental/codex/events/","title":"<code>Events</code>","text":""},{"location":"ref/extensions/experimental/codex/exec/","title":"<code>Exec</code>","text":""},{"location":"ref/extensions/experimental/codex/items/","title":"<code>Items</code>","text":""},{"location":"ref/extensions/experimental/codex/output_schema_file/","title":"<code>Output Schema File</code>","text":""},{"location":"ref/extensions/experimental/codex/output_schema_file/#agents.extensions.experimental.codex.output_schema_file.create_output_schema_file","title":"create_output_schema_file","text":"<pre><code>create_output_schema_file(\n    schema: dict[str, Any] | None,\n) -&gt; OutputSchemaFile\n</code></pre> <p>Materialize a JSON schema into a temp file for the Codex CLI.</p> Source code in <code>src/agents/extensions/experimental/codex/output_schema_file.py</code> <pre><code>def create_output_schema_file(schema: dict[str, Any] | None) -&gt; OutputSchemaFile:\n    \"\"\"Materialize a JSON schema into a temp file for the Codex CLI.\"\"\"\n    if schema is None:\n        # No schema means there is no temp file to manage.\n        return OutputSchemaFile(schema_path=None, cleanup=lambda: None)\n\n    if not _is_plain_json_object(schema):\n        raise UserError(\"output_schema must be a plain JSON object\")\n\n    # The Codex CLI expects a schema file path, so write to a temp directory.\n    schema_dir = tempfile.mkdtemp(prefix=\"codex-output-schema-\")\n    schema_path = os.path.join(schema_dir, \"schema.json\")\n\n    def cleanup() -&gt; None:\n        # Best-effort cleanup since this runs in finally blocks.\n        try:\n            shutil.rmtree(schema_dir, ignore_errors=True)\n        except Exception:\n            pass\n\n    try:\n        with open(schema_path, \"w\", encoding=\"utf-8\") as handle:\n            json.dump(schema, handle)\n        return OutputSchemaFile(schema_path=schema_path, cleanup=cleanup)\n    except Exception:\n        cleanup()\n        raise\n</code></pre>"},{"location":"ref/extensions/experimental/codex/payloads/","title":"<code>Payloads</code>","text":""},{"location":"ref/extensions/experimental/codex/thread/","title":"<code>Thread</code>","text":""},{"location":"ref/extensions/experimental/codex/thread_options/","title":"<code>Thread Options</code>","text":""},{"location":"ref/extensions/experimental/codex/turn_options/","title":"<code>Turn Options</code>","text":""},{"location":"ref/extensions/memory/advanced_sqlite_session/","title":"<code>AdvancedSQLiteSession</code>","text":"<p>               Bases: <code>SQLiteSession</code></p> <p>Enhanced SQLite session with conversation branching and usage analytics.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>class AdvancedSQLiteSession(SQLiteSession):\n    \"\"\"Enhanced SQLite session with conversation branching and usage analytics.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        session_id: str,\n        db_path: str | Path = \":memory:\",\n        create_tables: bool = False,\n        logger: logging.Logger | None = None,\n        session_settings: SessionSettings | None = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the AdvancedSQLiteSession.\n\n        Args:\n            session_id: The ID of the session\n            db_path: The path to the SQLite database file. Defaults to `:memory:` for in-memory storage\n            create_tables: Whether to create the structure tables\n            logger: The logger to use. Defaults to the module logger\n            **kwargs: Additional keyword arguments to pass to the superclass\n        \"\"\"  # noqa: E501\n        super().__init__(\n            session_id=session_id,\n            db_path=db_path,\n            session_settings=session_settings,\n            **kwargs,\n        )\n        if create_tables:\n            self._init_structure_tables()\n        self._current_branch_id = \"main\"\n        self._logger = logger or logging.getLogger(__name__)\n\n    def _init_structure_tables(self):\n        \"\"\"Add structure and usage tracking tables.\n\n        Creates the message_structure and turn_usage tables with appropriate\n        indexes for conversation branching and usage analytics.\n        \"\"\"\n        conn = self._get_connection()\n\n        # Message structure with branch support\n        conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS message_structure (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL,\n                message_id INTEGER NOT NULL,\n                branch_id TEXT NOT NULL DEFAULT 'main',\n                message_type TEXT NOT NULL,\n                sequence_number INTEGER NOT NULL,\n                user_turn_number INTEGER,\n                branch_turn_number INTEGER,\n                tool_name TEXT,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (session_id) REFERENCES agent_sessions(session_id) ON DELETE CASCADE,\n                FOREIGN KEY (message_id) REFERENCES agent_messages(id) ON DELETE CASCADE\n            )\n        \"\"\")\n\n        # Turn-level usage tracking with branch support and full JSON details\n        conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS turn_usage (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL,\n                branch_id TEXT NOT NULL DEFAULT 'main',\n                user_turn_number INTEGER NOT NULL,\n                requests INTEGER DEFAULT 0,\n                input_tokens INTEGER DEFAULT 0,\n                output_tokens INTEGER DEFAULT 0,\n                total_tokens INTEGER DEFAULT 0,\n                input_tokens_details JSON,\n                output_tokens_details JSON,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (session_id) REFERENCES agent_sessions(session_id) ON DELETE CASCADE,\n                UNIQUE(session_id, branch_id, user_turn_number)\n            )\n        \"\"\")\n\n        # Indexes\n        conn.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_structure_session_seq\n            ON message_structure(session_id, sequence_number)\n        \"\"\")\n        conn.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_structure_branch\n            ON message_structure(session_id, branch_id)\n        \"\"\")\n        conn.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_structure_turn\n            ON message_structure(session_id, branch_id, user_turn_number)\n        \"\"\")\n        conn.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_structure_branch_seq\n            ON message_structure(session_id, branch_id, sequence_number)\n        \"\"\")\n        conn.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_turn_usage_session_turn\n            ON turn_usage(session_id, branch_id, user_turn_number)\n        \"\"\")\n\n        conn.commit()\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add items to the session.\n\n        Args:\n            items: The items to add to the session\n        \"\"\"\n        # Add to base table first\n        await super().add_items(items)\n\n        # Extract structure metadata with precise sequencing\n        if items:\n            await self._add_structure_metadata(items)\n\n    async def get_items(\n        self,\n        limit: int | None = None,\n        branch_id: str | None = None,\n    ) -&gt; list[TResponseInputItem]:\n        \"\"\"Get items from current or specified branch.\n\n        Args:\n            limit: Maximum number of items to return. If None, uses session_settings.limit.\n            branch_id: Branch to get items from. If None, uses current branch.\n\n        Returns:\n            List of conversation items from the specified branch.\n        \"\"\"\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        if branch_id is None:\n            branch_id = self._current_branch_id\n\n            # Get all items for this branch\n            def _get_all_items_sync():\n                \"\"\"Synchronous helper to get all items for a branch.\"\"\"\n                conn = self._get_connection()\n                # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n                with self._lock if self._is_memory_db else threading.Lock():\n                    with closing(conn.cursor()) as cursor:\n                        if session_limit is None:\n                            cursor.execute(\n                                \"\"\"\n                                SELECT m.message_data\n                                FROM agent_messages m\n                                JOIN message_structure s ON m.id = s.message_id\n                                WHERE m.session_id = ? AND s.branch_id = ?\n                                ORDER BY s.sequence_number ASC\n                            \"\"\",\n                                (self.session_id, branch_id),\n                            )\n                        else:\n                            cursor.execute(\n                                \"\"\"\n                                SELECT m.message_data\n                                FROM agent_messages m\n                                JOIN message_structure s ON m.id = s.message_id\n                                WHERE m.session_id = ? AND s.branch_id = ?\n                                ORDER BY s.sequence_number DESC\n                                LIMIT ?\n                            \"\"\",\n                                (self.session_id, branch_id, session_limit),\n                            )\n\n                        rows = cursor.fetchall()\n                        if session_limit is not None:\n                            rows = list(reversed(rows))\n\n                    items = []\n                    for (message_data,) in rows:\n                        try:\n                            item = json.loads(message_data)\n                            items.append(item)\n                        except json.JSONDecodeError:\n                            continue\n                    return items\n\n            return await asyncio.to_thread(_get_all_items_sync)\n\n        def _get_items_sync():\n            \"\"\"Synchronous helper to get items for a specific branch.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                with closing(conn.cursor()) as cursor:\n                    # Get message IDs in correct order for this branch\n                    if session_limit is None:\n                        cursor.execute(\n                            \"\"\"\n                            SELECT m.message_data\n                            FROM agent_messages m\n                            JOIN message_structure s ON m.id = s.message_id\n                            WHERE m.session_id = ? AND s.branch_id = ?\n                            ORDER BY s.sequence_number ASC\n                        \"\"\",\n                            (self.session_id, branch_id),\n                        )\n                    else:\n                        cursor.execute(\n                            \"\"\"\n                            SELECT m.message_data\n                            FROM agent_messages m\n                            JOIN message_structure s ON m.id = s.message_id\n                            WHERE m.session_id = ? AND s.branch_id = ?\n                            ORDER BY s.sequence_number DESC\n                            LIMIT ?\n                        \"\"\",\n                            (self.session_id, branch_id, session_limit),\n                        )\n\n                    rows = cursor.fetchall()\n                    if session_limit is not None:\n                        rows = list(reversed(rows))\n\n                items = []\n                for (message_data,) in rows:\n                    try:\n                        item = json.loads(message_data)\n                        items.append(item)\n                    except json.JSONDecodeError:\n                        continue\n                return items\n\n        return await asyncio.to_thread(_get_items_sync)\n\n    async def store_run_usage(self, result: RunResult) -&gt; None:\n        \"\"\"Store usage data for the current conversation turn.\n\n        This is designed to be called after `Runner.run()` completes.\n        Session-level usage can be aggregated from turn data when needed.\n\n        Args:\n            result: The result from the run\n        \"\"\"\n        try:\n            if result.context_wrapper.usage is not None:\n                # Get the current turn number for this branch\n                current_turn = self._get_current_turn_number()\n                # Only update turn-level usage - session usage is aggregated on demand\n                await self._update_turn_usage_internal(current_turn, result.context_wrapper.usage)\n        except Exception as e:\n            self._logger.error(f\"Failed to store usage for session {self.session_id}: {e}\")\n\n    def _get_next_turn_number(self, branch_id: str) -&gt; int:\n        \"\"\"Get the next turn number for a specific branch.\n\n        Args:\n            branch_id: The branch ID to get the next turn number for.\n\n        Returns:\n            The next available turn number for the specified branch.\n        \"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT COALESCE(MAX(user_turn_number), 0)\n                FROM message_structure\n                WHERE session_id = ? AND branch_id = ?\n            \"\"\",\n                (self.session_id, branch_id),\n            )\n            result = cursor.fetchone()\n            max_turn = result[0] if result else 0\n            return max_turn + 1\n\n    def _get_next_branch_turn_number(self, branch_id: str) -&gt; int:\n        \"\"\"Get the next branch turn number for a specific branch.\n\n        Args:\n            branch_id: The branch ID to get the next branch turn number for.\n\n        Returns:\n            The next available branch turn number for the specified branch.\n        \"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT COALESCE(MAX(branch_turn_number), 0)\n                FROM message_structure\n                WHERE session_id = ? AND branch_id = ?\n            \"\"\",\n                (self.session_id, branch_id),\n            )\n            result = cursor.fetchone()\n            max_turn = result[0] if result else 0\n            return max_turn + 1\n\n    def _get_current_turn_number(self) -&gt; int:\n        \"\"\"Get the current turn number for the current branch.\n\n        Returns:\n            The current turn number for the active branch.\n        \"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT COALESCE(MAX(user_turn_number), 0)\n                FROM message_structure\n                WHERE session_id = ? AND branch_id = ?\n                \"\"\",\n                (self.session_id, self._current_branch_id),\n            )\n            result = cursor.fetchone()\n            return result[0] if result else 0\n\n    async def _add_structure_metadata(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Extract structure metadata with branch-aware turn tracking.\n\n        This method:\n        - Assigns turn numbers per branch (not globally)\n        - Assigns explicit sequence numbers for precise ordering\n        - Links messages to their database IDs for structure tracking\n        - Handles multiple user messages in a single batch correctly\n\n        Args:\n            items: The items to add to the session\n        \"\"\"\n\n        def _add_structure_sync():\n            \"\"\"Synchronous helper to add structure metadata to database.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Get the IDs of messages we just inserted, in order\n                with closing(conn.cursor()) as cursor:\n                    cursor.execute(\n                        f\"SELECT id FROM {self.messages_table} \"\n                        f\"WHERE session_id = ? ORDER BY id DESC LIMIT ?\",\n                        (self.session_id, len(items)),\n                    )\n                    message_ids = [row[0] for row in cursor.fetchall()]\n                    message_ids.reverse()  # Match order of items\n\n                # Get current max sequence number (global)\n                with closing(conn.cursor()) as cursor:\n                    cursor.execute(\n                        \"\"\"\n                        SELECT COALESCE(MAX(sequence_number), 0)\n                        FROM message_structure\n                        WHERE session_id = ?\n                    \"\"\",\n                        (self.session_id,),\n                    )\n                    seq_start = cursor.fetchone()[0]\n\n                # Get current turn numbers atomically with a single query\n                with closing(conn.cursor()) as cursor:\n                    cursor.execute(\n                        \"\"\"\n                        SELECT\n                            COALESCE(MAX(user_turn_number), 0) as max_global_turn,\n                            COALESCE(MAX(branch_turn_number), 0) as max_branch_turn\n                        FROM message_structure\n                        WHERE session_id = ? AND branch_id = ?\n                    \"\"\",\n                        (self.session_id, self._current_branch_id),\n                    )\n                    result = cursor.fetchone()\n                    current_turn = result[0] if result else 0\n                    current_branch_turn = result[1] if result else 0\n\n                # Process items and assign turn numbers correctly\n                structure_data = []\n                user_message_count = 0\n\n                for i, (item, msg_id) in enumerate(zip(items, message_ids)):\n                    msg_type = self._classify_message_type(item)\n                    tool_name = self._extract_tool_name(item)\n\n                    # If this is a user message, increment turn counters\n                    if self._is_user_message(item):\n                        user_message_count += 1\n                        item_turn = current_turn + user_message_count\n                        item_branch_turn = current_branch_turn + user_message_count\n                    else:\n                        # Non-user messages inherit the turn number of the most recent user message\n                        item_turn = current_turn + user_message_count\n                        item_branch_turn = current_branch_turn + user_message_count\n\n                    structure_data.append(\n                        (\n                            self.session_id,\n                            msg_id,\n                            self._current_branch_id,\n                            msg_type,\n                            seq_start + i + 1,  # Global sequence\n                            item_turn,  # Global turn number\n                            item_branch_turn,  # Branch-specific turn number\n                            tool_name,\n                        )\n                    )\n\n                with closing(conn.cursor()) as cursor:\n                    cursor.executemany(\n                        \"\"\"\n                        INSERT INTO message_structure\n                        (session_id, message_id, branch_id, message_type, sequence_number,\n                         user_turn_number, branch_turn_number, tool_name)\n                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n                    \"\"\",\n                        structure_data,\n                    )\n                    conn.commit()\n\n        try:\n            await asyncio.to_thread(_add_structure_sync)\n        except Exception as e:\n            self._logger.error(\n                f\"Failed to add structure metadata for session {self.session_id}: {e}\"\n            )\n            # Try to clean up any orphaned messages to maintain consistency\n            try:\n                await self._cleanup_orphaned_messages()\n            except Exception as cleanup_error:\n                self._logger.error(f\"Failed to cleanup orphaned messages: {cleanup_error}\")\n            # Don't re-raise - structure metadata is supplementary\n\n    async def _cleanup_orphaned_messages(self) -&gt; None:\n        \"\"\"Remove messages that exist in agent_messages but not in message_structure.\n\n        This can happen if _add_structure_metadata fails after super().add_items() succeeds.\n        Used for maintaining data consistency.\n        \"\"\"\n\n        def _cleanup_sync():\n            \"\"\"Synchronous helper to cleanup orphaned messages.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                with closing(conn.cursor()) as cursor:\n                    # Find messages without structure metadata\n                    cursor.execute(\n                        \"\"\"\n                        SELECT am.id\n                        FROM agent_messages am\n                        LEFT JOIN message_structure ms ON am.id = ms.message_id\n                        WHERE am.session_id = ? AND ms.message_id IS NULL\n                    \"\"\",\n                        (self.session_id,),\n                    )\n\n                    orphaned_ids = [row[0] for row in cursor.fetchall()]\n\n                    if orphaned_ids:\n                        # Delete orphaned messages\n                        placeholders = \",\".join(\"?\" * len(orphaned_ids))\n                        cursor.execute(\n                            f\"DELETE FROM agent_messages WHERE id IN ({placeholders})\", orphaned_ids\n                        )\n\n                        deleted_count = cursor.rowcount\n                        conn.commit()\n\n                        self._logger.info(f\"Cleaned up {deleted_count} orphaned messages\")\n                        return deleted_count\n\n                    return 0\n\n        return await asyncio.to_thread(_cleanup_sync)\n\n    def _classify_message_type(self, item: TResponseInputItem) -&gt; str:\n        \"\"\"Classify the type of a message item.\n\n        Args:\n            item: The message item to classify.\n\n        Returns:\n            String representing the message type (user, assistant, etc.).\n        \"\"\"\n        if isinstance(item, dict):\n            if item.get(\"role\") == \"user\":\n                return \"user\"\n            elif item.get(\"role\") == \"assistant\":\n                return \"assistant\"\n            elif item.get(\"type\"):\n                return str(item.get(\"type\"))\n        return \"other\"\n\n    def _extract_tool_name(self, item: TResponseInputItem) -&gt; str | None:\n        \"\"\"Extract tool name if this is a tool call/output.\n\n        Args:\n            item: The message item to extract tool name from.\n\n        Returns:\n            Tool name if item is a tool call, None otherwise.\n        \"\"\"\n        if isinstance(item, dict):\n            item_type = item.get(\"type\")\n\n            # For MCP tools, try to extract from server_label if available\n            if item_type in {\"mcp_call\", \"mcp_approval_request\"} and \"server_label\" in item:\n                server_label = item.get(\"server_label\")\n                tool_name = item.get(\"name\")\n                if tool_name and server_label:\n                    return f\"{server_label}.{tool_name}\"\n                elif server_label:\n                    return str(server_label)\n                elif tool_name:\n                    return str(tool_name)\n\n            # For tool types without a 'name' field, derive from the type\n            elif item_type in {\n                \"computer_call\",\n                \"file_search_call\",\n                \"web_search_call\",\n                \"code_interpreter_call\",\n            }:\n                return item_type\n\n            # Most other tool calls have a 'name' field\n            elif \"name\" in item:\n                name = item.get(\"name\")\n                return str(name) if name is not None else None\n\n        return None\n\n    def _is_user_message(self, item: TResponseInputItem) -&gt; bool:\n        \"\"\"Check if this is a user message.\n\n        Args:\n            item: The message item to check.\n\n        Returns:\n            True if the item is a user message, False otherwise.\n        \"\"\"\n        return isinstance(item, dict) and item.get(\"role\") == \"user\"\n\n    async def create_branch_from_turn(\n        self, turn_number: int, branch_name: str | None = None\n    ) -&gt; str:\n        \"\"\"Create a new branch starting from a specific user message turn.\n\n        Args:\n            turn_number: The branch turn number of the user message to branch from\n            branch_name: Optional name for the branch (auto-generated if None)\n\n        Returns:\n            The branch_id of the newly created branch\n\n        Raises:\n            ValueError: If turn doesn't exist or doesn't contain a user message\n        \"\"\"\n        import time\n\n        # Validate the turn exists and contains a user message\n        def _validate_turn():\n            \"\"\"Synchronous helper to validate turn exists and contains user message.\"\"\"\n            conn = self._get_connection()\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(\n                    \"\"\"\n                    SELECT am.message_data\n                    FROM message_structure ms\n                    JOIN agent_messages am ON ms.message_id = am.id\n                    WHERE ms.session_id = ? AND ms.branch_id = ?\n                    AND ms.branch_turn_number = ? AND ms.message_type = 'user'\n                    \"\"\",\n                    (self.session_id, self._current_branch_id, turn_number),\n                )\n\n                result = cursor.fetchone()\n                if not result:\n                    raise ValueError(\n                        f\"Turn {turn_number} does not contain a user message \"\n                        f\"in branch '{self._current_branch_id}'\"\n                    )\n\n                message_data = result[0]\n                try:\n                    content = json.loads(message_data).get(\"content\", \"\")\n                    return content[:50] + \"...\" if len(content) &gt; 50 else content\n                except Exception:\n                    return \"Unable to parse content\"\n\n        turn_content = await asyncio.to_thread(_validate_turn)\n\n        # Generate branch name if not provided\n        if branch_name is None:\n            timestamp = int(time.time())\n            branch_name = f\"branch_from_turn_{turn_number}_{timestamp}\"\n\n        # Copy messages before the branch point to the new branch\n        await self._copy_messages_to_new_branch(branch_name, turn_number)\n\n        # Switch to new branch\n        old_branch = self._current_branch_id\n        self._current_branch_id = branch_name\n\n        self._logger.debug(\n            f\"Created branch '{branch_name}' from turn {turn_number} ('{turn_content}') in '{old_branch}'\"  # noqa: E501\n        )\n        return branch_name\n\n    async def create_branch_from_content(\n        self, search_term: str, branch_name: str | None = None\n    ) -&gt; str:\n        \"\"\"Create branch from the first user turn matching the search term.\n\n        Args:\n            search_term: Text to search for in user messages.\n            branch_name: Optional name for the branch (auto-generated if None).\n\n        Returns:\n            The branch_id of the newly created branch.\n\n        Raises:\n            ValueError: If no matching turns are found.\n        \"\"\"\n        matching_turns = await self.find_turns_by_content(search_term)\n        if not matching_turns:\n            raise ValueError(f\"No user turns found containing '{search_term}'\")\n\n        # Use the first (earliest) match\n        turn_number = matching_turns[0][\"turn\"]\n        return await self.create_branch_from_turn(turn_number, branch_name)\n\n    async def switch_to_branch(self, branch_id: str) -&gt; None:\n        \"\"\"Switch to a different branch.\n\n        Args:\n            branch_id: The branch to switch to.\n\n        Raises:\n            ValueError: If the branch doesn't exist.\n        \"\"\"\n\n        # Validate branch exists\n        def _validate_branch():\n            \"\"\"Synchronous helper to validate branch exists.\"\"\"\n            conn = self._get_connection()\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(\n                    \"\"\"\n                    SELECT COUNT(*) FROM message_structure\n                    WHERE session_id = ? AND branch_id = ?\n                \"\"\",\n                    (self.session_id, branch_id),\n                )\n\n                count = cursor.fetchone()[0]\n                if count == 0:\n                    raise ValueError(f\"Branch '{branch_id}' does not exist\")\n\n        await asyncio.to_thread(_validate_branch)\n\n        old_branch = self._current_branch_id\n        self._current_branch_id = branch_id\n        self._logger.info(f\"Switched from branch '{old_branch}' to '{branch_id}'\")\n\n    async def delete_branch(self, branch_id: str, force: bool = False) -&gt; None:\n        \"\"\"Delete a branch and all its associated data.\n\n        Args:\n            branch_id: The branch to delete.\n            force: If True, allows deleting the current branch (will switch to 'main').\n\n        Raises:\n            ValueError: If branch doesn't exist, is 'main', or is current branch without force.\n        \"\"\"\n        if not branch_id or not branch_id.strip():\n            raise ValueError(\"Branch ID cannot be empty\")\n\n        branch_id = branch_id.strip()\n\n        # Protect main branch\n        if branch_id == \"main\":\n            raise ValueError(\"Cannot delete the 'main' branch\")\n\n        # Check if trying to delete current branch\n        if branch_id == self._current_branch_id:\n            if not force:\n                raise ValueError(\n                    f\"Cannot delete current branch '{branch_id}'. Use force=True or switch branches first\"  # noqa: E501\n                )\n            else:\n                # Switch to main before deleting\n                await self.switch_to_branch(\"main\")\n\n        def _delete_sync():\n            \"\"\"Synchronous helper to delete branch and associated data.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                with closing(conn.cursor()) as cursor:\n                    # First verify the branch exists\n                    cursor.execute(\n                        \"\"\"\n                        SELECT COUNT(*) FROM message_structure\n                        WHERE session_id = ? AND branch_id = ?\n                    \"\"\",\n                        (self.session_id, branch_id),\n                    )\n\n                    count = cursor.fetchone()[0]\n                    if count == 0:\n                        raise ValueError(f\"Branch '{branch_id}' does not exist\")\n\n                    # Delete from turn_usage first (foreign key constraint)\n                    cursor.execute(\n                        \"\"\"\n                        DELETE FROM turn_usage\n                        WHERE session_id = ? AND branch_id = ?\n                    \"\"\",\n                        (self.session_id, branch_id),\n                    )\n\n                    usage_deleted = cursor.rowcount\n\n                    # Delete from message_structure\n                    cursor.execute(\n                        \"\"\"\n                        DELETE FROM message_structure\n                        WHERE session_id = ? AND branch_id = ?\n                    \"\"\",\n                        (self.session_id, branch_id),\n                    )\n\n                    structure_deleted = cursor.rowcount\n\n                    conn.commit()\n\n                    return usage_deleted, structure_deleted\n\n        usage_deleted, structure_deleted = await asyncio.to_thread(_delete_sync)\n\n        self._logger.info(\n            f\"Deleted branch '{branch_id}': {structure_deleted} message entries, {usage_deleted} usage entries\"  # noqa: E501\n        )\n\n    async def list_branches(self) -&gt; list[dict[str, Any]]:\n        \"\"\"List all branches in this session.\n\n        Returns:\n            List of dicts with branch info containing:\n                - 'branch_id': Branch identifier\n                - 'message_count': Number of messages in branch\n                - 'user_turns': Number of user turns in branch\n                - 'is_current': Whether this is the current branch\n                - 'created_at': When the branch was first created\n        \"\"\"\n\n        def _list_branches_sync():\n            \"\"\"Synchronous helper to list all branches.\"\"\"\n            conn = self._get_connection()\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(\n                    \"\"\"\n                    SELECT\n                        ms.branch_id,\n                        COUNT(*) as message_count,\n                        COUNT(CASE WHEN ms.message_type = 'user' THEN 1 END) as user_turns,\n                        MIN(ms.created_at) as created_at\n                    FROM message_structure ms\n                    WHERE ms.session_id = ?\n                    GROUP BY ms.branch_id\n                    ORDER BY created_at\n                \"\"\",\n                    (self.session_id,),\n                )\n\n                branches = []\n                for row in cursor.fetchall():\n                    branch_id, msg_count, user_turns, created_at = row\n                    branches.append(\n                        {\n                            \"branch_id\": branch_id,\n                            \"message_count\": msg_count,\n                            \"user_turns\": user_turns,\n                            \"is_current\": branch_id == self._current_branch_id,\n                            \"created_at\": created_at,\n                        }\n                    )\n\n                return branches\n\n        return await asyncio.to_thread(_list_branches_sync)\n\n    async def _copy_messages_to_new_branch(self, new_branch_id: str, from_turn_number: int) -&gt; None:\n        \"\"\"Copy messages before the branch point to the new branch.\n\n        Args:\n            new_branch_id: The ID of the new branch to copy messages to.\n            from_turn_number: The turn number to copy messages up to (exclusive).\n        \"\"\"\n\n        def _copy_sync():\n            \"\"\"Synchronous helper to copy messages to new branch.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                with closing(conn.cursor()) as cursor:\n                    # Get all messages before the branch point\n                    cursor.execute(\n                        \"\"\"\n                        SELECT\n                            ms.message_id,\n                            ms.message_type,\n                            ms.sequence_number,\n                            ms.user_turn_number,\n                            ms.branch_turn_number,\n                            ms.tool_name\n                        FROM message_structure ms\n                        WHERE ms.session_id = ? AND ms.branch_id = ?\n                        AND ms.branch_turn_number &lt; ?\n                        ORDER BY ms.sequence_number\n                    \"\"\",\n                        (self.session_id, self._current_branch_id, from_turn_number),\n                    )\n\n                    messages_to_copy = cursor.fetchall()\n\n                    if messages_to_copy:\n                        # Get the max sequence number for the new inserts\n                        cursor.execute(\n                            \"\"\"\n                            SELECT COALESCE(MAX(sequence_number), 0)\n                            FROM message_structure\n                            WHERE session_id = ?\n                        \"\"\",\n                            (self.session_id,),\n                        )\n\n                        seq_start = cursor.fetchone()[0]\n\n                        # Insert copied messages with new branch_id\n                        new_structure_data = []\n                        for i, (\n                            msg_id,\n                            msg_type,\n                            _,\n                            user_turn,\n                            branch_turn,\n                            tool_name,\n                        ) in enumerate(messages_to_copy):\n                            new_structure_data.append(\n                                (\n                                    self.session_id,\n                                    msg_id,  # Same message_id (sharing the actual message data)\n                                    new_branch_id,\n                                    msg_type,\n                                    seq_start + i + 1,  # New sequence number\n                                    user_turn,  # Keep same global turn number\n                                    branch_turn,  # Keep same branch turn number\n                                    tool_name,\n                                )\n                            )\n\n                        cursor.executemany(\n                            \"\"\"\n                            INSERT INTO message_structure\n                            (session_id, message_id, branch_id, message_type, sequence_number,\n                             user_turn_number, branch_turn_number, tool_name)\n                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n                        \"\"\",\n                            new_structure_data,\n                        )\n\n                    conn.commit()\n\n        await asyncio.to_thread(_copy_sync)\n\n    async def get_conversation_turns(self, branch_id: str | None = None) -&gt; list[dict[str, Any]]:\n        \"\"\"Get user turns with content for easy browsing and branching decisions.\n\n        Args:\n            branch_id: Branch to get turns from (current branch if None).\n\n        Returns:\n            List of dicts with turn info containing:\n                - 'turn': Branch turn number\n                - 'content': User message content (truncated)\n                - 'full_content': Full user message content\n                - 'timestamp': When the turn was created\n                - 'can_branch': Always True (all user messages can branch)\n        \"\"\"\n        if branch_id is None:\n            branch_id = self._current_branch_id\n\n        def _get_turns_sync():\n            \"\"\"Synchronous helper to get conversation turns.\"\"\"\n            conn = self._get_connection()\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(\n                    \"\"\"\n                    SELECT\n                        ms.branch_turn_number,\n                        am.message_data,\n                        ms.created_at\n                    FROM message_structure ms\n                    JOIN agent_messages am ON ms.message_id = am.id\n                    WHERE ms.session_id = ? AND ms.branch_id = ?\n                    AND ms.message_type = 'user'\n                    ORDER BY ms.branch_turn_number\n                \"\"\",\n                    (self.session_id, branch_id),\n                )\n\n                turns = []\n                for row in cursor.fetchall():\n                    turn_num, message_data, created_at = row\n                    try:\n                        content = json.loads(message_data).get(\"content\", \"\")\n                        turns.append(\n                            {\n                                \"turn\": turn_num,\n                                \"content\": content[:100] + \"...\" if len(content) &gt; 100 else content,\n                                \"full_content\": content,\n                                \"timestamp\": created_at,\n                                \"can_branch\": True,\n                            }\n                        )\n                    except (json.JSONDecodeError, AttributeError):\n                        continue\n\n                return turns\n\n        return await asyncio.to_thread(_get_turns_sync)\n\n    async def find_turns_by_content(\n        self, search_term: str, branch_id: str | None = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Find user turns containing specific content.\n\n        Args:\n            search_term: Text to search for in user messages.\n            branch_id: Branch to search in (current branch if None).\n\n        Returns:\n            List of matching turns with same format as get_conversation_turns().\n        \"\"\"\n        if branch_id is None:\n            branch_id = self._current_branch_id\n\n        def _search_sync():\n            \"\"\"Synchronous helper to search turns by content.\"\"\"\n            conn = self._get_connection()\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(\n                    \"\"\"\n                    SELECT\n                        ms.branch_turn_number,\n                        am.message_data,\n                        ms.created_at\n                    FROM message_structure ms\n                    JOIN agent_messages am ON ms.message_id = am.id\n                    WHERE ms.session_id = ? AND ms.branch_id = ?\n                    AND ms.message_type = 'user'\n                    AND am.message_data LIKE ?\n                    ORDER BY ms.branch_turn_number\n                \"\"\",\n                    (self.session_id, branch_id, f\"%{search_term}%\"),\n                )\n\n                matches = []\n                for row in cursor.fetchall():\n                    turn_num, message_data, created_at = row\n                    try:\n                        content = json.loads(message_data).get(\"content\", \"\")\n                        matches.append(\n                            {\n                                \"turn\": turn_num,\n                                \"content\": content,\n                                \"full_content\": content,\n                                \"timestamp\": created_at,\n                                \"can_branch\": True,\n                            }\n                        )\n                    except (json.JSONDecodeError, AttributeError):\n                        continue\n\n                return matches\n\n        return await asyncio.to_thread(_search_sync)\n\n    async def get_conversation_by_turns(\n        self, branch_id: str | None = None\n    ) -&gt; dict[int, list[dict[str, str | None]]]:\n        \"\"\"Get conversation grouped by user turns for specified branch.\n\n        Args:\n            branch_id: Branch to get conversation from (current branch if None).\n\n        Returns:\n            Dictionary mapping turn numbers to lists of message metadata.\n        \"\"\"\n        if branch_id is None:\n            branch_id = self._current_branch_id\n\n        def _get_conversation_sync():\n            \"\"\"Synchronous helper to get conversation by turns.\"\"\"\n            conn = self._get_connection()\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(\n                    \"\"\"\n                    SELECT user_turn_number, message_type, tool_name\n                    FROM message_structure\n                    WHERE session_id = ? AND branch_id = ?\n                    ORDER BY sequence_number\n                \"\"\",\n                    (self.session_id, branch_id),\n                )\n\n                turns: dict[int, list[dict[str, str | None]]] = {}\n                for row in cursor.fetchall():\n                    turn_num, msg_type, tool_name = row\n                    if turn_num not in turns:\n                        turns[turn_num] = []\n                    turns[turn_num].append({\"type\": msg_type, \"tool_name\": tool_name})\n                return turns\n\n        return await asyncio.to_thread(_get_conversation_sync)\n\n    async def get_tool_usage(self, branch_id: str | None = None) -&gt; list[tuple[str, int, int]]:\n        \"\"\"Get all tool usage by turn for specified branch.\n\n        Args:\n            branch_id: Branch to get tool usage from (current branch if None).\n\n        Returns:\n            List of tuples containing (tool_name, usage_count, turn_number).\n        \"\"\"\n        if branch_id is None:\n            branch_id = self._current_branch_id\n\n        def _get_tool_usage_sync():\n            \"\"\"Synchronous helper to get tool usage statistics.\"\"\"\n            conn = self._get_connection()\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(\n                    \"\"\"\n                    SELECT tool_name, COUNT(*), user_turn_number\n                    FROM message_structure\n                    WHERE session_id = ? AND branch_id = ? AND message_type IN (\n                        'tool_call', 'function_call', 'computer_call', 'file_search_call',\n                        'web_search_call', 'code_interpreter_call', 'custom_tool_call',\n                        'mcp_call', 'mcp_approval_request'\n                    )\n                    GROUP BY tool_name, user_turn_number\n                    ORDER BY user_turn_number\n                \"\"\",\n                    (self.session_id, branch_id),\n                )\n                return cursor.fetchall()\n\n        return await asyncio.to_thread(_get_tool_usage_sync)\n\n    async def get_session_usage(self, branch_id: str | None = None) -&gt; dict[str, int] | None:\n        \"\"\"Get cumulative usage for session or specific branch.\n\n        Args:\n            branch_id: If provided, only get usage for that branch. If None, get all branches.\n\n        Returns:\n            Dictionary with usage statistics or None if no usage data found.\n        \"\"\"\n\n        def _get_usage_sync():\n            \"\"\"Synchronous helper to get session usage data.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                if branch_id:\n                    # Branch-specific usage\n                    query = \"\"\"\n                        SELECT\n                            SUM(requests) as total_requests,\n                            SUM(input_tokens) as total_input_tokens,\n                            SUM(output_tokens) as total_output_tokens,\n                            SUM(total_tokens) as total_total_tokens,\n                            COUNT(*) as total_turns\n                        FROM turn_usage\n                        WHERE session_id = ? AND branch_id = ?\n                    \"\"\"\n                    params: tuple[str, ...] = (self.session_id, branch_id)\n                else:\n                    # All branches\n                    query = \"\"\"\n                        SELECT\n                            SUM(requests) as total_requests,\n                            SUM(input_tokens) as total_input_tokens,\n                            SUM(output_tokens) as total_output_tokens,\n                            SUM(total_tokens) as total_total_tokens,\n                            COUNT(*) as total_turns\n                        FROM turn_usage\n                        WHERE session_id = ?\n                    \"\"\"\n                    params = (self.session_id,)\n\n                with closing(conn.cursor()) as cursor:\n                    cursor.execute(query, params)\n                    row = cursor.fetchone()\n\n                    if row and row[0] is not None:\n                        return {\n                            \"requests\": row[0] or 0,\n                            \"input_tokens\": row[1] or 0,\n                            \"output_tokens\": row[2] or 0,\n                            \"total_tokens\": row[3] or 0,\n                            \"total_turns\": row[4] or 0,\n                        }\n                    return None\n\n        result = await asyncio.to_thread(_get_usage_sync)\n\n        return cast(Union[dict[str, int], None], result)\n\n    async def get_turn_usage(\n        self,\n        user_turn_number: int | None = None,\n        branch_id: str | None = None,\n    ) -&gt; list[dict[str, Any]] | dict[str, Any]:\n        \"\"\"Get usage statistics by turn with full JSON token details.\n\n        Args:\n            user_turn_number: Specific turn to get usage for. If None, returns all turns.\n            branch_id: Branch to get usage from (current branch if None).\n\n        Returns:\n            Dictionary with usage data for specific turn, or list of dictionaries for all turns.\n        \"\"\"\n\n        if branch_id is None:\n            branch_id = self._current_branch_id\n\n        def _get_turn_usage_sync():\n            \"\"\"Synchronous helper to get turn usage statistics.\"\"\"\n            conn = self._get_connection()\n\n            if user_turn_number is not None:\n                query = \"\"\"\n                    SELECT requests, input_tokens, output_tokens, total_tokens,\n                           input_tokens_details, output_tokens_details\n                    FROM turn_usage\n                    WHERE session_id = ? AND branch_id = ? AND user_turn_number = ?\n                \"\"\"\n\n                with closing(conn.cursor()) as cursor:\n                    cursor.execute(query, (self.session_id, branch_id, user_turn_number))\n                    row = cursor.fetchone()\n\n                    if row:\n                        # Parse JSON details if present\n                        input_details = None\n                        output_details = None\n\n                        if row[4]:  # input_tokens_details\n                            try:\n                                input_details = json.loads(row[4])\n                            except json.JSONDecodeError:\n                                pass\n\n                        if row[5]:  # output_tokens_details\n                            try:\n                                output_details = json.loads(row[5])\n                            except json.JSONDecodeError:\n                                pass\n\n                        return {\n                            \"requests\": row[0],\n                            \"input_tokens\": row[1],\n                            \"output_tokens\": row[2],\n                            \"total_tokens\": row[3],\n                            \"input_tokens_details\": input_details,\n                            \"output_tokens_details\": output_details,\n                        }\n                    return {}\n            else:\n                query = \"\"\"\n                    SELECT user_turn_number, requests, input_tokens, output_tokens,\n                           total_tokens, input_tokens_details, output_tokens_details\n                    FROM turn_usage\n                    WHERE session_id = ? AND branch_id = ?\n                    ORDER BY user_turn_number\n                \"\"\"\n\n                with closing(conn.cursor()) as cursor:\n                    cursor.execute(query, (self.session_id, branch_id))\n                    results = []\n                    for row in cursor.fetchall():\n                        # Parse JSON details if present\n                        input_details = None\n                        output_details = None\n\n                        if row[5]:  # input_tokens_details\n                            try:\n                                input_details = json.loads(row[5])\n                            except json.JSONDecodeError:\n                                pass\n\n                        if row[6]:  # output_tokens_details\n                            try:\n                                output_details = json.loads(row[6])\n                            except json.JSONDecodeError:\n                                pass\n\n                        results.append(\n                            {\n                                \"user_turn_number\": row[0],\n                                \"requests\": row[1],\n                                \"input_tokens\": row[2],\n                                \"output_tokens\": row[3],\n                                \"total_tokens\": row[4],\n                                \"input_tokens_details\": input_details,\n                                \"output_tokens_details\": output_details,\n                            }\n                        )\n                    return results\n\n        result = await asyncio.to_thread(_get_turn_usage_sync)\n\n        return cast(Union[list[dict[str, Any]], dict[str, Any]], result)\n\n    async def _update_turn_usage_internal(self, user_turn_number: int, usage_data: Usage) -&gt; None:\n        \"\"\"Internal method to update usage for a specific turn with full JSON details.\n\n        Args:\n            user_turn_number: The turn number to update usage for.\n            usage_data: The usage data to store.\n        \"\"\"\n\n        def _update_sync():\n            \"\"\"Synchronous helper to update turn usage data.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Serialize token details as JSON\n                input_details_json = None\n                output_details_json = None\n\n                if hasattr(usage_data, \"input_tokens_details\") and usage_data.input_tokens_details:\n                    try:\n                        input_details_json = json.dumps(usage_data.input_tokens_details.__dict__)\n                    except (TypeError, ValueError) as e:\n                        self._logger.warning(f\"Failed to serialize input tokens details: {e}\")\n                        input_details_json = None\n\n                    if (\n                        hasattr(usage_data, \"output_tokens_details\")\n                        and usage_data.output_tokens_details\n                    ):\n                        try:\n                            output_details_json = json.dumps(\n                                usage_data.output_tokens_details.__dict__\n                            )\n                        except (TypeError, ValueError) as e:\n                            self._logger.warning(f\"Failed to serialize output tokens details: {e}\")\n                            output_details_json = None\n\n                with closing(conn.cursor()) as cursor:\n                    cursor.execute(\n                        \"\"\"\n                        INSERT OR REPLACE INTO turn_usage\n                        (session_id, branch_id, user_turn_number, requests, input_tokens, output_tokens,\n                         total_tokens, input_tokens_details, output_tokens_details)\n                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                    \"\"\",  # noqa: E501\n                        (\n                            self.session_id,\n                            self._current_branch_id,\n                            user_turn_number,\n                            usage_data.requests or 0,\n                            usage_data.input_tokens or 0,\n                            usage_data.output_tokens or 0,\n                            usage_data.total_tokens or 0,\n                            input_details_json,\n                            output_details_json,\n                        ),\n                    )\n                    conn.commit()\n\n        await asyncio.to_thread(_update_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    create_tables: bool = False,\n    logger: Logger | None = None,\n    session_settings: SessionSettings | None = None,\n    **kwargs,\n)\n</code></pre> <p>Initialize the AdvancedSQLiteSession.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The ID of the session</p> required <code>db_path</code> <code>str | Path</code> <p>The path to the SQLite database file. Defaults to <code>:memory:</code> for in-memory storage</p> <code>':memory:'</code> <code>create_tables</code> <code>bool</code> <p>Whether to create the structure tables</p> <code>False</code> <code>logger</code> <code>Logger | None</code> <p>The logger to use. Defaults to the module logger</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the superclass</p> <code>{}</code> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>def __init__(\n    self,\n    *,\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    create_tables: bool = False,\n    logger: logging.Logger | None = None,\n    session_settings: SessionSettings | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the AdvancedSQLiteSession.\n\n    Args:\n        session_id: The ID of the session\n        db_path: The path to the SQLite database file. Defaults to `:memory:` for in-memory storage\n        create_tables: Whether to create the structure tables\n        logger: The logger to use. Defaults to the module logger\n        **kwargs: Additional keyword arguments to pass to the superclass\n    \"\"\"  # noqa: E501\n    super().__init__(\n        session_id=session_id,\n        db_path=db_path,\n        session_settings=session_settings,\n        **kwargs,\n    )\n    if create_tables:\n        self._init_structure_tables()\n    self._current_branch_id = \"main\"\n    self._logger = logger or logging.getLogger(__name__)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add items to the session.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>The items to add to the session</p> required Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add items to the session.\n\n    Args:\n        items: The items to add to the session\n    \"\"\"\n    # Add to base table first\n    await super().add_items(items)\n\n    # Extract structure metadata with precise sequencing\n    if items:\n        await self._add_structure_metadata(items)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None, branch_id: str | None = None\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Get items from current or specified branch.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to return. If None, uses session_settings.limit.</p> <code>None</code> <code>branch_id</code> <code>str | None</code> <p>Branch to get items from. If None, uses current branch.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of conversation items from the specified branch.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def get_items(\n    self,\n    limit: int | None = None,\n    branch_id: str | None = None,\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Get items from current or specified branch.\n\n    Args:\n        limit: Maximum number of items to return. If None, uses session_settings.limit.\n        branch_id: Branch to get items from. If None, uses current branch.\n\n    Returns:\n        List of conversation items from the specified branch.\n    \"\"\"\n    session_limit = resolve_session_limit(limit, self.session_settings)\n\n    if branch_id is None:\n        branch_id = self._current_branch_id\n\n        # Get all items for this branch\n        def _get_all_items_sync():\n            \"\"\"Synchronous helper to get all items for a branch.\"\"\"\n            conn = self._get_connection()\n            # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n            with self._lock if self._is_memory_db else threading.Lock():\n                with closing(conn.cursor()) as cursor:\n                    if session_limit is None:\n                        cursor.execute(\n                            \"\"\"\n                            SELECT m.message_data\n                            FROM agent_messages m\n                            JOIN message_structure s ON m.id = s.message_id\n                            WHERE m.session_id = ? AND s.branch_id = ?\n                            ORDER BY s.sequence_number ASC\n                        \"\"\",\n                            (self.session_id, branch_id),\n                        )\n                    else:\n                        cursor.execute(\n                            \"\"\"\n                            SELECT m.message_data\n                            FROM agent_messages m\n                            JOIN message_structure s ON m.id = s.message_id\n                            WHERE m.session_id = ? AND s.branch_id = ?\n                            ORDER BY s.sequence_number DESC\n                            LIMIT ?\n                        \"\"\",\n                            (self.session_id, branch_id, session_limit),\n                        )\n\n                    rows = cursor.fetchall()\n                    if session_limit is not None:\n                        rows = list(reversed(rows))\n\n                items = []\n                for (message_data,) in rows:\n                    try:\n                        item = json.loads(message_data)\n                        items.append(item)\n                    except json.JSONDecodeError:\n                        continue\n                return items\n\n        return await asyncio.to_thread(_get_all_items_sync)\n\n    def _get_items_sync():\n        \"\"\"Synchronous helper to get items for a specific branch.\"\"\"\n        conn = self._get_connection()\n        # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n        with self._lock if self._is_memory_db else threading.Lock():\n            with closing(conn.cursor()) as cursor:\n                # Get message IDs in correct order for this branch\n                if session_limit is None:\n                    cursor.execute(\n                        \"\"\"\n                        SELECT m.message_data\n                        FROM agent_messages m\n                        JOIN message_structure s ON m.id = s.message_id\n                        WHERE m.session_id = ? AND s.branch_id = ?\n                        ORDER BY s.sequence_number ASC\n                    \"\"\",\n                        (self.session_id, branch_id),\n                    )\n                else:\n                    cursor.execute(\n                        \"\"\"\n                        SELECT m.message_data\n                        FROM agent_messages m\n                        JOIN message_structure s ON m.id = s.message_id\n                        WHERE m.session_id = ? AND s.branch_id = ?\n                        ORDER BY s.sequence_number DESC\n                        LIMIT ?\n                    \"\"\",\n                        (self.session_id, branch_id, session_limit),\n                    )\n\n                rows = cursor.fetchall()\n                if session_limit is not None:\n                    rows = list(reversed(rows))\n\n            items = []\n            for (message_data,) in rows:\n                try:\n                    item = json.loads(message_data)\n                    items.append(item)\n                except json.JSONDecodeError:\n                    continue\n            return items\n\n    return await asyncio.to_thread(_get_items_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.store_run_usage","title":"store_run_usage  <code>async</code>","text":"<pre><code>store_run_usage(result: RunResult) -&gt; None\n</code></pre> <p>Store usage data for the current conversation turn.</p> <p>This is designed to be called after <code>Runner.run()</code> completes. Session-level usage can be aggregated from turn data when needed.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>RunResult</code> <p>The result from the run</p> required Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def store_run_usage(self, result: RunResult) -&gt; None:\n    \"\"\"Store usage data for the current conversation turn.\n\n    This is designed to be called after `Runner.run()` completes.\n    Session-level usage can be aggregated from turn data when needed.\n\n    Args:\n        result: The result from the run\n    \"\"\"\n    try:\n        if result.context_wrapper.usage is not None:\n            # Get the current turn number for this branch\n            current_turn = self._get_current_turn_number()\n            # Only update turn-level usage - session usage is aggregated on demand\n            await self._update_turn_usage_internal(current_turn, result.context_wrapper.usage)\n    except Exception as e:\n        self._logger.error(f\"Failed to store usage for session {self.session_id}: {e}\")\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.create_branch_from_turn","title":"create_branch_from_turn  <code>async</code>","text":"<pre><code>create_branch_from_turn(\n    turn_number: int, branch_name: str | None = None\n) -&gt; str\n</code></pre> <p>Create a new branch starting from a specific user message turn.</p> <p>Parameters:</p> Name Type Description Default <code>turn_number</code> <code>int</code> <p>The branch turn number of the user message to branch from</p> required <code>branch_name</code> <code>str | None</code> <p>Optional name for the branch (auto-generated if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The branch_id of the newly created branch</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If turn doesn't exist or doesn't contain a user message</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def create_branch_from_turn(\n    self, turn_number: int, branch_name: str | None = None\n) -&gt; str:\n    \"\"\"Create a new branch starting from a specific user message turn.\n\n    Args:\n        turn_number: The branch turn number of the user message to branch from\n        branch_name: Optional name for the branch (auto-generated if None)\n\n    Returns:\n        The branch_id of the newly created branch\n\n    Raises:\n        ValueError: If turn doesn't exist or doesn't contain a user message\n    \"\"\"\n    import time\n\n    # Validate the turn exists and contains a user message\n    def _validate_turn():\n        \"\"\"Synchronous helper to validate turn exists and contains user message.\"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT am.message_data\n                FROM message_structure ms\n                JOIN agent_messages am ON ms.message_id = am.id\n                WHERE ms.session_id = ? AND ms.branch_id = ?\n                AND ms.branch_turn_number = ? AND ms.message_type = 'user'\n                \"\"\",\n                (self.session_id, self._current_branch_id, turn_number),\n            )\n\n            result = cursor.fetchone()\n            if not result:\n                raise ValueError(\n                    f\"Turn {turn_number} does not contain a user message \"\n                    f\"in branch '{self._current_branch_id}'\"\n                )\n\n            message_data = result[0]\n            try:\n                content = json.loads(message_data).get(\"content\", \"\")\n                return content[:50] + \"...\" if len(content) &gt; 50 else content\n            except Exception:\n                return \"Unable to parse content\"\n\n    turn_content = await asyncio.to_thread(_validate_turn)\n\n    # Generate branch name if not provided\n    if branch_name is None:\n        timestamp = int(time.time())\n        branch_name = f\"branch_from_turn_{turn_number}_{timestamp}\"\n\n    # Copy messages before the branch point to the new branch\n    await self._copy_messages_to_new_branch(branch_name, turn_number)\n\n    # Switch to new branch\n    old_branch = self._current_branch_id\n    self._current_branch_id = branch_name\n\n    self._logger.debug(\n        f\"Created branch '{branch_name}' from turn {turn_number} ('{turn_content}') in '{old_branch}'\"  # noqa: E501\n    )\n    return branch_name\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.create_branch_from_content","title":"create_branch_from_content  <code>async</code>","text":"<pre><code>create_branch_from_content(\n    search_term: str, branch_name: str | None = None\n) -&gt; str\n</code></pre> <p>Create branch from the first user turn matching the search term.</p> <p>Parameters:</p> Name Type Description Default <code>search_term</code> <code>str</code> <p>Text to search for in user messages.</p> required <code>branch_name</code> <code>str | None</code> <p>Optional name for the branch (auto-generated if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The branch_id of the newly created branch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching turns are found.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def create_branch_from_content(\n    self, search_term: str, branch_name: str | None = None\n) -&gt; str:\n    \"\"\"Create branch from the first user turn matching the search term.\n\n    Args:\n        search_term: Text to search for in user messages.\n        branch_name: Optional name for the branch (auto-generated if None).\n\n    Returns:\n        The branch_id of the newly created branch.\n\n    Raises:\n        ValueError: If no matching turns are found.\n    \"\"\"\n    matching_turns = await self.find_turns_by_content(search_term)\n    if not matching_turns:\n        raise ValueError(f\"No user turns found containing '{search_term}'\")\n\n    # Use the first (earliest) match\n    turn_number = matching_turns[0][\"turn\"]\n    return await self.create_branch_from_turn(turn_number, branch_name)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.switch_to_branch","title":"switch_to_branch  <code>async</code>","text":"<pre><code>switch_to_branch(branch_id: str) -&gt; None\n</code></pre> <p>Switch to a different branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str</code> <p>The branch to switch to.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the branch doesn't exist.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def switch_to_branch(self, branch_id: str) -&gt; None:\n    \"\"\"Switch to a different branch.\n\n    Args:\n        branch_id: The branch to switch to.\n\n    Raises:\n        ValueError: If the branch doesn't exist.\n    \"\"\"\n\n    # Validate branch exists\n    def _validate_branch():\n        \"\"\"Synchronous helper to validate branch exists.\"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT COUNT(*) FROM message_structure\n                WHERE session_id = ? AND branch_id = ?\n            \"\"\",\n                (self.session_id, branch_id),\n            )\n\n            count = cursor.fetchone()[0]\n            if count == 0:\n                raise ValueError(f\"Branch '{branch_id}' does not exist\")\n\n    await asyncio.to_thread(_validate_branch)\n\n    old_branch = self._current_branch_id\n    self._current_branch_id = branch_id\n    self._logger.info(f\"Switched from branch '{old_branch}' to '{branch_id}'\")\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.delete_branch","title":"delete_branch  <code>async</code>","text":"<pre><code>delete_branch(branch_id: str, force: bool = False) -&gt; None\n</code></pre> <p>Delete a branch and all its associated data.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str</code> <p>The branch to delete.</p> required <code>force</code> <code>bool</code> <p>If True, allows deleting the current branch (will switch to 'main').</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If branch doesn't exist, is 'main', or is current branch without force.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def delete_branch(self, branch_id: str, force: bool = False) -&gt; None:\n    \"\"\"Delete a branch and all its associated data.\n\n    Args:\n        branch_id: The branch to delete.\n        force: If True, allows deleting the current branch (will switch to 'main').\n\n    Raises:\n        ValueError: If branch doesn't exist, is 'main', or is current branch without force.\n    \"\"\"\n    if not branch_id or not branch_id.strip():\n        raise ValueError(\"Branch ID cannot be empty\")\n\n    branch_id = branch_id.strip()\n\n    # Protect main branch\n    if branch_id == \"main\":\n        raise ValueError(\"Cannot delete the 'main' branch\")\n\n    # Check if trying to delete current branch\n    if branch_id == self._current_branch_id:\n        if not force:\n            raise ValueError(\n                f\"Cannot delete current branch '{branch_id}'. Use force=True or switch branches first\"  # noqa: E501\n            )\n        else:\n            # Switch to main before deleting\n            await self.switch_to_branch(\"main\")\n\n    def _delete_sync():\n        \"\"\"Synchronous helper to delete branch and associated data.\"\"\"\n        conn = self._get_connection()\n        # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n        with self._lock if self._is_memory_db else threading.Lock():\n            with closing(conn.cursor()) as cursor:\n                # First verify the branch exists\n                cursor.execute(\n                    \"\"\"\n                    SELECT COUNT(*) FROM message_structure\n                    WHERE session_id = ? AND branch_id = ?\n                \"\"\",\n                    (self.session_id, branch_id),\n                )\n\n                count = cursor.fetchone()[0]\n                if count == 0:\n                    raise ValueError(f\"Branch '{branch_id}' does not exist\")\n\n                # Delete from turn_usage first (foreign key constraint)\n                cursor.execute(\n                    \"\"\"\n                    DELETE FROM turn_usage\n                    WHERE session_id = ? AND branch_id = ?\n                \"\"\",\n                    (self.session_id, branch_id),\n                )\n\n                usage_deleted = cursor.rowcount\n\n                # Delete from message_structure\n                cursor.execute(\n                    \"\"\"\n                    DELETE FROM message_structure\n                    WHERE session_id = ? AND branch_id = ?\n                \"\"\",\n                    (self.session_id, branch_id),\n                )\n\n                structure_deleted = cursor.rowcount\n\n                conn.commit()\n\n                return usage_deleted, structure_deleted\n\n    usage_deleted, structure_deleted = await asyncio.to_thread(_delete_sync)\n\n    self._logger.info(\n        f\"Deleted branch '{branch_id}': {structure_deleted} message entries, {usage_deleted} usage entries\"  # noqa: E501\n    )\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.list_branches","title":"list_branches  <code>async</code>","text":"<pre><code>list_branches() -&gt; list[dict[str, Any]]\n</code></pre> <p>List all branches in this session.</p> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dicts with branch info containing: - 'branch_id': Branch identifier - 'message_count': Number of messages in branch - 'user_turns': Number of user turns in branch - 'is_current': Whether this is the current branch - 'created_at': When the branch was first created</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def list_branches(self) -&gt; list[dict[str, Any]]:\n    \"\"\"List all branches in this session.\n\n    Returns:\n        List of dicts with branch info containing:\n            - 'branch_id': Branch identifier\n            - 'message_count': Number of messages in branch\n            - 'user_turns': Number of user turns in branch\n            - 'is_current': Whether this is the current branch\n            - 'created_at': When the branch was first created\n    \"\"\"\n\n    def _list_branches_sync():\n        \"\"\"Synchronous helper to list all branches.\"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT\n                    ms.branch_id,\n                    COUNT(*) as message_count,\n                    COUNT(CASE WHEN ms.message_type = 'user' THEN 1 END) as user_turns,\n                    MIN(ms.created_at) as created_at\n                FROM message_structure ms\n                WHERE ms.session_id = ?\n                GROUP BY ms.branch_id\n                ORDER BY created_at\n            \"\"\",\n                (self.session_id,),\n            )\n\n            branches = []\n            for row in cursor.fetchall():\n                branch_id, msg_count, user_turns, created_at = row\n                branches.append(\n                    {\n                        \"branch_id\": branch_id,\n                        \"message_count\": msg_count,\n                        \"user_turns\": user_turns,\n                        \"is_current\": branch_id == self._current_branch_id,\n                        \"created_at\": created_at,\n                    }\n                )\n\n            return branches\n\n    return await asyncio.to_thread(_list_branches_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.get_conversation_turns","title":"get_conversation_turns  <code>async</code>","text":"<pre><code>get_conversation_turns(\n    branch_id: str | None = None,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Get user turns with content for easy browsing and branching decisions.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str | None</code> <p>Branch to get turns from (current branch if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dicts with turn info containing: - 'turn': Branch turn number - 'content': User message content (truncated) - 'full_content': Full user message content - 'timestamp': When the turn was created - 'can_branch': Always True (all user messages can branch)</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def get_conversation_turns(self, branch_id: str | None = None) -&gt; list[dict[str, Any]]:\n    \"\"\"Get user turns with content for easy browsing and branching decisions.\n\n    Args:\n        branch_id: Branch to get turns from (current branch if None).\n\n    Returns:\n        List of dicts with turn info containing:\n            - 'turn': Branch turn number\n            - 'content': User message content (truncated)\n            - 'full_content': Full user message content\n            - 'timestamp': When the turn was created\n            - 'can_branch': Always True (all user messages can branch)\n    \"\"\"\n    if branch_id is None:\n        branch_id = self._current_branch_id\n\n    def _get_turns_sync():\n        \"\"\"Synchronous helper to get conversation turns.\"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT\n                    ms.branch_turn_number,\n                    am.message_data,\n                    ms.created_at\n                FROM message_structure ms\n                JOIN agent_messages am ON ms.message_id = am.id\n                WHERE ms.session_id = ? AND ms.branch_id = ?\n                AND ms.message_type = 'user'\n                ORDER BY ms.branch_turn_number\n            \"\"\",\n                (self.session_id, branch_id),\n            )\n\n            turns = []\n            for row in cursor.fetchall():\n                turn_num, message_data, created_at = row\n                try:\n                    content = json.loads(message_data).get(\"content\", \"\")\n                    turns.append(\n                        {\n                            \"turn\": turn_num,\n                            \"content\": content[:100] + \"...\" if len(content) &gt; 100 else content,\n                            \"full_content\": content,\n                            \"timestamp\": created_at,\n                            \"can_branch\": True,\n                        }\n                    )\n                except (json.JSONDecodeError, AttributeError):\n                    continue\n\n            return turns\n\n    return await asyncio.to_thread(_get_turns_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.find_turns_by_content","title":"find_turns_by_content  <code>async</code>","text":"<pre><code>find_turns_by_content(\n    search_term: str, branch_id: str | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Find user turns containing specific content.</p> <p>Parameters:</p> Name Type Description Default <code>search_term</code> <code>str</code> <p>Text to search for in user messages.</p> required <code>branch_id</code> <code>str | None</code> <p>Branch to search in (current branch if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of matching turns with same format as get_conversation_turns().</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def find_turns_by_content(\n    self, search_term: str, branch_id: str | None = None\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Find user turns containing specific content.\n\n    Args:\n        search_term: Text to search for in user messages.\n        branch_id: Branch to search in (current branch if None).\n\n    Returns:\n        List of matching turns with same format as get_conversation_turns().\n    \"\"\"\n    if branch_id is None:\n        branch_id = self._current_branch_id\n\n    def _search_sync():\n        \"\"\"Synchronous helper to search turns by content.\"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT\n                    ms.branch_turn_number,\n                    am.message_data,\n                    ms.created_at\n                FROM message_structure ms\n                JOIN agent_messages am ON ms.message_id = am.id\n                WHERE ms.session_id = ? AND ms.branch_id = ?\n                AND ms.message_type = 'user'\n                AND am.message_data LIKE ?\n                ORDER BY ms.branch_turn_number\n            \"\"\",\n                (self.session_id, branch_id, f\"%{search_term}%\"),\n            )\n\n            matches = []\n            for row in cursor.fetchall():\n                turn_num, message_data, created_at = row\n                try:\n                    content = json.loads(message_data).get(\"content\", \"\")\n                    matches.append(\n                        {\n                            \"turn\": turn_num,\n                            \"content\": content,\n                            \"full_content\": content,\n                            \"timestamp\": created_at,\n                            \"can_branch\": True,\n                        }\n                    )\n                except (json.JSONDecodeError, AttributeError):\n                    continue\n\n            return matches\n\n    return await asyncio.to_thread(_search_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.get_conversation_by_turns","title":"get_conversation_by_turns  <code>async</code>","text":"<pre><code>get_conversation_by_turns(\n    branch_id: str | None = None,\n) -&gt; dict[int, list[dict[str, str | None]]]\n</code></pre> <p>Get conversation grouped by user turns for specified branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str | None</code> <p>Branch to get conversation from (current branch if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, list[dict[str, str | None]]]</code> <p>Dictionary mapping turn numbers to lists of message metadata.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def get_conversation_by_turns(\n    self, branch_id: str | None = None\n) -&gt; dict[int, list[dict[str, str | None]]]:\n    \"\"\"Get conversation grouped by user turns for specified branch.\n\n    Args:\n        branch_id: Branch to get conversation from (current branch if None).\n\n    Returns:\n        Dictionary mapping turn numbers to lists of message metadata.\n    \"\"\"\n    if branch_id is None:\n        branch_id = self._current_branch_id\n\n    def _get_conversation_sync():\n        \"\"\"Synchronous helper to get conversation by turns.\"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT user_turn_number, message_type, tool_name\n                FROM message_structure\n                WHERE session_id = ? AND branch_id = ?\n                ORDER BY sequence_number\n            \"\"\",\n                (self.session_id, branch_id),\n            )\n\n            turns: dict[int, list[dict[str, str | None]]] = {}\n            for row in cursor.fetchall():\n                turn_num, msg_type, tool_name = row\n                if turn_num not in turns:\n                    turns[turn_num] = []\n                turns[turn_num].append({\"type\": msg_type, \"tool_name\": tool_name})\n            return turns\n\n    return await asyncio.to_thread(_get_conversation_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.get_tool_usage","title":"get_tool_usage  <code>async</code>","text":"<pre><code>get_tool_usage(\n    branch_id: str | None = None,\n) -&gt; list[tuple[str, int, int]]\n</code></pre> <p>Get all tool usage by turn for specified branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str | None</code> <p>Branch to get tool usage from (current branch if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, int, int]]</code> <p>List of tuples containing (tool_name, usage_count, turn_number).</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def get_tool_usage(self, branch_id: str | None = None) -&gt; list[tuple[str, int, int]]:\n    \"\"\"Get all tool usage by turn for specified branch.\n\n    Args:\n        branch_id: Branch to get tool usage from (current branch if None).\n\n    Returns:\n        List of tuples containing (tool_name, usage_count, turn_number).\n    \"\"\"\n    if branch_id is None:\n        branch_id = self._current_branch_id\n\n    def _get_tool_usage_sync():\n        \"\"\"Synchronous helper to get tool usage statistics.\"\"\"\n        conn = self._get_connection()\n        with closing(conn.cursor()) as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT tool_name, COUNT(*), user_turn_number\n                FROM message_structure\n                WHERE session_id = ? AND branch_id = ? AND message_type IN (\n                    'tool_call', 'function_call', 'computer_call', 'file_search_call',\n                    'web_search_call', 'code_interpreter_call', 'custom_tool_call',\n                    'mcp_call', 'mcp_approval_request'\n                )\n                GROUP BY tool_name, user_turn_number\n                ORDER BY user_turn_number\n            \"\"\",\n                (self.session_id, branch_id),\n            )\n            return cursor.fetchall()\n\n    return await asyncio.to_thread(_get_tool_usage_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.get_session_usage","title":"get_session_usage  <code>async</code>","text":"<pre><code>get_session_usage(\n    branch_id: str | None = None,\n) -&gt; dict[str, int] | None\n</code></pre> <p>Get cumulative usage for session or specific branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str | None</code> <p>If provided, only get usage for that branch. If None, get all branches.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int] | None</code> <p>Dictionary with usage statistics or None if no usage data found.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def get_session_usage(self, branch_id: str | None = None) -&gt; dict[str, int] | None:\n    \"\"\"Get cumulative usage for session or specific branch.\n\n    Args:\n        branch_id: If provided, only get usage for that branch. If None, get all branches.\n\n    Returns:\n        Dictionary with usage statistics or None if no usage data found.\n    \"\"\"\n\n    def _get_usage_sync():\n        \"\"\"Synchronous helper to get session usage data.\"\"\"\n        conn = self._get_connection()\n        # TODO: Refactor SQLiteSession to use asyncio.Lock instead of threading.Lock and update this code  # noqa: E501\n        with self._lock if self._is_memory_db else threading.Lock():\n            if branch_id:\n                # Branch-specific usage\n                query = \"\"\"\n                    SELECT\n                        SUM(requests) as total_requests,\n                        SUM(input_tokens) as total_input_tokens,\n                        SUM(output_tokens) as total_output_tokens,\n                        SUM(total_tokens) as total_total_tokens,\n                        COUNT(*) as total_turns\n                    FROM turn_usage\n                    WHERE session_id = ? AND branch_id = ?\n                \"\"\"\n                params: tuple[str, ...] = (self.session_id, branch_id)\n            else:\n                # All branches\n                query = \"\"\"\n                    SELECT\n                        SUM(requests) as total_requests,\n                        SUM(input_tokens) as total_input_tokens,\n                        SUM(output_tokens) as total_output_tokens,\n                        SUM(total_tokens) as total_total_tokens,\n                        COUNT(*) as total_turns\n                    FROM turn_usage\n                    WHERE session_id = ?\n                \"\"\"\n                params = (self.session_id,)\n\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(query, params)\n                row = cursor.fetchone()\n\n                if row and row[0] is not None:\n                    return {\n                        \"requests\": row[0] or 0,\n                        \"input_tokens\": row[1] or 0,\n                        \"output_tokens\": row[2] or 0,\n                        \"total_tokens\": row[3] or 0,\n                        \"total_turns\": row[4] or 0,\n                    }\n                return None\n\n    result = await asyncio.to_thread(_get_usage_sync)\n\n    return cast(Union[dict[str, int], None], result)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.get_turn_usage","title":"get_turn_usage  <code>async</code>","text":"<pre><code>get_turn_usage(\n    user_turn_number: int | None = None,\n    branch_id: str | None = None,\n) -&gt; list[dict[str, Any]] | dict[str, Any]\n</code></pre> <p>Get usage statistics by turn with full JSON token details.</p> <p>Parameters:</p> Name Type Description Default <code>user_turn_number</code> <code>int | None</code> <p>Specific turn to get usage for. If None, returns all turns.</p> <code>None</code> <code>branch_id</code> <code>str | None</code> <p>Branch to get usage from (current branch if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]] | dict[str, Any]</code> <p>Dictionary with usage data for specific turn, or list of dictionaries for all turns.</p> Source code in <code>src/agents/extensions/memory/advanced_sqlite_session.py</code> <pre><code>async def get_turn_usage(\n    self,\n    user_turn_number: int | None = None,\n    branch_id: str | None = None,\n) -&gt; list[dict[str, Any]] | dict[str, Any]:\n    \"\"\"Get usage statistics by turn with full JSON token details.\n\n    Args:\n        user_turn_number: Specific turn to get usage for. If None, returns all turns.\n        branch_id: Branch to get usage from (current branch if None).\n\n    Returns:\n        Dictionary with usage data for specific turn, or list of dictionaries for all turns.\n    \"\"\"\n\n    if branch_id is None:\n        branch_id = self._current_branch_id\n\n    def _get_turn_usage_sync():\n        \"\"\"Synchronous helper to get turn usage statistics.\"\"\"\n        conn = self._get_connection()\n\n        if user_turn_number is not None:\n            query = \"\"\"\n                SELECT requests, input_tokens, output_tokens, total_tokens,\n                       input_tokens_details, output_tokens_details\n                FROM turn_usage\n                WHERE session_id = ? AND branch_id = ? AND user_turn_number = ?\n            \"\"\"\n\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(query, (self.session_id, branch_id, user_turn_number))\n                row = cursor.fetchone()\n\n                if row:\n                    # Parse JSON details if present\n                    input_details = None\n                    output_details = None\n\n                    if row[4]:  # input_tokens_details\n                        try:\n                            input_details = json.loads(row[4])\n                        except json.JSONDecodeError:\n                            pass\n\n                    if row[5]:  # output_tokens_details\n                        try:\n                            output_details = json.loads(row[5])\n                        except json.JSONDecodeError:\n                            pass\n\n                    return {\n                        \"requests\": row[0],\n                        \"input_tokens\": row[1],\n                        \"output_tokens\": row[2],\n                        \"total_tokens\": row[3],\n                        \"input_tokens_details\": input_details,\n                        \"output_tokens_details\": output_details,\n                    }\n                return {}\n        else:\n            query = \"\"\"\n                SELECT user_turn_number, requests, input_tokens, output_tokens,\n                       total_tokens, input_tokens_details, output_tokens_details\n                FROM turn_usage\n                WHERE session_id = ? AND branch_id = ?\n                ORDER BY user_turn_number\n            \"\"\"\n\n            with closing(conn.cursor()) as cursor:\n                cursor.execute(query, (self.session_id, branch_id))\n                results = []\n                for row in cursor.fetchall():\n                    # Parse JSON details if present\n                    input_details = None\n                    output_details = None\n\n                    if row[5]:  # input_tokens_details\n                        try:\n                            input_details = json.loads(row[5])\n                        except json.JSONDecodeError:\n                            pass\n\n                    if row[6]:  # output_tokens_details\n                        try:\n                            output_details = json.loads(row[6])\n                        except json.JSONDecodeError:\n                            pass\n\n                    results.append(\n                        {\n                            \"user_turn_number\": row[0],\n                            \"requests\": row[1],\n                            \"input_tokens\": row[2],\n                            \"output_tokens\": row[3],\n                            \"total_tokens\": row[4],\n                            \"input_tokens_details\": input_details,\n                            \"output_tokens_details\": output_details,\n                        }\n                    )\n                return results\n\n    result = await asyncio.to_thread(_get_turn_usage_sync)\n\n    return cast(Union[list[dict[str, Any]], dict[str, Any]], result)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n\n    def _pop_item_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            # Use DELETE with RETURNING to atomically delete and return the most recent item\n            cursor = conn.execute(\n                f\"\"\"\n                DELETE FROM {self.messages_table}\n                WHERE id = (\n                    SELECT id FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id DESC\n                    LIMIT 1\n                )\n                RETURNING message_data\n                \"\"\",\n                (self.session_id,),\n            )\n\n            result = cursor.fetchone()\n            conn.commit()\n\n            if result:\n                message_data = result[0]\n                try:\n                    item = json.loads(message_data)\n                    return item\n                except json.JSONDecodeError:\n                    # Return None for corrupted JSON entries (already deleted)\n                    return None\n\n            return None\n\n    return await asyncio.to_thread(_pop_item_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n\n    def _clear_session_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            conn.execute(\n                f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.execute(\n                f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.commit()\n\n    await asyncio.to_thread(_clear_session_sync)\n</code></pre>"},{"location":"ref/extensions/memory/advanced_sqlite_session/#agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._is_memory_db:\n        if hasattr(self, \"_shared_connection\"):\n            self._shared_connection.close()\n    else:\n        if hasattr(self._local, \"connection\"):\n            self._local.connection.close()\n</code></pre>"},{"location":"ref/extensions/memory/async_sqlite_session/","title":"<code>Async Sqlite Session</code>","text":""},{"location":"ref/extensions/memory/async_sqlite_session/#agents.extensions.memory.async_sqlite_session.AsyncSQLiteSession","title":"AsyncSQLiteSession","text":"<p>               Bases: <code>SessionABC</code></p> <p>Async SQLite-based implementation of session storage.</p> <p>This implementation stores conversation history in a SQLite database. By default, uses an in-memory database that is lost when the process ends. For persistent storage, provide a file path.</p> Source code in <code>src/agents/extensions/memory/async_sqlite_session.py</code> <pre><code>class AsyncSQLiteSession(SessionABC):\n    \"\"\"Async SQLite-based implementation of session storage.\n\n    This implementation stores conversation history in a SQLite database.\n    By default, uses an in-memory database that is lost when the process ends.\n    For persistent storage, provide a file path.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        db_path: str | Path = \":memory:\",\n        sessions_table: str = \"agent_sessions\",\n        messages_table: str = \"agent_messages\",\n    ):\n        \"\"\"Initialize the async SQLite session.\n\n        Args:\n            session_id: Unique identifier for the conversation session\n            db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n            sessions_table: Name of the table to store session metadata. Defaults to\n                'agent_sessions'\n            messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n        \"\"\"\n        self.session_id = session_id\n        self.db_path = db_path\n        self.sessions_table = sessions_table\n        self.messages_table = messages_table\n        self._connection: aiosqlite.Connection | None = None\n        self._lock = asyncio.Lock()\n        self._init_lock = asyncio.Lock()\n\n    async def _init_db_for_connection(self, conn: aiosqlite.Connection) -&gt; None:\n        \"\"\"Initialize the database schema for a specific connection.\"\"\"\n        await conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.sessions_table} (\n                session_id TEXT PRIMARY KEY,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\"\n        )\n\n        await conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.messages_table} (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL,\n                message_data TEXT NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (session_id) REFERENCES {self.sessions_table} (session_id)\n                    ON DELETE CASCADE\n            )\n        \"\"\"\n        )\n\n        await conn.execute(\n            f\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_{self.messages_table}_session_id\n            ON {self.messages_table} (session_id, id)\n        \"\"\"\n        )\n\n        await conn.commit()\n\n    async def _get_connection(self) -&gt; aiosqlite.Connection:\n        \"\"\"Get or create a database connection.\"\"\"\n        if self._connection is not None:\n            return self._connection\n\n        async with self._init_lock:\n            if self._connection is None:\n                self._connection = await aiosqlite.connect(str(self.db_path))\n                await self._connection.execute(\"PRAGMA journal_mode=WAL\")\n                await self._init_db_for_connection(self._connection)\n\n        return self._connection\n\n    @asynccontextmanager\n    async def _locked_connection(self) -&gt; AsyncIterator[aiosqlite.Connection]:\n        \"\"\"Provide a connection under the session lock.\"\"\"\n        async with self._lock:\n            conn = await self._get_connection()\n            yield conn\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, retrieves all items.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n\n        async with self._locked_connection() as conn:\n            if limit is None:\n                cursor = await conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id ASC\n                \"\"\",\n                    (self.session_id,),\n                )\n            else:\n                cursor = await conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id DESC\n                    LIMIT ?\n                    \"\"\",\n                    (self.session_id, limit),\n                )\n\n            rows = list(await cursor.fetchall())\n            await cursor.close()\n\n        if limit is not None:\n            rows = rows[::-1]\n\n        items: list[TResponseInputItem] = []\n        for (message_data,) in rows:\n            try:\n                item = json.loads(message_data)\n                items.append(item)\n            except json.JSONDecodeError:\n                continue\n\n        return items\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        if not items:\n            return\n\n        async with self._locked_connection() as conn:\n            await conn.execute(\n                f\"\"\"\n                INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n            \"\"\",\n                (self.session_id,),\n            )\n\n            message_data = [(self.session_id, json.dumps(item)) for item in items]\n            await conn.executemany(\n                f\"\"\"\n                INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n            \"\"\",\n                message_data,\n            )\n\n            await conn.execute(\n                f\"\"\"\n                UPDATE {self.sessions_table}\n                SET updated_at = CURRENT_TIMESTAMP\n                WHERE session_id = ?\n            \"\"\",\n                (self.session_id,),\n            )\n\n            await conn.commit()\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        async with self._locked_connection() as conn:\n            cursor = await conn.execute(\n                f\"\"\"\n                DELETE FROM {self.messages_table}\n                WHERE id = (\n                    SELECT id FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id DESC\n                    LIMIT 1\n                )\n                RETURNING message_data\n                \"\"\",\n                (self.session_id,),\n            )\n\n            result = await cursor.fetchone()\n            await cursor.close()\n            await conn.commit()\n\n        if result:\n            message_data = result[0]\n            try:\n                return cast(TResponseInputItem, json.loads(message_data))\n            except json.JSONDecodeError:\n                return None\n\n        return None\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        async with self._locked_connection() as conn:\n            await conn.execute(\n                f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            await conn.execute(\n                f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            await conn.commit()\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the database connection.\"\"\"\n        if self._connection is None:\n            return\n        async with self._lock:\n            await self._connection.close()\n            self._connection = None\n</code></pre>"},{"location":"ref/extensions/memory/async_sqlite_session/#agents.extensions.memory.async_sqlite_session.AsyncSQLiteSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n)\n</code></pre> <p>Initialize the async SQLite session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Unique identifier for the conversation session</p> required <code>db_path</code> <code>str | Path</code> <p>Path to the SQLite database file. Defaults to ':memory:' (in-memory database)</p> <code>':memory:'</code> <code>sessions_table</code> <code>str</code> <p>Name of the table to store session metadata. Defaults to 'agent_sessions'</p> <code>'agent_sessions'</code> <code>messages_table</code> <code>str</code> <p>Name of the table to store message data. Defaults to 'agent_messages'</p> <code>'agent_messages'</code> Source code in <code>src/agents/extensions/memory/async_sqlite_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n):\n    \"\"\"Initialize the async SQLite session.\n\n    Args:\n        session_id: Unique identifier for the conversation session\n        db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n        sessions_table: Name of the table to store session metadata. Defaults to\n            'agent_sessions'\n        messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n    \"\"\"\n    self.session_id = session_id\n    self.db_path = db_path\n    self.sessions_table = sessions_table\n    self.messages_table = messages_table\n    self._connection: aiosqlite.Connection | None = None\n    self._lock = asyncio.Lock()\n    self._init_lock = asyncio.Lock()\n</code></pre>"},{"location":"ref/extensions/memory/async_sqlite_session/#agents.extensions.memory.async_sqlite_session.AsyncSQLiteSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, retrieves all items.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/extensions/memory/async_sqlite_session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, retrieves all items.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n\n    async with self._locked_connection() as conn:\n        if limit is None:\n            cursor = await conn.execute(\n                f\"\"\"\n                SELECT message_data FROM {self.messages_table}\n                WHERE session_id = ?\n                ORDER BY id ASC\n            \"\"\",\n                (self.session_id,),\n            )\n        else:\n            cursor = await conn.execute(\n                f\"\"\"\n                SELECT message_data FROM {self.messages_table}\n                WHERE session_id = ?\n                ORDER BY id DESC\n                LIMIT ?\n                \"\"\",\n                (self.session_id, limit),\n            )\n\n        rows = list(await cursor.fetchall())\n        await cursor.close()\n\n    if limit is not None:\n        rows = rows[::-1]\n\n    items: list[TResponseInputItem] = []\n    for (message_data,) in rows:\n        try:\n            item = json.loads(message_data)\n            items.append(item)\n        except json.JSONDecodeError:\n            continue\n\n    return items\n</code></pre>"},{"location":"ref/extensions/memory/async_sqlite_session/#agents.extensions.memory.async_sqlite_session.AsyncSQLiteSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/extensions/memory/async_sqlite_session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    if not items:\n        return\n\n    async with self._locked_connection() as conn:\n        await conn.execute(\n            f\"\"\"\n            INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n        \"\"\",\n            (self.session_id,),\n        )\n\n        message_data = [(self.session_id, json.dumps(item)) for item in items]\n        await conn.executemany(\n            f\"\"\"\n            INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n        \"\"\",\n            message_data,\n        )\n\n        await conn.execute(\n            f\"\"\"\n            UPDATE {self.sessions_table}\n            SET updated_at = CURRENT_TIMESTAMP\n            WHERE session_id = ?\n        \"\"\",\n            (self.session_id,),\n        )\n\n        await conn.commit()\n</code></pre>"},{"location":"ref/extensions/memory/async_sqlite_session/#agents.extensions.memory.async_sqlite_session.AsyncSQLiteSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/extensions/memory/async_sqlite_session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    async with self._locked_connection() as conn:\n        cursor = await conn.execute(\n            f\"\"\"\n            DELETE FROM {self.messages_table}\n            WHERE id = (\n                SELECT id FROM {self.messages_table}\n                WHERE session_id = ?\n                ORDER BY id DESC\n                LIMIT 1\n            )\n            RETURNING message_data\n            \"\"\",\n            (self.session_id,),\n        )\n\n        result = await cursor.fetchone()\n        await cursor.close()\n        await conn.commit()\n\n    if result:\n        message_data = result[0]\n        try:\n            return cast(TResponseInputItem, json.loads(message_data))\n        except json.JSONDecodeError:\n            return None\n\n    return None\n</code></pre>"},{"location":"ref/extensions/memory/async_sqlite_session/#agents.extensions.memory.async_sqlite_session.AsyncSQLiteSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/extensions/memory/async_sqlite_session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    async with self._locked_connection() as conn:\n        await conn.execute(\n            f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n            (self.session_id,),\n        )\n        await conn.execute(\n            f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n            (self.session_id,),\n        )\n        await conn.commit()\n</code></pre>"},{"location":"ref/extensions/memory/async_sqlite_session/#agents.extensions.memory.async_sqlite_session.AsyncSQLiteSession.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p> Source code in <code>src/agents/extensions/memory/async_sqlite_session.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._connection is None:\n        return\n    async with self._lock:\n        await self._connection.close()\n        self._connection = None\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/","title":"Dapr session","text":"<p>               Bases: <code>SessionABC</code></p> <p>Dapr State Store implementation of :pyclass:<code>agents.memory.session.Session</code>.</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>class DaprSession(SessionABC):\n    \"\"\"Dapr State Store implementation of :pyclass:`agents.memory.session.Session`.\"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        *,\n        state_store_name: str,\n        dapr_client: DaprClient,\n        ttl: int | None = None,\n        consistency: ConsistencyLevel = DAPR_CONSISTENCY_EVENTUAL,\n        session_settings: SessionSettings | None = None,\n    ):\n        \"\"\"Initializes a new DaprSession.\n\n        Args:\n            session_id (str): Unique identifier for the conversation.\n            state_store_name (str): Name of the Dapr state store component.\n            dapr_client (DaprClient): A pre-configured Dapr client.\n            ttl (int | None, optional): Time-to-live in seconds for session data.\n                If None, data persists indefinitely. Note that TTL support depends on\n                the underlying state store implementation. Defaults to None.\n            consistency (ConsistencyLevel, optional): Consistency level for state operations.\n                Use DAPR_CONSISTENCY_EVENTUAL or DAPR_CONSISTENCY_STRONG constants.\n                Defaults to DAPR_CONSISTENCY_EVENTUAL.\n            session_settings (SessionSettings | None): Session configuration settings including\n                default limit for retrieving items. If None, uses default SessionSettings().\n        \"\"\"\n        self.session_id = session_id\n        self.session_settings = session_settings or SessionSettings()\n        self._dapr_client = dapr_client\n        self._state_store_name = state_store_name\n        self._ttl = ttl\n        self._consistency = consistency\n        self._lock = asyncio.Lock()\n        self._owns_client = False  # Track if we own the Dapr client\n\n        # State keys\n        self._messages_key = f\"{self.session_id}:messages\"\n        self._metadata_key = f\"{self.session_id}:metadata\"\n\n    @classmethod\n    def from_address(\n        cls,\n        session_id: str,\n        *,\n        state_store_name: str,\n        dapr_address: str = \"localhost:50001\",\n        session_settings: SessionSettings | None = None,\n        **kwargs: Any,\n    ) -&gt; DaprSession:\n        \"\"\"Create a session from a Dapr sidecar address.\n\n        Args:\n            session_id (str): Conversation ID.\n            state_store_name (str): Name of the Dapr state store component.\n            dapr_address (str): Dapr sidecar gRPC address. Defaults to \"localhost:50001\".\n            session_settings (SessionSettings | None): Session configuration settings including\n                default limit for retrieving items. If None, uses default SessionSettings().\n            **kwargs: Additional keyword arguments forwarded to the main constructor\n                (e.g., ttl, consistency).\n\n        Returns:\n            DaprSession: An instance of DaprSession connected to the specified Dapr sidecar.\n\n        Note:\n            The Dapr Python SDK performs health checks on the HTTP endpoint (default: http://localhost:3500).\n            Ensure the Dapr sidecar is started with --dapr-http-port 3500. Alternatively, set one of\n            these environment variables: DAPR_HTTP_ENDPOINT (e.g., \"http://localhost:3500\") or\n            DAPR_HTTP_PORT (e.g., \"3500\") to avoid connection errors.\n        \"\"\"\n        dapr_client = DaprClient(address=dapr_address)\n        session = cls(\n            session_id,\n            state_store_name=state_store_name,\n            dapr_client=dapr_client,\n            session_settings=session_settings,\n            **kwargs,\n        )\n        session._owns_client = True  # We created the client, so we own it\n        return session\n\n    def _get_read_metadata(self) -&gt; dict[str, str]:\n        \"\"\"Get metadata for read operations including consistency.\n\n        The consistency level is passed through state_metadata as per Dapr's state API.\n        \"\"\"\n        metadata: dict[str, str] = {}\n        # Add consistency level to metadata for read operations\n        if self._consistency:\n            metadata[\"consistency\"] = self._consistency\n        return metadata\n\n    def _get_state_options(self, *, concurrency: Concurrency | None = None) -&gt; StateOptions | None:\n        \"\"\"Get StateOptions configured with consistency and optional concurrency.\"\"\"\n        options_kwargs: dict[str, Any] = {}\n        if self._consistency == DAPR_CONSISTENCY_STRONG:\n            options_kwargs[\"consistency\"] = Consistency.strong\n        elif self._consistency == DAPR_CONSISTENCY_EVENTUAL:\n            options_kwargs[\"consistency\"] = Consistency.eventual\n        if concurrency is not None:\n            options_kwargs[\"concurrency\"] = concurrency\n        if options_kwargs:\n            return StateOptions(**options_kwargs)\n        return None\n\n    def _get_metadata(self) -&gt; dict[str, str]:\n        \"\"\"Get metadata for state operations including TTL if configured.\"\"\"\n        metadata = {}\n        if self._ttl is not None:\n            metadata[\"ttlInSeconds\"] = str(self._ttl)\n        return metadata\n\n    async def _serialize_item(self, item: TResponseInputItem) -&gt; str:\n        \"\"\"Serialize an item to JSON string. Can be overridden by subclasses.\"\"\"\n        return json.dumps(item, separators=(\",\", \":\"))\n\n    async def _deserialize_item(self, item: str) -&gt; TResponseInputItem:\n        \"\"\"Deserialize a JSON string to an item. Can be overridden by subclasses.\"\"\"\n        return json.loads(item)  # type: ignore[no-any-return]\n\n    def _decode_messages(self, data: bytes | None) -&gt; list[Any]:\n        if not data:\n            return []\n        try:\n            messages_json = data.decode(\"utf-8\")\n            messages = json.loads(messages_json)\n            if isinstance(messages, list):\n                return list(messages)\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            return []\n        return []\n\n    def _calculate_retry_delay(self, attempt: int) -&gt; float:\n        base: float = _RETRY_BASE_DELAY_SECONDS * (2 ** max(0, attempt - 1))\n        delay: float = min(base, _RETRY_MAX_DELAY_SECONDS)\n        # Add jitter (10%) similar to tracing processors to avoid thundering herd.\n        return delay + random.uniform(0, 0.1 * delay)\n\n    def _is_concurrency_conflict(self, error: Exception) -&gt; bool:\n        code_attr = getattr(error, \"code\", None)\n        if callable(code_attr):\n            try:\n                status_code = code_attr()\n            except Exception:\n                status_code = None\n            if status_code is not None:\n                status_name = getattr(status_code, \"name\", str(status_code))\n                if status_name in {\"ABORTED\", \"FAILED_PRECONDITION\"}:\n                    return True\n        message = str(error).lower()\n        conflict_markers = (\n            \"etag mismatch\",\n            \"etag does not match\",\n            \"precondition failed\",\n            \"concurrency conflict\",\n            \"invalid etag\",\n            \"failed to set key\",  # Redis state store Lua script error during conditional write\n            \"user_script\",  # Redis script failure hint\n        )\n        return any(marker in message for marker in conflict_markers)\n\n    async def _handle_concurrency_conflict(self, error: Exception, attempt: int) -&gt; bool:\n        if not self._is_concurrency_conflict(error):\n            return False\n        if attempt &gt;= _MAX_WRITE_ATTEMPTS:\n            return False\n        delay = self._calculate_retry_delay(attempt)\n        if delay &gt; 0:\n            await asyncio.sleep(delay)\n        return True\n\n    # ------------------------------------------------------------------\n    # Session protocol implementation\n    # ------------------------------------------------------------------\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        async with self._lock:\n            # Get messages from state store with consistency level\n            response = await self._dapr_client.get_state(\n                store_name=self._state_store_name,\n                key=self._messages_key,\n                state_metadata=self._get_read_metadata(),\n            )\n\n            messages = self._decode_messages(response.data)\n            if not messages:\n                return []\n            if session_limit is not None:\n                if session_limit &lt;= 0:\n                    return []\n                messages = messages[-session_limit:]\n            items: list[TResponseInputItem] = []\n            for msg in messages:\n                try:\n                    if isinstance(msg, str):\n                        item = await self._deserialize_item(msg)\n                    else:\n                        item = msg\n                    items.append(item)\n                except (json.JSONDecodeError, TypeError):\n                    continue\n            return items\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        if not items:\n            return\n\n        async with self._lock:\n            serialized_items: list[str] = [await self._serialize_item(item) for item in items]\n            attempt = 0\n            while True:\n                attempt += 1\n                response = await self._dapr_client.get_state(\n                    store_name=self._state_store_name,\n                    key=self._messages_key,\n                    state_metadata=self._get_read_metadata(),\n                )\n                existing_messages = self._decode_messages(response.data)\n                updated_messages = existing_messages + serialized_items\n                messages_json = json.dumps(updated_messages, separators=(\",\", \":\"))\n                etag = response.etag\n                try:\n                    await self._dapr_client.save_state(\n                        store_name=self._state_store_name,\n                        key=self._messages_key,\n                        value=messages_json,\n                        etag=etag,\n                        state_metadata=self._get_metadata(),\n                        options=self._get_state_options(concurrency=Concurrency.first_write),\n                    )\n                    break\n                except Exception as error:\n                    should_retry = await self._handle_concurrency_conflict(error, attempt)\n                    if should_retry:\n                        continue\n                    raise\n\n            # Update metadata\n            metadata = {\n                \"session_id\": self.session_id,\n                \"created_at\": str(int(time.time())),\n                \"updated_at\": str(int(time.time())),\n            }\n            await self._dapr_client.save_state(\n                store_name=self._state_store_name,\n                key=self._metadata_key,\n                value=json.dumps(metadata),\n                state_metadata=self._get_metadata(),\n                options=self._get_state_options(),\n            )\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        async with self._lock:\n            attempt = 0\n            while True:\n                attempt += 1\n                response = await self._dapr_client.get_state(\n                    store_name=self._state_store_name,\n                    key=self._messages_key,\n                    state_metadata=self._get_read_metadata(),\n                )\n                messages = self._decode_messages(response.data)\n                if not messages:\n                    return None\n                last_item = messages.pop()\n                messages_json = json.dumps(messages, separators=(\",\", \":\"))\n                etag = getattr(response, \"etag\", None) or None\n                etag = getattr(response, \"etag\", None) or None\n                try:\n                    await self._dapr_client.save_state(\n                        store_name=self._state_store_name,\n                        key=self._messages_key,\n                        value=messages_json,\n                        etag=etag,\n                        state_metadata=self._get_metadata(),\n                        options=self._get_state_options(concurrency=Concurrency.first_write),\n                    )\n                    break\n                except Exception as error:\n                    should_retry = await self._handle_concurrency_conflict(error, attempt)\n                    if should_retry:\n                        continue\n                    raise\n            try:\n                if isinstance(last_item, str):\n                    return await self._deserialize_item(last_item)\n                return last_item  # type: ignore[no-any-return]\n            except (json.JSONDecodeError, TypeError):\n                return None\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        async with self._lock:\n            # Delete messages and metadata keys\n            await self._dapr_client.delete_state(\n                store_name=self._state_store_name,\n                key=self._messages_key,\n                options=self._get_state_options(),\n            )\n\n            await self._dapr_client.delete_state(\n                store_name=self._state_store_name,\n                key=self._metadata_key,\n                options=self._get_state_options(),\n            )\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the Dapr client connection.\n\n        Only closes the connection if this session owns the Dapr client\n        (i.e., created via from_address). If the client was injected externally,\n        the caller is responsible for managing its lifecycle.\n        \"\"\"\n        if self._owns_client:\n            await self._dapr_client.close()\n\n    async def __aenter__(self) -&gt; DaprSession:\n        \"\"\"Enter async context manager.\"\"\"\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Exit async context manager and close the connection.\"\"\"\n        await self.close()\n\n    async def ping(self) -&gt; bool:\n        \"\"\"Test Dapr connectivity by checking metadata.\n\n        Returns:\n            True if Dapr is reachable, False otherwise.\n        \"\"\"\n        try:\n            # First attempt a read; some stores may not be initialized yet.\n            await self._dapr_client.get_state(\n                store_name=self._state_store_name,\n                key=\"__ping__\",\n                state_metadata=self._get_read_metadata(),\n            )\n            return True\n        except Exception as initial_error:\n            # If relation/table is missing or store isn't initialized,\n            # attempt a write to initialize it, then read again.\n            try:\n                await self._dapr_client.save_state(\n                    store_name=self._state_store_name,\n                    key=\"__ping__\",\n                    value=\"ok\",\n                    state_metadata=self._get_metadata(),\n                    options=self._get_state_options(),\n                )\n                # Read again after write.\n                await self._dapr_client.get_state(\n                    store_name=self._state_store_name,\n                    key=\"__ping__\",\n                    state_metadata=self._get_read_metadata(),\n                )\n                return True\n            except Exception:\n                logger.error(\"Dapr connection failed: %s\", initial_error)\n                return False\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    *,\n    state_store_name: str,\n    dapr_client: DaprClient,\n    ttl: int | None = None,\n    consistency: ConsistencyLevel = DAPR_CONSISTENCY_EVENTUAL,\n    session_settings: SessionSettings | None = None,\n)\n</code></pre> <p>Initializes a new DaprSession.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Unique identifier for the conversation.</p> required <code>state_store_name</code> <code>str</code> <p>Name of the Dapr state store component.</p> required <code>dapr_client</code> <code>DaprClient</code> <p>A pre-configured Dapr client.</p> required <code>ttl</code> <code>int | None</code> <p>Time-to-live in seconds for session data. If None, data persists indefinitely. Note that TTL support depends on the underlying state store implementation. Defaults to None.</p> <code>None</code> <code>consistency</code> <code>ConsistencyLevel</code> <p>Consistency level for state operations. Use DAPR_CONSISTENCY_EVENTUAL or DAPR_CONSISTENCY_STRONG constants. Defaults to DAPR_CONSISTENCY_EVENTUAL.</p> <code>DAPR_CONSISTENCY_EVENTUAL</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings including default limit for retrieving items. If None, uses default SessionSettings().</p> <code>None</code> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    *,\n    state_store_name: str,\n    dapr_client: DaprClient,\n    ttl: int | None = None,\n    consistency: ConsistencyLevel = DAPR_CONSISTENCY_EVENTUAL,\n    session_settings: SessionSettings | None = None,\n):\n    \"\"\"Initializes a new DaprSession.\n\n    Args:\n        session_id (str): Unique identifier for the conversation.\n        state_store_name (str): Name of the Dapr state store component.\n        dapr_client (DaprClient): A pre-configured Dapr client.\n        ttl (int | None, optional): Time-to-live in seconds for session data.\n            If None, data persists indefinitely. Note that TTL support depends on\n            the underlying state store implementation. Defaults to None.\n        consistency (ConsistencyLevel, optional): Consistency level for state operations.\n            Use DAPR_CONSISTENCY_EVENTUAL or DAPR_CONSISTENCY_STRONG constants.\n            Defaults to DAPR_CONSISTENCY_EVENTUAL.\n        session_settings (SessionSettings | None): Session configuration settings including\n            default limit for retrieving items. If None, uses default SessionSettings().\n    \"\"\"\n    self.session_id = session_id\n    self.session_settings = session_settings or SessionSettings()\n    self._dapr_client = dapr_client\n    self._state_store_name = state_store_name\n    self._ttl = ttl\n    self._consistency = consistency\n    self._lock = asyncio.Lock()\n    self._owns_client = False  # Track if we own the Dapr client\n\n    # State keys\n    self._messages_key = f\"{self.session_id}:messages\"\n    self._metadata_key = f\"{self.session_id}:metadata\"\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.from_address","title":"from_address  <code>classmethod</code>","text":"<pre><code>from_address(\n    session_id: str,\n    *,\n    state_store_name: str,\n    dapr_address: str = \"localhost:50001\",\n    session_settings: SessionSettings | None = None,\n    **kwargs: Any,\n) -&gt; DaprSession\n</code></pre> <p>Create a session from a Dapr sidecar address.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Conversation ID.</p> required <code>state_store_name</code> <code>str</code> <p>Name of the Dapr state store component.</p> required <code>dapr_address</code> <code>str</code> <p>Dapr sidecar gRPC address. Defaults to \"localhost:50001\".</p> <code>'localhost:50001'</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings including default limit for retrieving items. If None, uses default SessionSettings().</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments forwarded to the main constructor (e.g., ttl, consistency).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DaprSession</code> <code>DaprSession</code> <p>An instance of DaprSession connected to the specified Dapr sidecar.</p> Note <p>The Dapr Python SDK performs health checks on the HTTP endpoint (default: http://localhost:3500). Ensure the Dapr sidecar is started with --dapr-http-port 3500. Alternatively, set one of these environment variables: DAPR_HTTP_ENDPOINT (e.g., \"http://localhost:3500\") or DAPR_HTTP_PORT (e.g., \"3500\") to avoid connection errors.</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>@classmethod\ndef from_address(\n    cls,\n    session_id: str,\n    *,\n    state_store_name: str,\n    dapr_address: str = \"localhost:50001\",\n    session_settings: SessionSettings | None = None,\n    **kwargs: Any,\n) -&gt; DaprSession:\n    \"\"\"Create a session from a Dapr sidecar address.\n\n    Args:\n        session_id (str): Conversation ID.\n        state_store_name (str): Name of the Dapr state store component.\n        dapr_address (str): Dapr sidecar gRPC address. Defaults to \"localhost:50001\".\n        session_settings (SessionSettings | None): Session configuration settings including\n            default limit for retrieving items. If None, uses default SessionSettings().\n        **kwargs: Additional keyword arguments forwarded to the main constructor\n            (e.g., ttl, consistency).\n\n    Returns:\n        DaprSession: An instance of DaprSession connected to the specified Dapr sidecar.\n\n    Note:\n        The Dapr Python SDK performs health checks on the HTTP endpoint (default: http://localhost:3500).\n        Ensure the Dapr sidecar is started with --dapr-http-port 3500. Alternatively, set one of\n        these environment variables: DAPR_HTTP_ENDPOINT (e.g., \"http://localhost:3500\") or\n        DAPR_HTTP_PORT (e.g., \"3500\") to avoid connection errors.\n    \"\"\"\n    dapr_client = DaprClient(address=dapr_address)\n    session = cls(\n        session_id,\n        state_store_name=state_store_name,\n        dapr_client=dapr_client,\n        session_settings=session_settings,\n        **kwargs,\n    )\n    session._owns_client = True  # We created the client, so we own it\n    return session\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, uses session_settings.limit.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    session_limit = resolve_session_limit(limit, self.session_settings)\n\n    async with self._lock:\n        # Get messages from state store with consistency level\n        response = await self._dapr_client.get_state(\n            store_name=self._state_store_name,\n            key=self._messages_key,\n            state_metadata=self._get_read_metadata(),\n        )\n\n        messages = self._decode_messages(response.data)\n        if not messages:\n            return []\n        if session_limit is not None:\n            if session_limit &lt;= 0:\n                return []\n            messages = messages[-session_limit:]\n        items: list[TResponseInputItem] = []\n        for msg in messages:\n            try:\n                if isinstance(msg, str):\n                    item = await self._deserialize_item(msg)\n                else:\n                    item = msg\n                items.append(item)\n            except (json.JSONDecodeError, TypeError):\n                continue\n        return items\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    if not items:\n        return\n\n    async with self._lock:\n        serialized_items: list[str] = [await self._serialize_item(item) for item in items]\n        attempt = 0\n        while True:\n            attempt += 1\n            response = await self._dapr_client.get_state(\n                store_name=self._state_store_name,\n                key=self._messages_key,\n                state_metadata=self._get_read_metadata(),\n            )\n            existing_messages = self._decode_messages(response.data)\n            updated_messages = existing_messages + serialized_items\n            messages_json = json.dumps(updated_messages, separators=(\",\", \":\"))\n            etag = response.etag\n            try:\n                await self._dapr_client.save_state(\n                    store_name=self._state_store_name,\n                    key=self._messages_key,\n                    value=messages_json,\n                    etag=etag,\n                    state_metadata=self._get_metadata(),\n                    options=self._get_state_options(concurrency=Concurrency.first_write),\n                )\n                break\n            except Exception as error:\n                should_retry = await self._handle_concurrency_conflict(error, attempt)\n                if should_retry:\n                    continue\n                raise\n\n        # Update metadata\n        metadata = {\n            \"session_id\": self.session_id,\n            \"created_at\": str(int(time.time())),\n            \"updated_at\": str(int(time.time())),\n        }\n        await self._dapr_client.save_state(\n            store_name=self._state_store_name,\n            key=self._metadata_key,\n            value=json.dumps(metadata),\n            state_metadata=self._get_metadata(),\n            options=self._get_state_options(),\n        )\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    async with self._lock:\n        attempt = 0\n        while True:\n            attempt += 1\n            response = await self._dapr_client.get_state(\n                store_name=self._state_store_name,\n                key=self._messages_key,\n                state_metadata=self._get_read_metadata(),\n            )\n            messages = self._decode_messages(response.data)\n            if not messages:\n                return None\n            last_item = messages.pop()\n            messages_json = json.dumps(messages, separators=(\",\", \":\"))\n            etag = getattr(response, \"etag\", None) or None\n            etag = getattr(response, \"etag\", None) or None\n            try:\n                await self._dapr_client.save_state(\n                    store_name=self._state_store_name,\n                    key=self._messages_key,\n                    value=messages_json,\n                    etag=etag,\n                    state_metadata=self._get_metadata(),\n                    options=self._get_state_options(concurrency=Concurrency.first_write),\n                )\n                break\n            except Exception as error:\n                should_retry = await self._handle_concurrency_conflict(error, attempt)\n                if should_retry:\n                    continue\n                raise\n        try:\n            if isinstance(last_item, str):\n                return await self._deserialize_item(last_item)\n            return last_item  # type: ignore[no-any-return]\n        except (json.JSONDecodeError, TypeError):\n            return None\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    async with self._lock:\n        # Delete messages and metadata keys\n        await self._dapr_client.delete_state(\n            store_name=self._state_store_name,\n            key=self._messages_key,\n            options=self._get_state_options(),\n        )\n\n        await self._dapr_client.delete_state(\n            store_name=self._state_store_name,\n            key=self._metadata_key,\n            options=self._get_state_options(),\n        )\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the Dapr client connection.</p> <p>Only closes the connection if this session owns the Dapr client (i.e., created via from_address). If the client was injected externally, the caller is responsible for managing its lifecycle.</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the Dapr client connection.\n\n    Only closes the connection if this session owns the Dapr client\n    (i.e., created via from_address). If the client was injected externally,\n    the caller is responsible for managing its lifecycle.\n    \"\"\"\n    if self._owns_client:\n        await self._dapr_client.close()\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; DaprSession\n</code></pre> <p>Enter async context manager.</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def __aenter__(self) -&gt; DaprSession:\n    \"\"\"Enter async context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Exit async context manager and close the connection.</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Exit async context manager and close the connection.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"ref/extensions/memory/dapr_session/#agents.extensions.memory.dapr_session.DaprSession.ping","title":"ping  <code>async</code>","text":"<pre><code>ping() -&gt; bool\n</code></pre> <p>Test Dapr connectivity by checking metadata.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if Dapr is reachable, False otherwise.</p> Source code in <code>src/agents/extensions/memory/dapr_session.py</code> <pre><code>async def ping(self) -&gt; bool:\n    \"\"\"Test Dapr connectivity by checking metadata.\n\n    Returns:\n        True if Dapr is reachable, False otherwise.\n    \"\"\"\n    try:\n        # First attempt a read; some stores may not be initialized yet.\n        await self._dapr_client.get_state(\n            store_name=self._state_store_name,\n            key=\"__ping__\",\n            state_metadata=self._get_read_metadata(),\n        )\n        return True\n    except Exception as initial_error:\n        # If relation/table is missing or store isn't initialized,\n        # attempt a write to initialize it, then read again.\n        try:\n            await self._dapr_client.save_state(\n                store_name=self._state_store_name,\n                key=\"__ping__\",\n                value=\"ok\",\n                state_metadata=self._get_metadata(),\n                options=self._get_state_options(),\n            )\n            # Read again after write.\n            await self._dapr_client.get_state(\n                store_name=self._state_store_name,\n                key=\"__ping__\",\n                state_metadata=self._get_read_metadata(),\n            )\n            return True\n        except Exception:\n            logger.error(\"Dapr connection failed: %s\", initial_error)\n            return False\n</code></pre>"},{"location":"ref/extensions/memory/encrypt_session/","title":"<code>EncryptedSession</code>","text":"<p>               Bases: <code>SessionABC</code></p> <p>Encrypted wrapper for Session implementations with TTL-based expiration.</p> <p>This class wraps any SessionABC implementation to provide transparent encryption/decryption of stored items using Fernet encryption with per-session key derivation and automatic expiration of old data.</p> <p>When items expire (exceed TTL), they are silently skipped during retrieval.</p> <p>Note: Expired tokens are rejected based on the system clock of the application server. To avoid valid tokens being rejected due to clock drift, ensure all servers in your environment are synchronized using NTP.</p> Source code in <code>src/agents/extensions/memory/encrypt_session.py</code> <pre><code>class EncryptedSession(SessionABC):\n    \"\"\"Encrypted wrapper for Session implementations with TTL-based expiration.\n\n    This class wraps any SessionABC implementation to provide transparent\n    encryption/decryption of stored items using Fernet encryption with\n    per-session key derivation and automatic expiration of old data.\n\n    When items expire (exceed TTL), they are silently skipped during retrieval.\n\n    Note: Expired tokens are rejected based on the system clock of the application server.\n    To avoid valid tokens being rejected due to clock drift, ensure all servers in\n    your environment are synchronized using NTP.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        underlying_session: SessionABC,\n        encryption_key: str,\n        ttl: int = 600,\n    ):\n        \"\"\"\n        Args:\n            session_id: ID for this session\n            underlying_session: The real session store (e.g. SQLiteSession, SQLAlchemySession)\n            encryption_key: Master key (Fernet key or raw secret)\n            ttl: Token time-to-live in seconds (default 10 min)\n        \"\"\"\n        self.session_id = session_id\n        self.underlying_session = underlying_session\n        self.ttl = ttl\n\n        master = _ensure_fernet_key_bytes(encryption_key)\n        self.cipher = _derive_session_fernet_key(master, session_id)\n        self._kid = \"hkdf-v1\"\n        self._ver = 1\n\n    def __getattr__(self, name):\n        return getattr(self.underlying_session, name)\n\n    @property\n    def session_settings(self) -&gt; SessionSettings | None:\n        \"\"\"Get session settings from the underlying session.\"\"\"\n        return self.underlying_session.session_settings\n\n    @session_settings.setter\n    def session_settings(self, value: SessionSettings | None) -&gt; None:\n        \"\"\"Set session settings on the underlying session.\"\"\"\n        self.underlying_session.session_settings = value\n\n    def _wrap(self, item: TResponseInputItem) -&gt; EncryptedEnvelope:\n        if isinstance(item, dict):\n            payload = item\n        elif hasattr(item, \"model_dump\"):\n            payload = item.model_dump()\n        elif hasattr(item, \"__dict__\"):\n            payload = item.__dict__\n        else:\n            payload = dict(item)\n\n        token = self.cipher.encrypt(_to_json_bytes(payload)).decode(\"utf-8\")\n        return {\"__enc__\": 1, \"v\": self._ver, \"kid\": self._kid, \"payload\": token}\n\n    def _unwrap(self, item: TResponseInputItem | EncryptedEnvelope) -&gt; TResponseInputItem | None:\n        if not _is_encrypted_envelope(item):\n            return cast(TResponseInputItem, item)\n\n        try:\n            token = item[\"payload\"].encode(\"utf-8\")\n            plaintext = self.cipher.decrypt(token, ttl=self.ttl)\n            return cast(TResponseInputItem, _from_json_bytes(plaintext))\n        except (InvalidToken, KeyError):\n            return None\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        encrypted_items = await self.underlying_session.get_items(limit)\n        valid_items: list[TResponseInputItem] = []\n        for enc in encrypted_items:\n            item = self._unwrap(enc)\n            if item is not None:\n                valid_items.append(item)\n        return valid_items\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        wrapped: list[EncryptedEnvelope] = [self._wrap(it) for it in items]\n        await self.underlying_session.add_items(cast(list[TResponseInputItem], wrapped))\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        while True:\n            enc = await self.underlying_session.pop_item()\n            if not enc:\n                return None\n            item = self._unwrap(enc)\n            if item is not None:\n                return item\n\n    async def clear_session(self) -&gt; None:\n        await self.underlying_session.clear_session()\n</code></pre>"},{"location":"ref/extensions/memory/encrypt_session/#agents.extensions.memory.encrypt_session.EncryptedSession.session_settings","title":"session_settings  <code>property</code> <code>writable</code>","text":"<pre><code>session_settings: SessionSettings | None\n</code></pre> <p>Get session settings from the underlying session.</p>"},{"location":"ref/extensions/memory/encrypt_session/#agents.extensions.memory.encrypt_session.EncryptedSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    underlying_session: SessionABC,\n    encryption_key: str,\n    ttl: int = 600,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>ID for this session</p> required <code>underlying_session</code> <code>SessionABC</code> <p>The real session store (e.g. SQLiteSession, SQLAlchemySession)</p> required <code>encryption_key</code> <code>str</code> <p>Master key (Fernet key or raw secret)</p> required <code>ttl</code> <code>int</code> <p>Token time-to-live in seconds (default 10 min)</p> <code>600</code> Source code in <code>src/agents/extensions/memory/encrypt_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    underlying_session: SessionABC,\n    encryption_key: str,\n    ttl: int = 600,\n):\n    \"\"\"\n    Args:\n        session_id: ID for this session\n        underlying_session: The real session store (e.g. SQLiteSession, SQLAlchemySession)\n        encryption_key: Master key (Fernet key or raw secret)\n        ttl: Token time-to-live in seconds (default 10 min)\n    \"\"\"\n    self.session_id = session_id\n    self.underlying_session = underlying_session\n    self.ttl = ttl\n\n    master = _ensure_fernet_key_bytes(encryption_key)\n    self.cipher = _derive_session_fernet_key(master, session_id)\n    self._kid = \"hkdf-v1\"\n    self._ver = 1\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/","title":"<code>RedisSession</code>","text":"<p>               Bases: <code>SessionABC</code></p> <p>Redis implementation of :pyclass:<code>agents.memory.session.Session</code>.</p> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>class RedisSession(SessionABC):\n    \"\"\"Redis implementation of :pyclass:`agents.memory.session.Session`.\"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        *,\n        redis_client: Redis,\n        key_prefix: str = \"agents:session\",\n        ttl: int | None = None,\n        session_settings: SessionSettings | None = None,\n    ):\n        \"\"\"Initializes a new RedisSession.\n\n        Args:\n            session_id (str): Unique identifier for the conversation.\n            redis_client (Redis[bytes]): A pre-configured Redis async client.\n            key_prefix (str, optional): Prefix for Redis keys to avoid collisions.\n                Defaults to \"agents:session\".\n            ttl (int | None, optional): Time-to-live in seconds for session data.\n                If None, data persists indefinitely. Defaults to None.\n            session_settings (SessionSettings | None): Session configuration settings including\n                default limit for retrieving items. If None, uses default SessionSettings().\n        \"\"\"\n        self.session_id = session_id\n        self.session_settings = session_settings or SessionSettings()\n        self._redis = redis_client\n        self._key_prefix = key_prefix\n        self._ttl = ttl\n        self._lock = asyncio.Lock()\n        self._owns_client = False  # Track if we own the Redis client\n\n        # Redis key patterns\n        self._session_key = f\"{self._key_prefix}:{self.session_id}\"\n        self._messages_key = f\"{self._session_key}:messages\"\n        self._counter_key = f\"{self._session_key}:counter\"\n\n    @classmethod\n    def from_url(\n        cls,\n        session_id: str,\n        *,\n        url: str,\n        redis_kwargs: dict[str, Any] | None = None,\n        session_settings: SessionSettings | None = None,\n        **kwargs: Any,\n    ) -&gt; RedisSession:\n        \"\"\"Create a session from a Redis URL string.\n\n        Args:\n            session_id (str): Conversation ID.\n            url (str): Redis URL, e.g. \"redis://localhost:6379/0\" or \"rediss://host:6380\".\n            redis_kwargs (dict[str, Any] | None): Additional keyword arguments forwarded to\n                redis.asyncio.from_url.\n            session_settings (SessionSettings | None): Session configuration settings including\n                default limit for retrieving items. If None, uses default SessionSettings().\n            **kwargs: Additional keyword arguments forwarded to the main constructor\n                (e.g., key_prefix, ttl, etc.).\n\n        Returns:\n            RedisSession: An instance of RedisSession connected to the specified Redis server.\n        \"\"\"\n        redis_kwargs = redis_kwargs or {}\n\n        redis_client = redis.from_url(url, **redis_kwargs)\n        session = cls(\n            session_id,\n            redis_client=redis_client,\n            session_settings=session_settings,\n            **kwargs,\n        )\n        session._owns_client = True  # We created the client, so we own it\n        return session\n\n    async def _serialize_item(self, item: TResponseInputItem) -&gt; str:\n        \"\"\"Serialize an item to JSON string. Can be overridden by subclasses.\"\"\"\n        return json.dumps(item, separators=(\",\", \":\"))\n\n    async def _deserialize_item(self, item: str) -&gt; TResponseInputItem:\n        \"\"\"Deserialize a JSON string to an item. Can be overridden by subclasses.\"\"\"\n        return json.loads(item)  # type: ignore[no-any-return]  # json.loads returns Any but we know the structure\n\n    async def _get_next_id(self) -&gt; int:\n        \"\"\"Get the next message ID using Redis INCR for atomic increment.\"\"\"\n        result = await self._redis.incr(self._counter_key)\n        return int(result)\n\n    async def _set_ttl_if_configured(self, *keys: str) -&gt; None:\n        \"\"\"Set TTL on keys if configured.\"\"\"\n        if self._ttl is not None:\n            pipe = self._redis.pipeline()\n            for key in keys:\n                pipe.expire(key, self._ttl)\n            await pipe.execute()\n\n    # ------------------------------------------------------------------\n    # Session protocol implementation\n    # ------------------------------------------------------------------\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        async with self._lock:\n            if session_limit is None:\n                # Get all messages in chronological order\n                raw_messages = await self._redis.lrange(self._messages_key, 0, -1)  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n            else:\n                if session_limit &lt;= 0:\n                    return []\n                # Get the latest N messages (Redis list is ordered chronologically)\n                # Use negative indices to get from the end - Redis uses -N to -1 for last N items\n                raw_messages = await self._redis.lrange(self._messages_key, -session_limit, -1)  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n\n            items: list[TResponseInputItem] = []\n            for raw_msg in raw_messages:\n                try:\n                    # Handle both bytes (default) and str (decode_responses=True) Redis clients\n                    if isinstance(raw_msg, bytes):\n                        msg_str = raw_msg.decode(\"utf-8\")\n                    else:\n                        msg_str = raw_msg  # Already a string\n                    item = await self._deserialize_item(msg_str)\n                    items.append(item)\n                except (json.JSONDecodeError, UnicodeDecodeError):\n                    # Skip corrupted messages\n                    continue\n\n            return items\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        if not items:\n            return\n\n        async with self._lock:\n            pipe = self._redis.pipeline()\n\n            # Set session metadata with current timestamp\n            pipe.hset(\n                self._session_key,\n                mapping={\n                    \"session_id\": self.session_id,\n                    \"created_at\": str(int(time.time())),\n                    \"updated_at\": str(int(time.time())),\n                },\n            )\n\n            # Add all items to the messages list\n            serialized_items = []\n            for item in items:\n                serialized = await self._serialize_item(item)\n                serialized_items.append(serialized)\n\n            if serialized_items:\n                pipe.rpush(self._messages_key, *serialized_items)\n\n            # Update the session timestamp\n            pipe.hset(self._session_key, \"updated_at\", str(int(time.time())))\n\n            # Execute all commands\n            await pipe.execute()\n\n            # Set TTL if configured\n            await self._set_ttl_if_configured(\n                self._session_key, self._messages_key, self._counter_key\n            )\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        async with self._lock:\n            # Use RPOP to atomically remove and return the rightmost (most recent) item\n            raw_msg = await self._redis.rpop(self._messages_key)  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n\n            if raw_msg is None:\n                return None\n\n            try:\n                # Handle both bytes (default) and str (decode_responses=True) Redis clients\n                if isinstance(raw_msg, bytes):\n                    msg_str = raw_msg.decode(\"utf-8\")\n                else:\n                    msg_str = raw_msg  # Already a string\n                return await self._deserialize_item(msg_str)\n            except (json.JSONDecodeError, UnicodeDecodeError):\n                # Return None for corrupted messages (already removed)\n                return None\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        async with self._lock:\n            # Delete all keys associated with this session\n            await self._redis.delete(\n                self._session_key,\n                self._messages_key,\n                self._counter_key,\n            )\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the Redis connection.\n\n        Only closes the connection if this session owns the Redis client\n        (i.e., created via from_url). If the client was injected externally,\n        the caller is responsible for managing its lifecycle.\n        \"\"\"\n        if self._owns_client:\n            await self._redis.aclose()\n\n    async def ping(self) -&gt; bool:\n        \"\"\"Test Redis connectivity.\n\n        Returns:\n            True if Redis is reachable, False otherwise.\n        \"\"\"\n        try:\n            await self._redis.ping()  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    *,\n    redis_client: Redis,\n    key_prefix: str = \"agents:session\",\n    ttl: int | None = None,\n    session_settings: SessionSettings | None = None,\n)\n</code></pre> <p>Initializes a new RedisSession.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Unique identifier for the conversation.</p> required <code>redis_client</code> <code>Redis[bytes]</code> <p>A pre-configured Redis async client.</p> required <code>key_prefix</code> <code>str</code> <p>Prefix for Redis keys to avoid collisions. Defaults to \"agents:session\".</p> <code>'agents:session'</code> <code>ttl</code> <code>int | None</code> <p>Time-to-live in seconds for session data. If None, data persists indefinitely. Defaults to None.</p> <code>None</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings including default limit for retrieving items. If None, uses default SessionSettings().</p> <code>None</code> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    *,\n    redis_client: Redis,\n    key_prefix: str = \"agents:session\",\n    ttl: int | None = None,\n    session_settings: SessionSettings | None = None,\n):\n    \"\"\"Initializes a new RedisSession.\n\n    Args:\n        session_id (str): Unique identifier for the conversation.\n        redis_client (Redis[bytes]): A pre-configured Redis async client.\n        key_prefix (str, optional): Prefix for Redis keys to avoid collisions.\n            Defaults to \"agents:session\".\n        ttl (int | None, optional): Time-to-live in seconds for session data.\n            If None, data persists indefinitely. Defaults to None.\n        session_settings (SessionSettings | None): Session configuration settings including\n            default limit for retrieving items. If None, uses default SessionSettings().\n    \"\"\"\n    self.session_id = session_id\n    self.session_settings = session_settings or SessionSettings()\n    self._redis = redis_client\n    self._key_prefix = key_prefix\n    self._ttl = ttl\n    self._lock = asyncio.Lock()\n    self._owns_client = False  # Track if we own the Redis client\n\n    # Redis key patterns\n    self._session_key = f\"{self._key_prefix}:{self.session_id}\"\n    self._messages_key = f\"{self._session_key}:messages\"\n    self._counter_key = f\"{self._session_key}:counter\"\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    session_id: str,\n    *,\n    url: str,\n    redis_kwargs: dict[str, Any] | None = None,\n    session_settings: SessionSettings | None = None,\n    **kwargs: Any,\n) -&gt; RedisSession\n</code></pre> <p>Create a session from a Redis URL string.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Conversation ID.</p> required <code>url</code> <code>str</code> <p>Redis URL, e.g. \"redis://localhost:6379/0\" or \"rediss://host:6380\".</p> required <code>redis_kwargs</code> <code>dict[str, Any] | None</code> <p>Additional keyword arguments forwarded to redis.asyncio.from_url.</p> <code>None</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings including default limit for retrieving items. If None, uses default SessionSettings().</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments forwarded to the main constructor (e.g., key_prefix, ttl, etc.).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RedisSession</code> <code>RedisSession</code> <p>An instance of RedisSession connected to the specified Redis server.</p> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>@classmethod\ndef from_url(\n    cls,\n    session_id: str,\n    *,\n    url: str,\n    redis_kwargs: dict[str, Any] | None = None,\n    session_settings: SessionSettings | None = None,\n    **kwargs: Any,\n) -&gt; RedisSession:\n    \"\"\"Create a session from a Redis URL string.\n\n    Args:\n        session_id (str): Conversation ID.\n        url (str): Redis URL, e.g. \"redis://localhost:6379/0\" or \"rediss://host:6380\".\n        redis_kwargs (dict[str, Any] | None): Additional keyword arguments forwarded to\n            redis.asyncio.from_url.\n        session_settings (SessionSettings | None): Session configuration settings including\n            default limit for retrieving items. If None, uses default SessionSettings().\n        **kwargs: Additional keyword arguments forwarded to the main constructor\n            (e.g., key_prefix, ttl, etc.).\n\n    Returns:\n        RedisSession: An instance of RedisSession connected to the specified Redis server.\n    \"\"\"\n    redis_kwargs = redis_kwargs or {}\n\n    redis_client = redis.from_url(url, **redis_kwargs)\n    session = cls(\n        session_id,\n        redis_client=redis_client,\n        session_settings=session_settings,\n        **kwargs,\n    )\n    session._owns_client = True  # We created the client, so we own it\n    return session\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, uses session_settings.limit.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    session_limit = resolve_session_limit(limit, self.session_settings)\n\n    async with self._lock:\n        if session_limit is None:\n            # Get all messages in chronological order\n            raw_messages = await self._redis.lrange(self._messages_key, 0, -1)  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n        else:\n            if session_limit &lt;= 0:\n                return []\n            # Get the latest N messages (Redis list is ordered chronologically)\n            # Use negative indices to get from the end - Redis uses -N to -1 for last N items\n            raw_messages = await self._redis.lrange(self._messages_key, -session_limit, -1)  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n\n        items: list[TResponseInputItem] = []\n        for raw_msg in raw_messages:\n            try:\n                # Handle both bytes (default) and str (decode_responses=True) Redis clients\n                if isinstance(raw_msg, bytes):\n                    msg_str = raw_msg.decode(\"utf-8\")\n                else:\n                    msg_str = raw_msg  # Already a string\n                item = await self._deserialize_item(msg_str)\n                items.append(item)\n            except (json.JSONDecodeError, UnicodeDecodeError):\n                # Skip corrupted messages\n                continue\n\n        return items\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    if not items:\n        return\n\n    async with self._lock:\n        pipe = self._redis.pipeline()\n\n        # Set session metadata with current timestamp\n        pipe.hset(\n            self._session_key,\n            mapping={\n                \"session_id\": self.session_id,\n                \"created_at\": str(int(time.time())),\n                \"updated_at\": str(int(time.time())),\n            },\n        )\n\n        # Add all items to the messages list\n        serialized_items = []\n        for item in items:\n            serialized = await self._serialize_item(item)\n            serialized_items.append(serialized)\n\n        if serialized_items:\n            pipe.rpush(self._messages_key, *serialized_items)\n\n        # Update the session timestamp\n        pipe.hset(self._session_key, \"updated_at\", str(int(time.time())))\n\n        # Execute all commands\n        await pipe.execute()\n\n        # Set TTL if configured\n        await self._set_ttl_if_configured(\n            self._session_key, self._messages_key, self._counter_key\n        )\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    async with self._lock:\n        # Use RPOP to atomically remove and return the rightmost (most recent) item\n        raw_msg = await self._redis.rpop(self._messages_key)  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n\n        if raw_msg is None:\n            return None\n\n        try:\n            # Handle both bytes (default) and str (decode_responses=True) Redis clients\n            if isinstance(raw_msg, bytes):\n                msg_str = raw_msg.decode(\"utf-8\")\n            else:\n                msg_str = raw_msg  # Already a string\n            return await self._deserialize_item(msg_str)\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            # Return None for corrupted messages (already removed)\n            return None\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    async with self._lock:\n        # Delete all keys associated with this session\n        await self._redis.delete(\n            self._session_key,\n            self._messages_key,\n            self._counter_key,\n        )\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the Redis connection.</p> <p>Only closes the connection if this session owns the Redis client (i.e., created via from_url). If the client was injected externally, the caller is responsible for managing its lifecycle.</p> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the Redis connection.\n\n    Only closes the connection if this session owns the Redis client\n    (i.e., created via from_url). If the client was injected externally,\n    the caller is responsible for managing its lifecycle.\n    \"\"\"\n    if self._owns_client:\n        await self._redis.aclose()\n</code></pre>"},{"location":"ref/extensions/memory/redis_session/#agents.extensions.memory.redis_session.RedisSession.ping","title":"ping  <code>async</code>","text":"<pre><code>ping() -&gt; bool\n</code></pre> <p>Test Redis connectivity.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if Redis is reachable, False otherwise.</p> Source code in <code>src/agents/extensions/memory/redis_session.py</code> <pre><code>async def ping(self) -&gt; bool:\n    \"\"\"Test Redis connectivity.\n\n    Returns:\n        True if Redis is reachable, False otherwise.\n    \"\"\"\n    try:\n        await self._redis.ping()  # type: ignore[misc]  # Redis library returns Union[Awaitable[T], T] in async context\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"ref/extensions/memory/sqlalchemy_session/","title":"<code>SQLAlchemySession</code>","text":"<p>               Bases: <code>SessionABC</code></p> <p>SQLAlchemy implementation of :pyclass:<code>agents.memory.session.Session</code>.</p> Source code in <code>src/agents/extensions/memory/sqlalchemy_session.py</code> <pre><code>class SQLAlchemySession(SessionABC):\n    \"\"\"SQLAlchemy implementation of :pyclass:`agents.memory.session.Session`.\"\"\"\n\n    _metadata: MetaData\n    _sessions: Table\n    _messages: Table\n\n    def __init__(\n        self,\n        session_id: str,\n        *,\n        engine: AsyncEngine,\n        create_tables: bool = False,\n        sessions_table: str = \"agent_sessions\",\n        messages_table: str = \"agent_messages\",\n        session_settings: SessionSettings | None = None,\n    ):\n        \"\"\"Initializes a new SQLAlchemySession.\n\n        Args:\n            session_id (str): Unique identifier for the conversation.\n            engine (AsyncEngine): A pre-configured SQLAlchemy async engine. The engine\n                must be created with an async driver (e.g., 'postgresql+asyncpg://',\n                'mysql+aiomysql://', or 'sqlite+aiosqlite://').\n            create_tables (bool, optional): Whether to automatically create the required\n                tables and indexes. Defaults to False for production use. Set to True for\n                development and testing when migrations aren't used.\n            sessions_table (str, optional): Override the default table name for sessions if needed.\n            messages_table (str, optional): Override the default table name for messages if needed.\n            session_settings (SessionSettings | None, optional): Session configuration settings\n        \"\"\"\n        self.session_id = session_id\n        self.session_settings = session_settings or SessionSettings()\n        self._engine = engine\n        self._lock = asyncio.Lock()\n\n        self._metadata = MetaData()\n        self._sessions = Table(\n            sessions_table,\n            self._metadata,\n            Column(\"session_id\", String, primary_key=True),\n            Column(\n                \"created_at\",\n                TIMESTAMP(timezone=False),\n                server_default=sql_text(\"CURRENT_TIMESTAMP\"),\n                nullable=False,\n            ),\n            Column(\n                \"updated_at\",\n                TIMESTAMP(timezone=False),\n                server_default=sql_text(\"CURRENT_TIMESTAMP\"),\n                onupdate=sql_text(\"CURRENT_TIMESTAMP\"),\n                nullable=False,\n            ),\n        )\n\n        self._messages = Table(\n            messages_table,\n            self._metadata,\n            Column(\"id\", Integer, primary_key=True, autoincrement=True),\n            Column(\n                \"session_id\",\n                String,\n                ForeignKey(f\"{sessions_table}.session_id\", ondelete=\"CASCADE\"),\n                nullable=False,\n            ),\n            Column(\"message_data\", Text, nullable=False),\n            Column(\n                \"created_at\",\n                TIMESTAMP(timezone=False),\n                server_default=sql_text(\"CURRENT_TIMESTAMP\"),\n                nullable=False,\n            ),\n            Index(\n                f\"idx_{messages_table}_session_time\",\n                \"session_id\",\n                \"created_at\",\n            ),\n            sqlite_autoincrement=True,\n        )\n\n        # Async session factory\n        self._session_factory = async_sessionmaker(self._engine, expire_on_commit=False)\n\n        self._create_tables = create_tables\n\n    # ---------------------------------------------------------------------\n    # Convenience constructors\n    # ---------------------------------------------------------------------\n    @classmethod\n    def from_url(\n        cls,\n        session_id: str,\n        *,\n        url: str,\n        engine_kwargs: dict[str, Any] | None = None,\n        session_settings: SessionSettings | None = None,\n        **kwargs: Any,\n    ) -&gt; SQLAlchemySession:\n        \"\"\"Create a session from a database URL string.\n\n        Args:\n            session_id (str): Conversation ID.\n            url (str): Any SQLAlchemy async URL, e.g. \"postgresql+asyncpg://user:pass@host/db\".\n            engine_kwargs (dict[str, Any] | None): Additional keyword arguments forwarded to\n                sqlalchemy.ext.asyncio.create_async_engine.\n            session_settings (SessionSettings | None): Session configuration settings including\n                default limit for retrieving items. If None, uses default SessionSettings().\n            **kwargs: Additional keyword arguments forwarded to the main constructor\n                (e.g., create_tables, custom table names, etc.).\n\n        Returns:\n            SQLAlchemySession: An instance of SQLAlchemySession connected to the specified database.\n        \"\"\"\n        engine_kwargs = engine_kwargs or {}\n        engine = create_async_engine(url, **engine_kwargs)\n        return cls(session_id, engine=engine, session_settings=session_settings, **kwargs)\n\n    async def _serialize_item(self, item: TResponseInputItem) -&gt; str:\n        \"\"\"Serialize an item to JSON string. Can be overridden by subclasses.\"\"\"\n        return json.dumps(item, separators=(\",\", \":\"))\n\n    async def _deserialize_item(self, item: str) -&gt; TResponseInputItem:\n        \"\"\"Deserialize a JSON string to an item. Can be overridden by subclasses.\"\"\"\n        return json.loads(item)  # type: ignore[no-any-return]\n\n    # ------------------------------------------------------------------\n    # Session protocol implementation\n    # ------------------------------------------------------------------\n    async def _ensure_tables(self) -&gt; None:\n        \"\"\"Ensure tables are created before any database operations.\"\"\"\n        if self._create_tables:\n            async with self._engine.begin() as conn:\n                await conn.run_sync(self._metadata.create_all)\n            self._create_tables = False  # Only create once\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        await self._ensure_tables()\n\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        async with self._session_factory() as sess:\n            if session_limit is None:\n                stmt = (\n                    select(self._messages.c.message_data)\n                    .where(self._messages.c.session_id == self.session_id)\n                    .order_by(\n                        self._messages.c.created_at.asc(),\n                        self._messages.c.id.asc(),\n                    )\n                )\n            else:\n                stmt = (\n                    select(self._messages.c.message_data)\n                    .where(self._messages.c.session_id == self.session_id)\n                    # Use DESC + LIMIT to get the latest N\n                    # then reverse later for chronological order.\n                    .order_by(\n                        self._messages.c.created_at.desc(),\n                        self._messages.c.id.desc(),\n                    )\n                    .limit(session_limit)\n                )\n\n            result = await sess.execute(stmt)\n            rows: list[str] = [row[0] for row in result.all()]\n\n            if session_limit is not None:\n                rows.reverse()\n\n            items: list[TResponseInputItem] = []\n            for raw in rows:\n                try:\n                    items.append(await self._deserialize_item(raw))\n                except json.JSONDecodeError:\n                    # Skip corrupted rows\n                    continue\n            return items\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        if not items:\n            return\n\n        await self._ensure_tables()\n        payload = [\n            {\n                \"session_id\": self.session_id,\n                \"message_data\": await self._serialize_item(item),\n            }\n            for item in items\n        ]\n\n        async with self._session_factory() as sess:\n            async with sess.begin():\n                # Ensure the parent session row exists - use merge for cross-DB compatibility\n                # Check if session exists\n                existing = await sess.execute(\n                    select(self._sessions.c.session_id).where(\n                        self._sessions.c.session_id == self.session_id\n                    )\n                )\n                if not existing.scalar_one_or_none():\n                    # Session doesn't exist, create it\n                    await sess.execute(\n                        insert(self._sessions).values({\"session_id\": self.session_id})\n                    )\n\n                # Insert messages in bulk\n                await sess.execute(insert(self._messages), payload)\n\n                # Touch updated_at column\n                await sess.execute(\n                    update(self._sessions)\n                    .where(self._sessions.c.session_id == self.session_id)\n                    .values(updated_at=sql_text(\"CURRENT_TIMESTAMP\"))\n                )\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        await self._ensure_tables()\n        async with self._session_factory() as sess:\n            async with sess.begin():\n                # Fallback for all dialects - get ID first, then delete\n                subq = (\n                    select(self._messages.c.id)\n                    .where(self._messages.c.session_id == self.session_id)\n                    .order_by(\n                        self._messages.c.created_at.desc(),\n                        self._messages.c.id.desc(),\n                    )\n                    .limit(1)\n                )\n                res = await sess.execute(subq)\n                row_id = res.scalar_one_or_none()\n                if row_id is None:\n                    return None\n                # Fetch data before deleting\n                res_data = await sess.execute(\n                    select(self._messages.c.message_data).where(self._messages.c.id == row_id)\n                )\n                row = res_data.scalar_one_or_none()\n                await sess.execute(delete(self._messages).where(self._messages.c.id == row_id))\n\n                if row is None:\n                    return None\n                try:\n                    return await self._deserialize_item(row)\n                except json.JSONDecodeError:\n                    return None\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        await self._ensure_tables()\n        async with self._session_factory() as sess:\n            async with sess.begin():\n                await sess.execute(\n                    delete(self._messages).where(self._messages.c.session_id == self.session_id)\n                )\n                await sess.execute(\n                    delete(self._sessions).where(self._sessions.c.session_id == self.session_id)\n                )\n\n    @property\n    def engine(self) -&gt; AsyncEngine:\n        \"\"\"Access the underlying SQLAlchemy AsyncEngine.\n\n        This property provides direct access to the engine for advanced use cases,\n        such as checking connection pool status, configuring engine settings,\n        or manually disposing the engine when needed.\n\n        Returns:\n            AsyncEngine: The SQLAlchemy async engine instance.\n        \"\"\"\n        return self._engine\n</code></pre>"},{"location":"ref/extensions/memory/sqlalchemy_session/#agents.extensions.memory.sqlalchemy_session.SQLAlchemySession.engine","title":"engine  <code>property</code>","text":"<pre><code>engine: AsyncEngine\n</code></pre> <p>Access the underlying SQLAlchemy AsyncEngine.</p> <p>This property provides direct access to the engine for advanced use cases, such as checking connection pool status, configuring engine settings, or manually disposing the engine when needed.</p> <p>Returns:</p> Name Type Description <code>AsyncEngine</code> <code>AsyncEngine</code> <p>The SQLAlchemy async engine instance.</p>"},{"location":"ref/extensions/memory/sqlalchemy_session/#agents.extensions.memory.sqlalchemy_session.SQLAlchemySession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    *,\n    engine: AsyncEngine,\n    create_tables: bool = False,\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n    session_settings: SessionSettings | None = None,\n)\n</code></pre> <p>Initializes a new SQLAlchemySession.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Unique identifier for the conversation.</p> required <code>engine</code> <code>AsyncEngine</code> <p>A pre-configured SQLAlchemy async engine. The engine must be created with an async driver (e.g., 'postgresql+asyncpg://', 'mysql+aiomysql://', or 'sqlite+aiosqlite://').</p> required <code>create_tables</code> <code>bool</code> <p>Whether to automatically create the required tables and indexes. Defaults to False for production use. Set to True for development and testing when migrations aren't used.</p> <code>False</code> <code>sessions_table</code> <code>str</code> <p>Override the default table name for sessions if needed.</p> <code>'agent_sessions'</code> <code>messages_table</code> <code>str</code> <p>Override the default table name for messages if needed.</p> <code>'agent_messages'</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings</p> <code>None</code> Source code in <code>src/agents/extensions/memory/sqlalchemy_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    *,\n    engine: AsyncEngine,\n    create_tables: bool = False,\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n    session_settings: SessionSettings | None = None,\n):\n    \"\"\"Initializes a new SQLAlchemySession.\n\n    Args:\n        session_id (str): Unique identifier for the conversation.\n        engine (AsyncEngine): A pre-configured SQLAlchemy async engine. The engine\n            must be created with an async driver (e.g., 'postgresql+asyncpg://',\n            'mysql+aiomysql://', or 'sqlite+aiosqlite://').\n        create_tables (bool, optional): Whether to automatically create the required\n            tables and indexes. Defaults to False for production use. Set to True for\n            development and testing when migrations aren't used.\n        sessions_table (str, optional): Override the default table name for sessions if needed.\n        messages_table (str, optional): Override the default table name for messages if needed.\n        session_settings (SessionSettings | None, optional): Session configuration settings\n    \"\"\"\n    self.session_id = session_id\n    self.session_settings = session_settings or SessionSettings()\n    self._engine = engine\n    self._lock = asyncio.Lock()\n\n    self._metadata = MetaData()\n    self._sessions = Table(\n        sessions_table,\n        self._metadata,\n        Column(\"session_id\", String, primary_key=True),\n        Column(\n            \"created_at\",\n            TIMESTAMP(timezone=False),\n            server_default=sql_text(\"CURRENT_TIMESTAMP\"),\n            nullable=False,\n        ),\n        Column(\n            \"updated_at\",\n            TIMESTAMP(timezone=False),\n            server_default=sql_text(\"CURRENT_TIMESTAMP\"),\n            onupdate=sql_text(\"CURRENT_TIMESTAMP\"),\n            nullable=False,\n        ),\n    )\n\n    self._messages = Table(\n        messages_table,\n        self._metadata,\n        Column(\"id\", Integer, primary_key=True, autoincrement=True),\n        Column(\n            \"session_id\",\n            String,\n            ForeignKey(f\"{sessions_table}.session_id\", ondelete=\"CASCADE\"),\n            nullable=False,\n        ),\n        Column(\"message_data\", Text, nullable=False),\n        Column(\n            \"created_at\",\n            TIMESTAMP(timezone=False),\n            server_default=sql_text(\"CURRENT_TIMESTAMP\"),\n            nullable=False,\n        ),\n        Index(\n            f\"idx_{messages_table}_session_time\",\n            \"session_id\",\n            \"created_at\",\n        ),\n        sqlite_autoincrement=True,\n    )\n\n    # Async session factory\n    self._session_factory = async_sessionmaker(self._engine, expire_on_commit=False)\n\n    self._create_tables = create_tables\n</code></pre>"},{"location":"ref/extensions/memory/sqlalchemy_session/#agents.extensions.memory.sqlalchemy_session.SQLAlchemySession.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    session_id: str,\n    *,\n    url: str,\n    engine_kwargs: dict[str, Any] | None = None,\n    session_settings: SessionSettings | None = None,\n    **kwargs: Any,\n) -&gt; SQLAlchemySession\n</code></pre> <p>Create a session from a database URL string.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Conversation ID.</p> required <code>url</code> <code>str</code> <p>Any SQLAlchemy async URL, e.g. \"postgresql+asyncpg://user:pass@host/db\".</p> required <code>engine_kwargs</code> <code>dict[str, Any] | None</code> <p>Additional keyword arguments forwarded to sqlalchemy.ext.asyncio.create_async_engine.</p> <code>None</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings including default limit for retrieving items. If None, uses default SessionSettings().</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments forwarded to the main constructor (e.g., create_tables, custom table names, etc.).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SQLAlchemySession</code> <code>SQLAlchemySession</code> <p>An instance of SQLAlchemySession connected to the specified database.</p> Source code in <code>src/agents/extensions/memory/sqlalchemy_session.py</code> <pre><code>@classmethod\ndef from_url(\n    cls,\n    session_id: str,\n    *,\n    url: str,\n    engine_kwargs: dict[str, Any] | None = None,\n    session_settings: SessionSettings | None = None,\n    **kwargs: Any,\n) -&gt; SQLAlchemySession:\n    \"\"\"Create a session from a database URL string.\n\n    Args:\n        session_id (str): Conversation ID.\n        url (str): Any SQLAlchemy async URL, e.g. \"postgresql+asyncpg://user:pass@host/db\".\n        engine_kwargs (dict[str, Any] | None): Additional keyword arguments forwarded to\n            sqlalchemy.ext.asyncio.create_async_engine.\n        session_settings (SessionSettings | None): Session configuration settings including\n            default limit for retrieving items. If None, uses default SessionSettings().\n        **kwargs: Additional keyword arguments forwarded to the main constructor\n            (e.g., create_tables, custom table names, etc.).\n\n    Returns:\n        SQLAlchemySession: An instance of SQLAlchemySession connected to the specified database.\n    \"\"\"\n    engine_kwargs = engine_kwargs or {}\n    engine = create_async_engine(url, **engine_kwargs)\n    return cls(session_id, engine=engine, session_settings=session_settings, **kwargs)\n</code></pre>"},{"location":"ref/extensions/memory/sqlalchemy_session/#agents.extensions.memory.sqlalchemy_session.SQLAlchemySession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, uses session_settings.limit.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/extensions/memory/sqlalchemy_session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    await self._ensure_tables()\n\n    session_limit = resolve_session_limit(limit, self.session_settings)\n\n    async with self._session_factory() as sess:\n        if session_limit is None:\n            stmt = (\n                select(self._messages.c.message_data)\n                .where(self._messages.c.session_id == self.session_id)\n                .order_by(\n                    self._messages.c.created_at.asc(),\n                    self._messages.c.id.asc(),\n                )\n            )\n        else:\n            stmt = (\n                select(self._messages.c.message_data)\n                .where(self._messages.c.session_id == self.session_id)\n                # Use DESC + LIMIT to get the latest N\n                # then reverse later for chronological order.\n                .order_by(\n                    self._messages.c.created_at.desc(),\n                    self._messages.c.id.desc(),\n                )\n                .limit(session_limit)\n            )\n\n        result = await sess.execute(stmt)\n        rows: list[str] = [row[0] for row in result.all()]\n\n        if session_limit is not None:\n            rows.reverse()\n\n        items: list[TResponseInputItem] = []\n        for raw in rows:\n            try:\n                items.append(await self._deserialize_item(raw))\n            except json.JSONDecodeError:\n                # Skip corrupted rows\n                continue\n        return items\n</code></pre>"},{"location":"ref/extensions/memory/sqlalchemy_session/#agents.extensions.memory.sqlalchemy_session.SQLAlchemySession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/extensions/memory/sqlalchemy_session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    if not items:\n        return\n\n    await self._ensure_tables()\n    payload = [\n        {\n            \"session_id\": self.session_id,\n            \"message_data\": await self._serialize_item(item),\n        }\n        for item in items\n    ]\n\n    async with self._session_factory() as sess:\n        async with sess.begin():\n            # Ensure the parent session row exists - use merge for cross-DB compatibility\n            # Check if session exists\n            existing = await sess.execute(\n                select(self._sessions.c.session_id).where(\n                    self._sessions.c.session_id == self.session_id\n                )\n            )\n            if not existing.scalar_one_or_none():\n                # Session doesn't exist, create it\n                await sess.execute(\n                    insert(self._sessions).values({\"session_id\": self.session_id})\n                )\n\n            # Insert messages in bulk\n            await sess.execute(insert(self._messages), payload)\n\n            # Touch updated_at column\n            await sess.execute(\n                update(self._sessions)\n                .where(self._sessions.c.session_id == self.session_id)\n                .values(updated_at=sql_text(\"CURRENT_TIMESTAMP\"))\n            )\n</code></pre>"},{"location":"ref/extensions/memory/sqlalchemy_session/#agents.extensions.memory.sqlalchemy_session.SQLAlchemySession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/extensions/memory/sqlalchemy_session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    await self._ensure_tables()\n    async with self._session_factory() as sess:\n        async with sess.begin():\n            # Fallback for all dialects - get ID first, then delete\n            subq = (\n                select(self._messages.c.id)\n                .where(self._messages.c.session_id == self.session_id)\n                .order_by(\n                    self._messages.c.created_at.desc(),\n                    self._messages.c.id.desc(),\n                )\n                .limit(1)\n            )\n            res = await sess.execute(subq)\n            row_id = res.scalar_one_or_none()\n            if row_id is None:\n                return None\n            # Fetch data before deleting\n            res_data = await sess.execute(\n                select(self._messages.c.message_data).where(self._messages.c.id == row_id)\n            )\n            row = res_data.scalar_one_or_none()\n            await sess.execute(delete(self._messages).where(self._messages.c.id == row_id))\n\n            if row is None:\n                return None\n            try:\n                return await self._deserialize_item(row)\n            except json.JSONDecodeError:\n                return None\n</code></pre>"},{"location":"ref/extensions/memory/sqlalchemy_session/#agents.extensions.memory.sqlalchemy_session.SQLAlchemySession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/extensions/memory/sqlalchemy_session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    await self._ensure_tables()\n    async with self._session_factory() as sess:\n        async with sess.begin():\n            await sess.execute(\n                delete(self._messages).where(self._messages.c.session_id == self.session_id)\n            )\n            await sess.execute(\n                delete(self._sessions).where(self._sessions.c.session_id == self.session_id)\n            )\n</code></pre>"},{"location":"ref/extensions/models/litellm_model/","title":"<code>LiteLLM Model</code>","text":""},{"location":"ref/extensions/models/litellm_model/#agents.extensions.models.litellm_model.InternalChatCompletionMessage","title":"InternalChatCompletionMessage","text":"<p>               Bases: <code>ChatCompletionMessage</code></p> <p>An internal subclass to carry reasoning_content and thinking_blocks without modifying the original model.</p> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class InternalChatCompletionMessage(ChatCompletionMessage):\n    \"\"\"\n    An internal subclass to carry reasoning_content and thinking_blocks without modifying the original model.\n    \"\"\"  # noqa: E501\n\n    reasoning_content: str\n    thinking_blocks: list[dict[str, Any]] | None = None\n</code></pre>"},{"location":"ref/extensions/models/litellm_model/#agents.extensions.models.litellm_model.InternalToolCall","title":"InternalToolCall","text":"<p>               Bases: <code>ChatCompletionMessageFunctionToolCall</code></p> <p>An internal subclass to carry provider-specific metadata (e.g., Gemini thought signatures) without modifying the original model.</p> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class InternalToolCall(ChatCompletionMessageFunctionToolCall):\n    \"\"\"\n    An internal subclass to carry provider-specific metadata (e.g., Gemini thought signatures)\n    without modifying the original model.\n    \"\"\"\n\n    extra_content: dict[str, Any] | None = None\n</code></pre>"},{"location":"ref/extensions/models/litellm_model/#agents.extensions.models.litellm_model.LitellmModel","title":"LitellmModel","text":"<p>               Bases: <code>Model</code></p> <p>This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI, Anthropic, Gemini, Mistral, and many other models. See supported models here: litellm models.</p> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class LitellmModel(Model):\n    \"\"\"This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI,\n    Anthropic, Gemini, Mistral, and many other models.\n    See supported models here: [litellm models](https://docs.litellm.ai/docs/providers).\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        base_url: str | None = None,\n        api_key: str | None = None,\n    ):\n        self.model = model\n        self.base_url = base_url\n        self.api_key = api_key\n\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,  # unused\n        conversation_id: str | None = None,  # unused\n        prompt: Any | None = None,\n    ) -&gt; ModelResponse:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict()\n            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=False,\n                prompt=prompt,\n            )\n\n            message: litellm.types.utils.Message | None = None\n            first_choice: litellm.types.utils.Choices | None = None\n            if response.choices and len(response.choices) &gt; 0:\n                choice = response.choices[0]\n                if isinstance(choice, litellm.types.utils.Choices):\n                    first_choice = choice\n                    message = first_choice.message\n\n            if _debug.DONT_LOG_MODEL_DATA:\n                logger.debug(\"Received model response\")\n            else:\n                if message is not None:\n                    logger.debug(\n                        f\"\"\"LLM resp:\\n{\n                            json.dumps(message.model_dump(), indent=2, ensure_ascii=False)\n                        }\\n\"\"\"\n                    )\n                else:\n                    finish_reason = first_choice.finish_reason if first_choice else \"-\"\n                    logger.debug(f\"LLM resp had no message. finish_reason: {finish_reason}\")\n\n            if hasattr(response, \"usage\"):\n                response_usage = response.usage\n                usage = (\n                    Usage(\n                        requests=1,\n                        input_tokens=response_usage.prompt_tokens,\n                        output_tokens=response_usage.completion_tokens,\n                        total_tokens=response_usage.total_tokens,\n                        input_tokens_details=InputTokensDetails(\n                            cached_tokens=getattr(\n                                response_usage.prompt_tokens_details, \"cached_tokens\", 0\n                            )\n                            or 0\n                        ),\n                        output_tokens_details=OutputTokensDetails(\n                            reasoning_tokens=getattr(\n                                response_usage.completion_tokens_details, \"reasoning_tokens\", 0\n                            )\n                            or 0\n                        ),\n                    )\n                    if response.usage\n                    else Usage()\n                )\n            else:\n                usage = Usage()\n                logger.warning(\"No usage information returned from Litellm\")\n\n            if tracing.include_data():\n                span_generation.span_data.output = (\n                    [message.model_dump()] if message is not None else []\n                )\n            span_generation.span_data.usage = {\n                \"requests\": usage.requests,\n                \"input_tokens\": usage.input_tokens,\n                \"output_tokens\": usage.output_tokens,\n                \"total_tokens\": usage.total_tokens,\n                \"input_tokens_details\": usage.input_tokens_details.model_dump(),\n                \"output_tokens_details\": usage.output_tokens_details.model_dump(),\n            }\n\n            # Build provider_data for provider specific fields\n            provider_data: dict[str, Any] = {\"model\": self.model}\n            if message is not None and hasattr(response, \"id\"):\n                provider_data[\"response_id\"] = response.id\n\n            items = (\n                Converter.message_to_output_items(\n                    LitellmConverter.convert_message_to_openai(message, model=self.model),\n                    provider_data=provider_data,\n                )\n                if message is not None\n                else []\n            )\n\n            return ModelResponse(\n                output=items,\n                usage=usage,\n                response_id=None,\n            )\n\n    async def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,  # unused\n        conversation_id: str | None = None,  # unused\n        prompt: Any | None = None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict()\n            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response, stream = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=True,\n                prompt=prompt,\n            )\n\n            final_response: Response | None = None\n            async for chunk in ChatCmplStreamHandler.handle_stream(\n                response, stream, model=self.model\n            ):\n                yield chunk\n\n                if chunk.type == \"response.completed\":\n                    final_response = chunk.response\n\n            if tracing.include_data() and final_response:\n                span_generation.span_data.output = [final_response.model_dump()]\n\n            if final_response and final_response.usage:\n                span_generation.span_data.usage = {\n                    \"requests\": 1,\n                    \"input_tokens\": final_response.usage.input_tokens,\n                    \"output_tokens\": final_response.usage.output_tokens,\n                    \"total_tokens\": final_response.usage.total_tokens,\n                    \"input_tokens_details\": (\n                        final_response.usage.input_tokens_details.model_dump()\n                        if final_response.usage.input_tokens_details\n                        else {\"cached_tokens\": 0}\n                    ),\n                    \"output_tokens_details\": (\n                        final_response.usage.output_tokens_details.model_dump()\n                        if final_response.usage.output_tokens_details\n                        else {\"reasoning_tokens\": 0}\n                    ),\n                }\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[True],\n        prompt: Any | None = None,\n    ) -&gt; tuple[Response, AsyncStream[ChatCompletionChunk]]: ...\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[False],\n        prompt: Any | None = None,\n    ) -&gt; litellm.types.utils.ModelResponse: ...\n\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: bool = False,\n        prompt: Any | None = None,\n    ) -&gt; litellm.types.utils.ModelResponse | tuple[Response, AsyncStream[ChatCompletionChunk]]:\n        # Preserve reasoning messages for tool calls when reasoning is on\n        # This is needed for models like Claude 4 Sonnet/Opus which support interleaved thinking\n        preserve_thinking_blocks = (\n            model_settings.reasoning is not None and model_settings.reasoning.effort is not None\n        )\n\n        converted_messages = Converter.items_to_messages(\n            input,\n            preserve_thinking_blocks=preserve_thinking_blocks,\n            preserve_tool_output_all_content=True,\n            model=self.model,\n        )\n\n        # Fix message ordering: reorder to ensure tool_use comes before tool_result.\n        # Required for Anthropic and Vertex AI Gemini APIs which reject tool responses without preceding tool calls.  # noqa: E501\n        if any(model.lower() in self.model.lower() for model in [\"anthropic\", \"claude\", \"gemini\"]):\n            converted_messages = self._fix_tool_message_ordering(converted_messages)\n\n        # Convert Google's extra_content to litellm's provider_specific_fields format\n        if \"gemini\" in self.model.lower():\n            converted_messages = self._convert_gemini_extra_content_to_provider_specific_fields(\n                converted_messages\n            )\n\n        if system_instructions:\n            converted_messages.insert(\n                0,\n                {\n                    \"content\": system_instructions,\n                    \"role\": \"system\",\n                },\n            )\n        converted_messages = _to_dump_compatible(converted_messages)\n\n        if tracing.include_data():\n            span.span_data.input = converted_messages\n\n        parallel_tool_calls = (\n            True\n            if model_settings.parallel_tool_calls and tools and len(tools) &gt; 0\n            else False\n            if model_settings.parallel_tool_calls is False\n            else None\n        )\n        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)\n        response_format = Converter.convert_response_format(output_schema)\n\n        converted_tools = [Converter.tool_to_openai(tool) for tool in tools] if tools else []\n\n        for handoff in handoffs:\n            converted_tools.append(Converter.convert_handoff_tool(handoff))\n\n        converted_tools = _to_dump_compatible(converted_tools)\n\n        if _debug.DONT_LOG_MODEL_DATA:\n            logger.debug(\"Calling LLM\")\n        else:\n            messages_json = json.dumps(\n                converted_messages,\n                indent=2,\n                ensure_ascii=False,\n            )\n            tools_json = json.dumps(\n                converted_tools,\n                indent=2,\n                ensure_ascii=False,\n            )\n            logger.debug(\n                f\"Calling Litellm model: {self.model}\\n\"\n                f\"{messages_json}\\n\"\n                f\"Tools:\\n{tools_json}\\n\"\n                f\"Stream: {stream}\\n\"\n                f\"Tool choice: {tool_choice}\\n\"\n                f\"Response format: {response_format}\\n\"\n            )\n\n        # Build reasoning_effort - use dict only when summary is present (OpenAI feature)\n        # Otherwise pass string for backward compatibility with all providers\n        reasoning_effort: dict[str, Any] | str | None = None\n        if model_settings.reasoning:\n            if model_settings.reasoning.summary is not None:\n                # Dict format when summary is needed (OpenAI only)\n                reasoning_effort = {\n                    \"effort\": model_settings.reasoning.effort,\n                    \"summary\": model_settings.reasoning.summary,\n                }\n            elif model_settings.reasoning.effort is not None:\n                # String format for compatibility with all providers\n                reasoning_effort = model_settings.reasoning.effort\n\n        # Enable developers to pass non-OpenAI compatible reasoning_effort data like \"none\"\n        # Priority order:\n        #  1. model_settings.reasoning (effort + summary)\n        #  2. model_settings.extra_body[\"reasoning_effort\"]\n        #  3. model_settings.extra_args[\"reasoning_effort\"]\n        if (\n            reasoning_effort is None  # Unset in model_settings\n            and isinstance(model_settings.extra_body, dict)\n            and \"reasoning_effort\" in model_settings.extra_body\n        ):\n            reasoning_effort = model_settings.extra_body[\"reasoning_effort\"]\n        if (\n            reasoning_effort is None  # Unset in both model_settings and model_settings.extra_body\n            and model_settings.extra_args\n            and \"reasoning_effort\" in model_settings.extra_args\n        ):\n            reasoning_effort = model_settings.extra_args[\"reasoning_effort\"]\n\n        stream_options = None\n        if stream and model_settings.include_usage is not None:\n            stream_options = {\"include_usage\": model_settings.include_usage}\n\n        extra_kwargs = {}\n        if model_settings.extra_query:\n            extra_kwargs[\"extra_query\"] = copy(model_settings.extra_query)\n        if model_settings.metadata:\n            extra_kwargs[\"metadata\"] = copy(model_settings.metadata)\n        if model_settings.extra_body and isinstance(model_settings.extra_body, dict):\n            extra_kwargs.update(model_settings.extra_body)\n\n        # Add kwargs from model_settings.extra_args, filtering out None values\n        if model_settings.extra_args:\n            extra_kwargs.update(model_settings.extra_args)\n\n        # Prevent duplicate reasoning_effort kwargs when it was promoted to a top-level argument.\n        extra_kwargs.pop(\"reasoning_effort\", None)\n\n        ret = await litellm.acompletion(\n            model=self.model,\n            messages=converted_messages,\n            tools=converted_tools or None,\n            temperature=model_settings.temperature,\n            top_p=model_settings.top_p,\n            frequency_penalty=model_settings.frequency_penalty,\n            presence_penalty=model_settings.presence_penalty,\n            max_tokens=model_settings.max_tokens,\n            tool_choice=self._remove_not_given(tool_choice),\n            response_format=self._remove_not_given(response_format),\n            parallel_tool_calls=parallel_tool_calls,\n            stream=stream,\n            stream_options=stream_options,\n            reasoning_effort=reasoning_effort,\n            top_logprobs=model_settings.top_logprobs,\n            extra_headers=self._merge_headers(model_settings),\n            api_key=self.api_key,\n            base_url=self.base_url,\n            **extra_kwargs,\n        )\n\n        if isinstance(ret, litellm.types.utils.ModelResponse):\n            return ret\n\n        responses_tool_choice = OpenAIResponsesConverter.convert_tool_choice(\n            model_settings.tool_choice\n        )\n        if responses_tool_choice is None or responses_tool_choice is omit:\n            responses_tool_choice = \"auto\"\n\n        response = Response(\n            id=FAKE_RESPONSES_ID,\n            created_at=time.time(),\n            model=self.model,\n            object=\"response\",\n            output=[],\n            tool_choice=responses_tool_choice,  # type: ignore[arg-type]\n            top_p=model_settings.top_p,\n            temperature=model_settings.temperature,\n            tools=[],\n            parallel_tool_calls=parallel_tool_calls or False,\n            reasoning=model_settings.reasoning,\n        )\n        return response, ret\n\n    def _convert_gemini_extra_content_to_provider_specific_fields(\n        self, messages: list[ChatCompletionMessageParam]\n    ) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"\n        Convert Gemini model's extra_content format to provider_specific_fields format for litellm.\n\n        Transforms tool calls from internal format:\n            extra_content={\"google\": {\"thought_signature\": \"...\"}}\n        To litellm format:\n            provider_specific_fields={\"thought_signature\": \"...\"}\n\n        Only processes tool_calls that appear after the last user message.\n        See: https://ai.google.dev/gemini-api/docs/thought-signatures\n        \"\"\"\n\n        # Find the index of the last user message\n        last_user_index = -1\n        for i in range(len(messages) - 1, -1, -1):\n            if isinstance(messages[i], dict) and messages[i].get(\"role\") == \"user\":\n                last_user_index = i\n                break\n\n        for i, message in enumerate(messages):\n            if not isinstance(message, dict):\n                continue\n\n            # Only process assistant messages that come after the last user message\n            # If no user message found (last_user_index == -1), process all messages\n            if last_user_index != -1 and i &lt;= last_user_index:\n                continue\n\n            # Check if this is an assistant message with tool calls\n            if message.get(\"role\") == \"assistant\" and message.get(\"tool_calls\"):\n                tool_calls = message.get(\"tool_calls\", [])\n\n                for tool_call in tool_calls:  # type: ignore[attr-defined]\n                    if not isinstance(tool_call, dict):\n                        continue\n\n                    # Default to skip validator, overridden if valid thought signature exists\n                    tool_call[\"provider_specific_fields\"] = {\n                        \"thought_signature\": \"skip_thought_signature_validator\"\n                    }\n\n                    # Override with actual thought signature if extra_content exists\n                    if \"extra_content\" in tool_call:\n                        extra_content = tool_call.pop(\"extra_content\")\n                        if isinstance(extra_content, dict):\n                            # Extract google-specific fields\n                            google_fields = extra_content.get(\"google\")\n                            if google_fields and isinstance(google_fields, dict):\n                                thought_sig = google_fields.get(\"thought_signature\")\n                                if thought_sig:\n                                    tool_call[\"provider_specific_fields\"] = {\n                                        \"thought_signature\": thought_sig\n                                    }\n\n        return messages\n\n    def _fix_tool_message_ordering(\n        self, messages: list[ChatCompletionMessageParam]\n    ) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"\n        Fix the ordering of tool messages to ensure tool_use messages come before tool_result messages.\n\n        Required for Anthropic and Vertex AI Gemini APIs which require tool calls to immediately\n        precede their corresponding tool responses in conversation history.\n        \"\"\"  # noqa: E501\n        if not messages:\n            return messages\n\n        # Collect all tool calls and tool results\n        tool_call_messages = {}  # tool_id -&gt; (index, message)\n        tool_result_messages = {}  # tool_id -&gt; (index, message)\n        other_messages = []  # (index, message) for non-tool messages\n\n        for i, message in enumerate(messages):\n            if not isinstance(message, dict):\n                other_messages.append((i, message))\n                continue\n\n            role = message.get(\"role\")\n\n            if role == \"assistant\" and message.get(\"tool_calls\"):\n                # Extract tool calls from this assistant message\n                tool_calls = message.get(\"tool_calls\", [])\n                if isinstance(tool_calls, list):\n                    for tool_call in tool_calls:\n                        if isinstance(tool_call, dict):\n                            tool_id = tool_call.get(\"id\")\n                            if tool_id:\n                                # Create a separate assistant message for each tool call\n                                single_tool_msg = cast(dict[str, Any], message.copy())\n                                single_tool_msg[\"tool_calls\"] = [tool_call]\n                                tool_call_messages[tool_id] = (\n                                    i,\n                                    cast(ChatCompletionMessageParam, single_tool_msg),\n                                )\n\n            elif role == \"tool\":\n                tool_call_id = message.get(\"tool_call_id\")\n                if tool_call_id:\n                    tool_result_messages[tool_call_id] = (i, message)\n                else:\n                    other_messages.append((i, message))\n            else:\n                other_messages.append((i, message))\n\n        # First, identify which tool results will be paired to avoid duplicates\n        paired_tool_result_indices = set()\n        for tool_id in tool_call_messages:\n            if tool_id in tool_result_messages:\n                tool_result_idx, _ = tool_result_messages[tool_id]\n                paired_tool_result_indices.add(tool_result_idx)\n\n        # Create the fixed message sequence\n        fixed_messages: list[ChatCompletionMessageParam] = []\n        used_indices = set()\n\n        # Add messages in their original order, but ensure tool_use \u2192 tool_result pairing\n        for i, original_message in enumerate(messages):\n            if i in used_indices:\n                continue\n\n            if not isinstance(original_message, dict):\n                fixed_messages.append(original_message)\n                used_indices.add(i)\n                continue\n\n            role = original_message.get(\"role\")\n\n            if role == \"assistant\" and original_message.get(\"tool_calls\"):\n                # Process each tool call in this assistant message\n                tool_calls = original_message.get(\"tool_calls\", [])\n                if isinstance(tool_calls, list):\n                    for tool_call in tool_calls:\n                        if isinstance(tool_call, dict):\n                            tool_id = tool_call.get(\"id\")\n                            if (\n                                tool_id\n                                and tool_id in tool_call_messages\n                                and tool_id in tool_result_messages\n                            ):\n                                # Add tool_use \u2192 tool_result pair\n                                _, tool_call_msg = tool_call_messages[tool_id]\n                                tool_result_idx, tool_result_msg = tool_result_messages[tool_id]\n\n                                fixed_messages.append(tool_call_msg)\n                                fixed_messages.append(tool_result_msg)\n\n                                # Mark both as used\n                                used_indices.add(tool_call_messages[tool_id][0])\n                                used_indices.add(tool_result_idx)\n                            elif tool_id and tool_id in tool_call_messages:\n                                # Tool call without result - add just the tool call\n                                _, tool_call_msg = tool_call_messages[tool_id]\n                                fixed_messages.append(tool_call_msg)\n                                used_indices.add(tool_call_messages[tool_id][0])\n\n                used_indices.add(i)  # Mark original multi-tool message as used\n\n            elif role == \"tool\":\n                # Only preserve unmatched tool results to avoid duplicates\n                if i not in paired_tool_result_indices:\n                    fixed_messages.append(original_message)\n                used_indices.add(i)\n\n            else:\n                # Regular message - add it normally\n                fixed_messages.append(original_message)\n                used_indices.add(i)\n\n        return fixed_messages\n\n    def _remove_not_given(self, value: Any) -&gt; Any:\n        if value is omit or isinstance(value, NotGiven):\n            return None\n        return value\n\n    def _merge_headers(self, model_settings: ModelSettings):\n        return {**HEADERS, **(model_settings.extra_headers or {}), **(HEADERS_OVERRIDE.get() or {})}\n</code></pre>"},{"location":"ref/extensions/models/litellm_model/#agents.extensions.models.litellm_model.LitellmConverter","title":"LitellmConverter","text":"Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class LitellmConverter:\n    @classmethod\n    def convert_message_to_openai(\n        cls, message: litellm.types.utils.Message, model: str | None = None\n    ) -&gt; ChatCompletionMessage:\n        \"\"\"\n        Convert a LiteLLM message to OpenAI ChatCompletionMessage format.\n\n        Args:\n            message: The LiteLLM message to convert\n            model: The target model to convert to. Used to handle provider-specific\n                transformations.\n        \"\"\"\n        if message.role != \"assistant\":\n            raise ModelBehaviorError(f\"Unsupported role: {message.role}\")\n\n        tool_calls: (\n            list[ChatCompletionMessageFunctionToolCall | ChatCompletionMessageCustomToolCall] | None\n        ) = (\n            [\n                LitellmConverter.convert_tool_call_to_openai(tool, model=model)\n                for tool in message.tool_calls\n            ]\n            if message.tool_calls\n            else None\n        )\n\n        provider_specific_fields = message.get(\"provider_specific_fields\", None)\n        refusal = (\n            provider_specific_fields.get(\"refusal\", None) if provider_specific_fields else None\n        )\n\n        reasoning_content = \"\"\n        if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n            reasoning_content = message.reasoning_content\n\n        # Extract full thinking blocks including signatures (for Anthropic)\n        thinking_blocks: list[dict[str, Any]] | None = None\n        if hasattr(message, \"thinking_blocks\") and message.thinking_blocks:\n            # Convert thinking blocks to dict format for compatibility\n            thinking_blocks = []\n            for block in message.thinking_blocks:\n                if isinstance(block, dict):\n                    thinking_blocks.append(cast(dict[str, Any], block))\n                else:\n                    # Convert object to dict by accessing its attributes\n                    block_dict: dict[str, Any] = {}\n                    if hasattr(block, \"__dict__\"):\n                        block_dict = dict(block.__dict__.items())\n                    elif hasattr(block, \"model_dump\"):\n                        block_dict = block.model_dump()\n                    else:\n                        # Last resort: convert to string representation\n                        block_dict = {\"thinking\": str(block)}\n                    thinking_blocks.append(block_dict)\n\n        return InternalChatCompletionMessage(\n            content=message.content,\n            refusal=refusal,\n            role=\"assistant\",\n            annotations=cls.convert_annotations_to_openai(message),\n            audio=message.get(\"audio\", None),  # litellm deletes audio if not present\n            tool_calls=tool_calls,\n            reasoning_content=reasoning_content,\n            thinking_blocks=thinking_blocks,\n        )\n\n    @classmethod\n    def convert_annotations_to_openai(\n        cls, message: litellm.types.utils.Message\n    ) -&gt; list[Annotation] | None:\n        annotations: list[litellm.types.llms.openai.ChatCompletionAnnotation] | None = message.get(\n            \"annotations\", None\n        )\n        if not annotations:\n            return None\n\n        return [\n            Annotation(\n                type=\"url_citation\",\n                url_citation=AnnotationURLCitation(\n                    start_index=annotation[\"url_citation\"][\"start_index\"],\n                    end_index=annotation[\"url_citation\"][\"end_index\"],\n                    url=annotation[\"url_citation\"][\"url\"],\n                    title=annotation[\"url_citation\"][\"title\"],\n                ),\n            )\n            for annotation in annotations\n        ]\n\n    @classmethod\n    def convert_tool_call_to_openai(\n        cls, tool_call: litellm.types.utils.ChatCompletionMessageToolCall, model: str | None = None\n    ) -&gt; ChatCompletionMessageFunctionToolCall:\n        # Clean up litellm's addition of __thought__ suffix to tool_call.id for\n        # Gemini models. See: https://github.com/BerriAI/litellm/pull/16895\n        # This suffix is redundant since we can get thought_signature from\n        # provider_specific_fields, and this hack causes validation errors when\n        # cross-model passing to other models.\n        tool_call_id = tool_call.id\n        if model and \"gemini\" in model.lower() and \"__thought__\" in tool_call_id:\n            tool_call_id = tool_call_id.split(\"__thought__\")[0]\n\n        # Convert litellm's tool call format to chat completion message format\n        base_tool_call = ChatCompletionMessageFunctionToolCall(\n            id=tool_call_id,\n            type=\"function\",\n            function=Function(\n                name=tool_call.function.name or \"\",\n                arguments=tool_call.function.arguments,\n            ),\n        )\n\n        # Preserve provider-specific fields if present (e.g., Gemini thought signatures)\n        if hasattr(tool_call, \"provider_specific_fields\") and tool_call.provider_specific_fields:\n            # Convert to nested extra_content structure\n            extra_content: dict[str, Any] = {}\n            provider_fields = tool_call.provider_specific_fields\n\n            # Check for thought_signature (Gemini specific)\n            if model and \"gemini\" in model.lower():\n                if \"thought_signature\" in provider_fields:\n                    extra_content[\"google\"] = {\n                        \"thought_signature\": provider_fields[\"thought_signature\"]\n                    }\n\n            return InternalToolCall(\n                **base_tool_call.model_dump(),\n                extra_content=extra_content if extra_content else None,\n            )\n\n        return base_tool_call\n</code></pre>"},{"location":"ref/extensions/models/litellm_model/#agents.extensions.models.litellm_model.LitellmConverter.convert_message_to_openai","title":"convert_message_to_openai  <code>classmethod</code>","text":"<pre><code>convert_message_to_openai(\n    message: Message, model: str | None = None\n) -&gt; ChatCompletionMessage\n</code></pre> <p>Convert a LiteLLM message to OpenAI ChatCompletionMessage format.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The LiteLLM message to convert</p> required <code>model</code> <code>str | None</code> <p>The target model to convert to. Used to handle provider-specific transformations.</p> <code>None</code> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>@classmethod\ndef convert_message_to_openai(\n    cls, message: litellm.types.utils.Message, model: str | None = None\n) -&gt; ChatCompletionMessage:\n    \"\"\"\n    Convert a LiteLLM message to OpenAI ChatCompletionMessage format.\n\n    Args:\n        message: The LiteLLM message to convert\n        model: The target model to convert to. Used to handle provider-specific\n            transformations.\n    \"\"\"\n    if message.role != \"assistant\":\n        raise ModelBehaviorError(f\"Unsupported role: {message.role}\")\n\n    tool_calls: (\n        list[ChatCompletionMessageFunctionToolCall | ChatCompletionMessageCustomToolCall] | None\n    ) = (\n        [\n            LitellmConverter.convert_tool_call_to_openai(tool, model=model)\n            for tool in message.tool_calls\n        ]\n        if message.tool_calls\n        else None\n    )\n\n    provider_specific_fields = message.get(\"provider_specific_fields\", None)\n    refusal = (\n        provider_specific_fields.get(\"refusal\", None) if provider_specific_fields else None\n    )\n\n    reasoning_content = \"\"\n    if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n        reasoning_content = message.reasoning_content\n\n    # Extract full thinking blocks including signatures (for Anthropic)\n    thinking_blocks: list[dict[str, Any]] | None = None\n    if hasattr(message, \"thinking_blocks\") and message.thinking_blocks:\n        # Convert thinking blocks to dict format for compatibility\n        thinking_blocks = []\n        for block in message.thinking_blocks:\n            if isinstance(block, dict):\n                thinking_blocks.append(cast(dict[str, Any], block))\n            else:\n                # Convert object to dict by accessing its attributes\n                block_dict: dict[str, Any] = {}\n                if hasattr(block, \"__dict__\"):\n                    block_dict = dict(block.__dict__.items())\n                elif hasattr(block, \"model_dump\"):\n                    block_dict = block.model_dump()\n                else:\n                    # Last resort: convert to string representation\n                    block_dict = {\"thinking\": str(block)}\n                thinking_blocks.append(block_dict)\n\n    return InternalChatCompletionMessage(\n        content=message.content,\n        refusal=refusal,\n        role=\"assistant\",\n        annotations=cls.convert_annotations_to_openai(message),\n        audio=message.get(\"audio\", None),  # litellm deletes audio if not present\n        tool_calls=tool_calls,\n        reasoning_content=reasoning_content,\n        thinking_blocks=thinking_blocks,\n    )\n</code></pre>"},{"location":"ref/extensions/models/litellm_provider/","title":"<code>LiteLLM Provider</code>","text":""},{"location":"ref/extensions/models/litellm_provider/#agents.extensions.models.litellm_provider.LitellmProvider","title":"LitellmProvider","text":"<p>               Bases: <code>ModelProvider</code></p> <p>A ModelProvider that uses LiteLLM to route to any model provider. You can use it via: <pre><code>Runner.run(agent, input, run_config=RunConfig(model_provider=LitellmProvider()))\n</code></pre> See supported models here: litellm models.</p> <p>NOTE: API keys must be set via environment variables. If you're using models that require additional configuration (e.g. Azure API base or version), those must also be set via the environment variables that LiteLLM expects. If you have more advanced needs, we recommend copy-pasting this class and making any modifications you need.</p> Source code in <code>src/agents/extensions/models/litellm_provider.py</code> <pre><code>class LitellmProvider(ModelProvider):\n    \"\"\"A ModelProvider that uses LiteLLM to route to any model provider. You can use it via:\n    ```python\n    Runner.run(agent, input, run_config=RunConfig(model_provider=LitellmProvider()))\n    ```\n    See supported models here: [litellm models](https://docs.litellm.ai/docs/providers).\n\n    NOTE: API keys must be set via environment variables. If you're using models that require\n    additional configuration (e.g. Azure API base or version), those must also be set via the\n    environment variables that LiteLLM expects. If you have more advanced needs, we recommend\n    copy-pasting this class and making any modifications you need.\n    \"\"\"\n\n    def get_model(self, model_name: str | None) -&gt; Model:\n        return LitellmModel(model_name or get_default_model())\n</code></pre>"},{"location":"ref/handoffs/history/","title":"<code>History</code>","text":""},{"location":"ref/handoffs/history/#agents.handoffs.history.set_conversation_history_wrappers","title":"set_conversation_history_wrappers","text":"<pre><code>set_conversation_history_wrappers(\n    *, start: str | None = None, end: str | None = None\n) -&gt; None\n</code></pre> <p>Override the markers that wrap the generated conversation summary.</p> <p>Pass <code>None</code> to leave either side unchanged.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def set_conversation_history_wrappers(\n    *,\n    start: str | None = None,\n    end: str | None = None,\n) -&gt; None:\n    \"\"\"Override the markers that wrap the generated conversation summary.\n\n    Pass ``None`` to leave either side unchanged.\n    \"\"\"\n\n    global _conversation_history_start, _conversation_history_end\n    if start is not None:\n        _conversation_history_start = start\n    if end is not None:\n        _conversation_history_end = end\n</code></pre>"},{"location":"ref/handoffs/history/#agents.handoffs.history.reset_conversation_history_wrappers","title":"reset_conversation_history_wrappers","text":"<pre><code>reset_conversation_history_wrappers() -&gt; None\n</code></pre> <p>Restore the default <code>&lt;CONVERSATION HISTORY&gt;</code> markers.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def reset_conversation_history_wrappers() -&gt; None:\n    \"\"\"Restore the default ``&lt;CONVERSATION HISTORY&gt;`` markers.\"\"\"\n\n    global _conversation_history_start, _conversation_history_end\n    _conversation_history_start = _DEFAULT_CONVERSATION_HISTORY_START\n    _conversation_history_end = _DEFAULT_CONVERSATION_HISTORY_END\n</code></pre>"},{"location":"ref/handoffs/history/#agents.handoffs.history.get_conversation_history_wrappers","title":"get_conversation_history_wrappers","text":"<pre><code>get_conversation_history_wrappers() -&gt; tuple[str, str]\n</code></pre> <p>Return the current start/end markers used for the nested conversation summary.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def get_conversation_history_wrappers() -&gt; tuple[str, str]:\n    \"\"\"Return the current start/end markers used for the nested conversation summary.\"\"\"\n\n    return (_conversation_history_start, _conversation_history_end)\n</code></pre>"},{"location":"ref/handoffs/history/#agents.handoffs.history.nest_handoff_history","title":"nest_handoff_history","text":"<pre><code>nest_handoff_history(\n    handoff_input_data: HandoffInputData,\n    *,\n    history_mapper: HandoffHistoryMapper | None = None,\n) -&gt; HandoffInputData\n</code></pre> <p>Summarize the previous transcript for the next agent.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def nest_handoff_history(\n    handoff_input_data: HandoffInputData,\n    *,\n    history_mapper: HandoffHistoryMapper | None = None,\n) -&gt; HandoffInputData:\n    \"\"\"Summarize the previous transcript for the next agent.\"\"\"\n\n    normalized_history = _normalize_input_history(handoff_input_data.input_history)\n    flattened_history = _flatten_nested_history_messages(normalized_history)\n\n    # Convert items to plain inputs for the transcript summary.\n    pre_items_as_inputs: list[TResponseInputItem] = []\n    filtered_pre_items: list[RunItem] = []\n    for run_item in handoff_input_data.pre_handoff_items:\n        if isinstance(run_item, ToolApprovalItem):\n            continue\n        plain_input = _run_item_to_plain_input(run_item)\n        pre_items_as_inputs.append(plain_input)\n        if _should_forward_pre_item(plain_input):\n            filtered_pre_items.append(run_item)\n\n    new_items_as_inputs: list[TResponseInputItem] = []\n    filtered_input_items: list[RunItem] = []\n    for run_item in handoff_input_data.new_items:\n        if isinstance(run_item, ToolApprovalItem):\n            continue\n        plain_input = _run_item_to_plain_input(run_item)\n        new_items_as_inputs.append(plain_input)\n        if _should_forward_new_item(plain_input):\n            filtered_input_items.append(run_item)\n\n    transcript = flattened_history + pre_items_as_inputs + new_items_as_inputs\n\n    mapper = history_mapper or default_handoff_history_mapper\n    history_items = mapper(transcript)\n\n    return handoff_input_data.clone(\n        input_history=tuple(deepcopy(item) for item in history_items),\n        pre_handoff_items=tuple(filtered_pre_items),\n        # new_items stays unchanged for session history.\n        input_items=tuple(filtered_input_items),\n    )\n</code></pre>"},{"location":"ref/handoffs/history/#agents.handoffs.history.default_handoff_history_mapper","title":"default_handoff_history_mapper","text":"<pre><code>default_handoff_history_mapper(\n    transcript: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Return a single assistant message summarizing the transcript.</p> Source code in <code>src/agents/handoffs/history.py</code> <pre><code>def default_handoff_history_mapper(\n    transcript: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Return a single assistant message summarizing the transcript.\"\"\"\n\n    summary_message = _build_summary_message(transcript)\n    return [summary_message]\n</code></pre>"},{"location":"ref/mcp/manager/","title":"<code>Manager</code>","text":""},{"location":"ref/mcp/manager/#agents.mcp.manager.MCPServerManager","title":"MCPServerManager","text":"<p>               Bases: <code>AbstractAsyncContextManager['MCPServerManager']</code></p> <p>Manage MCP server lifecycles and expose only connected servers.</p> <p>Use this helper to keep MCP connect/cleanup on the same task and avoid run failures when a server is unavailable. The manager will attempt to connect each server and then expose the connected subset via <code>active_servers</code>.</p> Basic usage <p>async with MCPServerManager([server_a, server_b]) as manager:     agent = Agent(         name=\"Assistant\",         instructions=\"...\",         mcp_servers=manager.active_servers,     )</p> FastAPI lifespan example <p>@asynccontextmanager async def lifespan(app: FastAPI):     async with MCPServerManager([server_a, server_b]) as manager:         app.state.mcp_manager = manager         yield</p> <p>app = FastAPI(lifespan=lifespan)</p> <p>Important behaviors: - <code>active_servers</code> only includes servers that connected successfully.   <code>failed_servers</code> holds the failures and <code>errors</code> maps servers to errors. - <code>drop_failed_servers=True</code> removes failed servers from <code>active_servers</code>   (recommended). If False, <code>active_servers</code> will still include all servers. - <code>strict=True</code> raises on the first connection failure. If False, failures   are recorded and the run can proceed with the remaining servers. - <code>reconnect(failed_only=True)</code> retries failed servers and refreshes   <code>active_servers</code>. - <code>connect_in_parallel=True</code> uses a dedicated worker task per server to   allow concurrent connects while preserving task affinity for cleanup.</p> Source code in <code>src/agents/mcp/manager.py</code> <pre><code>class MCPServerManager(AbstractAsyncContextManager[\"MCPServerManager\"]):\n    \"\"\"Manage MCP server lifecycles and expose only connected servers.\n\n    Use this helper to keep MCP connect/cleanup on the same task and avoid\n    run failures when a server is unavailable. The manager will attempt to\n    connect each server and then expose the connected subset via\n    `active_servers`.\n\n    Basic usage:\n        async with MCPServerManager([server_a, server_b]) as manager:\n            agent = Agent(\n                name=\"Assistant\",\n                instructions=\"...\",\n                mcp_servers=manager.active_servers,\n            )\n\n    FastAPI lifespan example:\n        @asynccontextmanager\n        async def lifespan(app: FastAPI):\n            async with MCPServerManager([server_a, server_b]) as manager:\n                app.state.mcp_manager = manager\n                yield\n\n        app = FastAPI(lifespan=lifespan)\n\n    Important behaviors:\n    - `active_servers` only includes servers that connected successfully.\n      `failed_servers` holds the failures and `errors` maps servers to errors.\n    - `drop_failed_servers=True` removes failed servers from `active_servers`\n      (recommended). If False, `active_servers` will still include all servers.\n    - `strict=True` raises on the first connection failure. If False, failures\n      are recorded and the run can proceed with the remaining servers.\n    - `reconnect(failed_only=True)` retries failed servers and refreshes\n      `active_servers`.\n    - `connect_in_parallel=True` uses a dedicated worker task per server to\n      allow concurrent connects while preserving task affinity for cleanup.\n    \"\"\"\n\n    def __init__(\n        self,\n        servers: Iterable[MCPServer],\n        *,\n        connect_timeout_seconds: float | None = 10.0,\n        cleanup_timeout_seconds: float | None = 10.0,\n        drop_failed_servers: bool = True,\n        strict: bool = False,\n        suppress_cancelled_error: bool = True,\n        connect_in_parallel: bool = False,\n    ) -&gt; None:\n        self._all_servers = list(servers)\n        self._active_servers = list(servers)\n        self.connect_timeout_seconds = connect_timeout_seconds\n        self.cleanup_timeout_seconds = cleanup_timeout_seconds\n        self.drop_failed_servers = drop_failed_servers\n        self.strict = strict\n        self.suppress_cancelled_error = suppress_cancelled_error\n        self.connect_in_parallel = connect_in_parallel\n        self._workers: dict[MCPServer, _ServerWorker] = {}\n\n        self.failed_servers: list[MCPServer] = []\n        self._failed_server_set: set[MCPServer] = set()\n        self._connected_servers: set[MCPServer] = set()\n        self.errors: dict[MCPServer, BaseException] = {}\n\n    @property\n    def active_servers(self) -&gt; list[MCPServer]:\n        \"\"\"Return the active MCP servers after connection attempts.\"\"\"\n        return list(self._active_servers)\n\n    @property\n    def all_servers(self) -&gt; list[MCPServer]:\n        \"\"\"Return all MCP servers managed by this instance.\"\"\"\n        return list(self._all_servers)\n\n    async def __aenter__(self) -&gt; MCPServerManager:\n        await self.connect_all()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; bool | None:\n        await self.cleanup_all()\n        return None\n\n    async def connect_all(self) -&gt; list[MCPServer]:\n        \"\"\"Connect all servers in order and return the active list.\"\"\"\n        previous_connected_servers = set(self._connected_servers)\n        previous_active_servers = list(self._active_servers)\n        self.failed_servers = []\n        self._failed_server_set = set()\n        self.errors = {}\n\n        servers_to_connect = self._servers_to_connect(self._all_servers)\n        connected_servers: list[MCPServer] = []\n        try:\n            if self.connect_in_parallel:\n                await self._connect_all_parallel(servers_to_connect)\n            else:\n                for server in servers_to_connect:\n                    await self._attempt_connect(server)\n                    if server not in self._failed_server_set:\n                        connected_servers.append(server)\n        except BaseException:\n            if self.connect_in_parallel:\n                await self._cleanup_servers(servers_to_connect)\n            else:\n                servers_to_cleanup = self._unique_servers(\n                    [*connected_servers, *self.failed_servers]\n                )\n                await self._cleanup_servers(servers_to_cleanup)\n            if self.drop_failed_servers:\n                self._active_servers = [\n                    server for server in self._all_servers if server in previous_connected_servers\n                ]\n            else:\n                self._active_servers = previous_active_servers\n            raise\n\n        self._refresh_active_servers()\n\n        return self._active_servers\n\n    async def reconnect(self, *, failed_only: bool = True) -&gt; list[MCPServer]:\n        \"\"\"Reconnect servers and return the active list.\n\n        Args:\n            failed_only: If True, only retry servers that previously failed.\n                If False, cleanup and retry all servers.\n        \"\"\"\n        if failed_only:\n            servers_to_retry = self._unique_servers(self.failed_servers)\n        else:\n            await self.cleanup_all()\n            servers_to_retry = list(self._all_servers)\n            self.failed_servers = []\n            self._failed_server_set = set()\n            self.errors = {}\n\n        servers_to_retry = self._servers_to_connect(servers_to_retry)\n        try:\n            if self.connect_in_parallel:\n                await self._connect_all_parallel(servers_to_retry)\n            else:\n                for server in servers_to_retry:\n                    await self._attempt_connect(server)\n        finally:\n            self._refresh_active_servers()\n        return self._active_servers\n\n    async def cleanup_all(self) -&gt; None:\n        \"\"\"Cleanup all servers in reverse order.\"\"\"\n        for server in reversed(self._all_servers):\n            try:\n                await self._cleanup_server(server)\n            except asyncio.CancelledError as exc:\n                if not self.suppress_cancelled_error:\n                    raise\n                logger.debug(f\"Cleanup cancelled for MCP server '{server.name}': {exc}\")\n                self.errors[server] = exc\n            except Exception as exc:\n                logger.exception(f\"Failed to cleanup MCP server '{server.name}': {exc}\")\n                self.errors[server] = exc\n\n    async def _run_with_timeout(\n        self, func: Callable[[], Awaitable[Any]], timeout_seconds: float | None\n    ) -&gt; None:\n        await _run_with_timeout_in_task(func, timeout_seconds)\n\n    async def _attempt_connect(\n        self, server: MCPServer, *, raise_on_error: bool | None = None\n    ) -&gt; None:\n        if raise_on_error is None:\n            raise_on_error = self.strict\n        try:\n            await self._run_connect(server)\n            self._connected_servers.add(server)\n            if server in self.failed_servers:\n                self._remove_failed_server(server)\n                self.errors.pop(server, None)\n        except asyncio.CancelledError as exc:\n            if not self.suppress_cancelled_error:\n                raise\n            self._record_failure(server, exc, phase=\"connect\")\n        except Exception as exc:\n            self._record_failure(server, exc, phase=\"connect\")\n            if raise_on_error:\n                raise\n        except BaseException as exc:\n            self._record_failure(server, exc, phase=\"connect\")\n            raise\n\n    def _refresh_active_servers(self) -&gt; None:\n        if self.drop_failed_servers:\n            failed = set(self._failed_server_set)\n            self._active_servers = [server for server in self._all_servers if server not in failed]\n        else:\n            self._active_servers = list(self._all_servers)\n\n    def _record_failure(self, server: MCPServer, exc: BaseException, phase: str) -&gt; None:\n        logger.exception(f\"Failed to {phase} MCP server '{server.name}': {exc}\")\n        if server not in self._failed_server_set:\n            self.failed_servers.append(server)\n            self._failed_server_set.add(server)\n        self.errors[server] = exc\n\n    async def _run_connect(self, server: MCPServer) -&gt; None:\n        if self.connect_in_parallel:\n            worker = self._get_worker(server)\n            await worker.connect()\n        else:\n            await self._run_with_timeout(server.connect, self.connect_timeout_seconds)\n\n    async def _cleanup_server(self, server: MCPServer) -&gt; None:\n        if self.connect_in_parallel and server in self._workers:\n            worker = self._workers[server]\n            if worker.is_done:\n                self._workers.pop(server, None)\n                self._connected_servers.discard(server)\n                return\n            try:\n                await worker.cleanup()\n            finally:\n                self._workers.pop(server, None)\n                self._connected_servers.discard(server)\n            return\n        try:\n            await self._run_with_timeout(server.cleanup, self.cleanup_timeout_seconds)\n        finally:\n            self._connected_servers.discard(server)\n\n    async def _cleanup_servers(self, servers: Iterable[MCPServer]) -&gt; None:\n        for server in reversed(list(servers)):\n            try:\n                await self._cleanup_server(server)\n            except asyncio.CancelledError as exc:\n                if not self.suppress_cancelled_error:\n                    raise\n                logger.debug(f\"Cleanup cancelled for MCP server '{server.name}': {exc}\")\n                self.errors[server] = exc\n            except Exception as exc:\n                logger.exception(f\"Failed to cleanup MCP server '{server.name}': {exc}\")\n                self.errors[server] = exc\n\n    async def _connect_all_parallel(self, servers: list[MCPServer]) -&gt; None:\n        tasks = [\n            asyncio.create_task(self._attempt_connect(server, raise_on_error=False))\n            for server in servers\n        ]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        if not self.suppress_cancelled_error:\n            for result in results:\n                if isinstance(result, asyncio.CancelledError):\n                    raise result\n        for result in results:\n            if isinstance(result, BaseException) and not isinstance(result, asyncio.CancelledError):\n                raise result\n        if self.strict and self.failed_servers:\n            first_failure = None\n            if self.suppress_cancelled_error:\n                for server in self.failed_servers:\n                    error = self.errors.get(server)\n                    if error is None or isinstance(error, asyncio.CancelledError):\n                        continue\n                    first_failure = server\n                    break\n            else:\n                first_failure = self.failed_servers[0]\n            if first_failure is not None:\n                error = self.errors.get(first_failure)\n                if error is not None:\n                    raise error\n                raise RuntimeError(f\"Failed to connect MCP server '{first_failure.name}'\")\n\n    def _get_worker(self, server: MCPServer) -&gt; _ServerWorker:\n        worker = self._workers.get(server)\n        if worker is None or worker.is_done:\n            worker = _ServerWorker(\n                server=server,\n                connect_timeout_seconds=self.connect_timeout_seconds,\n                cleanup_timeout_seconds=self.cleanup_timeout_seconds,\n            )\n            self._workers[server] = worker\n        return worker\n\n    def _remove_failed_server(self, server: MCPServer) -&gt; None:\n        if server in self._failed_server_set:\n            self._failed_server_set.remove(server)\n        self.failed_servers = [\n            failed_server for failed_server in self.failed_servers if failed_server != server\n        ]\n\n    def _servers_to_connect(self, servers: Iterable[MCPServer]) -&gt; list[MCPServer]:\n        unique = self._unique_servers(servers)\n        if not self._connected_servers:\n            return unique\n        return [server for server in unique if server not in self._connected_servers]\n\n    @staticmethod\n    def _unique_servers(servers: Iterable[MCPServer]) -&gt; list[MCPServer]:\n        seen: set[MCPServer] = set()\n        unique: list[MCPServer] = []\n        for server in servers:\n            if server not in seen:\n                seen.add(server)\n                unique.append(server)\n        return unique\n</code></pre>"},{"location":"ref/mcp/manager/#agents.mcp.manager.MCPServerManager.active_servers","title":"active_servers  <code>property</code>","text":"<pre><code>active_servers: list[MCPServer]\n</code></pre> <p>Return the active MCP servers after connection attempts.</p>"},{"location":"ref/mcp/manager/#agents.mcp.manager.MCPServerManager.all_servers","title":"all_servers  <code>property</code>","text":"<pre><code>all_servers: list[MCPServer]\n</code></pre> <p>Return all MCP servers managed by this instance.</p>"},{"location":"ref/mcp/manager/#agents.mcp.manager.MCPServerManager.connect_all","title":"connect_all  <code>async</code>","text":"<pre><code>connect_all() -&gt; list[MCPServer]\n</code></pre> <p>Connect all servers in order and return the active list.</p> Source code in <code>src/agents/mcp/manager.py</code> <pre><code>async def connect_all(self) -&gt; list[MCPServer]:\n    \"\"\"Connect all servers in order and return the active list.\"\"\"\n    previous_connected_servers = set(self._connected_servers)\n    previous_active_servers = list(self._active_servers)\n    self.failed_servers = []\n    self._failed_server_set = set()\n    self.errors = {}\n\n    servers_to_connect = self._servers_to_connect(self._all_servers)\n    connected_servers: list[MCPServer] = []\n    try:\n        if self.connect_in_parallel:\n            await self._connect_all_parallel(servers_to_connect)\n        else:\n            for server in servers_to_connect:\n                await self._attempt_connect(server)\n                if server not in self._failed_server_set:\n                    connected_servers.append(server)\n    except BaseException:\n        if self.connect_in_parallel:\n            await self._cleanup_servers(servers_to_connect)\n        else:\n            servers_to_cleanup = self._unique_servers(\n                [*connected_servers, *self.failed_servers]\n            )\n            await self._cleanup_servers(servers_to_cleanup)\n        if self.drop_failed_servers:\n            self._active_servers = [\n                server for server in self._all_servers if server in previous_connected_servers\n            ]\n        else:\n            self._active_servers = previous_active_servers\n        raise\n\n    self._refresh_active_servers()\n\n    return self._active_servers\n</code></pre>"},{"location":"ref/mcp/manager/#agents.mcp.manager.MCPServerManager.reconnect","title":"reconnect  <code>async</code>","text":"<pre><code>reconnect(*, failed_only: bool = True) -&gt; list[MCPServer]\n</code></pre> <p>Reconnect servers and return the active list.</p> <p>Parameters:</p> Name Type Description Default <code>failed_only</code> <code>bool</code> <p>If True, only retry servers that previously failed. If False, cleanup and retry all servers.</p> <code>True</code> Source code in <code>src/agents/mcp/manager.py</code> <pre><code>async def reconnect(self, *, failed_only: bool = True) -&gt; list[MCPServer]:\n    \"\"\"Reconnect servers and return the active list.\n\n    Args:\n        failed_only: If True, only retry servers that previously failed.\n            If False, cleanup and retry all servers.\n    \"\"\"\n    if failed_only:\n        servers_to_retry = self._unique_servers(self.failed_servers)\n    else:\n        await self.cleanup_all()\n        servers_to_retry = list(self._all_servers)\n        self.failed_servers = []\n        self._failed_server_set = set()\n        self.errors = {}\n\n    servers_to_retry = self._servers_to_connect(servers_to_retry)\n    try:\n        if self.connect_in_parallel:\n            await self._connect_all_parallel(servers_to_retry)\n        else:\n            for server in servers_to_retry:\n                await self._attempt_connect(server)\n    finally:\n        self._refresh_active_servers()\n    return self._active_servers\n</code></pre>"},{"location":"ref/mcp/manager/#agents.mcp.manager.MCPServerManager.cleanup_all","title":"cleanup_all  <code>async</code>","text":"<pre><code>cleanup_all() -&gt; None\n</code></pre> <p>Cleanup all servers in reverse order.</p> Source code in <code>src/agents/mcp/manager.py</code> <pre><code>async def cleanup_all(self) -&gt; None:\n    \"\"\"Cleanup all servers in reverse order.\"\"\"\n    for server in reversed(self._all_servers):\n        try:\n            await self._cleanup_server(server)\n        except asyncio.CancelledError as exc:\n            if not self.suppress_cancelled_error:\n                raise\n            logger.debug(f\"Cleanup cancelled for MCP server '{server.name}': {exc}\")\n            self.errors[server] = exc\n        except Exception as exc:\n            logger.exception(f\"Failed to cleanup MCP server '{server.name}': {exc}\")\n            self.errors[server] = exc\n</code></pre>"},{"location":"ref/mcp/server/","title":"<code>MCP Servers</code>","text":""},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer","title":"MCPServer","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for Model Context Protocol servers.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServer(abc.ABC):\n    \"\"\"Base class for Model Context Protocol servers.\"\"\"\n\n    def __init__(\n        self,\n        use_structured_content: bool = False,\n        require_approval: RequireApprovalSetting = None,\n        failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n        tool_meta_resolver: MCPToolMetaResolver | None = None,\n    ):\n        \"\"\"\n        Args:\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool.Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n            require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n                a dict of tool names to those values, a boolean, or an object with always/never\n                tool lists (mirroring TS requireApproval). Normalized into a needs_approval policy.\n            failure_error_function: Optional function used to convert MCP tool failures into\n                a model-visible error message. If explicitly set to None, tool errors will be\n                raised instead of converted. If left unset, the agent-level configuration (or\n                SDK default) will be used.\n            tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n                tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n        \"\"\"\n        self.use_structured_content = use_structured_content\n        self._needs_approval_policy = self._normalize_needs_approval(\n            require_approval=require_approval\n        )\n        self._failure_error_function = failure_error_function\n        self.tool_meta_resolver = tool_meta_resolver\n\n    @abc.abstractmethod\n    async def connect(self):\n        \"\"\"Connect to the server. For example, this might mean spawning a subprocess or\n        opening a network connection. The server is expected to remain connected until\n        `cleanup()` is called.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def cleanup(self):\n        \"\"\"Cleanup the server. For example, this might mean closing a subprocess or\n        closing a network connection.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def list_tools(\n        self,\n        run_context: RunContextWrapper[Any] | None = None,\n        agent: AgentBase | None = None,\n    ) -&gt; list[MCPTool]:\n        \"\"\"List the tools available on the server.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def call_tool(\n        self,\n        tool_name: str,\n        arguments: dict[str, Any] | None,\n        meta: dict[str, Any] | None = None,\n    ) -&gt; CallToolResult:\n        \"\"\"Invoke a tool on the server.\"\"\"\n        pass\n\n    @property\n    def cached_tools(self) -&gt; list[MCPTool] | None:\n        \"\"\"Return the most recently fetched tools list, if available.\n\n        Implementations may return `None` when tools have not been fetched yet or caching is\n        disabled.\n        \"\"\"\n\n        return None\n\n    @abc.abstractmethod\n    async def list_prompts(\n        self,\n    ) -&gt; ListPromptsResult:\n        \"\"\"List the prompts available on the server.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -&gt; GetPromptResult:\n        \"\"\"Get a specific prompt from the server.\"\"\"\n        pass\n\n    @staticmethod\n    def _normalize_needs_approval(\n        *,\n        require_approval: RequireApprovalSetting,\n    ) -&gt; (\n        bool\n        | dict[str, bool]\n        | Callable[[RunContextWrapper[Any], AgentBase, MCPTool], MaybeAwaitable[bool]]\n    ):\n        \"\"\"Normalize approval inputs to booleans or a name-&gt;bool map.\"\"\"\n\n        if require_approval is None:\n            return False\n\n        def _to_bool(value: str) -&gt; bool:\n            return value == \"always\"\n\n        def _is_tool_list_schema(value: object) -&gt; bool:\n            if not isinstance(value, dict):\n                return False\n            for key in (\"always\", \"never\"):\n                if key not in value:\n                    continue\n                entry = value.get(key)\n                if isinstance(entry, dict) and \"tool_names\" in entry:\n                    return True\n            return False\n\n        if isinstance(require_approval, dict) and _is_tool_list_schema(require_approval):\n            always_entry: RequireApprovalToolList | Any = require_approval.get(\"always\", {})\n            never_entry: RequireApprovalToolList | Any = require_approval.get(\"never\", {})\n            always_names = (\n                always_entry.get(\"tool_names\", []) if isinstance(always_entry, dict) else []\n            )\n            never_names = never_entry.get(\"tool_names\", []) if isinstance(never_entry, dict) else []\n            tool_list_mapping: dict[str, bool] = {}\n            for name in always_names:\n                tool_list_mapping[str(name)] = True\n            for name in never_names:\n                tool_list_mapping[str(name)] = False\n            return tool_list_mapping\n\n        if isinstance(require_approval, dict):\n            tool_mapping: dict[str, bool] = {}\n            for name, value in require_approval.items():\n                if isinstance(value, bool):\n                    tool_mapping[str(name)] = value\n                elif isinstance(value, str) and value in (\"always\", \"never\"):\n                    tool_mapping[str(name)] = _to_bool(value)\n            return tool_mapping\n\n        if isinstance(require_approval, bool):\n            return require_approval\n\n        return _to_bool(require_approval)\n\n    def _get_needs_approval_for_tool(\n        self,\n        tool: MCPTool,\n        agent: AgentBase | None,\n    ) -&gt; bool | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]]:\n        \"\"\"Return a FunctionTool.needs_approval value for a given MCP tool.\"\"\"\n\n        policy = self._needs_approval_policy\n\n        if callable(policy):\n            if agent is None:\n                return False\n\n            async def _needs_approval(\n                run_context: RunContextWrapper[Any], _args: dict[str, Any], _call_id: str\n            ) -&gt; bool:\n                result = policy(run_context, agent, tool)\n                if inspect.isawaitable(result):\n                    result = await result\n                return bool(result)\n\n            return _needs_approval\n\n        if isinstance(policy, dict):\n            return bool(policy.get(tool.name, False))\n\n        return bool(policy)\n\n    def _get_failure_error_function(\n        self, agent_failure_error_function: ToolErrorFunction | None\n    ) -&gt; ToolErrorFunction | None:\n        \"\"\"Return the effective error handler for MCP tool failures.\"\"\"\n        if self._failure_error_function is _UNSET:\n            return agent_failure_error_function\n        return cast(ToolErrorFunction | None, self._failure_error_function)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.cached_tools","title":"cached_tools  <code>property</code>","text":"<pre><code>cached_tools: list[Tool] | None\n</code></pre> <p>Return the most recently fetched tools list, if available.</p> <p>Implementations may return <code>None</code> when tools have not been fetched yet or caching is disabled.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.__init__","title":"__init__","text":"<pre><code>__init__(\n    use_structured_content: bool = False,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction\n    | None\n    | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool.Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> <code>require_approval</code> <code>RequireApprovalSetting</code> <p>Approval policy for tools on this server. Accepts \"always\"/\"never\", a dict of tool names to those values, a boolean, or an object with always/never tool lists (mirroring TS requireApproval). Normalized into a needs_approval policy.</p> <code>None</code> <code>failure_error_function</code> <code>ToolErrorFunction | None | _UnsetType</code> <p>Optional function used to convert MCP tool failures into a model-visible error message. If explicitly set to None, tool errors will be raised instead of converted. If left unset, the agent-level configuration (or SDK default) will be used.</p> <code>_UNSET</code> <code>tool_meta_resolver</code> <code>MCPToolMetaResolver | None</code> <p>Optional callable that produces MCP request metadata (<code>_meta</code>) for tool calls. It is invoked by the Agents SDK before calling <code>call_tool</code>.</p> <code>None</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    use_structured_content: bool = False,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n):\n    \"\"\"\n    Args:\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool.Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n        require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n            a dict of tool names to those values, a boolean, or an object with always/never\n            tool lists (mirroring TS requireApproval). Normalized into a needs_approval policy.\n        failure_error_function: Optional function used to convert MCP tool failures into\n            a model-visible error message. If explicitly set to None, tool errors will be\n            raised instead of converted. If left unset, the agent-level configuration (or\n            SDK default) will be used.\n        tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n            tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n    \"\"\"\n    self.use_structured_content = use_structured_content\n    self._needs_approval_policy = self._normalize_needs_approval(\n        require_approval=require_approval\n    )\n    self._failure_error_function = failure_error_function\n    self.tool_meta_resolver = tool_meta_resolver\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.connect","title":"connect  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server. For example, this might mean spawning a subprocess or opening a network connection. The server is expected to remain connected until <code>cleanup()</code> is called.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def connect(self):\n    \"\"\"Connect to the server. For example, this might mean spawning a subprocess or\n    opening a network connection. The server is expected to remain connected until\n    `cleanup()` is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.cleanup","title":"cleanup  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server. For example, this might mean closing a subprocess or closing a network connection.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def cleanup(self):\n    \"\"\"Cleanup the server. For example, this might mean closing a subprocess or\n    closing a network connection.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.list_tools","title":"list_tools  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.call_tool","title":"call_tool  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def call_tool(\n    self,\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.list_prompts","title":"list_prompts  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.get_prompt","title":"get_prompt  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams","title":"MCPServerStdioParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Mirrors <code>mcp.client.stdio.StdioServerParameters</code>, but lets you pass params without another import.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStdioParams(TypedDict):\n    \"\"\"Mirrors `mcp.client.stdio.StdioServerParameters`, but lets you pass params without another\n    import.\n    \"\"\"\n\n    command: str\n    \"\"\"The executable to run to start the server. For example, `python` or `node`.\"\"\"\n\n    args: NotRequired[list[str]]\n    \"\"\"Command line args to pass to the `command` executable. For example, `['foo.py']` or\n    `['server.js', '--port', '8080']`.\"\"\"\n\n    env: NotRequired[dict[str, str]]\n    \"\"\"The environment variables to set for the server. .\"\"\"\n\n    cwd: NotRequired[str | Path]\n    \"\"\"The working directory to use when spawning the process.\"\"\"\n\n    encoding: NotRequired[str]\n    \"\"\"The text encoding used when sending/receiving messages to the server. Defaults to `utf-8`.\"\"\"\n\n    encoding_error_handler: NotRequired[Literal[\"strict\", \"ignore\", \"replace\"]]\n    \"\"\"The text encoding error handler. Defaults to `strict`.\n\n    See https://docs.python.org/3/library/codecs.html#codec-base-classes for\n    explanations of possible values.\n    \"\"\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.command","title":"command  <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre> <p>The executable to run to start the server. For example, <code>python</code> or <code>node</code>.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.args","title":"args  <code>instance-attribute</code>","text":"<pre><code>args: NotRequired[list[str]]\n</code></pre> <p>Command line args to pass to the <code>command</code> executable. For example, <code>['foo.py']</code> or <code>['server.js', '--port', '8080']</code>.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env: NotRequired[dict[str, str]]\n</code></pre> <p>The environment variables to set for the server. .</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.cwd","title":"cwd  <code>instance-attribute</code>","text":"<pre><code>cwd: NotRequired[str | Path]\n</code></pre> <p>The working directory to use when spawning the process.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding: NotRequired[str]\n</code></pre> <p>The text encoding used when sending/receiving messages to the server. Defaults to <code>utf-8</code>.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.encoding_error_handler","title":"encoding_error_handler  <code>instance-attribute</code>","text":"<pre><code>encoding_error_handler: NotRequired[\n    Literal[\"strict\", \"ignore\", \"replace\"]\n]\n</code></pre> <p>The text encoding error handler. Defaults to <code>strict</code>.</p> <p>See https://docs.python.org/3/library/codecs.html#codec-base-classes for explanations of possible values.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio","title":"MCPServerStdio","text":"<p>               Bases: <code>_MCPServerWithClientSession</code></p> <p>MCP server implementation that uses the stdio transport. See the [spec] (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) for details.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStdio(_MCPServerWithClientSession):\n    \"\"\"MCP server implementation that uses the stdio transport. See the [spec]\n    (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) for\n    details.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: MCPServerStdioParams,\n        cache_tools_list: bool = False,\n        name: str | None = None,\n        client_session_timeout_seconds: float | None = 5,\n        tool_filter: ToolFilter = None,\n        use_structured_content: bool = False,\n        max_retry_attempts: int = 0,\n        retry_backoff_seconds_base: float = 1.0,\n        message_handler: MessageHandlerFnT | None = None,\n        require_approval: RequireApprovalSetting = None,\n        failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n        tool_meta_resolver: MCPToolMetaResolver | None = None,\n    ):\n        \"\"\"Create a new MCP server based on the stdio transport.\n\n        Args:\n            params: The params that configure the server. This includes the command to run to\n                start the server, the args to pass to the command, the environment variables to\n                set for the server, the working directory to use when spawning the process, and\n                the text encoding used when sending/receiving messages to the server.\n            cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n                cached and only fetched from the server once. If `False`, the tools list will be\n                fetched from the server on each call to `list_tools()`. The cache can be\n                invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n                if you know the server will not change its tools list, because it can drastically\n                improve latency (by avoiding a round-trip to the server every time).\n            name: A readable name for the server. If not provided, we'll create one from the\n                command.\n            client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n            tool_filter: The tool filter to use for filtering tools.\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n            max_retry_attempts: Number of times to retry failed list_tools/call_tool calls.\n                Defaults to no retries.\n            retry_backoff_seconds_base: The base delay, in seconds, for exponential\n                backoff between retries.\n            message_handler: Optional handler invoked for session messages as delivered by the\n                ClientSession.\n            require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n                a dict of tool names to those values, or an object with always/never tool lists.\n            failure_error_function: Optional function used to convert MCP tool failures into\n                a model-visible error message. If explicitly set to None, tool errors will be\n                raised instead of converted. If left unset, the agent-level configuration (or\n                SDK default) will be used.\n            tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n                tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n        \"\"\"\n        super().__init__(\n            cache_tools_list=cache_tools_list,\n            client_session_timeout_seconds=client_session_timeout_seconds,\n            tool_filter=tool_filter,\n            use_structured_content=use_structured_content,\n            max_retry_attempts=max_retry_attempts,\n            retry_backoff_seconds_base=retry_backoff_seconds_base,\n            message_handler=message_handler,\n            require_approval=require_approval,\n            failure_error_function=failure_error_function,\n            tool_meta_resolver=tool_meta_resolver,\n        )\n\n        self.params = StdioServerParameters(\n            command=params[\"command\"],\n            args=params.get(\"args\", []),\n            env=params.get(\"env\"),\n            cwd=params.get(\"cwd\"),\n            encoding=params.get(\"encoding\", \"utf-8\"),\n            encoding_error_handler=params.get(\"encoding_error_handler\", \"strict\"),\n        )\n\n        self._name = name or f\"stdio: {self.params.command}\"\n\n    def create_streams(\n        self,\n    ) -&gt; AbstractAsyncContextManager[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n            GetSessionIdCallback | None,\n        ]\n    ]:\n        \"\"\"Create the streams for the server.\"\"\"\n        return stdio_client(self.params)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        return self._name\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.__init__","title":"__init__","text":"<pre><code>__init__(\n    params: MCPServerStdioParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n    max_retry_attempts: int = 0,\n    retry_backoff_seconds_base: float = 1.0,\n    message_handler: MessageHandlerFnT | None = None,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction\n    | None\n    | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n)\n</code></pre> <p>Create a new MCP server based on the stdio transport.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MCPServerStdioParams</code> <p>The params that configure the server. This includes the command to run to start the server, the args to pass to the command, the environment variables to set for the server, the working directory to use when spawning the process, and the text encoding used when sending/receiving messages to the server.</p> required <code>cache_tools_list</code> <code>bool</code> <p>Whether to cache the tools list. If <code>True</code>, the tools list will be cached and only fetched from the server once. If <code>False</code>, the tools list will be fetched from the server on each call to <code>list_tools()</code>. The cache can be invalidated by calling <code>invalidate_tools_cache()</code>. You should set this to <code>True</code> if you know the server will not change its tools list, because it can drastically improve latency (by avoiding a round-trip to the server every time).</p> <code>False</code> <code>name</code> <code>str | None</code> <p>A readable name for the server. If not provided, we'll create one from the command.</p> <code>None</code> <code>client_session_timeout_seconds</code> <code>float | None</code> <p>the read timeout passed to the MCP ClientSession.</p> <code>5</code> <code>tool_filter</code> <code>ToolFilter</code> <p>The tool filter to use for filtering tools.</p> <code>None</code> <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool. Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> <code>max_retry_attempts</code> <code>int</code> <p>Number of times to retry failed list_tools/call_tool calls. Defaults to no retries.</p> <code>0</code> <code>retry_backoff_seconds_base</code> <code>float</code> <p>The base delay, in seconds, for exponential backoff between retries.</p> <code>1.0</code> <code>message_handler</code> <code>MessageHandlerFnT | None</code> <p>Optional handler invoked for session messages as delivered by the ClientSession.</p> <code>None</code> <code>require_approval</code> <code>RequireApprovalSetting</code> <p>Approval policy for tools on this server. Accepts \"always\"/\"never\", a dict of tool names to those values, or an object with always/never tool lists.</p> <code>None</code> <code>failure_error_function</code> <code>ToolErrorFunction | None | _UnsetType</code> <p>Optional function used to convert MCP tool failures into a model-visible error message. If explicitly set to None, tool errors will be raised instead of converted. If left unset, the agent-level configuration (or SDK default) will be used.</p> <code>_UNSET</code> <code>tool_meta_resolver</code> <code>MCPToolMetaResolver | None</code> <p>Optional callable that produces MCP request metadata (<code>_meta</code>) for tool calls. It is invoked by the Agents SDK before calling <code>call_tool</code>.</p> <code>None</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    params: MCPServerStdioParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n    max_retry_attempts: int = 0,\n    retry_backoff_seconds_base: float = 1.0,\n    message_handler: MessageHandlerFnT | None = None,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n):\n    \"\"\"Create a new MCP server based on the stdio transport.\n\n    Args:\n        params: The params that configure the server. This includes the command to run to\n            start the server, the args to pass to the command, the environment variables to\n            set for the server, the working directory to use when spawning the process, and\n            the text encoding used when sending/receiving messages to the server.\n        cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n            cached and only fetched from the server once. If `False`, the tools list will be\n            fetched from the server on each call to `list_tools()`. The cache can be\n            invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n            if you know the server will not change its tools list, because it can drastically\n            improve latency (by avoiding a round-trip to the server every time).\n        name: A readable name for the server. If not provided, we'll create one from the\n            command.\n        client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n        tool_filter: The tool filter to use for filtering tools.\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n        max_retry_attempts: Number of times to retry failed list_tools/call_tool calls.\n            Defaults to no retries.\n        retry_backoff_seconds_base: The base delay, in seconds, for exponential\n            backoff between retries.\n        message_handler: Optional handler invoked for session messages as delivered by the\n            ClientSession.\n        require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n            a dict of tool names to those values, or an object with always/never tool lists.\n        failure_error_function: Optional function used to convert MCP tool failures into\n            a model-visible error message. If explicitly set to None, tool errors will be\n            raised instead of converted. If left unset, the agent-level configuration (or\n            SDK default) will be used.\n        tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n            tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n    \"\"\"\n    super().__init__(\n        cache_tools_list=cache_tools_list,\n        client_session_timeout_seconds=client_session_timeout_seconds,\n        tool_filter=tool_filter,\n        use_structured_content=use_structured_content,\n        max_retry_attempts=max_retry_attempts,\n        retry_backoff_seconds_base=retry_backoff_seconds_base,\n        message_handler=message_handler,\n        require_approval=require_approval,\n        failure_error_function=failure_error_function,\n        tool_meta_resolver=tool_meta_resolver,\n    )\n\n    self.params = StdioServerParameters(\n        command=params[\"command\"],\n        args=params.get(\"args\", []),\n        env=params.get(\"env\"),\n        cwd=params.get(\"cwd\"),\n        encoding=params.get(\"encoding\", \"utf-8\"),\n        encoding_error_handler=params.get(\"encoding_error_handler\", \"strict\"),\n    )\n\n    self._name = name or f\"stdio: {self.params.command}\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.create_streams","title":"create_streams","text":"<pre><code>create_streams() -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]\n</code></pre> <p>Create the streams for the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def create_streams(\n    self,\n) -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[SessionMessage | Exception],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]:\n    \"\"\"Create the streams for the server.\"\"\"\n    return stdio_client(self.params)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def connect(self):\n    \"\"\"Connect to the server.\"\"\"\n    connection_succeeded = False\n    try:\n        transport = await self.exit_stack.enter_async_context(self.create_streams())\n        # streamablehttp_client returns (read, write, get_session_id)\n        # sse_client returns (read, write)\n\n        read, write, *_ = transport\n\n        session = await self.exit_stack.enter_async_context(\n            ClientSession(\n                read,\n                write,\n                timedelta(seconds=self.client_session_timeout_seconds)\n                if self.client_session_timeout_seconds\n                else None,\n                message_handler=self.message_handler,\n            )\n        )\n        server_result = await session.initialize()\n        self.server_initialize_result = server_result\n        self.session = session\n        connection_succeeded = True\n    except Exception as e:\n        # Try to extract HTTP error from exception or ExceptionGroup\n        http_error = self._extract_http_error_from_exception(e)\n        if http_error:\n            self._raise_user_error_for_http_error(http_error)\n\n        # For CancelledError, preserve cancellation semantics - don't wrap it.\n        # If it's masking an HTTP error, cleanup() will extract and raise UserError.\n        if isinstance(e, asyncio.CancelledError):\n            raise\n\n        # For HTTP-related errors, wrap them\n        if isinstance(e, (httpx.HTTPStatusError, httpx.ConnectError, httpx.TimeoutException)):\n            self._raise_user_error_for_http_error(e)\n\n        # For other errors, re-raise as-is (don't wrap non-HTTP errors)\n        raise\n    finally:\n        # Always attempt cleanup on error, but suppress cleanup errors that mask the original\n        if not connection_succeeded:\n            try:\n                await self.cleanup()\n            except UserError:\n                # Re-raise UserError from cleanup (contains the real HTTP error)\n                raise\n            except Exception as cleanup_error:\n                # Suppress RuntimeError about cancel scopes during cleanup - this is a known\n                # issue with the MCP library's async generator cleanup and shouldn't mask the\n                # original error\n                if isinstance(cleanup_error, RuntimeError) and \"cancel scope\" in str(\n                    cleanup_error\n                ):\n                    logger.debug(\n                        f\"Ignoring cancel scope error during cleanup of MCP server \"\n                        f\"'{self.name}': {cleanup_error}\"\n                    )\n                else:\n                    # Log other cleanup errors but don't raise - original error is more\n                    # important\n                    logger.warning(\n                        f\"Error during cleanup of MCP server '{self.name}': {cleanup_error}\"\n                    )\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the server.\"\"\"\n    async with self._cleanup_lock:\n        # Only raise HTTP errors if we're cleaning up after a failed connection.\n        # During normal teardown (via __aexit__), log but don't raise to avoid\n        # masking the original exception.\n        is_failed_connection_cleanup = self.session is None\n\n        try:\n            await self.exit_stack.aclose()\n        except asyncio.CancelledError as e:\n            logger.debug(f\"Cleanup cancelled for MCP server '{self.name}': {e}\")\n            raise\n        except BaseExceptionGroup as eg:\n            # Extract HTTP errors from ExceptionGroup raised during cleanup\n            # This happens when background tasks fail (e.g., HTTP errors)\n            http_error = None\n            connect_error = None\n            timeout_error = None\n            error_message = f\"Failed to connect to MCP server '{self.name}': \"\n\n            for exc in eg.exceptions:\n                if isinstance(exc, httpx.HTTPStatusError):\n                    http_error = exc\n                elif isinstance(exc, httpx.ConnectError):\n                    connect_error = exc\n                elif isinstance(exc, httpx.TimeoutException):\n                    timeout_error = exc\n\n            # Only raise HTTP errors if we're cleaning up after a failed connection.\n            # During normal teardown, log them instead.\n            if http_error:\n                if is_failed_connection_cleanup:\n                    error_message += f\"HTTP error {http_error.response.status_code} ({http_error.response.reason_phrase})\"  # noqa: E501\n                    raise UserError(error_message) from http_error\n                else:\n                    # Normal teardown - log but don't raise\n                    logger.warning(\n                        f\"HTTP error during cleanup of MCP server '{self.name}': {http_error}\"\n                    )\n            elif connect_error:\n                if is_failed_connection_cleanup:\n                    error_message += \"Could not reach the server.\"\n                    raise UserError(error_message) from connect_error\n                else:\n                    logger.warning(\n                        f\"Connection error during cleanup of MCP server '{self.name}': {connect_error}\"  # noqa: E501\n                    )\n            elif timeout_error:\n                if is_failed_connection_cleanup:\n                    error_message += \"Connection timeout.\"\n                    raise UserError(error_message) from timeout_error\n                else:\n                    logger.warning(\n                        f\"Timeout error during cleanup of MCP server '{self.name}': {timeout_error}\"  # noqa: E501\n                    )\n            else:\n                # No HTTP error found, suppress RuntimeError about cancel scopes\n                has_cancel_scope_error = any(\n                    isinstance(exc, RuntimeError) and \"cancel scope\" in str(exc)\n                    for exc in eg.exceptions\n                )\n                if has_cancel_scope_error:\n                    logger.debug(f\"Ignoring cancel scope error during cleanup: {eg}\")\n                else:\n                    logger.error(f\"Error cleaning up server: {eg}\")\n        except Exception as e:\n            # Suppress RuntimeError about cancel scopes - this is a known issue with the MCP\n            # library when background tasks fail during async generator cleanup\n            if isinstance(e, RuntimeError) and \"cancel scope\" in str(e):\n                logger.debug(f\"Ignoring cancel scope error during cleanup: {e}\")\n            else:\n                logger.error(f\"Error cleaning up server: {e}\")\n        finally:\n            self.session = None\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.list_tools","title":"list_tools  <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n    session = self.session\n    assert session is not None\n\n    try:\n        # Return from cache if caching is enabled, we have tools, and the cache is not dirty\n        if self.cache_tools_list and not self._cache_dirty and self._tools_list:\n            tools = self._tools_list\n        else:\n            # Fetch the tools from the server\n            result = await self._run_with_retries(lambda: session.list_tools())\n            self._tools_list = result.tools\n            self._cache_dirty = False\n            tools = self._tools_list\n\n        # Filter tools based on tool_filter\n        filtered_tools = tools\n        if self.tool_filter is not None:\n            filtered_tools = await self._apply_tool_filter(filtered_tools, run_context, agent)\n        return filtered_tools\n    except httpx.HTTPStatusError as e:\n        status_code = e.response.status_code\n        raise UserError(\n            f\"Failed to list tools from MCP server '{self.name}': HTTP error {status_code}\"\n        ) from e\n    except httpx.ConnectError as e:\n        raise UserError(\n            f\"Failed to list tools from MCP server '{self.name}': Connection lost. \"\n            f\"The server may have disconnected.\"\n        ) from e\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def call_tool(\n    self,\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n    session = self.session\n    assert session is not None\n\n    try:\n        if meta is None:\n            return await self._run_with_retries(lambda: session.call_tool(tool_name, arguments))\n        return await self._run_with_retries(\n            lambda: session.call_tool(tool_name, arguments, meta=meta)\n        )\n    except httpx.HTTPStatusError as e:\n        status_code = e.response.status_code\n        raise UserError(\n            f\"Failed to call tool '{tool_name}' on MCP server '{self.name}': \"\n            f\"HTTP error {status_code}\"\n        ) from e\n    except httpx.ConnectError as e:\n        raise UserError(\n            f\"Failed to call tool '{tool_name}' on MCP server '{self.name}': Connection lost. \"\n            f\"The server may have disconnected.\"\n        ) from e\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.list_prompts","title":"list_prompts  <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.list_prompts()\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.get_prompt(name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.invalidate_tools_cache","title":"invalidate_tools_cache","text":"<pre><code>invalidate_tools_cache()\n</code></pre> <p>Invalidate the tools cache.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def invalidate_tools_cache(self):\n    \"\"\"Invalidate the tools cache.\"\"\"\n    self._cache_dirty = True\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams","title":"MCPServerSseParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Mirrors the params in<code>mcp.client.sse.sse_client</code>.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerSseParams(TypedDict):\n    \"\"\"Mirrors the params in`mcp.client.sse.sse_client`.\"\"\"\n\n    url: str\n    \"\"\"The URL of the server.\"\"\"\n\n    headers: NotRequired[dict[str, str]]\n    \"\"\"The headers to send to the server.\"\"\"\n\n    timeout: NotRequired[float]\n    \"\"\"The timeout for the HTTP request. Defaults to 5 seconds.\"\"\"\n\n    sse_read_timeout: NotRequired[float]\n    \"\"\"The timeout for the SSE connection, in seconds. Defaults to 5 minutes.\"\"\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>The URL of the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: NotRequired[dict[str, str]]\n</code></pre> <p>The headers to send to the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: NotRequired[float]\n</code></pre> <p>The timeout for the HTTP request. Defaults to 5 seconds.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.sse_read_timeout","title":"sse_read_timeout  <code>instance-attribute</code>","text":"<pre><code>sse_read_timeout: NotRequired[float]\n</code></pre> <p>The timeout for the SSE connection, in seconds. Defaults to 5 minutes.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse","title":"MCPServerSse","text":"<p>               Bases: <code>_MCPServerWithClientSession</code></p> <p>MCP server implementation that uses the HTTP with SSE transport. See the [spec] (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) for details.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerSse(_MCPServerWithClientSession):\n    \"\"\"MCP server implementation that uses the HTTP with SSE transport. See the [spec]\n    (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse)\n    for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: MCPServerSseParams,\n        cache_tools_list: bool = False,\n        name: str | None = None,\n        client_session_timeout_seconds: float | None = 5,\n        tool_filter: ToolFilter = None,\n        use_structured_content: bool = False,\n        max_retry_attempts: int = 0,\n        retry_backoff_seconds_base: float = 1.0,\n        message_handler: MessageHandlerFnT | None = None,\n        require_approval: RequireApprovalSetting = None,\n        failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n        tool_meta_resolver: MCPToolMetaResolver | None = None,\n    ):\n        \"\"\"Create a new MCP server based on the HTTP with SSE transport.\n\n        Args:\n            params: The params that configure the server. This includes the URL of the server,\n                the headers to send to the server, the timeout for the HTTP request, and the\n                timeout for the SSE connection.\n\n            cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n                cached and only fetched from the server once. If `False`, the tools list will be\n                fetched from the server on each call to `list_tools()`. The cache can be\n                invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n                if you know the server will not change its tools list, because it can drastically\n                improve latency (by avoiding a round-trip to the server every time).\n\n            name: A readable name for the server. If not provided, we'll create one from the\n                URL.\n\n            client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n            tool_filter: The tool filter to use for filtering tools.\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n            max_retry_attempts: Number of times to retry failed list_tools/call_tool calls.\n                Defaults to no retries.\n            retry_backoff_seconds_base: The base delay, in seconds, for exponential\n                backoff between retries.\n            message_handler: Optional handler invoked for session messages as delivered by the\n                ClientSession.\n            require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n                a dict of tool names to those values, or an object with always/never tool lists.\n            failure_error_function: Optional function used to convert MCP tool failures into\n                a model-visible error message. If explicitly set to None, tool errors will be\n                raised instead of converted. If left unset, the agent-level configuration (or\n                SDK default) will be used.\n            tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n                tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n        \"\"\"\n        super().__init__(\n            cache_tools_list=cache_tools_list,\n            client_session_timeout_seconds=client_session_timeout_seconds,\n            tool_filter=tool_filter,\n            use_structured_content=use_structured_content,\n            max_retry_attempts=max_retry_attempts,\n            retry_backoff_seconds_base=retry_backoff_seconds_base,\n            message_handler=message_handler,\n            require_approval=require_approval,\n            failure_error_function=failure_error_function,\n            tool_meta_resolver=tool_meta_resolver,\n        )\n\n        self.params = params\n        self._name = name or f\"sse: {self.params['url']}\"\n\n    def create_streams(\n        self,\n    ) -&gt; AbstractAsyncContextManager[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n            GetSessionIdCallback | None,\n        ]\n    ]:\n        \"\"\"Create the streams for the server.\"\"\"\n        return sse_client(\n            url=self.params[\"url\"],\n            headers=self.params.get(\"headers\", None),\n            timeout=self.params.get(\"timeout\", 5),\n            sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        return self._name\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.__init__","title":"__init__","text":"<pre><code>__init__(\n    params: MCPServerSseParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n    max_retry_attempts: int = 0,\n    retry_backoff_seconds_base: float = 1.0,\n    message_handler: MessageHandlerFnT | None = None,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction\n    | None\n    | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n)\n</code></pre> <p>Create a new MCP server based on the HTTP with SSE transport.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MCPServerSseParams</code> <p>The params that configure the server. This includes the URL of the server, the headers to send to the server, the timeout for the HTTP request, and the timeout for the SSE connection.</p> required <code>cache_tools_list</code> <code>bool</code> <p>Whether to cache the tools list. If <code>True</code>, the tools list will be cached and only fetched from the server once. If <code>False</code>, the tools list will be fetched from the server on each call to <code>list_tools()</code>. The cache can be invalidated by calling <code>invalidate_tools_cache()</code>. You should set this to <code>True</code> if you know the server will not change its tools list, because it can drastically improve latency (by avoiding a round-trip to the server every time).</p> <code>False</code> <code>name</code> <code>str | None</code> <p>A readable name for the server. If not provided, we'll create one from the URL.</p> <code>None</code> <code>client_session_timeout_seconds</code> <code>float | None</code> <p>the read timeout passed to the MCP ClientSession.</p> <code>5</code> <code>tool_filter</code> <code>ToolFilter</code> <p>The tool filter to use for filtering tools.</p> <code>None</code> <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool. Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> <code>max_retry_attempts</code> <code>int</code> <p>Number of times to retry failed list_tools/call_tool calls. Defaults to no retries.</p> <code>0</code> <code>retry_backoff_seconds_base</code> <code>float</code> <p>The base delay, in seconds, for exponential backoff between retries.</p> <code>1.0</code> <code>message_handler</code> <code>MessageHandlerFnT | None</code> <p>Optional handler invoked for session messages as delivered by the ClientSession.</p> <code>None</code> <code>require_approval</code> <code>RequireApprovalSetting</code> <p>Approval policy for tools on this server. Accepts \"always\"/\"never\", a dict of tool names to those values, or an object with always/never tool lists.</p> <code>None</code> <code>failure_error_function</code> <code>ToolErrorFunction | None | _UnsetType</code> <p>Optional function used to convert MCP tool failures into a model-visible error message. If explicitly set to None, tool errors will be raised instead of converted. If left unset, the agent-level configuration (or SDK default) will be used.</p> <code>_UNSET</code> <code>tool_meta_resolver</code> <code>MCPToolMetaResolver | None</code> <p>Optional callable that produces MCP request metadata (<code>_meta</code>) for tool calls. It is invoked by the Agents SDK before calling <code>call_tool</code>.</p> <code>None</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    params: MCPServerSseParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n    max_retry_attempts: int = 0,\n    retry_backoff_seconds_base: float = 1.0,\n    message_handler: MessageHandlerFnT | None = None,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n):\n    \"\"\"Create a new MCP server based on the HTTP with SSE transport.\n\n    Args:\n        params: The params that configure the server. This includes the URL of the server,\n            the headers to send to the server, the timeout for the HTTP request, and the\n            timeout for the SSE connection.\n\n        cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n            cached and only fetched from the server once. If `False`, the tools list will be\n            fetched from the server on each call to `list_tools()`. The cache can be\n            invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n            if you know the server will not change its tools list, because it can drastically\n            improve latency (by avoiding a round-trip to the server every time).\n\n        name: A readable name for the server. If not provided, we'll create one from the\n            URL.\n\n        client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n        tool_filter: The tool filter to use for filtering tools.\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n        max_retry_attempts: Number of times to retry failed list_tools/call_tool calls.\n            Defaults to no retries.\n        retry_backoff_seconds_base: The base delay, in seconds, for exponential\n            backoff between retries.\n        message_handler: Optional handler invoked for session messages as delivered by the\n            ClientSession.\n        require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n            a dict of tool names to those values, or an object with always/never tool lists.\n        failure_error_function: Optional function used to convert MCP tool failures into\n            a model-visible error message. If explicitly set to None, tool errors will be\n            raised instead of converted. If left unset, the agent-level configuration (or\n            SDK default) will be used.\n        tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n            tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n    \"\"\"\n    super().__init__(\n        cache_tools_list=cache_tools_list,\n        client_session_timeout_seconds=client_session_timeout_seconds,\n        tool_filter=tool_filter,\n        use_structured_content=use_structured_content,\n        max_retry_attempts=max_retry_attempts,\n        retry_backoff_seconds_base=retry_backoff_seconds_base,\n        message_handler=message_handler,\n        require_approval=require_approval,\n        failure_error_function=failure_error_function,\n        tool_meta_resolver=tool_meta_resolver,\n    )\n\n    self.params = params\n    self._name = name or f\"sse: {self.params['url']}\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.create_streams","title":"create_streams","text":"<pre><code>create_streams() -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]\n</code></pre> <p>Create the streams for the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def create_streams(\n    self,\n) -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[SessionMessage | Exception],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]:\n    \"\"\"Create the streams for the server.\"\"\"\n    return sse_client(\n        url=self.params[\"url\"],\n        headers=self.params.get(\"headers\", None),\n        timeout=self.params.get(\"timeout\", 5),\n        sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n    )\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def connect(self):\n    \"\"\"Connect to the server.\"\"\"\n    connection_succeeded = False\n    try:\n        transport = await self.exit_stack.enter_async_context(self.create_streams())\n        # streamablehttp_client returns (read, write, get_session_id)\n        # sse_client returns (read, write)\n\n        read, write, *_ = transport\n\n        session = await self.exit_stack.enter_async_context(\n            ClientSession(\n                read,\n                write,\n                timedelta(seconds=self.client_session_timeout_seconds)\n                if self.client_session_timeout_seconds\n                else None,\n                message_handler=self.message_handler,\n            )\n        )\n        server_result = await session.initialize()\n        self.server_initialize_result = server_result\n        self.session = session\n        connection_succeeded = True\n    except Exception as e:\n        # Try to extract HTTP error from exception or ExceptionGroup\n        http_error = self._extract_http_error_from_exception(e)\n        if http_error:\n            self._raise_user_error_for_http_error(http_error)\n\n        # For CancelledError, preserve cancellation semantics - don't wrap it.\n        # If it's masking an HTTP error, cleanup() will extract and raise UserError.\n        if isinstance(e, asyncio.CancelledError):\n            raise\n\n        # For HTTP-related errors, wrap them\n        if isinstance(e, (httpx.HTTPStatusError, httpx.ConnectError, httpx.TimeoutException)):\n            self._raise_user_error_for_http_error(e)\n\n        # For other errors, re-raise as-is (don't wrap non-HTTP errors)\n        raise\n    finally:\n        # Always attempt cleanup on error, but suppress cleanup errors that mask the original\n        if not connection_succeeded:\n            try:\n                await self.cleanup()\n            except UserError:\n                # Re-raise UserError from cleanup (contains the real HTTP error)\n                raise\n            except Exception as cleanup_error:\n                # Suppress RuntimeError about cancel scopes during cleanup - this is a known\n                # issue with the MCP library's async generator cleanup and shouldn't mask the\n                # original error\n                if isinstance(cleanup_error, RuntimeError) and \"cancel scope\" in str(\n                    cleanup_error\n                ):\n                    logger.debug(\n                        f\"Ignoring cancel scope error during cleanup of MCP server \"\n                        f\"'{self.name}': {cleanup_error}\"\n                    )\n                else:\n                    # Log other cleanup errors but don't raise - original error is more\n                    # important\n                    logger.warning(\n                        f\"Error during cleanup of MCP server '{self.name}': {cleanup_error}\"\n                    )\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the server.\"\"\"\n    async with self._cleanup_lock:\n        # Only raise HTTP errors if we're cleaning up after a failed connection.\n        # During normal teardown (via __aexit__), log but don't raise to avoid\n        # masking the original exception.\n        is_failed_connection_cleanup = self.session is None\n\n        try:\n            await self.exit_stack.aclose()\n        except asyncio.CancelledError as e:\n            logger.debug(f\"Cleanup cancelled for MCP server '{self.name}': {e}\")\n            raise\n        except BaseExceptionGroup as eg:\n            # Extract HTTP errors from ExceptionGroup raised during cleanup\n            # This happens when background tasks fail (e.g., HTTP errors)\n            http_error = None\n            connect_error = None\n            timeout_error = None\n            error_message = f\"Failed to connect to MCP server '{self.name}': \"\n\n            for exc in eg.exceptions:\n                if isinstance(exc, httpx.HTTPStatusError):\n                    http_error = exc\n                elif isinstance(exc, httpx.ConnectError):\n                    connect_error = exc\n                elif isinstance(exc, httpx.TimeoutException):\n                    timeout_error = exc\n\n            # Only raise HTTP errors if we're cleaning up after a failed connection.\n            # During normal teardown, log them instead.\n            if http_error:\n                if is_failed_connection_cleanup:\n                    error_message += f\"HTTP error {http_error.response.status_code} ({http_error.response.reason_phrase})\"  # noqa: E501\n                    raise UserError(error_message) from http_error\n                else:\n                    # Normal teardown - log but don't raise\n                    logger.warning(\n                        f\"HTTP error during cleanup of MCP server '{self.name}': {http_error}\"\n                    )\n            elif connect_error:\n                if is_failed_connection_cleanup:\n                    error_message += \"Could not reach the server.\"\n                    raise UserError(error_message) from connect_error\n                else:\n                    logger.warning(\n                        f\"Connection error during cleanup of MCP server '{self.name}': {connect_error}\"  # noqa: E501\n                    )\n            elif timeout_error:\n                if is_failed_connection_cleanup:\n                    error_message += \"Connection timeout.\"\n                    raise UserError(error_message) from timeout_error\n                else:\n                    logger.warning(\n                        f\"Timeout error during cleanup of MCP server '{self.name}': {timeout_error}\"  # noqa: E501\n                    )\n            else:\n                # No HTTP error found, suppress RuntimeError about cancel scopes\n                has_cancel_scope_error = any(\n                    isinstance(exc, RuntimeError) and \"cancel scope\" in str(exc)\n                    for exc in eg.exceptions\n                )\n                if has_cancel_scope_error:\n                    logger.debug(f\"Ignoring cancel scope error during cleanup: {eg}\")\n                else:\n                    logger.error(f\"Error cleaning up server: {eg}\")\n        except Exception as e:\n            # Suppress RuntimeError about cancel scopes - this is a known issue with the MCP\n            # library when background tasks fail during async generator cleanup\n            if isinstance(e, RuntimeError) and \"cancel scope\" in str(e):\n                logger.debug(f\"Ignoring cancel scope error during cleanup: {e}\")\n            else:\n                logger.error(f\"Error cleaning up server: {e}\")\n        finally:\n            self.session = None\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.list_tools","title":"list_tools  <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n    session = self.session\n    assert session is not None\n\n    try:\n        # Return from cache if caching is enabled, we have tools, and the cache is not dirty\n        if self.cache_tools_list and not self._cache_dirty and self._tools_list:\n            tools = self._tools_list\n        else:\n            # Fetch the tools from the server\n            result = await self._run_with_retries(lambda: session.list_tools())\n            self._tools_list = result.tools\n            self._cache_dirty = False\n            tools = self._tools_list\n\n        # Filter tools based on tool_filter\n        filtered_tools = tools\n        if self.tool_filter is not None:\n            filtered_tools = await self._apply_tool_filter(filtered_tools, run_context, agent)\n        return filtered_tools\n    except httpx.HTTPStatusError as e:\n        status_code = e.response.status_code\n        raise UserError(\n            f\"Failed to list tools from MCP server '{self.name}': HTTP error {status_code}\"\n        ) from e\n    except httpx.ConnectError as e:\n        raise UserError(\n            f\"Failed to list tools from MCP server '{self.name}': Connection lost. \"\n            f\"The server may have disconnected.\"\n        ) from e\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def call_tool(\n    self,\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n    session = self.session\n    assert session is not None\n\n    try:\n        if meta is None:\n            return await self._run_with_retries(lambda: session.call_tool(tool_name, arguments))\n        return await self._run_with_retries(\n            lambda: session.call_tool(tool_name, arguments, meta=meta)\n        )\n    except httpx.HTTPStatusError as e:\n        status_code = e.response.status_code\n        raise UserError(\n            f\"Failed to call tool '{tool_name}' on MCP server '{self.name}': \"\n            f\"HTTP error {status_code}\"\n        ) from e\n    except httpx.ConnectError as e:\n        raise UserError(\n            f\"Failed to call tool '{tool_name}' on MCP server '{self.name}': Connection lost. \"\n            f\"The server may have disconnected.\"\n        ) from e\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.list_prompts","title":"list_prompts  <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.list_prompts()\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.get_prompt(name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.invalidate_tools_cache","title":"invalidate_tools_cache","text":"<pre><code>invalidate_tools_cache()\n</code></pre> <p>Invalidate the tools cache.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def invalidate_tools_cache(self):\n    \"\"\"Invalidate the tools cache.\"\"\"\n    self._cache_dirty = True\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams","title":"MCPServerStreamableHttpParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Mirrors the params in<code>mcp.client.streamable_http.streamablehttp_client</code>.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStreamableHttpParams(TypedDict):\n    \"\"\"Mirrors the params in`mcp.client.streamable_http.streamablehttp_client`.\"\"\"\n\n    url: str\n    \"\"\"The URL of the server.\"\"\"\n\n    headers: NotRequired[dict[str, str]]\n    \"\"\"The headers to send to the server.\"\"\"\n\n    timeout: NotRequired[timedelta | float]\n    \"\"\"The timeout for the HTTP request. Defaults to 5 seconds.\"\"\"\n\n    sse_read_timeout: NotRequired[timedelta | float]\n    \"\"\"The timeout for the SSE connection, in seconds. Defaults to 5 minutes.\"\"\"\n\n    terminate_on_close: NotRequired[bool]\n    \"\"\"Terminate on close\"\"\"\n\n    httpx_client_factory: NotRequired[HttpClientFactory]\n    \"\"\"Custom HTTP client factory for configuring httpx.AsyncClient behavior.\"\"\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>The URL of the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: NotRequired[dict[str, str]]\n</code></pre> <p>The headers to send to the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: NotRequired[timedelta | float]\n</code></pre> <p>The timeout for the HTTP request. Defaults to 5 seconds.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.sse_read_timeout","title":"sse_read_timeout  <code>instance-attribute</code>","text":"<pre><code>sse_read_timeout: NotRequired[timedelta | float]\n</code></pre> <p>The timeout for the SSE connection, in seconds. Defaults to 5 minutes.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.terminate_on_close","title":"terminate_on_close  <code>instance-attribute</code>","text":"<pre><code>terminate_on_close: NotRequired[bool]\n</code></pre> <p>Terminate on close</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.httpx_client_factory","title":"httpx_client_factory  <code>instance-attribute</code>","text":"<pre><code>httpx_client_factory: NotRequired[HttpClientFactory]\n</code></pre> <p>Custom HTTP client factory for configuring httpx.AsyncClient behavior.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp","title":"MCPServerStreamableHttp","text":"<p>               Bases: <code>_MCPServerWithClientSession</code></p> <p>MCP server implementation that uses the Streamable HTTP transport. See the [spec] (https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) for details.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStreamableHttp(_MCPServerWithClientSession):\n    \"\"\"MCP server implementation that uses the Streamable HTTP transport. See the [spec]\n    (https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http)\n    for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: MCPServerStreamableHttpParams,\n        cache_tools_list: bool = False,\n        name: str | None = None,\n        client_session_timeout_seconds: float | None = 5,\n        tool_filter: ToolFilter = None,\n        use_structured_content: bool = False,\n        max_retry_attempts: int = 0,\n        retry_backoff_seconds_base: float = 1.0,\n        message_handler: MessageHandlerFnT | None = None,\n        require_approval: RequireApprovalSetting = None,\n        failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n        tool_meta_resolver: MCPToolMetaResolver | None = None,\n    ):\n        \"\"\"Create a new MCP server based on the Streamable HTTP transport.\n\n        Args:\n            params: The params that configure the server. This includes the URL of the server,\n                the headers to send to the server, the timeout for the HTTP request, the\n                timeout for the Streamable HTTP connection, whether we need to\n                terminate on close, and an optional custom HTTP client factory.\n\n            cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n                cached and only fetched from the server once. If `False`, the tools list will be\n                fetched from the server on each call to `list_tools()`. The cache can be\n                invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n                if you know the server will not change its tools list, because it can drastically\n                improve latency (by avoiding a round-trip to the server every time).\n\n            name: A readable name for the server. If not provided, we'll create one from the\n                URL.\n\n            client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n            tool_filter: The tool filter to use for filtering tools.\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n            max_retry_attempts: Number of times to retry failed list_tools/call_tool calls.\n                Defaults to no retries.\n            retry_backoff_seconds_base: The base delay, in seconds, for exponential\n                backoff between retries.\n            message_handler: Optional handler invoked for session messages as delivered by the\n                ClientSession.\n            require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n                a dict of tool names to those values, or an object with always/never tool lists.\n            failure_error_function: Optional function used to convert MCP tool failures into\n                a model-visible error message. If explicitly set to None, tool errors will be\n                raised instead of converted. If left unset, the agent-level configuration (or\n                SDK default) will be used.\n            tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n                tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n        \"\"\"\n        super().__init__(\n            cache_tools_list=cache_tools_list,\n            client_session_timeout_seconds=client_session_timeout_seconds,\n            tool_filter=tool_filter,\n            use_structured_content=use_structured_content,\n            max_retry_attempts=max_retry_attempts,\n            retry_backoff_seconds_base=retry_backoff_seconds_base,\n            message_handler=message_handler,\n            require_approval=require_approval,\n            failure_error_function=failure_error_function,\n            tool_meta_resolver=tool_meta_resolver,\n        )\n\n        self.params = params\n        self._name = name or f\"streamable_http: {self.params['url']}\"\n\n    def create_streams(\n        self,\n    ) -&gt; AbstractAsyncContextManager[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n            GetSessionIdCallback | None,\n        ]\n    ]:\n        \"\"\"Create the streams for the server.\"\"\"\n        # Only pass httpx_client_factory if it's provided\n        if \"httpx_client_factory\" in self.params:\n            return streamablehttp_client(\n                url=self.params[\"url\"],\n                headers=self.params.get(\"headers\", None),\n                timeout=self.params.get(\"timeout\", 5),\n                sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n                terminate_on_close=self.params.get(\"terminate_on_close\", True),\n                httpx_client_factory=self.params[\"httpx_client_factory\"],\n            )\n        else:\n            return streamablehttp_client(\n                url=self.params[\"url\"],\n                headers=self.params.get(\"headers\", None),\n                timeout=self.params.get(\"timeout\", 5),\n                sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n                terminate_on_close=self.params.get(\"terminate_on_close\", True),\n            )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        return self._name\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.__init__","title":"__init__","text":"<pre><code>__init__(\n    params: MCPServerStreamableHttpParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n    max_retry_attempts: int = 0,\n    retry_backoff_seconds_base: float = 1.0,\n    message_handler: MessageHandlerFnT | None = None,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction\n    | None\n    | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n)\n</code></pre> <p>Create a new MCP server based on the Streamable HTTP transport.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MCPServerStreamableHttpParams</code> <p>The params that configure the server. This includes the URL of the server, the headers to send to the server, the timeout for the HTTP request, the timeout for the Streamable HTTP connection, whether we need to terminate on close, and an optional custom HTTP client factory.</p> required <code>cache_tools_list</code> <code>bool</code> <p>Whether to cache the tools list. If <code>True</code>, the tools list will be cached and only fetched from the server once. If <code>False</code>, the tools list will be fetched from the server on each call to <code>list_tools()</code>. The cache can be invalidated by calling <code>invalidate_tools_cache()</code>. You should set this to <code>True</code> if you know the server will not change its tools list, because it can drastically improve latency (by avoiding a round-trip to the server every time).</p> <code>False</code> <code>name</code> <code>str | None</code> <p>A readable name for the server. If not provided, we'll create one from the URL.</p> <code>None</code> <code>client_session_timeout_seconds</code> <code>float | None</code> <p>the read timeout passed to the MCP ClientSession.</p> <code>5</code> <code>tool_filter</code> <code>ToolFilter</code> <p>The tool filter to use for filtering tools.</p> <code>None</code> <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool. Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> <code>max_retry_attempts</code> <code>int</code> <p>Number of times to retry failed list_tools/call_tool calls. Defaults to no retries.</p> <code>0</code> <code>retry_backoff_seconds_base</code> <code>float</code> <p>The base delay, in seconds, for exponential backoff between retries.</p> <code>1.0</code> <code>message_handler</code> <code>MessageHandlerFnT | None</code> <p>Optional handler invoked for session messages as delivered by the ClientSession.</p> <code>None</code> <code>require_approval</code> <code>RequireApprovalSetting</code> <p>Approval policy for tools on this server. Accepts \"always\"/\"never\", a dict of tool names to those values, or an object with always/never tool lists.</p> <code>None</code> <code>failure_error_function</code> <code>ToolErrorFunction | None | _UnsetType</code> <p>Optional function used to convert MCP tool failures into a model-visible error message. If explicitly set to None, tool errors will be raised instead of converted. If left unset, the agent-level configuration (or SDK default) will be used.</p> <code>_UNSET</code> <code>tool_meta_resolver</code> <code>MCPToolMetaResolver | None</code> <p>Optional callable that produces MCP request metadata (<code>_meta</code>) for tool calls. It is invoked by the Agents SDK before calling <code>call_tool</code>.</p> <code>None</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    params: MCPServerStreamableHttpParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n    max_retry_attempts: int = 0,\n    retry_backoff_seconds_base: float = 1.0,\n    message_handler: MessageHandlerFnT | None = None,\n    require_approval: RequireApprovalSetting = None,\n    failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n    tool_meta_resolver: MCPToolMetaResolver | None = None,\n):\n    \"\"\"Create a new MCP server based on the Streamable HTTP transport.\n\n    Args:\n        params: The params that configure the server. This includes the URL of the server,\n            the headers to send to the server, the timeout for the HTTP request, the\n            timeout for the Streamable HTTP connection, whether we need to\n            terminate on close, and an optional custom HTTP client factory.\n\n        cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n            cached and only fetched from the server once. If `False`, the tools list will be\n            fetched from the server on each call to `list_tools()`. The cache can be\n            invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n            if you know the server will not change its tools list, because it can drastically\n            improve latency (by avoiding a round-trip to the server every time).\n\n        name: A readable name for the server. If not provided, we'll create one from the\n            URL.\n\n        client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n        tool_filter: The tool filter to use for filtering tools.\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n        max_retry_attempts: Number of times to retry failed list_tools/call_tool calls.\n            Defaults to no retries.\n        retry_backoff_seconds_base: The base delay, in seconds, for exponential\n            backoff between retries.\n        message_handler: Optional handler invoked for session messages as delivered by the\n            ClientSession.\n        require_approval: Approval policy for tools on this server. Accepts \"always\"/\"never\",\n            a dict of tool names to those values, or an object with always/never tool lists.\n        failure_error_function: Optional function used to convert MCP tool failures into\n            a model-visible error message. If explicitly set to None, tool errors will be\n            raised instead of converted. If left unset, the agent-level configuration (or\n            SDK default) will be used.\n        tool_meta_resolver: Optional callable that produces MCP request metadata (`_meta`) for\n            tool calls. It is invoked by the Agents SDK before calling `call_tool`.\n    \"\"\"\n    super().__init__(\n        cache_tools_list=cache_tools_list,\n        client_session_timeout_seconds=client_session_timeout_seconds,\n        tool_filter=tool_filter,\n        use_structured_content=use_structured_content,\n        max_retry_attempts=max_retry_attempts,\n        retry_backoff_seconds_base=retry_backoff_seconds_base,\n        message_handler=message_handler,\n        require_approval=require_approval,\n        failure_error_function=failure_error_function,\n        tool_meta_resolver=tool_meta_resolver,\n    )\n\n    self.params = params\n    self._name = name or f\"streamable_http: {self.params['url']}\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.create_streams","title":"create_streams","text":"<pre><code>create_streams() -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]\n</code></pre> <p>Create the streams for the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def create_streams(\n    self,\n) -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[SessionMessage | Exception],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]:\n    \"\"\"Create the streams for the server.\"\"\"\n    # Only pass httpx_client_factory if it's provided\n    if \"httpx_client_factory\" in self.params:\n        return streamablehttp_client(\n            url=self.params[\"url\"],\n            headers=self.params.get(\"headers\", None),\n            timeout=self.params.get(\"timeout\", 5),\n            sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n            terminate_on_close=self.params.get(\"terminate_on_close\", True),\n            httpx_client_factory=self.params[\"httpx_client_factory\"],\n        )\n    else:\n        return streamablehttp_client(\n            url=self.params[\"url\"],\n            headers=self.params.get(\"headers\", None),\n            timeout=self.params.get(\"timeout\", 5),\n            sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n            terminate_on_close=self.params.get(\"terminate_on_close\", True),\n        )\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def connect(self):\n    \"\"\"Connect to the server.\"\"\"\n    connection_succeeded = False\n    try:\n        transport = await self.exit_stack.enter_async_context(self.create_streams())\n        # streamablehttp_client returns (read, write, get_session_id)\n        # sse_client returns (read, write)\n\n        read, write, *_ = transport\n\n        session = await self.exit_stack.enter_async_context(\n            ClientSession(\n                read,\n                write,\n                timedelta(seconds=self.client_session_timeout_seconds)\n                if self.client_session_timeout_seconds\n                else None,\n                message_handler=self.message_handler,\n            )\n        )\n        server_result = await session.initialize()\n        self.server_initialize_result = server_result\n        self.session = session\n        connection_succeeded = True\n    except Exception as e:\n        # Try to extract HTTP error from exception or ExceptionGroup\n        http_error = self._extract_http_error_from_exception(e)\n        if http_error:\n            self._raise_user_error_for_http_error(http_error)\n\n        # For CancelledError, preserve cancellation semantics - don't wrap it.\n        # If it's masking an HTTP error, cleanup() will extract and raise UserError.\n        if isinstance(e, asyncio.CancelledError):\n            raise\n\n        # For HTTP-related errors, wrap them\n        if isinstance(e, (httpx.HTTPStatusError, httpx.ConnectError, httpx.TimeoutException)):\n            self._raise_user_error_for_http_error(e)\n\n        # For other errors, re-raise as-is (don't wrap non-HTTP errors)\n        raise\n    finally:\n        # Always attempt cleanup on error, but suppress cleanup errors that mask the original\n        if not connection_succeeded:\n            try:\n                await self.cleanup()\n            except UserError:\n                # Re-raise UserError from cleanup (contains the real HTTP error)\n                raise\n            except Exception as cleanup_error:\n                # Suppress RuntimeError about cancel scopes during cleanup - this is a known\n                # issue with the MCP library's async generator cleanup and shouldn't mask the\n                # original error\n                if isinstance(cleanup_error, RuntimeError) and \"cancel scope\" in str(\n                    cleanup_error\n                ):\n                    logger.debug(\n                        f\"Ignoring cancel scope error during cleanup of MCP server \"\n                        f\"'{self.name}': {cleanup_error}\"\n                    )\n                else:\n                    # Log other cleanup errors but don't raise - original error is more\n                    # important\n                    logger.warning(\n                        f\"Error during cleanup of MCP server '{self.name}': {cleanup_error}\"\n                    )\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the server.\"\"\"\n    async with self._cleanup_lock:\n        # Only raise HTTP errors if we're cleaning up after a failed connection.\n        # During normal teardown (via __aexit__), log but don't raise to avoid\n        # masking the original exception.\n        is_failed_connection_cleanup = self.session is None\n\n        try:\n            await self.exit_stack.aclose()\n        except asyncio.CancelledError as e:\n            logger.debug(f\"Cleanup cancelled for MCP server '{self.name}': {e}\")\n            raise\n        except BaseExceptionGroup as eg:\n            # Extract HTTP errors from ExceptionGroup raised during cleanup\n            # This happens when background tasks fail (e.g., HTTP errors)\n            http_error = None\n            connect_error = None\n            timeout_error = None\n            error_message = f\"Failed to connect to MCP server '{self.name}': \"\n\n            for exc in eg.exceptions:\n                if isinstance(exc, httpx.HTTPStatusError):\n                    http_error = exc\n                elif isinstance(exc, httpx.ConnectError):\n                    connect_error = exc\n                elif isinstance(exc, httpx.TimeoutException):\n                    timeout_error = exc\n\n            # Only raise HTTP errors if we're cleaning up after a failed connection.\n            # During normal teardown, log them instead.\n            if http_error:\n                if is_failed_connection_cleanup:\n                    error_message += f\"HTTP error {http_error.response.status_code} ({http_error.response.reason_phrase})\"  # noqa: E501\n                    raise UserError(error_message) from http_error\n                else:\n                    # Normal teardown - log but don't raise\n                    logger.warning(\n                        f\"HTTP error during cleanup of MCP server '{self.name}': {http_error}\"\n                    )\n            elif connect_error:\n                if is_failed_connection_cleanup:\n                    error_message += \"Could not reach the server.\"\n                    raise UserError(error_message) from connect_error\n                else:\n                    logger.warning(\n                        f\"Connection error during cleanup of MCP server '{self.name}': {connect_error}\"  # noqa: E501\n                    )\n            elif timeout_error:\n                if is_failed_connection_cleanup:\n                    error_message += \"Connection timeout.\"\n                    raise UserError(error_message) from timeout_error\n                else:\n                    logger.warning(\n                        f\"Timeout error during cleanup of MCP server '{self.name}': {timeout_error}\"  # noqa: E501\n                    )\n            else:\n                # No HTTP error found, suppress RuntimeError about cancel scopes\n                has_cancel_scope_error = any(\n                    isinstance(exc, RuntimeError) and \"cancel scope\" in str(exc)\n                    for exc in eg.exceptions\n                )\n                if has_cancel_scope_error:\n                    logger.debug(f\"Ignoring cancel scope error during cleanup: {eg}\")\n                else:\n                    logger.error(f\"Error cleaning up server: {eg}\")\n        except Exception as e:\n            # Suppress RuntimeError about cancel scopes - this is a known issue with the MCP\n            # library when background tasks fail during async generator cleanup\n            if isinstance(e, RuntimeError) and \"cancel scope\" in str(e):\n                logger.debug(f\"Ignoring cancel scope error during cleanup: {e}\")\n            else:\n                logger.error(f\"Error cleaning up server: {e}\")\n        finally:\n            self.session = None\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.list_tools","title":"list_tools  <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n    session = self.session\n    assert session is not None\n\n    try:\n        # Return from cache if caching is enabled, we have tools, and the cache is not dirty\n        if self.cache_tools_list and not self._cache_dirty and self._tools_list:\n            tools = self._tools_list\n        else:\n            # Fetch the tools from the server\n            result = await self._run_with_retries(lambda: session.list_tools())\n            self._tools_list = result.tools\n            self._cache_dirty = False\n            tools = self._tools_list\n\n        # Filter tools based on tool_filter\n        filtered_tools = tools\n        if self.tool_filter is not None:\n            filtered_tools = await self._apply_tool_filter(filtered_tools, run_context, agent)\n        return filtered_tools\n    except httpx.HTTPStatusError as e:\n        status_code = e.response.status_code\n        raise UserError(\n            f\"Failed to list tools from MCP server '{self.name}': HTTP error {status_code}\"\n        ) from e\n    except httpx.ConnectError as e:\n        raise UserError(\n            f\"Failed to list tools from MCP server '{self.name}': Connection lost. \"\n            f\"The server may have disconnected.\"\n        ) from e\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def call_tool(\n    self,\n    tool_name: str,\n    arguments: dict[str, Any] | None,\n    meta: dict[str, Any] | None = None,\n) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n    session = self.session\n    assert session is not None\n\n    try:\n        if meta is None:\n            return await self._run_with_retries(lambda: session.call_tool(tool_name, arguments))\n        return await self._run_with_retries(\n            lambda: session.call_tool(tool_name, arguments, meta=meta)\n        )\n    except httpx.HTTPStatusError as e:\n        status_code = e.response.status_code\n        raise UserError(\n            f\"Failed to call tool '{tool_name}' on MCP server '{self.name}': \"\n            f\"HTTP error {status_code}\"\n        ) from e\n    except httpx.ConnectError as e:\n        raise UserError(\n            f\"Failed to call tool '{tool_name}' on MCP server '{self.name}': Connection lost. \"\n            f\"The server may have disconnected.\"\n        ) from e\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.list_prompts","title":"list_prompts  <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.list_prompts()\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.get_prompt(name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.invalidate_tools_cache","title":"invalidate_tools_cache","text":"<pre><code>invalidate_tools_cache()\n</code></pre> <p>Invalidate the tools cache.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def invalidate_tools_cache(self):\n    \"\"\"Invalidate the tools cache.\"\"\"\n    self._cache_dirty = True\n</code></pre>"},{"location":"ref/mcp/util/","title":"<code>MCP Util</code>","text":""},{"location":"ref/mcp/util/#agents.mcp.util.HttpClientFactory","title":"HttpClientFactory","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for HTTP client factory functions.</p> <p>This interface matches the MCP SDK's McpHttpClientFactory but is defined locally to avoid accessing internal MCP SDK modules.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>class HttpClientFactory(Protocol):\n    \"\"\"Protocol for HTTP client factory functions.\n\n    This interface matches the MCP SDK's McpHttpClientFactory but is defined locally\n    to avoid accessing internal MCP SDK modules.\n    \"\"\"\n\n    def __call__(\n        self,\n        headers: dict[str, str] | None = None,\n        timeout: httpx.Timeout | None = None,\n        auth: httpx.Auth | None = None,\n    ) -&gt; httpx.AsyncClient: ...\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext","title":"ToolFilterContext  <code>dataclass</code>","text":"<p>Context information available to tool filter functions.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@dataclass\nclass ToolFilterContext:\n    \"\"\"Context information available to tool filter functions.\"\"\"\n\n    run_context: RunContextWrapper[Any]\n    \"\"\"The current run context.\"\"\"\n\n    agent: AgentBase\n    \"\"\"The agent that is requesting the tool list.\"\"\"\n\n    server_name: str\n    \"\"\"The name of the MCP server.\"\"\"\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext.run_context","title":"run_context  <code>instance-attribute</code>","text":"<pre><code>run_context: RunContextWrapper[Any]\n</code></pre> <p>The current run context.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: AgentBase\n</code></pre> <p>The agent that is requesting the tool list.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext.server_name","title":"server_name  <code>instance-attribute</code>","text":"<pre><code>server_name: str\n</code></pre> <p>The name of the MCP server.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterStatic","title":"ToolFilterStatic","text":"<p>               Bases: <code>TypedDict</code></p> <p>Static tool filter configuration using allowlists and blocklists.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>class ToolFilterStatic(TypedDict):\n    \"\"\"Static tool filter configuration using allowlists and blocklists.\"\"\"\n\n    allowed_tool_names: NotRequired[list[str]]\n    \"\"\"Optional list of tool names to allow (whitelist).\n    If set, only these tools will be available.\"\"\"\n\n    blocked_tool_names: NotRequired[list[str]]\n    \"\"\"Optional list of tool names to exclude (blacklist).\n    If set, these tools will be filtered out.\"\"\"\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterStatic.allowed_tool_names","title":"allowed_tool_names  <code>instance-attribute</code>","text":"<pre><code>allowed_tool_names: NotRequired[list[str]]\n</code></pre> <p>Optional list of tool names to allow (whitelist). If set, only these tools will be available.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterStatic.blocked_tool_names","title":"blocked_tool_names  <code>instance-attribute</code>","text":"<pre><code>blocked_tool_names: NotRequired[list[str]]\n</code></pre> <p>Optional list of tool names to exclude (blacklist). If set, these tools will be filtered out.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPToolMetaContext","title":"MCPToolMetaContext  <code>dataclass</code>","text":"<p>Context information available to MCP tool meta resolver functions.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@dataclass\nclass MCPToolMetaContext:\n    \"\"\"Context information available to MCP tool meta resolver functions.\"\"\"\n\n    run_context: RunContextWrapper[Any]\n    \"\"\"The current run context.\"\"\"\n\n    server_name: str\n    \"\"\"The name of the MCP server.\"\"\"\n\n    tool_name: str\n    \"\"\"The name of the tool being invoked.\"\"\"\n\n    arguments: dict[str, Any] | None\n    \"\"\"The parsed tool arguments.\"\"\"\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPToolMetaContext.run_context","title":"run_context  <code>instance-attribute</code>","text":"<pre><code>run_context: RunContextWrapper[Any]\n</code></pre> <p>The current run context.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPToolMetaContext.server_name","title":"server_name  <code>instance-attribute</code>","text":"<pre><code>server_name: str\n</code></pre> <p>The name of the MCP server.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPToolMetaContext.tool_name","title":"tool_name  <code>instance-attribute</code>","text":"<pre><code>tool_name: str\n</code></pre> <p>The name of the tool being invoked.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPToolMetaContext.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: dict[str, Any] | None\n</code></pre> <p>The parsed tool arguments.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil","title":"MCPUtil","text":"<p>Set of utilities for interop between MCP and Agents SDK tools.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>class MCPUtil:\n    \"\"\"Set of utilities for interop between MCP and Agents SDK tools.\"\"\"\n\n    @classmethod\n    async def get_all_function_tools(\n        cls,\n        servers: list[MCPServer],\n        convert_schemas_to_strict: bool,\n        run_context: RunContextWrapper[Any],\n        agent: AgentBase,\n        failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n    ) -&gt; list[Tool]:\n        \"\"\"Get all function tools from a list of MCP servers.\"\"\"\n        tools = []\n        tool_names: set[str] = set()\n        for server in servers:\n            server_tools = await cls.get_function_tools(\n                server,\n                convert_schemas_to_strict,\n                run_context,\n                agent,\n                failure_error_function=failure_error_function,\n            )\n            server_tool_names = {tool.name for tool in server_tools}\n            if len(server_tool_names &amp; tool_names) &gt; 0:\n                raise UserError(\n                    f\"Duplicate tool names found across MCP servers: \"\n                    f\"{server_tool_names &amp; tool_names}\"\n                )\n            tool_names.update(server_tool_names)\n            tools.extend(server_tools)\n\n        return tools\n\n    @classmethod\n    async def get_function_tools(\n        cls,\n        server: MCPServer,\n        convert_schemas_to_strict: bool,\n        run_context: RunContextWrapper[Any],\n        agent: AgentBase,\n        failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n    ) -&gt; list[Tool]:\n        \"\"\"Get all function tools from a single MCP server.\"\"\"\n\n        with mcp_tools_span(server=server.name) as span:\n            tools = await server.list_tools(run_context, agent)\n            span.span_data.result = [tool.name for tool in tools]\n\n        return [\n            cls.to_function_tool(\n                tool,\n                server,\n                convert_schemas_to_strict,\n                agent,\n                failure_error_function=failure_error_function,\n            )\n            for tool in tools\n        ]\n\n    @classmethod\n    def to_function_tool(\n        cls,\n        tool: MCPTool,\n        server: MCPServer,\n        convert_schemas_to_strict: bool,\n        agent: AgentBase | None = None,\n        failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n    ) -&gt; FunctionTool:\n        \"\"\"Convert an MCP tool to an Agents SDK function tool.\n\n        The ``agent`` parameter is optional for backward compatibility with older\n        call sites that used ``MCPUtil.to_function_tool(tool, server, strict)``.\n        When omitted, this helper preserves the historical behavior and leaves\n        ``needs_approval`` disabled.\n        \"\"\"\n        invoke_func_impl = functools.partial(cls.invoke_mcp_tool, server, tool)\n        effective_failure_error_function = server._get_failure_error_function(\n            failure_error_function\n        )\n        schema, is_strict = tool.inputSchema, False\n\n        # MCP spec doesn't require the inputSchema to have `properties`, but OpenAI spec does.\n        if \"properties\" not in schema:\n            schema[\"properties\"] = {}\n\n        if convert_schemas_to_strict:\n            try:\n                schema = ensure_strict_json_schema(schema)\n                is_strict = True\n            except Exception as e:\n                logger.info(f\"Error converting MCP schema to strict mode: {e}\")\n\n        # Wrap the invoke function with error handling, similar to regular function tools.\n        # This ensures that MCP tool errors (like timeouts) are handled gracefully instead\n        # of halting the entire agent flow.\n        async def invoke_func(ctx: ToolContext[Any], input_json: str) -&gt; ToolOutput:\n            try:\n                return await invoke_func_impl(ctx, input_json)\n            except Exception as e:\n                if effective_failure_error_function is None:\n                    raise\n\n                # Use configured error handling function to convert exception to error message.\n                result = effective_failure_error_function(ctx, e)\n                if inspect.isawaitable(result):\n                    result = await result\n\n                # Attach error to tracing span.\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Error running tool (non-fatal)\",\n                        data={\n                            \"tool_name\": tool.name,\n                            \"error\": str(e),\n                        },\n                    )\n                )\n\n                # Log the error.\n                if _debug.DONT_LOG_TOOL_DATA:\n                    logger.debug(f\"MCP tool {tool.name} failed\")\n                else:\n                    logger.error(\n                        f\"MCP tool {tool.name} failed: {input_json} {e}\",\n                        exc_info=e,\n                    )\n\n                return result\n\n        needs_approval: (\n            bool | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]]\n        ) = server._get_needs_approval_for_tool(tool, agent)\n\n        return FunctionTool(\n            name=tool.name,\n            description=tool.description or \"\",\n            params_json_schema=schema,\n            on_invoke_tool=invoke_func,\n            strict_json_schema=is_strict,\n            needs_approval=needs_approval,\n        )\n\n    @staticmethod\n    def _merge_mcp_meta(\n        resolved_meta: dict[str, Any] | None,\n        explicit_meta: dict[str, Any] | None,\n    ) -&gt; dict[str, Any] | None:\n        if resolved_meta is None and explicit_meta is None:\n            return None\n        merged: dict[str, Any] = {}\n        if resolved_meta is not None:\n            merged.update(resolved_meta)\n        if explicit_meta is not None:\n            merged.update(explicit_meta)\n        return merged\n\n    @classmethod\n    async def _resolve_meta(\n        cls,\n        server: MCPServer,\n        context: RunContextWrapper[Any],\n        tool_name: str,\n        arguments: dict[str, Any] | None,\n    ) -&gt; dict[str, Any] | None:\n        meta_resolver = getattr(server, \"tool_meta_resolver\", None)\n        if meta_resolver is None:\n            return None\n\n        arguments_copy = copy.deepcopy(arguments) if arguments is not None else None\n        resolver_context = MCPToolMetaContext(\n            run_context=context,\n            server_name=server.name,\n            tool_name=tool_name,\n            arguments=arguments_copy,\n        )\n        result = meta_resolver(resolver_context)\n        if inspect.isawaitable(result):\n            result = await result\n        if result is None:\n            return None\n        if not isinstance(result, dict):\n            raise TypeError(\"MCP meta resolver must return a dict or None.\")\n        return result\n\n    @classmethod\n    async def invoke_mcp_tool(\n        cls,\n        server: MCPServer,\n        tool: MCPTool,\n        context: RunContextWrapper[Any],\n        input_json: str,\n        *,\n        meta: dict[str, Any] | None = None,\n    ) -&gt; ToolOutput:\n        \"\"\"Invoke an MCP tool and return the result as ToolOutput.\"\"\"\n        try:\n            json_data: dict[str, Any] = json.loads(input_json) if input_json else {}\n        except Exception as e:\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Invalid JSON input for tool {tool.name}\")\n            else:\n                logger.debug(f\"Invalid JSON input for tool {tool.name}: {input_json}\")\n            raise ModelBehaviorError(\n                f\"Invalid JSON input for tool {tool.name}: {input_json}\"\n            ) from e\n\n        if _debug.DONT_LOG_TOOL_DATA:\n            logger.debug(f\"Invoking MCP tool {tool.name}\")\n        else:\n            logger.debug(f\"Invoking MCP tool {tool.name} with input {input_json}\")\n\n        try:\n            resolved_meta = await cls._resolve_meta(server, context, tool.name, json_data)\n            merged_meta = cls._merge_mcp_meta(resolved_meta, meta)\n            if merged_meta is None:\n                result = await server.call_tool(tool.name, json_data)\n            else:\n                result = await server.call_tool(tool.name, json_data, meta=merged_meta)\n        except UserError:\n            # Re-raise UserError as-is (it already has a good message)\n            raise\n        except Exception as e:\n            logger.error(f\"Error invoking MCP tool {tool.name} on server '{server.name}': {e}\")\n            raise AgentsException(\n                f\"Error invoking MCP tool {tool.name} on server '{server.name}': {e}\"\n            ) from e\n\n        if _debug.DONT_LOG_TOOL_DATA:\n            logger.debug(f\"MCP tool {tool.name} completed.\")\n        else:\n            logger.debug(f\"MCP tool {tool.name} returned {result}\")\n\n        # If structured content is requested and available, use it exclusively\n        tool_output: ToolOutput\n        if server.use_structured_content and result.structuredContent:\n            tool_output = json.dumps(result.structuredContent)\n        else:\n            tool_output_list: list[ToolOutputItem] = []\n            for item in result.content:\n                if item.type == \"text\":\n                    tool_output_list.append(ToolOutputTextDict(type=\"text\", text=item.text))\n                elif item.type == \"image\":\n                    tool_output_list.append(\n                        ToolOutputImageDict(\n                            type=\"image\", image_url=f\"data:{item.mimeType};base64,{item.data}\"\n                        )\n                    )\n                else:\n                    # Fall back to regular text content\n                    tool_output_list.append(\n                        ToolOutputTextDict(type=\"text\", text=str(item.model_dump(mode=\"json\")))\n                    )\n            if len(tool_output_list) == 1:\n                tool_output = tool_output_list[0]\n            else:\n                tool_output = tool_output_list\n\n        current_span = get_current_span()\n        if current_span:\n            if isinstance(current_span.span_data, FunctionSpanData):\n                current_span.span_data.output = tool_output\n                current_span.span_data.mcp_data = {\n                    \"server\": server.name,\n                }\n            else:\n                logger.warning(\n                    f\"Current span is not a FunctionSpanData, skipping tool output: {current_span}\"\n                )\n\n        return tool_output\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.get_all_function_tools","title":"get_all_function_tools  <code>async</code> <code>classmethod</code>","text":"<pre><code>get_all_function_tools(\n    servers: list[MCPServer],\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: AgentBase,\n    failure_error_function: ToolErrorFunction\n    | None = default_tool_error_function,\n) -&gt; list[Tool]\n</code></pre> <p>Get all function tools from a list of MCP servers.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\nasync def get_all_function_tools(\n    cls,\n    servers: list[MCPServer],\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: AgentBase,\n    failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n) -&gt; list[Tool]:\n    \"\"\"Get all function tools from a list of MCP servers.\"\"\"\n    tools = []\n    tool_names: set[str] = set()\n    for server in servers:\n        server_tools = await cls.get_function_tools(\n            server,\n            convert_schemas_to_strict,\n            run_context,\n            agent,\n            failure_error_function=failure_error_function,\n        )\n        server_tool_names = {tool.name for tool in server_tools}\n        if len(server_tool_names &amp; tool_names) &gt; 0:\n            raise UserError(\n                f\"Duplicate tool names found across MCP servers: \"\n                f\"{server_tool_names &amp; tool_names}\"\n            )\n        tool_names.update(server_tool_names)\n        tools.extend(server_tools)\n\n    return tools\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.get_function_tools","title":"get_function_tools  <code>async</code> <code>classmethod</code>","text":"<pre><code>get_function_tools(\n    server: MCPServer,\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: AgentBase,\n    failure_error_function: ToolErrorFunction\n    | None = default_tool_error_function,\n) -&gt; list[Tool]\n</code></pre> <p>Get all function tools from a single MCP server.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\nasync def get_function_tools(\n    cls,\n    server: MCPServer,\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: AgentBase,\n    failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n) -&gt; list[Tool]:\n    \"\"\"Get all function tools from a single MCP server.\"\"\"\n\n    with mcp_tools_span(server=server.name) as span:\n        tools = await server.list_tools(run_context, agent)\n        span.span_data.result = [tool.name for tool in tools]\n\n    return [\n        cls.to_function_tool(\n            tool,\n            server,\n            convert_schemas_to_strict,\n            agent,\n            failure_error_function=failure_error_function,\n        )\n        for tool in tools\n    ]\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.to_function_tool","title":"to_function_tool  <code>classmethod</code>","text":"<pre><code>to_function_tool(\n    tool: Tool,\n    server: MCPServer,\n    convert_schemas_to_strict: bool,\n    agent: AgentBase | None = None,\n    failure_error_function: ToolErrorFunction\n    | None = default_tool_error_function,\n) -&gt; FunctionTool\n</code></pre> <p>Convert an MCP tool to an Agents SDK function tool.</p> <p>The <code>agent</code> parameter is optional for backward compatibility with older call sites that used <code>MCPUtil.to_function_tool(tool, server, strict)</code>. When omitted, this helper preserves the historical behavior and leaves <code>needs_approval</code> disabled.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\ndef to_function_tool(\n    cls,\n    tool: MCPTool,\n    server: MCPServer,\n    convert_schemas_to_strict: bool,\n    agent: AgentBase | None = None,\n    failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n) -&gt; FunctionTool:\n    \"\"\"Convert an MCP tool to an Agents SDK function tool.\n\n    The ``agent`` parameter is optional for backward compatibility with older\n    call sites that used ``MCPUtil.to_function_tool(tool, server, strict)``.\n    When omitted, this helper preserves the historical behavior and leaves\n    ``needs_approval`` disabled.\n    \"\"\"\n    invoke_func_impl = functools.partial(cls.invoke_mcp_tool, server, tool)\n    effective_failure_error_function = server._get_failure_error_function(\n        failure_error_function\n    )\n    schema, is_strict = tool.inputSchema, False\n\n    # MCP spec doesn't require the inputSchema to have `properties`, but OpenAI spec does.\n    if \"properties\" not in schema:\n        schema[\"properties\"] = {}\n\n    if convert_schemas_to_strict:\n        try:\n            schema = ensure_strict_json_schema(schema)\n            is_strict = True\n        except Exception as e:\n            logger.info(f\"Error converting MCP schema to strict mode: {e}\")\n\n    # Wrap the invoke function with error handling, similar to regular function tools.\n    # This ensures that MCP tool errors (like timeouts) are handled gracefully instead\n    # of halting the entire agent flow.\n    async def invoke_func(ctx: ToolContext[Any], input_json: str) -&gt; ToolOutput:\n        try:\n            return await invoke_func_impl(ctx, input_json)\n        except Exception as e:\n            if effective_failure_error_function is None:\n                raise\n\n            # Use configured error handling function to convert exception to error message.\n            result = effective_failure_error_function(ctx, e)\n            if inspect.isawaitable(result):\n                result = await result\n\n            # Attach error to tracing span.\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Error running tool (non-fatal)\",\n                    data={\n                        \"tool_name\": tool.name,\n                        \"error\": str(e),\n                    },\n                )\n            )\n\n            # Log the error.\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"MCP tool {tool.name} failed\")\n            else:\n                logger.error(\n                    f\"MCP tool {tool.name} failed: {input_json} {e}\",\n                    exc_info=e,\n                )\n\n            return result\n\n    needs_approval: (\n        bool | Callable[[RunContextWrapper[Any], dict[str, Any], str], Awaitable[bool]]\n    ) = server._get_needs_approval_for_tool(tool, agent)\n\n    return FunctionTool(\n        name=tool.name,\n        description=tool.description or \"\",\n        params_json_schema=schema,\n        on_invoke_tool=invoke_func,\n        strict_json_schema=is_strict,\n        needs_approval=needs_approval,\n    )\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.invoke_mcp_tool","title":"invoke_mcp_tool  <code>async</code> <code>classmethod</code>","text":"<pre><code>invoke_mcp_tool(\n    server: MCPServer,\n    tool: Tool,\n    context: RunContextWrapper[Any],\n    input_json: str,\n    *,\n    meta: dict[str, Any] | None = None,\n) -&gt; ToolOutput\n</code></pre> <p>Invoke an MCP tool and return the result as ToolOutput.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\nasync def invoke_mcp_tool(\n    cls,\n    server: MCPServer,\n    tool: MCPTool,\n    context: RunContextWrapper[Any],\n    input_json: str,\n    *,\n    meta: dict[str, Any] | None = None,\n) -&gt; ToolOutput:\n    \"\"\"Invoke an MCP tool and return the result as ToolOutput.\"\"\"\n    try:\n        json_data: dict[str, Any] = json.loads(input_json) if input_json else {}\n    except Exception as e:\n        if _debug.DONT_LOG_TOOL_DATA:\n            logger.debug(f\"Invalid JSON input for tool {tool.name}\")\n        else:\n            logger.debug(f\"Invalid JSON input for tool {tool.name}: {input_json}\")\n        raise ModelBehaviorError(\n            f\"Invalid JSON input for tool {tool.name}: {input_json}\"\n        ) from e\n\n    if _debug.DONT_LOG_TOOL_DATA:\n        logger.debug(f\"Invoking MCP tool {tool.name}\")\n    else:\n        logger.debug(f\"Invoking MCP tool {tool.name} with input {input_json}\")\n\n    try:\n        resolved_meta = await cls._resolve_meta(server, context, tool.name, json_data)\n        merged_meta = cls._merge_mcp_meta(resolved_meta, meta)\n        if merged_meta is None:\n            result = await server.call_tool(tool.name, json_data)\n        else:\n            result = await server.call_tool(tool.name, json_data, meta=merged_meta)\n    except UserError:\n        # Re-raise UserError as-is (it already has a good message)\n        raise\n    except Exception as e:\n        logger.error(f\"Error invoking MCP tool {tool.name} on server '{server.name}': {e}\")\n        raise AgentsException(\n            f\"Error invoking MCP tool {tool.name} on server '{server.name}': {e}\"\n        ) from e\n\n    if _debug.DONT_LOG_TOOL_DATA:\n        logger.debug(f\"MCP tool {tool.name} completed.\")\n    else:\n        logger.debug(f\"MCP tool {tool.name} returned {result}\")\n\n    # If structured content is requested and available, use it exclusively\n    tool_output: ToolOutput\n    if server.use_structured_content and result.structuredContent:\n        tool_output = json.dumps(result.structuredContent)\n    else:\n        tool_output_list: list[ToolOutputItem] = []\n        for item in result.content:\n            if item.type == \"text\":\n                tool_output_list.append(ToolOutputTextDict(type=\"text\", text=item.text))\n            elif item.type == \"image\":\n                tool_output_list.append(\n                    ToolOutputImageDict(\n                        type=\"image\", image_url=f\"data:{item.mimeType};base64,{item.data}\"\n                    )\n                )\n            else:\n                # Fall back to regular text content\n                tool_output_list.append(\n                    ToolOutputTextDict(type=\"text\", text=str(item.model_dump(mode=\"json\")))\n                )\n        if len(tool_output_list) == 1:\n            tool_output = tool_output_list[0]\n        else:\n            tool_output = tool_output_list\n\n    current_span = get_current_span()\n    if current_span:\n        if isinstance(current_span.span_data, FunctionSpanData):\n            current_span.span_data.output = tool_output\n            current_span.span_data.mcp_data = {\n                \"server\": server.name,\n            }\n        else:\n            logger.warning(\n                f\"Current span is not a FunctionSpanData, skipping tool output: {current_span}\"\n            )\n\n    return tool_output\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.create_static_tool_filter","title":"create_static_tool_filter","text":"<pre><code>create_static_tool_filter(\n    allowed_tool_names: list[str] | None = None,\n    blocked_tool_names: list[str] | None = None,\n) -&gt; ToolFilterStatic | None\n</code></pre> <p>Create a static tool filter from allowlist and blocklist parameters.</p> <p>This is a convenience function for creating a ToolFilterStatic.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_tool_names</code> <code>list[str] | None</code> <p>Optional list of tool names to allow (whitelist).</p> <code>None</code> <code>blocked_tool_names</code> <code>list[str] | None</code> <p>Optional list of tool names to exclude (blacklist).</p> <code>None</code> <p>Returns:</p> Type Description <code>ToolFilterStatic | None</code> <p>A ToolFilterStatic if any filtering is specified, None otherwise.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>def create_static_tool_filter(\n    allowed_tool_names: list[str] | None = None,\n    blocked_tool_names: list[str] | None = None,\n) -&gt; ToolFilterStatic | None:\n    \"\"\"Create a static tool filter from allowlist and blocklist parameters.\n\n    This is a convenience function for creating a ToolFilterStatic.\n\n    Args:\n        allowed_tool_names: Optional list of tool names to allow (whitelist).\n        blocked_tool_names: Optional list of tool names to exclude (blacklist).\n\n    Returns:\n        A ToolFilterStatic if any filtering is specified, None otherwise.\n    \"\"\"\n    if allowed_tool_names is None and blocked_tool_names is None:\n        return None\n\n    filter_dict: ToolFilterStatic = {}\n    if allowed_tool_names is not None:\n        filter_dict[\"allowed_tool_names\"] = allowed_tool_names\n    if blocked_tool_names is not None:\n        filter_dict[\"blocked_tool_names\"] = blocked_tool_names\n\n    return filter_dict\n</code></pre>"},{"location":"ref/memory/openai_conversations_session/","title":"<code>Openai Conversations Session</code>","text":""},{"location":"ref/memory/openai_conversations_session/#agents.memory.openai_conversations_session.OpenAIConversationsSession","title":"OpenAIConversationsSession","text":"<p>               Bases: <code>SessionABC</code></p> Source code in <code>src/agents/memory/openai_conversations_session.py</code> <pre><code>class OpenAIConversationsSession(SessionABC):\n    def __init__(\n        self,\n        *,\n        conversation_id: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        session_settings: SessionSettings | None = None,\n    ):\n        self._session_id: str | None = conversation_id\n        self.session_settings = session_settings or SessionSettings()\n        _openai_client = openai_client\n        if _openai_client is None:\n            _openai_client = get_default_openai_client() or AsyncOpenAI()\n        # this never be None here\n        self._openai_client: AsyncOpenAI = _openai_client\n\n    @property\n    def session_id(self) -&gt; str:\n        \"\"\"Get the session ID (conversation ID).\n\n        Returns:\n            The conversation ID for this session.\n\n        Raises:\n            ValueError: If the session has not been initialized yet.\n                Call any session method (get_items, add_items, etc.) first\n                to trigger lazy initialization.\n        \"\"\"\n        if self._session_id is None:\n            raise ValueError(\n                \"Session ID not yet available. The session is lazily initialized \"\n                \"on first API call. Call get_items(), add_items(), or similar first.\"\n            )\n        return self._session_id\n\n    @session_id.setter\n    def session_id(self, value: str) -&gt; None:\n        \"\"\"Set the session ID (conversation ID).\"\"\"\n        self._session_id = value\n\n    async def _get_session_id(self) -&gt; str:\n        if self._session_id is None:\n            self._session_id = await start_openai_conversations_session(self._openai_client)\n        return self._session_id\n\n    async def _clear_session_id(self) -&gt; None:\n        self._session_id = None\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        session_id = await self._get_session_id()\n\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        all_items = []\n        if session_limit is None:\n            async for item in self._openai_client.conversations.items.list(\n                conversation_id=session_id,\n                order=\"asc\",\n            ):\n                # calling model_dump() to make this serializable\n                all_items.append(item.model_dump(exclude_unset=True))\n        else:\n            async for item in self._openai_client.conversations.items.list(\n                conversation_id=session_id,\n                limit=session_limit,\n                order=\"desc\",\n            ):\n                # calling model_dump() to make this serializable\n                all_items.append(item.model_dump(exclude_unset=True))\n                if session_limit is not None and len(all_items) &gt;= session_limit:\n                    break\n            all_items.reverse()\n\n        return all_items  # type: ignore\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        session_id = await self._get_session_id()\n        if not items:\n            return\n\n        await self._openai_client.conversations.items.create(\n            conversation_id=session_id,\n            items=items,\n        )\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        session_id = await self._get_session_id()\n        items = await self.get_items(limit=1)\n        if not items:\n            return None\n        item_id: str = str(items[0][\"id\"])  # type: ignore [typeddict-item]\n        await self._openai_client.conversations.items.delete(\n            conversation_id=session_id, item_id=item_id\n        )\n        return items[0]\n\n    async def clear_session(self) -&gt; None:\n        session_id = await self._get_session_id()\n        await self._openai_client.conversations.delete(\n            conversation_id=session_id,\n        )\n        await self._clear_session_id()\n</code></pre>"},{"location":"ref/memory/openai_conversations_session/#agents.memory.openai_conversations_session.OpenAIConversationsSession.session_id","title":"session_id  <code>property</code> <code>writable</code>","text":"<pre><code>session_id: str\n</code></pre> <p>Get the session ID (conversation ID).</p> <p>Returns:</p> Type Description <code>str</code> <p>The conversation ID for this session.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the session has not been initialized yet. Call any session method (get_items, add_items, etc.) first to trigger lazy initialization.</p>"},{"location":"ref/memory/openai_responses_compaction_session/","title":"<code>Openai Responses Compaction Session</code>","text":""},{"location":"ref/memory/openai_responses_compaction_session/#agents.memory.openai_responses_compaction_session.OpenAIResponsesCompactionSession","title":"OpenAIResponsesCompactionSession","text":"<p>               Bases: <code>SessionABC</code>, <code>OpenAIResponsesCompactionAwareSession</code></p> <p>Session decorator that triggers responses.compact when stored history grows.</p> <p>Works with OpenAI Responses API models only. Wraps any Session (except OpenAIConversationsSession) and automatically calls the OpenAI responses.compact API after each turn when the decision hook returns True.</p> Source code in <code>src/agents/memory/openai_responses_compaction_session.py</code> <pre><code>class OpenAIResponsesCompactionSession(SessionABC, OpenAIResponsesCompactionAwareSession):\n    \"\"\"Session decorator that triggers responses.compact when stored history grows.\n\n    Works with OpenAI Responses API models only. Wraps any Session (except\n    OpenAIConversationsSession) and automatically calls the OpenAI responses.compact\n    API after each turn when the decision hook returns True.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        underlying_session: Session,\n        *,\n        client: AsyncOpenAI | None = None,\n        model: str = \"gpt-4.1\",\n        compaction_mode: OpenAIResponsesCompactionMode = \"auto\",\n        should_trigger_compaction: Callable[[dict[str, Any]], bool] | None = None,\n    ):\n        \"\"\"Initialize the compaction session.\n\n        Args:\n            session_id: Identifier for this session.\n            underlying_session: Session store that holds the compacted history. Cannot be\n                OpenAIConversationsSession.\n            client: OpenAI client for responses.compact API calls. Defaults to\n                get_default_openai_client() or new AsyncOpenAI().\n            model: Model to use for responses.compact. Defaults to \"gpt-4.1\". Must be an\n                OpenAI model name (gpt-*, o*, or ft:gpt-*).\n            compaction_mode: Controls how the compaction request provides conversation\n                history. \"auto\" (default) uses input when the last response was not\n                stored or no response_id is available.\n            should_trigger_compaction: Custom decision hook. Defaults to triggering when\n                10+ compaction candidates exist.\n        \"\"\"\n        if isinstance(underlying_session, OpenAIConversationsSession):\n            raise ValueError(\n                \"OpenAIResponsesCompactionSession cannot wrap OpenAIConversationsSession \"\n                \"because it manages its own history on the server.\"\n            )\n\n        if not is_openai_model_name(model):\n            raise ValueError(f\"Unsupported model for OpenAI responses compaction: {model}\")\n\n        self.session_id = session_id\n        self.underlying_session = underlying_session\n        self._client = client\n        self.model = model\n        self.compaction_mode = compaction_mode\n        self.should_trigger_compaction = (\n            should_trigger_compaction or default_should_trigger_compaction\n        )\n\n        # cache for incremental candidate tracking\n        self._compaction_candidate_items: list[TResponseInputItem] | None = None\n        self._session_items: list[TResponseInputItem] | None = None\n        self._response_id: str | None = None\n        self._deferred_response_id: str | None = None\n        self._last_unstored_response_id: str | None = None\n\n    @property\n    def client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = get_default_openai_client() or AsyncOpenAI()\n        return self._client\n\n    def _resolve_compaction_mode_for_response(\n        self,\n        *,\n        response_id: str | None,\n        store: bool | None,\n        requested_mode: OpenAIResponsesCompactionMode | None,\n    ) -&gt; _ResolvedCompactionMode:\n        mode = requested_mode or self.compaction_mode\n        if (\n            mode == \"auto\"\n            and store is None\n            and response_id is not None\n            and response_id == self._last_unstored_response_id\n        ):\n            return \"input\"\n        return _resolve_compaction_mode(mode, response_id=response_id, store=store)\n\n    async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None) -&gt; None:\n        \"\"\"Run compaction using responses.compact API.\"\"\"\n        if args and args.get(\"response_id\"):\n            self._response_id = args[\"response_id\"]\n        requested_mode = args.get(\"compaction_mode\") if args else None\n        if args and \"store\" in args:\n            store = args[\"store\"]\n            if store is False and self._response_id:\n                self._last_unstored_response_id = self._response_id\n            elif store is True and self._response_id == self._last_unstored_response_id:\n                self._last_unstored_response_id = None\n        else:\n            store = None\n        resolved_mode = self._resolve_compaction_mode_for_response(\n            response_id=self._response_id,\n            store=store,\n            requested_mode=requested_mode,\n        )\n\n        if resolved_mode == \"previous_response_id\" and not self._response_id:\n            raise ValueError(\n                \"OpenAIResponsesCompactionSession.run_compaction requires a response_id \"\n                \"when using previous_response_id compaction.\"\n            )\n\n        compaction_candidate_items, session_items = await self._ensure_compaction_candidates()\n\n        force = args.get(\"force\", False) if args else False\n        should_compact = force or self.should_trigger_compaction(\n            {\n                \"response_id\": self._response_id,\n                \"compaction_mode\": resolved_mode,\n                \"compaction_candidate_items\": compaction_candidate_items,\n                \"session_items\": session_items,\n            }\n        )\n\n        if not should_compact:\n            logger.debug(\n                f\"skip: decision hook declined compaction for {self._response_id} \"\n                f\"(mode={resolved_mode})\"\n            )\n            return\n\n        self._deferred_response_id = None\n        logger.debug(\n            f\"compact: start for {self._response_id} using {self.model} (mode={resolved_mode})\"\n        )\n\n        compact_kwargs: dict[str, Any] = {\"model\": self.model}\n        if resolved_mode == \"previous_response_id\":\n            compact_kwargs[\"previous_response_id\"] = self._response_id\n        else:\n            compact_kwargs[\"input\"] = session_items\n\n        compacted = await self.client.responses.compact(**compact_kwargs)\n\n        await self.underlying_session.clear_session()\n        output_items: list[TResponseInputItem] = []\n        if compacted.output:\n            for item in compacted.output:\n                if isinstance(item, dict):\n                    output_items.append(item)\n                else:\n                    # Suppress Pydantic literal warnings: responses.compact can return\n                    # user-style input_text content inside ResponseOutputMessage.\n                    output_items.append(\n                        item.model_dump(exclude_unset=True, warnings=False)  # type: ignore\n                    )\n\n        if output_items:\n            await self.underlying_session.add_items(output_items)\n\n        self._compaction_candidate_items = select_compaction_candidate_items(output_items)\n        self._session_items = output_items\n\n        logger.debug(\n            f\"compact: done for {self._response_id} \"\n            f\"(mode={resolved_mode}, output={len(output_items)}, \"\n            f\"candidates={len(self._compaction_candidate_items)})\"\n        )\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        return await self.underlying_session.get_items(limit)\n\n    async def _defer_compaction(self, response_id: str, store: bool | None = None) -&gt; None:\n        if self._deferred_response_id is not None:\n            return\n        compaction_candidate_items, session_items = await self._ensure_compaction_candidates()\n        resolved_mode = self._resolve_compaction_mode_for_response(\n            response_id=response_id,\n            store=store,\n            requested_mode=None,\n        )\n        should_compact = self.should_trigger_compaction(\n            {\n                \"response_id\": response_id,\n                \"compaction_mode\": resolved_mode,\n                \"compaction_candidate_items\": compaction_candidate_items,\n                \"session_items\": session_items,\n            }\n        )\n        if should_compact:\n            self._deferred_response_id = response_id\n\n    def _get_deferred_compaction_response_id(self) -&gt; str | None:\n        return self._deferred_response_id\n\n    def _clear_deferred_compaction(self) -&gt; None:\n        self._deferred_response_id = None\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        await self.underlying_session.add_items(items)\n        if self._compaction_candidate_items is not None:\n            new_candidates = select_compaction_candidate_items(items)\n            if new_candidates:\n                self._compaction_candidate_items.extend(new_candidates)\n        if self._session_items is not None:\n            self._session_items.extend(items)\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        popped = await self.underlying_session.pop_item()\n        if popped:\n            self._compaction_candidate_items = None\n            self._session_items = None\n        return popped\n\n    async def clear_session(self) -&gt; None:\n        await self.underlying_session.clear_session()\n        self._compaction_candidate_items = []\n        self._session_items = []\n        self._deferred_response_id = None\n\n    async def _ensure_compaction_candidates(\n        self,\n    ) -&gt; tuple[list[TResponseInputItem], list[TResponseInputItem]]:\n        \"\"\"Lazy-load and cache compaction candidates.\"\"\"\n        if self._compaction_candidate_items is not None and self._session_items is not None:\n            return (self._compaction_candidate_items[:], self._session_items[:])\n\n        history = await self.underlying_session.get_items()\n        candidates = select_compaction_candidate_items(history)\n        self._compaction_candidate_items = candidates\n        self._session_items = history\n\n        logger.debug(\n            f\"candidates: initialized (history={len(history)}, candidates={len(candidates)})\"\n        )\n        return (candidates[:], history[:])\n</code></pre>"},{"location":"ref/memory/openai_responses_compaction_session/#agents.memory.openai_responses_compaction_session.OpenAIResponsesCompactionSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    underlying_session: Session,\n    *,\n    client: AsyncOpenAI | None = None,\n    model: str = \"gpt-4.1\",\n    compaction_mode: OpenAIResponsesCompactionMode = \"auto\",\n    should_trigger_compaction: Callable[\n        [dict[str, Any]], bool\n    ]\n    | None = None,\n)\n</code></pre> <p>Initialize the compaction session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Identifier for this session.</p> required <code>underlying_session</code> <code>Session</code> <p>Session store that holds the compacted history. Cannot be OpenAIConversationsSession.</p> required <code>client</code> <code>AsyncOpenAI | None</code> <p>OpenAI client for responses.compact API calls. Defaults to get_default_openai_client() or new AsyncOpenAI().</p> <code>None</code> <code>model</code> <code>str</code> <p>Model to use for responses.compact. Defaults to \"gpt-4.1\". Must be an OpenAI model name (gpt-, o, or ft:gpt-*).</p> <code>'gpt-4.1'</code> <code>compaction_mode</code> <code>OpenAIResponsesCompactionMode</code> <p>Controls how the compaction request provides conversation history. \"auto\" (default) uses input when the last response was not stored or no response_id is available.</p> <code>'auto'</code> <code>should_trigger_compaction</code> <code>Callable[[dict[str, Any]], bool] | None</code> <p>Custom decision hook. Defaults to triggering when 10+ compaction candidates exist.</p> <code>None</code> Source code in <code>src/agents/memory/openai_responses_compaction_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    underlying_session: Session,\n    *,\n    client: AsyncOpenAI | None = None,\n    model: str = \"gpt-4.1\",\n    compaction_mode: OpenAIResponsesCompactionMode = \"auto\",\n    should_trigger_compaction: Callable[[dict[str, Any]], bool] | None = None,\n):\n    \"\"\"Initialize the compaction session.\n\n    Args:\n        session_id: Identifier for this session.\n        underlying_session: Session store that holds the compacted history. Cannot be\n            OpenAIConversationsSession.\n        client: OpenAI client for responses.compact API calls. Defaults to\n            get_default_openai_client() or new AsyncOpenAI().\n        model: Model to use for responses.compact. Defaults to \"gpt-4.1\". Must be an\n            OpenAI model name (gpt-*, o*, or ft:gpt-*).\n        compaction_mode: Controls how the compaction request provides conversation\n            history. \"auto\" (default) uses input when the last response was not\n            stored or no response_id is available.\n        should_trigger_compaction: Custom decision hook. Defaults to triggering when\n            10+ compaction candidates exist.\n    \"\"\"\n    if isinstance(underlying_session, OpenAIConversationsSession):\n        raise ValueError(\n            \"OpenAIResponsesCompactionSession cannot wrap OpenAIConversationsSession \"\n            \"because it manages its own history on the server.\"\n        )\n\n    if not is_openai_model_name(model):\n        raise ValueError(f\"Unsupported model for OpenAI responses compaction: {model}\")\n\n    self.session_id = session_id\n    self.underlying_session = underlying_session\n    self._client = client\n    self.model = model\n    self.compaction_mode = compaction_mode\n    self.should_trigger_compaction = (\n        should_trigger_compaction or default_should_trigger_compaction\n    )\n\n    # cache for incremental candidate tracking\n    self._compaction_candidate_items: list[TResponseInputItem] | None = None\n    self._session_items: list[TResponseInputItem] | None = None\n    self._response_id: str | None = None\n    self._deferred_response_id: str | None = None\n    self._last_unstored_response_id: str | None = None\n</code></pre>"},{"location":"ref/memory/openai_responses_compaction_session/#agents.memory.openai_responses_compaction_session.OpenAIResponsesCompactionSession.run_compaction","title":"run_compaction  <code>async</code>","text":"<pre><code>run_compaction(\n    args: OpenAIResponsesCompactionArgs | None = None,\n) -&gt; None\n</code></pre> <p>Run compaction using responses.compact API.</p> Source code in <code>src/agents/memory/openai_responses_compaction_session.py</code> <pre><code>async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None) -&gt; None:\n    \"\"\"Run compaction using responses.compact API.\"\"\"\n    if args and args.get(\"response_id\"):\n        self._response_id = args[\"response_id\"]\n    requested_mode = args.get(\"compaction_mode\") if args else None\n    if args and \"store\" in args:\n        store = args[\"store\"]\n        if store is False and self._response_id:\n            self._last_unstored_response_id = self._response_id\n        elif store is True and self._response_id == self._last_unstored_response_id:\n            self._last_unstored_response_id = None\n    else:\n        store = None\n    resolved_mode = self._resolve_compaction_mode_for_response(\n        response_id=self._response_id,\n        store=store,\n        requested_mode=requested_mode,\n    )\n\n    if resolved_mode == \"previous_response_id\" and not self._response_id:\n        raise ValueError(\n            \"OpenAIResponsesCompactionSession.run_compaction requires a response_id \"\n            \"when using previous_response_id compaction.\"\n        )\n\n    compaction_candidate_items, session_items = await self._ensure_compaction_candidates()\n\n    force = args.get(\"force\", False) if args else False\n    should_compact = force or self.should_trigger_compaction(\n        {\n            \"response_id\": self._response_id,\n            \"compaction_mode\": resolved_mode,\n            \"compaction_candidate_items\": compaction_candidate_items,\n            \"session_items\": session_items,\n        }\n    )\n\n    if not should_compact:\n        logger.debug(\n            f\"skip: decision hook declined compaction for {self._response_id} \"\n            f\"(mode={resolved_mode})\"\n        )\n        return\n\n    self._deferred_response_id = None\n    logger.debug(\n        f\"compact: start for {self._response_id} using {self.model} (mode={resolved_mode})\"\n    )\n\n    compact_kwargs: dict[str, Any] = {\"model\": self.model}\n    if resolved_mode == \"previous_response_id\":\n        compact_kwargs[\"previous_response_id\"] = self._response_id\n    else:\n        compact_kwargs[\"input\"] = session_items\n\n    compacted = await self.client.responses.compact(**compact_kwargs)\n\n    await self.underlying_session.clear_session()\n    output_items: list[TResponseInputItem] = []\n    if compacted.output:\n        for item in compacted.output:\n            if isinstance(item, dict):\n                output_items.append(item)\n            else:\n                # Suppress Pydantic literal warnings: responses.compact can return\n                # user-style input_text content inside ResponseOutputMessage.\n                output_items.append(\n                    item.model_dump(exclude_unset=True, warnings=False)  # type: ignore\n                )\n\n    if output_items:\n        await self.underlying_session.add_items(output_items)\n\n    self._compaction_candidate_items = select_compaction_candidate_items(output_items)\n    self._session_items = output_items\n\n    logger.debug(\n        f\"compact: done for {self._response_id} \"\n        f\"(mode={resolved_mode}, output={len(output_items)}, \"\n        f\"candidates={len(self._compaction_candidate_items)})\"\n    )\n</code></pre>"},{"location":"ref/memory/openai_responses_compaction_session/#agents.memory.openai_responses_compaction_session.select_compaction_candidate_items","title":"select_compaction_candidate_items","text":"<pre><code>select_compaction_candidate_items(\n    items: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Select compaction candidate items.</p> <p>Excludes user messages and compaction items.</p> Source code in <code>src/agents/memory/openai_responses_compaction_session.py</code> <pre><code>def select_compaction_candidate_items(\n    items: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Select compaction candidate items.\n\n    Excludes user messages and compaction items.\n    \"\"\"\n\n    def _is_user_message(item: TResponseInputItem) -&gt; bool:\n        if not isinstance(item, dict):\n            return False\n        if item.get(\"type\") == \"message\":\n            return item.get(\"role\") == \"user\"\n        return item.get(\"role\") == \"user\" and \"content\" in item\n\n    return [\n        item\n        for item in items\n        if not (\n            _is_user_message(item) or (isinstance(item, dict) and item.get(\"type\") == \"compaction\")\n        )\n    ]\n</code></pre>"},{"location":"ref/memory/openai_responses_compaction_session/#agents.memory.openai_responses_compaction_session.default_should_trigger_compaction","title":"default_should_trigger_compaction","text":"<pre><code>default_should_trigger_compaction(\n    context: dict[str, Any],\n) -&gt; bool\n</code></pre> <p>Default decision: compact when &gt;= 10 candidate items exist.</p> Source code in <code>src/agents/memory/openai_responses_compaction_session.py</code> <pre><code>def default_should_trigger_compaction(context: dict[str, Any]) -&gt; bool:\n    \"\"\"Default decision: compact when &gt;= 10 candidate items exist.\"\"\"\n    return len(context[\"compaction_candidate_items\"]) &gt;= DEFAULT_COMPACTION_THRESHOLD\n</code></pre>"},{"location":"ref/memory/openai_responses_compaction_session/#agents.memory.openai_responses_compaction_session.is_openai_model_name","title":"is_openai_model_name","text":"<pre><code>is_openai_model_name(model: str) -&gt; bool\n</code></pre> <p>Validate model name follows OpenAI conventions.</p> Source code in <code>src/agents/memory/openai_responses_compaction_session.py</code> <pre><code>def is_openai_model_name(model: str) -&gt; bool:\n    \"\"\"Validate model name follows OpenAI conventions.\"\"\"\n    trimmed = model.strip()\n    if not trimmed:\n        return False\n\n    # Handle fine-tuned models: ft:gpt-4.1:org:proj:suffix\n    without_ft_prefix = trimmed[3:] if trimmed.startswith(\"ft:\") else trimmed\n    root = without_ft_prefix.split(\":\", 1)[0]\n\n    # Allow gpt-* and o* models\n    if root.startswith(\"gpt-\"):\n        return True\n    if root.startswith(\"o\") and root[1:2].isdigit():\n        return True\n\n    return False\n</code></pre>"},{"location":"ref/memory/session/","title":"<code>Session</code>","text":""},{"location":"ref/memory/session/#agents.memory.session.Session","title":"Session","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for session implementations.</p> <p>Session stores conversation history for a specific session, allowing agents to maintain context without requiring explicit manual memory management.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>@runtime_checkable\nclass Session(Protocol):\n    \"\"\"Protocol for session implementations.\n\n    Session stores conversation history for a specific session, allowing\n    agents to maintain context without requiring explicit manual memory management.\n    \"\"\"\n\n    session_id: str\n    session_settings: SessionSettings | None = None\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, retrieves all items.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        ...\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        ...\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        ...\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.Session.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, retrieves all items.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, retrieves all items.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.Session.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.Session.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.Session.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.SessionABC","title":"SessionABC","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for session implementations.</p> <p>Session stores conversation history for a specific session, allowing agents to maintain context without requiring explicit manual memory management.</p> <p>This ABC is intended for internal use and as a base class for concrete implementations. Third-party libraries should implement the Session protocol instead.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>class SessionABC(ABC):\n    \"\"\"Abstract base class for session implementations.\n\n    Session stores conversation history for a specific session, allowing\n    agents to maintain context without requiring explicit manual memory management.\n\n    This ABC is intended for internal use and as a base class for concrete implementations.\n    Third-party libraries should implement the Session protocol instead.\n    \"\"\"\n\n    session_id: str\n    session_settings: SessionSettings | None = None\n\n    @abstractmethod\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, retrieves all items.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.SessionABC.get_items","title":"get_items  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, retrieves all items.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>@abstractmethod\nasync def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, retrieves all items.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.SessionABC.add_items","title":"add_items  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/session.py</code> <pre><code>@abstractmethod\nasync def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.SessionABC.pop_item","title":"pop_item  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>@abstractmethod\nasync def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.SessionABC.clear_session","title":"clear_session  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>@abstractmethod\nasync def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionArgs","title":"OpenAIResponsesCompactionArgs","text":"<p>               Bases: <code>TypedDict</code></p> <p>Arguments for the run_compaction method.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>class OpenAIResponsesCompactionArgs(TypedDict, total=False):\n    \"\"\"Arguments for the run_compaction method.\"\"\"\n\n    response_id: str\n    \"\"\"The ID of the last response to use for compaction.\"\"\"\n\n    compaction_mode: Literal[\"previous_response_id\", \"input\", \"auto\"]\n    \"\"\"How to provide history for compaction.\n\n    - \"auto\": Use input when the last response was not stored or no response ID is available.\n    - \"previous_response_id\": Use server-managed response history.\n    - \"input\": Send locally stored session items as input.\n    \"\"\"\n\n    store: bool\n    \"\"\"Whether the last model response was stored on the server.\n\n    When set to False, compaction should avoid \"previous_response_id\" unless explicitly requested.\n    \"\"\"\n\n    force: bool\n    \"\"\"Whether to force compaction even if the threshold is not met.\"\"\"\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionArgs.response_id","title":"response_id  <code>instance-attribute</code>","text":"<pre><code>response_id: str\n</code></pre> <p>The ID of the last response to use for compaction.</p>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionArgs.compaction_mode","title":"compaction_mode  <code>instance-attribute</code>","text":"<pre><code>compaction_mode: Literal[\n    \"previous_response_id\", \"input\", \"auto\"\n]\n</code></pre> <p>How to provide history for compaction.</p> <ul> <li>\"auto\": Use input when the last response was not stored or no response ID is available.</li> <li>\"previous_response_id\": Use server-managed response history.</li> <li>\"input\": Send locally stored session items as input.</li> </ul>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionArgs.store","title":"store  <code>instance-attribute</code>","text":"<pre><code>store: bool\n</code></pre> <p>Whether the last model response was stored on the server.</p> <p>When set to False, compaction should avoid \"previous_response_id\" unless explicitly requested.</p>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionArgs.force","title":"force  <code>instance-attribute</code>","text":"<pre><code>force: bool\n</code></pre> <p>Whether to force compaction even if the threshold is not met.</p>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionAwareSession","title":"OpenAIResponsesCompactionAwareSession","text":"<p>               Bases: <code>Session</code>, <code>Protocol</code></p> <p>Protocol for session implementations that support responses compaction.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>@runtime_checkable\nclass OpenAIResponsesCompactionAwareSession(Session, Protocol):\n    \"\"\"Protocol for session implementations that support responses compaction.\"\"\"\n\n    async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None) -&gt; None:\n        \"\"\"Run the compaction process for the session.\"\"\"\n        ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionAwareSession.run_compaction","title":"run_compaction  <code>async</code>","text":"<pre><code>run_compaction(\n    args: OpenAIResponsesCompactionArgs | None = None,\n) -&gt; None\n</code></pre> <p>Run the compaction process for the session.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None) -&gt; None:\n    \"\"\"Run the compaction process for the session.\"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionAwareSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, retrieves all items.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, retrieves all items.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionAwareSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionAwareSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.OpenAIResponsesCompactionAwareSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/session/#agents.memory.session.is_openai_responses_compaction_aware_session","title":"is_openai_responses_compaction_aware_session","text":"<pre><code>is_openai_responses_compaction_aware_session(\n    session: Session | None,\n) -&gt; TypeGuard[OpenAIResponsesCompactionAwareSession]\n</code></pre> <p>Check if a session supports responses compaction.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>def is_openai_responses_compaction_aware_session(\n    session: Session | None,\n) -&gt; TypeGuard[OpenAIResponsesCompactionAwareSession]:\n    \"\"\"Check if a session supports responses compaction.\"\"\"\n    if session is None:\n        return False\n    try:\n        run_compaction = getattr(session, \"run_compaction\", None)\n    except Exception:\n        return False\n    return callable(run_compaction)\n</code></pre>"},{"location":"ref/memory/session_settings/","title":"<code>Session Settings</code>","text":"<p>Session configuration settings.</p>"},{"location":"ref/memory/session_settings/#agents.memory.session_settings.SessionSettings","title":"SessionSettings","text":"<p>Settings for session operations.</p> <p>This class holds optional session configuration parameters that can be used when interacting with session methods.</p> Source code in <code>src/agents/memory/session_settings.py</code> <pre><code>@dataclass\nclass SessionSettings:\n    \"\"\"Settings for session operations.\n\n    This class holds optional session configuration parameters that can be used\n    when interacting with session methods.\n    \"\"\"\n\n    limit: int | None = None\n    \"\"\"Maximum number of items to retrieve. If None, retrieves all items.\"\"\"\n\n    def resolve(self, override: SessionSettings | None) -&gt; SessionSettings:\n        \"\"\"Produce a new SessionSettings by overlaying any non-None values from the\n        override on top of this instance.\"\"\"\n        if override is None:\n            return self\n\n        changes = {\n            field.name: getattr(override, field.name)\n            for field in fields(self)\n            if getattr(override, field.name) is not None\n        }\n\n        return replace(self, **changes)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert settings to a dictionary.\"\"\"\n        return dataclasses.asdict(self)\n</code></pre>"},{"location":"ref/memory/session_settings/#agents.memory.session_settings.SessionSettings.limit","title":"limit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>limit: int | None = None\n</code></pre> <p>Maximum number of items to retrieve. If None, retrieves all items.</p>"},{"location":"ref/memory/session_settings/#agents.memory.session_settings.SessionSettings.resolve","title":"resolve","text":"<pre><code>resolve(\n    override: SessionSettings | None,\n) -&gt; SessionSettings\n</code></pre> <p>Produce a new SessionSettings by overlaying any non-None values from the override on top of this instance.</p> Source code in <code>src/agents/memory/session_settings.py</code> <pre><code>def resolve(self, override: SessionSettings | None) -&gt; SessionSettings:\n    \"\"\"Produce a new SessionSettings by overlaying any non-None values from the\n    override on top of this instance.\"\"\"\n    if override is None:\n        return self\n\n    changes = {\n        field.name: getattr(override, field.name)\n        for field in fields(self)\n        if getattr(override, field.name) is not None\n    }\n\n    return replace(self, **changes)\n</code></pre>"},{"location":"ref/memory/session_settings/#agents.memory.session_settings.SessionSettings.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert settings to a dictionary.</p> Source code in <code>src/agents/memory/session_settings.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert settings to a dictionary.\"\"\"\n    return dataclasses.asdict(self)\n</code></pre>"},{"location":"ref/memory/session_settings/#agents.memory.session_settings.resolve_session_limit","title":"resolve_session_limit","text":"<pre><code>resolve_session_limit(\n    explicit_limit: int | None,\n    settings: SessionSettings | None,\n) -&gt; int | None\n</code></pre> <p>Safely resolve the effective limit for session operations.</p> Source code in <code>src/agents/memory/session_settings.py</code> <pre><code>def resolve_session_limit(\n    explicit_limit: int | None,\n    settings: SessionSettings | None,\n) -&gt; int | None:\n    \"\"\"Safely resolve the effective limit for session operations.\"\"\"\n    if explicit_limit is not None:\n        return explicit_limit\n    if settings is not None:\n        return settings.limit\n    return None\n</code></pre>"},{"location":"ref/memory/sqlite_session/","title":"<code>Sqlite Session</code>","text":""},{"location":"ref/memory/sqlite_session/#agents.memory.sqlite_session.SQLiteSession","title":"SQLiteSession","text":"<p>               Bases: <code>SessionABC</code></p> <p>SQLite-based implementation of session storage.</p> <p>This implementation stores conversation history in a SQLite database. By default, uses an in-memory database that is lost when the process ends. For persistent storage, provide a file path.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>class SQLiteSession(SessionABC):\n    \"\"\"SQLite-based implementation of session storage.\n\n    This implementation stores conversation history in a SQLite database.\n    By default, uses an in-memory database that is lost when the process ends.\n    For persistent storage, provide a file path.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        db_path: str | Path = \":memory:\",\n        sessions_table: str = \"agent_sessions\",\n        messages_table: str = \"agent_messages\",\n        session_settings: SessionSettings | None = None,\n    ):\n        \"\"\"Initialize the SQLite session.\n\n        Args:\n            session_id: Unique identifier for the conversation session\n            db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n            sessions_table: Name of the table to store session metadata. Defaults to\n                'agent_sessions'\n            messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n            session_settings: Session configuration settings including default limit for\n                retrieving items. If None, uses default SessionSettings().\n        \"\"\"\n        self.session_id = session_id\n        self.session_settings = session_settings or SessionSettings()\n        self.db_path = db_path\n        self.sessions_table = sessions_table\n        self.messages_table = messages_table\n        self._local = threading.local()\n        self._lock = threading.Lock()\n\n        # For in-memory databases, we need a shared connection to avoid thread isolation\n        # For file databases, we use thread-local connections for better concurrency\n        self._is_memory_db = str(db_path) == \":memory:\"\n        if self._is_memory_db:\n            self._shared_connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n            self._shared_connection.execute(\"PRAGMA journal_mode=WAL\")\n            self._init_db_for_connection(self._shared_connection)\n        else:\n            # For file databases, initialize the schema once since it persists\n            init_conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n            init_conn.execute(\"PRAGMA journal_mode=WAL\")\n            self._init_db_for_connection(init_conn)\n            init_conn.close()\n\n    def _get_connection(self) -&gt; sqlite3.Connection:\n        \"\"\"Get a database connection.\"\"\"\n        if self._is_memory_db:\n            # Use shared connection for in-memory database to avoid thread isolation\n            return self._shared_connection\n        else:\n            # Use thread-local connections for file databases\n            if not hasattr(self._local, \"connection\"):\n                self._local.connection = sqlite3.connect(\n                    str(self.db_path),\n                    check_same_thread=False,\n                )\n                self._local.connection.execute(\"PRAGMA journal_mode=WAL\")\n            assert isinstance(self._local.connection, sqlite3.Connection), (\n                f\"Expected sqlite3.Connection, got {type(self._local.connection)}\"\n            )\n            return self._local.connection\n\n    def _init_db_for_connection(self, conn: sqlite3.Connection) -&gt; None:\n        \"\"\"Initialize the database schema for a specific connection.\"\"\"\n        conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.sessions_table} (\n                session_id TEXT PRIMARY KEY,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\"\n        )\n\n        conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.messages_table} (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL,\n                message_data TEXT NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (session_id) REFERENCES {self.sessions_table} (session_id)\n                    ON DELETE CASCADE\n            )\n        \"\"\"\n        )\n\n        conn.execute(\n            f\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_{self.messages_table}_session_id\n            ON {self.messages_table} (session_id, id)\n        \"\"\"\n        )\n\n        conn.commit()\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        session_limit = resolve_session_limit(limit, self.session_settings)\n\n        def _get_items_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                if session_limit is None:\n                    # Fetch all items in chronological order\n                    cursor = conn.execute(\n                        f\"\"\"\n                        SELECT message_data FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY id ASC\n                    \"\"\",\n                        (self.session_id,),\n                    )\n                else:\n                    # Fetch the latest N items in chronological order\n                    cursor = conn.execute(\n                        f\"\"\"\n                        SELECT message_data FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY id DESC\n                        LIMIT ?\n                        \"\"\",\n                        (self.session_id, session_limit),\n                    )\n\n                rows = cursor.fetchall()\n\n                # Reverse to get chronological order when using DESC\n                if session_limit is not None:\n                    rows = list(reversed(rows))\n\n                items = []\n                for (message_data,) in rows:\n                    try:\n                        item = json.loads(message_data)\n                        items.append(item)\n                    except json.JSONDecodeError:\n                        # Skip invalid JSON entries\n                        continue\n\n                return items\n\n        return await asyncio.to_thread(_get_items_sync)\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        if not items:\n            return\n\n        def _add_items_sync():\n            conn = self._get_connection()\n\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Ensure session exists\n                conn.execute(\n                    f\"\"\"\n                    INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n                \"\"\",\n                    (self.session_id,),\n                )\n\n                # Add items\n                message_data = [(self.session_id, json.dumps(item)) for item in items]\n                conn.executemany(\n                    f\"\"\"\n                    INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n                \"\"\",\n                    message_data,\n                )\n\n                # Update session timestamp\n                conn.execute(\n                    f\"\"\"\n                    UPDATE {self.sessions_table}\n                    SET updated_at = CURRENT_TIMESTAMP\n                    WHERE session_id = ?\n                \"\"\",\n                    (self.session_id,),\n                )\n\n                conn.commit()\n\n        await asyncio.to_thread(_add_items_sync)\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n\n        def _pop_item_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Use DELETE with RETURNING to atomically delete and return the most recent item\n                cursor = conn.execute(\n                    f\"\"\"\n                    DELETE FROM {self.messages_table}\n                    WHERE id = (\n                        SELECT id FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY id DESC\n                        LIMIT 1\n                    )\n                    RETURNING message_data\n                    \"\"\",\n                    (self.session_id,),\n                )\n\n                result = cursor.fetchone()\n                conn.commit()\n\n                if result:\n                    message_data = result[0]\n                    try:\n                        item = json.loads(message_data)\n                        return item\n                    except json.JSONDecodeError:\n                        # Return None for corrupted JSON entries (already deleted)\n                        return None\n\n                return None\n\n        return await asyncio.to_thread(_pop_item_sync)\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n\n        def _clear_session_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                conn.execute(\n                    f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                    (self.session_id,),\n                )\n                conn.execute(\n                    f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                    (self.session_id,),\n                )\n                conn.commit()\n\n        await asyncio.to_thread(_clear_session_sync)\n\n    def close(self) -&gt; None:\n        \"\"\"Close the database connection.\"\"\"\n        if self._is_memory_db:\n            if hasattr(self, \"_shared_connection\"):\n                self._shared_connection.close()\n        else:\n            if hasattr(self._local, \"connection\"):\n                self._local.connection.close()\n</code></pre>"},{"location":"ref/memory/sqlite_session/#agents.memory.sqlite_session.SQLiteSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n    session_settings: SessionSettings | None = None,\n)\n</code></pre> <p>Initialize the SQLite session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Unique identifier for the conversation session</p> required <code>db_path</code> <code>str | Path</code> <p>Path to the SQLite database file. Defaults to ':memory:' (in-memory database)</p> <code>':memory:'</code> <code>sessions_table</code> <code>str</code> <p>Name of the table to store session metadata. Defaults to 'agent_sessions'</p> <code>'agent_sessions'</code> <code>messages_table</code> <code>str</code> <p>Name of the table to store message data. Defaults to 'agent_messages'</p> <code>'agent_messages'</code> <code>session_settings</code> <code>SessionSettings | None</code> <p>Session configuration settings including default limit for retrieving items. If None, uses default SessionSettings().</p> <code>None</code> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n    session_settings: SessionSettings | None = None,\n):\n    \"\"\"Initialize the SQLite session.\n\n    Args:\n        session_id: Unique identifier for the conversation session\n        db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n        sessions_table: Name of the table to store session metadata. Defaults to\n            'agent_sessions'\n        messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n        session_settings: Session configuration settings including default limit for\n            retrieving items. If None, uses default SessionSettings().\n    \"\"\"\n    self.session_id = session_id\n    self.session_settings = session_settings or SessionSettings()\n    self.db_path = db_path\n    self.sessions_table = sessions_table\n    self.messages_table = messages_table\n    self._local = threading.local()\n    self._lock = threading.Lock()\n\n    # For in-memory databases, we need a shared connection to avoid thread isolation\n    # For file databases, we use thread-local connections for better concurrency\n    self._is_memory_db = str(db_path) == \":memory:\"\n    if self._is_memory_db:\n        self._shared_connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n        self._shared_connection.execute(\"PRAGMA journal_mode=WAL\")\n        self._init_db_for_connection(self._shared_connection)\n    else:\n        # For file databases, initialize the schema once since it persists\n        init_conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n        init_conn.execute(\"PRAGMA journal_mode=WAL\")\n        self._init_db_for_connection(init_conn)\n        init_conn.close()\n</code></pre>"},{"location":"ref/memory/sqlite_session/#agents.memory.sqlite_session.SQLiteSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, uses session_settings.limit.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, uses session_settings.limit.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    session_limit = resolve_session_limit(limit, self.session_settings)\n\n    def _get_items_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            if session_limit is None:\n                # Fetch all items in chronological order\n                cursor = conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id ASC\n                \"\"\",\n                    (self.session_id,),\n                )\n            else:\n                # Fetch the latest N items in chronological order\n                cursor = conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id DESC\n                    LIMIT ?\n                    \"\"\",\n                    (self.session_id, session_limit),\n                )\n\n            rows = cursor.fetchall()\n\n            # Reverse to get chronological order when using DESC\n            if session_limit is not None:\n                rows = list(reversed(rows))\n\n            items = []\n            for (message_data,) in rows:\n                try:\n                    item = json.loads(message_data)\n                    items.append(item)\n                except json.JSONDecodeError:\n                    # Skip invalid JSON entries\n                    continue\n\n            return items\n\n    return await asyncio.to_thread(_get_items_sync)\n</code></pre>"},{"location":"ref/memory/sqlite_session/#agents.memory.sqlite_session.SQLiteSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    if not items:\n        return\n\n    def _add_items_sync():\n        conn = self._get_connection()\n\n        with self._lock if self._is_memory_db else threading.Lock():\n            # Ensure session exists\n            conn.execute(\n                f\"\"\"\n                INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n            \"\"\",\n                (self.session_id,),\n            )\n\n            # Add items\n            message_data = [(self.session_id, json.dumps(item)) for item in items]\n            conn.executemany(\n                f\"\"\"\n                INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n            \"\"\",\n                message_data,\n            )\n\n            # Update session timestamp\n            conn.execute(\n                f\"\"\"\n                UPDATE {self.sessions_table}\n                SET updated_at = CURRENT_TIMESTAMP\n                WHERE session_id = ?\n            \"\"\",\n                (self.session_id,),\n            )\n\n            conn.commit()\n\n    await asyncio.to_thread(_add_items_sync)\n</code></pre>"},{"location":"ref/memory/sqlite_session/#agents.memory.sqlite_session.SQLiteSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n\n    def _pop_item_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            # Use DELETE with RETURNING to atomically delete and return the most recent item\n            cursor = conn.execute(\n                f\"\"\"\n                DELETE FROM {self.messages_table}\n                WHERE id = (\n                    SELECT id FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY id DESC\n                    LIMIT 1\n                )\n                RETURNING message_data\n                \"\"\",\n                (self.session_id,),\n            )\n\n            result = cursor.fetchone()\n            conn.commit()\n\n            if result:\n                message_data = result[0]\n                try:\n                    item = json.loads(message_data)\n                    return item\n                except json.JSONDecodeError:\n                    # Return None for corrupted JSON entries (already deleted)\n                    return None\n\n            return None\n\n    return await asyncio.to_thread(_pop_item_sync)\n</code></pre>"},{"location":"ref/memory/sqlite_session/#agents.memory.sqlite_session.SQLiteSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n\n    def _clear_session_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            conn.execute(\n                f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.execute(\n                f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.commit()\n\n    await asyncio.to_thread(_clear_session_sync)\n</code></pre>"},{"location":"ref/memory/sqlite_session/#agents.memory.sqlite_session.SQLiteSession.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p> Source code in <code>src/agents/memory/sqlite_session.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._is_memory_db:\n        if hasattr(self, \"_shared_connection\"):\n            self._shared_connection.close()\n    else:\n        if hasattr(self._local, \"connection\"):\n            self._local.connection.close()\n</code></pre>"},{"location":"ref/memory/util/","title":"<code>Util</code>","text":""},{"location":"ref/memory/util/#agents.memory.util.SessionInputCallback","title":"SessionInputCallback  <code>module-attribute</code>","text":"<pre><code>SessionInputCallback = Callable[\n    [list[TResponseInputItem], list[TResponseInputItem]],\n    MaybeAwaitable[list[TResponseInputItem]],\n]\n</code></pre> <p>A function that combines session history with new input items.</p> <p>Parameters:</p> Name Type Description Default <code>history_items</code> <p>The list of items from the session history.</p> required <code>new_items</code> <p>The list of new input items for the current turn.</p> required <p>Returns:</p> Type Description <p>A list of combined items to be used as input for the agent. Can be sync or async.</p>"},{"location":"ref/models/chatcmpl_converter/","title":"<code>Chatcmpl Converter</code>","text":""},{"location":"ref/models/chatcmpl_converter/#agents.models.chatcmpl_converter.Converter","title":"Converter","text":"Source code in <code>src/agents/models/chatcmpl_converter.py</code> <pre><code>class Converter:\n    @classmethod\n    def convert_tool_choice(\n        cls, tool_choice: Literal[\"auto\", \"required\", \"none\"] | str | MCPToolChoice | None\n    ) -&gt; ChatCompletionToolChoiceOptionParam | Omit:\n        if tool_choice is None:\n            return omit\n        elif isinstance(tool_choice, MCPToolChoice):\n            raise UserError(\"MCPToolChoice is not supported for Chat Completions models\")\n        elif tool_choice == \"auto\":\n            return \"auto\"\n        elif tool_choice == \"required\":\n            return \"required\"\n        elif tool_choice == \"none\":\n            return \"none\"\n        else:\n            return {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool_choice,\n                },\n            }\n\n    @classmethod\n    def convert_response_format(\n        cls, final_output_schema: AgentOutputSchemaBase | None\n    ) -&gt; ResponseFormat | Omit:\n        if not final_output_schema or final_output_schema.is_plain_text():\n            return omit\n\n        return {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"name\": \"final_output\",\n                \"strict\": final_output_schema.is_strict_json_schema(),\n                \"schema\": final_output_schema.json_schema(),\n            },\n        }\n\n    @classmethod\n    def message_to_output_items(\n        cls,\n        message: ChatCompletionMessage,\n        provider_data: dict[str, Any] | None = None,\n    ) -&gt; list[TResponseOutputItem]:\n        \"\"\"\n        Convert a ChatCompletionMessage to a list of response output items.\n\n        Args:\n            message: The chat completion message to convert\n            provider_data: Metadata indicating the source model that generated this message.\n                Contains provider-specific information like model name and response_id,\n                which is attached to output items.\n        \"\"\"\n        items: list[TResponseOutputItem] = []\n\n        # Check if message is agents.extentions.models.litellm_model.InternalChatCompletionMessage\n        # We can't actually import it here because litellm is an optional dependency\n        # So we use hasattr to check for reasoning_content and thinking_blocks\n        if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n            reasoning_kwargs: dict[str, Any] = {\n                \"id\": FAKE_RESPONSES_ID,\n                \"summary\": [Summary(text=message.reasoning_content, type=\"summary_text\")],\n                \"type\": \"reasoning\",\n            }\n\n            # Add provider_data if available\n            if provider_data:\n                reasoning_kwargs[\"provider_data\"] = provider_data\n\n            reasoning_item = ResponseReasoningItem(**reasoning_kwargs)\n\n            # Store thinking blocks for Anthropic compatibility\n            if hasattr(message, \"thinking_blocks\") and message.thinking_blocks:\n                # Store thinking text in content and signature in encrypted_content\n                reasoning_item.content = []\n                signatures: list[str] = []\n                for block in message.thinking_blocks:\n                    if isinstance(block, dict):\n                        thinking_text = block.get(\"thinking\", \"\")\n                        if thinking_text:\n                            reasoning_item.content.append(\n                                Content(text=thinking_text, type=\"reasoning_text\")\n                            )\n                        # Store the signature if present\n                        if signature := block.get(\"signature\"):\n                            signatures.append(signature)\n\n                # Store the signatures in encrypted_content with newline delimiter\n                if signatures:\n                    reasoning_item.encrypted_content = \"\\n\".join(signatures)\n\n            items.append(reasoning_item)\n\n        message_kwargs: dict[str, Any] = {\n            \"id\": FAKE_RESPONSES_ID,\n            \"content\": [],\n            \"role\": \"assistant\",\n            \"type\": \"message\",\n            \"status\": \"completed\",\n        }\n\n        # Add provider_data if available\n        if provider_data:\n            message_kwargs[\"provider_data\"] = provider_data\n\n        message_item = ResponseOutputMessage(**message_kwargs)\n        if message.content:\n            message_item.content.append(\n                ResponseOutputText(\n                    text=message.content, type=\"output_text\", annotations=[], logprobs=[]\n                )\n            )\n        if message.refusal:\n            message_item.content.append(\n                ResponseOutputRefusal(refusal=message.refusal, type=\"refusal\")\n            )\n        if message.audio:\n            raise AgentsException(\"Audio is not currently supported\")\n\n        if message_item.content:\n            items.append(message_item)\n\n        if message.tool_calls:\n            for tool_call in message.tool_calls:\n                if tool_call.type == \"function\":\n                    # Create base function call item\n                    func_call_kwargs: dict[str, Any] = {\n                        \"id\": FAKE_RESPONSES_ID,\n                        \"call_id\": tool_call.id,\n                        \"arguments\": tool_call.function.arguments,\n                        \"name\": tool_call.function.name,\n                        \"type\": \"function_call\",\n                    }\n\n                    # Build provider_data for function call\n                    func_provider_data: dict[str, Any] = {}\n\n                    # Start with provider_data (if provided)\n                    if provider_data:\n                        func_provider_data.update(provider_data)\n\n                    # Convert Google's extra_content field data to item's provider_data field\n                    if hasattr(tool_call, \"extra_content\") and tool_call.extra_content:\n                        google_fields = tool_call.extra_content.get(\"google\")\n                        if google_fields and isinstance(google_fields, dict):\n                            thought_sig = google_fields.get(\"thought_signature\")\n                            if thought_sig:\n                                func_provider_data[\"thought_signature\"] = thought_sig\n\n                    # Add provider_data if we have any\n                    if func_provider_data:\n                        func_call_kwargs[\"provider_data\"] = func_provider_data\n\n                    items.append(ResponseFunctionToolCall(**func_call_kwargs))\n                elif tool_call.type == \"custom\":\n                    pass\n\n        return items\n\n    @classmethod\n    def maybe_easy_input_message(cls, item: Any) -&gt; EasyInputMessageParam | None:\n        if not isinstance(item, dict):\n            return None\n\n        keys = item.keys()\n        # EasyInputMessageParam only has these two keys\n        if keys != {\"content\", \"role\"}:\n            return None\n\n        role = item.get(\"role\", None)\n        if role not in (\"user\", \"assistant\", \"system\", \"developer\"):\n            return None\n\n        if \"content\" not in item:\n            return None\n\n        return cast(EasyInputMessageParam, item)\n\n    @classmethod\n    def maybe_input_message(cls, item: Any) -&gt; Message | None:\n        if (\n            isinstance(item, dict)\n            and item.get(\"type\") == \"message\"\n            and item.get(\"role\")\n            in (\n                \"user\",\n                \"system\",\n                \"developer\",\n            )\n        ):\n            return cast(Message, item)\n\n        return None\n\n    @classmethod\n    def maybe_file_search_call(cls, item: Any) -&gt; ResponseFileSearchToolCallParam | None:\n        if isinstance(item, dict) and item.get(\"type\") == \"file_search_call\":\n            return cast(ResponseFileSearchToolCallParam, item)\n        return None\n\n    @classmethod\n    def maybe_function_tool_call(cls, item: Any) -&gt; ResponseFunctionToolCallParam | None:\n        if isinstance(item, dict) and item.get(\"type\") == \"function_call\":\n            return cast(ResponseFunctionToolCallParam, item)\n        return None\n\n    @classmethod\n    def maybe_function_tool_call_output(\n        cls,\n        item: Any,\n    ) -&gt; FunctionCallOutput | None:\n        if isinstance(item, dict) and item.get(\"type\") == \"function_call_output\":\n            return cast(FunctionCallOutput, item)\n        return None\n\n    @classmethod\n    def maybe_item_reference(cls, item: Any) -&gt; ItemReference | None:\n        if isinstance(item, dict) and item.get(\"type\") == \"item_reference\":\n            return cast(ItemReference, item)\n        return None\n\n    @classmethod\n    def maybe_response_output_message(cls, item: Any) -&gt; ResponseOutputMessageParam | None:\n        # ResponseOutputMessage is only used for messages with role assistant\n        if (\n            isinstance(item, dict)\n            and item.get(\"type\") == \"message\"\n            and item.get(\"role\") == \"assistant\"\n        ):\n            return cast(ResponseOutputMessageParam, item)\n        return None\n\n    @classmethod\n    def maybe_reasoning_message(cls, item: Any) -&gt; ResponseReasoningItemParam | None:\n        if isinstance(item, dict) and item.get(\"type\") == \"reasoning\":\n            return cast(ResponseReasoningItemParam, item)\n        return None\n\n    @classmethod\n    def extract_text_content(\n        cls, content: str | Iterable[ResponseInputContentWithAudioParam]\n    ) -&gt; str | list[ChatCompletionContentPartTextParam]:\n        all_content = cls.extract_all_content(content)\n        if isinstance(all_content, str):\n            return all_content\n        out: list[ChatCompletionContentPartTextParam] = []\n        for c in all_content:\n            if c.get(\"type\") == \"text\":\n                out.append(cast(ChatCompletionContentPartTextParam, c))\n        return out\n\n    @classmethod\n    def extract_all_content(\n        cls, content: str | Iterable[ResponseInputContentWithAudioParam]\n    ) -&gt; str | list[ChatCompletionContentPartParam]:\n        if isinstance(content, str):\n            return content\n        out: list[ChatCompletionContentPartParam] = []\n\n        for c in content:\n            if isinstance(c, dict) and c.get(\"type\") == \"input_text\":\n                casted_text_param = cast(ResponseInputTextParam, c)\n                out.append(\n                    ChatCompletionContentPartTextParam(\n                        type=\"text\",\n                        text=casted_text_param[\"text\"],\n                    )\n                )\n            elif isinstance(c, dict) and c.get(\"type\") == \"input_image\":\n                casted_image_param = cast(ResponseInputImageParam, c)\n                if \"image_url\" not in casted_image_param or not casted_image_param[\"image_url\"]:\n                    raise UserError(\n                        f\"Only image URLs are supported for input_image {casted_image_param}\"\n                    )\n                out.append(\n                    ChatCompletionContentPartImageParam(\n                        type=\"image_url\",\n                        image_url={\n                            \"url\": casted_image_param[\"image_url\"],\n                            \"detail\": casted_image_param.get(\"detail\", \"auto\"),\n                        },\n                    )\n                )\n            elif isinstance(c, dict) and c.get(\"type\") == \"input_audio\":\n                casted_audio_param = cast(ResponseInputAudioParam, c)\n                audio_payload = casted_audio_param.get(\"input_audio\")\n                if not audio_payload:\n                    raise UserError(\n                        f\"Only audio data is supported for input_audio {casted_audio_param}\"\n                    )\n                if not isinstance(audio_payload, dict):\n                    raise UserError(\n                        f\"input_audio must provide audio data and format {casted_audio_param}\"\n                    )\n                audio_data = audio_payload.get(\"data\")\n                audio_format = audio_payload.get(\"format\")\n                if not audio_data or not audio_format:\n                    raise UserError(\n                        f\"input_audio requires both data and format {casted_audio_param}\"\n                    )\n                out.append(\n                    ChatCompletionContentPartInputAudioParam(\n                        type=\"input_audio\",\n                        input_audio={\n                            \"data\": audio_data,\n                            \"format\": audio_format,\n                        },\n                    )\n                )\n            elif isinstance(c, dict) and c.get(\"type\") == \"input_file\":\n                casted_file_param = cast(ResponseInputFileParam, c)\n                if \"file_data\" not in casted_file_param or not casted_file_param[\"file_data\"]:\n                    raise UserError(\n                        f\"Only file_data is supported for input_file {casted_file_param}\"\n                    )\n                filedata = FileFile(file_data=casted_file_param[\"file_data\"])\n\n                if \"filename\" in casted_file_param and casted_file_param[\"filename\"]:\n                    filedata[\"filename\"] = casted_file_param[\"filename\"]\n\n                out.append(File(type=\"file\", file=filedata))\n            else:\n                raise UserError(f\"Unknown content: {c}\")\n        return out\n\n    @classmethod\n    def items_to_messages(\n        cls,\n        items: str | Iterable[TResponseInputItem],\n        model: str | None = None,\n        preserve_thinking_blocks: bool = False,\n        preserve_tool_output_all_content: bool = False,\n    ) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"\n        Convert a sequence of 'Item' objects into a list of ChatCompletionMessageParam.\n\n        Args:\n            items: A string or iterable of response input items to convert\n            model: The target model to convert to. Used to restore provider-specific data\n                (e.g., Gemini thought signatures, Claude thinking blocks) when converting\n                items back to chat completion messages for the target model.\n            preserve_thinking_blocks: Whether to preserve thinking blocks in tool calls\n                for reasoning models like Claude 4 Sonnet/Opus which support interleaved\n                thinking. When True, thinking blocks are reconstructed and included in\n                assistant messages with tool calls.\n            preserve_tool_output_all_content: Whether to preserve non-text content (like images)\n                in tool outputs. When False (default), only text content is extracted.\n                OpenAI Chat Completions API doesn't support non-text content in tool results.\n                When True, all content types including images are preserved. This is useful\n                for model providers (e.g. Anthropic via LiteLLM) that support processing\n                non-text content in tool results.\n\n        Rules:\n        - EasyInputMessage or InputMessage (role=user) =&gt; ChatCompletionUserMessageParam\n        - EasyInputMessage or InputMessage (role=system) =&gt; ChatCompletionSystemMessageParam\n        - EasyInputMessage or InputMessage (role=developer) =&gt; ChatCompletionDeveloperMessageParam\n        - InputMessage (role=assistant) =&gt; Start or flush a ChatCompletionAssistantMessageParam\n        - response_output_message =&gt; Also produces/flushes a ChatCompletionAssistantMessageParam\n        - tool calls get attached to the *current* assistant message, or create one if none.\n        - tool outputs =&gt; ChatCompletionToolMessageParam\n        \"\"\"\n\n        if isinstance(items, str):\n            return [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=items,\n                )\n            ]\n\n        result: list[ChatCompletionMessageParam] = []\n        current_assistant_msg: ChatCompletionAssistantMessageParam | None = None\n        pending_thinking_blocks: list[dict[str, str]] | None = None\n        pending_reasoning_content: str | None = None  # For DeepSeek reasoning_content\n\n        def flush_assistant_message() -&gt; None:\n            nonlocal current_assistant_msg, pending_reasoning_content\n            if current_assistant_msg is not None:\n                # The API doesn't support empty arrays for tool_calls\n                if not current_assistant_msg.get(\"tool_calls\"):\n                    del current_assistant_msg[\"tool_calls\"]\n                    # prevents stale reasoning_content from contaminating later turns\n                    pending_reasoning_content = None\n                result.append(current_assistant_msg)\n                current_assistant_msg = None\n            else:\n                pending_reasoning_content = None\n\n        def ensure_assistant_message() -&gt; ChatCompletionAssistantMessageParam:\n            nonlocal current_assistant_msg, pending_thinking_blocks\n            if current_assistant_msg is None:\n                current_assistant_msg = ChatCompletionAssistantMessageParam(role=\"assistant\")\n                current_assistant_msg[\"content\"] = None\n                current_assistant_msg[\"tool_calls\"] = []\n\n            return current_assistant_msg\n\n        for item in items:\n            # 1) Check easy input message\n            if easy_msg := cls.maybe_easy_input_message(item):\n                role = easy_msg[\"role\"]\n                content = easy_msg[\"content\"]\n\n                if role == \"user\":\n                    flush_assistant_message()\n                    msg_user: ChatCompletionUserMessageParam = {\n                        \"role\": \"user\",\n                        \"content\": cls.extract_all_content(content),\n                    }\n                    result.append(msg_user)\n                elif role == \"system\":\n                    flush_assistant_message()\n                    msg_system: ChatCompletionSystemMessageParam = {\n                        \"role\": \"system\",\n                        \"content\": cls.extract_text_content(content),\n                    }\n                    result.append(msg_system)\n                elif role == \"developer\":\n                    flush_assistant_message()\n                    msg_developer: ChatCompletionDeveloperMessageParam = {\n                        \"role\": \"developer\",\n                        \"content\": cls.extract_text_content(content),\n                    }\n                    result.append(msg_developer)\n                elif role == \"assistant\":\n                    flush_assistant_message()\n                    msg_assistant: ChatCompletionAssistantMessageParam = {\n                        \"role\": \"assistant\",\n                        \"content\": cls.extract_text_content(content),\n                    }\n                    result.append(msg_assistant)\n                else:\n                    raise UserError(f\"Unexpected role in easy_input_message: {role}\")\n\n            # 2) Check input message\n            elif in_msg := cls.maybe_input_message(item):\n                role = in_msg[\"role\"]\n                content = in_msg[\"content\"]\n                flush_assistant_message()\n\n                if role == \"user\":\n                    msg_user = {\n                        \"role\": \"user\",\n                        \"content\": cls.extract_all_content(content),\n                    }\n                    result.append(msg_user)\n                elif role == \"system\":\n                    msg_system = {\n                        \"role\": \"system\",\n                        \"content\": cls.extract_text_content(content),\n                    }\n                    result.append(msg_system)\n                elif role == \"developer\":\n                    msg_developer = {\n                        \"role\": \"developer\",\n                        \"content\": cls.extract_text_content(content),\n                    }\n                    result.append(msg_developer)\n                else:\n                    raise UserError(f\"Unexpected role in input_message: {role}\")\n\n            # 3) response output message =&gt; assistant\n            elif resp_msg := cls.maybe_response_output_message(item):\n                flush_assistant_message()\n                new_asst = ChatCompletionAssistantMessageParam(role=\"assistant\")\n                contents = resp_msg[\"content\"]\n\n                text_segments = []\n                for c in contents:\n                    if c[\"type\"] == \"output_text\":\n                        text_segments.append(c[\"text\"])\n                    elif c[\"type\"] == \"refusal\":\n                        new_asst[\"refusal\"] = c[\"refusal\"]\n                    elif c[\"type\"] == \"output_audio\":\n                        # Can't handle this, b/c chat completions expects an ID which we dont have\n                        raise UserError(\n                            f\"Only audio IDs are supported for chat completions, but got: {c}\"\n                        )\n                    else:\n                        raise UserError(f\"Unknown content type in ResponseOutputMessage: {c}\")\n\n                if text_segments:\n                    combined = \"\\n\".join(text_segments)\n                    new_asst[\"content\"] = combined\n\n                # If we have pending thinking blocks, prepend them to the content\n                # This is required for Anthropic API with interleaved thinking\n                if pending_thinking_blocks:\n                    # If there is a text content, convert it to a list to prepend thinking blocks\n                    if \"content\" in new_asst and isinstance(new_asst[\"content\"], str):\n                        text_content = ChatCompletionContentPartTextParam(\n                            text=new_asst[\"content\"], type=\"text\"\n                        )\n                        new_asst[\"content\"] = [text_content]\n\n                    if \"content\" not in new_asst or new_asst[\"content\"] is None:\n                        new_asst[\"content\"] = []\n\n                    # Thinking blocks MUST come before any other content\n                    # We ignore type errors because pending_thinking_blocks is not openai standard\n                    new_asst[\"content\"] = pending_thinking_blocks + new_asst[\"content\"]  # type: ignore\n                    pending_thinking_blocks = None  # Clear after using\n\n                new_asst[\"tool_calls\"] = []\n                current_assistant_msg = new_asst\n\n            # 4) function/file-search calls =&gt; attach to assistant\n            elif file_search := cls.maybe_file_search_call(item):\n                asst = ensure_assistant_message()\n                tool_calls = list(asst.get(\"tool_calls\", []))\n                new_tool_call = ChatCompletionMessageFunctionToolCallParam(\n                    id=file_search[\"id\"],\n                    type=\"function\",\n                    function={\n                        \"name\": \"file_search_call\",\n                        \"arguments\": json.dumps(\n                            {\n                                \"queries\": file_search.get(\"queries\", []),\n                                \"status\": file_search.get(\"status\"),\n                            }\n                        ),\n                    },\n                )\n                tool_calls.append(new_tool_call)\n                asst[\"tool_calls\"] = tool_calls\n\n            elif func_call := cls.maybe_function_tool_call(item):\n                asst = ensure_assistant_message()\n\n                # If we have pending reasoning content for DeepSeek, add it to the assistant message\n                if pending_reasoning_content:\n                    asst[\"reasoning_content\"] = pending_reasoning_content  # type: ignore[typeddict-unknown-key]\n                    pending_reasoning_content = None  # Clear after using\n\n                # If we have pending thinking blocks, use them as the content\n                # This is required for Anthropic API tool calls with interleaved thinking\n                if pending_thinking_blocks:\n                    # If there is a text content, save it to append after thinking blocks\n                    # content type is Union[str, Iterable[ContentArrayOfContentPart], None]\n                    if \"content\" in asst and isinstance(asst[\"content\"], str):\n                        text_content = ChatCompletionContentPartTextParam(\n                            text=asst[\"content\"], type=\"text\"\n                        )\n                        asst[\"content\"] = [text_content]\n\n                    if \"content\" not in asst or asst[\"content\"] is None:\n                        asst[\"content\"] = []\n\n                    # Thinking blocks MUST come before any other content\n                    # We ignore type errors because pending_thinking_blocks is not openai standard\n                    asst[\"content\"] = pending_thinking_blocks + asst[\"content\"]  # type: ignore\n                    pending_thinking_blocks = None  # Clear after using\n\n                tool_calls = list(asst.get(\"tool_calls\", []))\n                arguments = func_call[\"arguments\"] if func_call[\"arguments\"] else \"{}\"\n                new_tool_call = ChatCompletionMessageFunctionToolCallParam(\n                    id=func_call[\"call_id\"],\n                    type=\"function\",\n                    function={\n                        \"name\": func_call[\"name\"],\n                        \"arguments\": arguments,\n                    },\n                )\n\n                # Restore provider_data back to chat completion message for non-OpenAI models\n                if \"provider_data\" in func_call:\n                    provider_fields = func_call[\"provider_data\"]  # type: ignore[typeddict-item]\n                    if isinstance(provider_fields, dict):\n                        # Restore thought_signature for Gemini in Google's extra_content format\n                        if model and \"gemini\" in model.lower():\n                            thought_sig = provider_fields.get(\"thought_signature\")\n\n                            if thought_sig:\n                                new_tool_call[\"extra_content\"] = {  # type: ignore[typeddict-unknown-key]\n                                    \"google\": {\"thought_signature\": thought_sig}\n                                }\n\n                tool_calls.append(new_tool_call)\n                asst[\"tool_calls\"] = tool_calls\n            # 5) function call output =&gt; tool message\n            elif func_output := cls.maybe_function_tool_call_output(item):\n                flush_assistant_message()\n                output_content = cast(\n                    Union[str, Iterable[ResponseInputContentWithAudioParam]], func_output[\"output\"]\n                )\n                if preserve_tool_output_all_content:\n                    tool_result_content = cls.extract_all_content(output_content)\n                else:\n                    tool_result_content = cls.extract_text_content(output_content)  # type: ignore[assignment]\n                msg: ChatCompletionToolMessageParam = {\n                    \"role\": \"tool\",\n                    \"tool_call_id\": func_output[\"call_id\"],\n                    \"content\": tool_result_content,  # type: ignore[typeddict-item]\n                }\n                result.append(msg)\n\n            # 6) item reference =&gt; handle or raise\n            elif item_ref := cls.maybe_item_reference(item):\n                raise UserError(\n                    f\"Encountered an item_reference, which is not supported: {item_ref}\"\n                )\n\n            # 7) reasoning message =&gt; extract thinking blocks if present\n            elif reasoning_item := cls.maybe_reasoning_message(item):\n                # Reconstruct thinking blocks from content (text) and encrypted_content (signature)\n                content_items = reasoning_item.get(\"content\", [])\n                encrypted_content = reasoning_item.get(\"encrypted_content\")\n\n                item_provider_data: dict[str, Any] = reasoning_item.get(\"provider_data\", {})  # type: ignore[assignment]\n                item_model = item_provider_data.get(\"model\", \"\")\n\n                if (\n                    model\n                    and (\"claude\" in model.lower() or \"anthropic\" in model.lower())\n                    and content_items\n                    and preserve_thinking_blocks\n                    # Items may not all originate from Claude, so we need to check for model match.\n                    # For backward compatibility, if provider_data is missing, we ignore the check.\n                    and (model == item_model or item_provider_data == {})\n                ):\n                    signatures = encrypted_content.split(\"\\n\") if encrypted_content else []\n\n                    # Reconstruct thinking blocks from content and signature\n                    reconstructed_thinking_blocks = []\n                    for content_item in content_items:\n                        if (\n                            isinstance(content_item, dict)\n                            and content_item.get(\"type\") == \"reasoning_text\"\n                        ):\n                            thinking_block = {\n                                \"type\": \"thinking\",\n                                \"thinking\": content_item.get(\"text\", \"\"),\n                            }\n                            # Add signatures if available\n                            if signatures:\n                                thinking_block[\"signature\"] = signatures.pop(0)\n                            reconstructed_thinking_blocks.append(thinking_block)\n\n                    # Store thinking blocks as pending for the next assistant message\n                    # This preserves the original behavior\n                    pending_thinking_blocks = reconstructed_thinking_blocks\n\n                # DeepSeek requires reasoning_content field in assistant messages with tool calls\n                # Items may not all originate from DeepSeek, so need to check for model match.\n                # For backward compatibility, if provider_data is missing, ignore the check.\n                elif (\n                    model\n                    and \"deepseek\" in model.lower()\n                    and (\n                        (item_model and \"deepseek\" in item_model.lower())\n                        or item_provider_data == {}\n                    )\n                ):\n                    summary_items = reasoning_item.get(\"summary\", [])\n                    if summary_items:\n                        reasoning_texts = []\n                        for summary_item in summary_items:\n                            if isinstance(summary_item, dict) and summary_item.get(\"text\"):\n                                reasoning_texts.append(summary_item[\"text\"])\n                        if reasoning_texts:\n                            pending_reasoning_content = \"\\n\".join(reasoning_texts)\n\n            # 8) compaction items =&gt; reject for chat completions\n            elif isinstance(item, dict) and item.get(\"type\") == \"compaction\":\n                raise UserError(\n                    \"Compaction items are not supported for chat completions. \"\n                    \"Please use the Responses API to handle compaction.\"\n                )\n\n            # 9) If we haven't recognized it =&gt; fail or ignore\n            else:\n                raise UserError(f\"Unhandled item type or structure: {item}\")\n\n        flush_assistant_message()\n        return result\n\n    @classmethod\n    def tool_to_openai(cls, tool: Tool) -&gt; ChatCompletionToolParam:\n        if isinstance(tool, FunctionTool):\n            return {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description or \"\",\n                    \"parameters\": tool.params_json_schema,\n                    \"strict\": tool.strict_json_schema,\n                },\n            }\n\n        raise UserError(\n            f\"Hosted tools are not supported with the ChatCompletions API. Got tool type: \"\n            f\"{type(tool)}, tool: {tool}\"\n        )\n\n    @classmethod\n    def convert_handoff_tool(cls, handoff: Handoff[Any, Any]) -&gt; ChatCompletionToolParam:\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": handoff.tool_name,\n                \"description\": handoff.tool_description,\n                \"parameters\": handoff.input_json_schema,\n                \"strict\": handoff.strict_json_schema,\n            },\n        }\n</code></pre>"},{"location":"ref/models/chatcmpl_converter/#agents.models.chatcmpl_converter.Converter.message_to_output_items","title":"message_to_output_items  <code>classmethod</code>","text":"<pre><code>message_to_output_items(\n    message: ChatCompletionMessage,\n    provider_data: dict[str, Any] | None = None,\n) -&gt; list[TResponseOutputItem]\n</code></pre> <p>Convert a ChatCompletionMessage to a list of response output items.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ChatCompletionMessage</code> <p>The chat completion message to convert</p> required <code>provider_data</code> <code>dict[str, Any] | None</code> <p>Metadata indicating the source model that generated this message. Contains provider-specific information like model name and response_id, which is attached to output items.</p> <code>None</code> Source code in <code>src/agents/models/chatcmpl_converter.py</code> <pre><code>@classmethod\ndef message_to_output_items(\n    cls,\n    message: ChatCompletionMessage,\n    provider_data: dict[str, Any] | None = None,\n) -&gt; list[TResponseOutputItem]:\n    \"\"\"\n    Convert a ChatCompletionMessage to a list of response output items.\n\n    Args:\n        message: The chat completion message to convert\n        provider_data: Metadata indicating the source model that generated this message.\n            Contains provider-specific information like model name and response_id,\n            which is attached to output items.\n    \"\"\"\n    items: list[TResponseOutputItem] = []\n\n    # Check if message is agents.extentions.models.litellm_model.InternalChatCompletionMessage\n    # We can't actually import it here because litellm is an optional dependency\n    # So we use hasattr to check for reasoning_content and thinking_blocks\n    if hasattr(message, \"reasoning_content\") and message.reasoning_content:\n        reasoning_kwargs: dict[str, Any] = {\n            \"id\": FAKE_RESPONSES_ID,\n            \"summary\": [Summary(text=message.reasoning_content, type=\"summary_text\")],\n            \"type\": \"reasoning\",\n        }\n\n        # Add provider_data if available\n        if provider_data:\n            reasoning_kwargs[\"provider_data\"] = provider_data\n\n        reasoning_item = ResponseReasoningItem(**reasoning_kwargs)\n\n        # Store thinking blocks for Anthropic compatibility\n        if hasattr(message, \"thinking_blocks\") and message.thinking_blocks:\n            # Store thinking text in content and signature in encrypted_content\n            reasoning_item.content = []\n            signatures: list[str] = []\n            for block in message.thinking_blocks:\n                if isinstance(block, dict):\n                    thinking_text = block.get(\"thinking\", \"\")\n                    if thinking_text:\n                        reasoning_item.content.append(\n                            Content(text=thinking_text, type=\"reasoning_text\")\n                        )\n                    # Store the signature if present\n                    if signature := block.get(\"signature\"):\n                        signatures.append(signature)\n\n            # Store the signatures in encrypted_content with newline delimiter\n            if signatures:\n                reasoning_item.encrypted_content = \"\\n\".join(signatures)\n\n        items.append(reasoning_item)\n\n    message_kwargs: dict[str, Any] = {\n        \"id\": FAKE_RESPONSES_ID,\n        \"content\": [],\n        \"role\": \"assistant\",\n        \"type\": \"message\",\n        \"status\": \"completed\",\n    }\n\n    # Add provider_data if available\n    if provider_data:\n        message_kwargs[\"provider_data\"] = provider_data\n\n    message_item = ResponseOutputMessage(**message_kwargs)\n    if message.content:\n        message_item.content.append(\n            ResponseOutputText(\n                text=message.content, type=\"output_text\", annotations=[], logprobs=[]\n            )\n        )\n    if message.refusal:\n        message_item.content.append(\n            ResponseOutputRefusal(refusal=message.refusal, type=\"refusal\")\n        )\n    if message.audio:\n        raise AgentsException(\"Audio is not currently supported\")\n\n    if message_item.content:\n        items.append(message_item)\n\n    if message.tool_calls:\n        for tool_call in message.tool_calls:\n            if tool_call.type == \"function\":\n                # Create base function call item\n                func_call_kwargs: dict[str, Any] = {\n                    \"id\": FAKE_RESPONSES_ID,\n                    \"call_id\": tool_call.id,\n                    \"arguments\": tool_call.function.arguments,\n                    \"name\": tool_call.function.name,\n                    \"type\": \"function_call\",\n                }\n\n                # Build provider_data for function call\n                func_provider_data: dict[str, Any] = {}\n\n                # Start with provider_data (if provided)\n                if provider_data:\n                    func_provider_data.update(provider_data)\n\n                # Convert Google's extra_content field data to item's provider_data field\n                if hasattr(tool_call, \"extra_content\") and tool_call.extra_content:\n                    google_fields = tool_call.extra_content.get(\"google\")\n                    if google_fields and isinstance(google_fields, dict):\n                        thought_sig = google_fields.get(\"thought_signature\")\n                        if thought_sig:\n                            func_provider_data[\"thought_signature\"] = thought_sig\n\n                # Add provider_data if we have any\n                if func_provider_data:\n                    func_call_kwargs[\"provider_data\"] = func_provider_data\n\n                items.append(ResponseFunctionToolCall(**func_call_kwargs))\n            elif tool_call.type == \"custom\":\n                pass\n\n    return items\n</code></pre>"},{"location":"ref/models/chatcmpl_converter/#agents.models.chatcmpl_converter.Converter.items_to_messages","title":"items_to_messages  <code>classmethod</code>","text":"<pre><code>items_to_messages(\n    items: str | Iterable[TResponseInputItem],\n    model: str | None = None,\n    preserve_thinking_blocks: bool = False,\n    preserve_tool_output_all_content: bool = False,\n) -&gt; list[ChatCompletionMessageParam]\n</code></pre> <p>Convert a sequence of 'Item' objects into a list of ChatCompletionMessageParam.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>str | Iterable[TResponseInputItem]</code> <p>A string or iterable of response input items to convert</p> required <code>model</code> <code>str | None</code> <p>The target model to convert to. Used to restore provider-specific data (e.g., Gemini thought signatures, Claude thinking blocks) when converting items back to chat completion messages for the target model.</p> <code>None</code> <code>preserve_thinking_blocks</code> <code>bool</code> <p>Whether to preserve thinking blocks in tool calls for reasoning models like Claude 4 Sonnet/Opus which support interleaved thinking. When True, thinking blocks are reconstructed and included in assistant messages with tool calls.</p> <code>False</code> <code>preserve_tool_output_all_content</code> <code>bool</code> <p>Whether to preserve non-text content (like images) in tool outputs. When False (default), only text content is extracted. OpenAI Chat Completions API doesn't support non-text content in tool results. When True, all content types including images are preserved. This is useful for model providers (e.g. Anthropic via LiteLLM) that support processing non-text content in tool results.</p> <code>False</code> <p>Rules: - EasyInputMessage or InputMessage (role=user) =&gt; ChatCompletionUserMessageParam - EasyInputMessage or InputMessage (role=system) =&gt; ChatCompletionSystemMessageParam - EasyInputMessage or InputMessage (role=developer) =&gt; ChatCompletionDeveloperMessageParam - InputMessage (role=assistant) =&gt; Start or flush a ChatCompletionAssistantMessageParam - response_output_message =&gt; Also produces/flushes a ChatCompletionAssistantMessageParam - tool calls get attached to the current assistant message, or create one if none. - tool outputs =&gt; ChatCompletionToolMessageParam</p> Source code in <code>src/agents/models/chatcmpl_converter.py</code> <pre><code>@classmethod\ndef items_to_messages(\n    cls,\n    items: str | Iterable[TResponseInputItem],\n    model: str | None = None,\n    preserve_thinking_blocks: bool = False,\n    preserve_tool_output_all_content: bool = False,\n) -&gt; list[ChatCompletionMessageParam]:\n    \"\"\"\n    Convert a sequence of 'Item' objects into a list of ChatCompletionMessageParam.\n\n    Args:\n        items: A string or iterable of response input items to convert\n        model: The target model to convert to. Used to restore provider-specific data\n            (e.g., Gemini thought signatures, Claude thinking blocks) when converting\n            items back to chat completion messages for the target model.\n        preserve_thinking_blocks: Whether to preserve thinking blocks in tool calls\n            for reasoning models like Claude 4 Sonnet/Opus which support interleaved\n            thinking. When True, thinking blocks are reconstructed and included in\n            assistant messages with tool calls.\n        preserve_tool_output_all_content: Whether to preserve non-text content (like images)\n            in tool outputs. When False (default), only text content is extracted.\n            OpenAI Chat Completions API doesn't support non-text content in tool results.\n            When True, all content types including images are preserved. This is useful\n            for model providers (e.g. Anthropic via LiteLLM) that support processing\n            non-text content in tool results.\n\n    Rules:\n    - EasyInputMessage or InputMessage (role=user) =&gt; ChatCompletionUserMessageParam\n    - EasyInputMessage or InputMessage (role=system) =&gt; ChatCompletionSystemMessageParam\n    - EasyInputMessage or InputMessage (role=developer) =&gt; ChatCompletionDeveloperMessageParam\n    - InputMessage (role=assistant) =&gt; Start or flush a ChatCompletionAssistantMessageParam\n    - response_output_message =&gt; Also produces/flushes a ChatCompletionAssistantMessageParam\n    - tool calls get attached to the *current* assistant message, or create one if none.\n    - tool outputs =&gt; ChatCompletionToolMessageParam\n    \"\"\"\n\n    if isinstance(items, str):\n        return [\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=items,\n            )\n        ]\n\n    result: list[ChatCompletionMessageParam] = []\n    current_assistant_msg: ChatCompletionAssistantMessageParam | None = None\n    pending_thinking_blocks: list[dict[str, str]] | None = None\n    pending_reasoning_content: str | None = None  # For DeepSeek reasoning_content\n\n    def flush_assistant_message() -&gt; None:\n        nonlocal current_assistant_msg, pending_reasoning_content\n        if current_assistant_msg is not None:\n            # The API doesn't support empty arrays for tool_calls\n            if not current_assistant_msg.get(\"tool_calls\"):\n                del current_assistant_msg[\"tool_calls\"]\n                # prevents stale reasoning_content from contaminating later turns\n                pending_reasoning_content = None\n            result.append(current_assistant_msg)\n            current_assistant_msg = None\n        else:\n            pending_reasoning_content = None\n\n    def ensure_assistant_message() -&gt; ChatCompletionAssistantMessageParam:\n        nonlocal current_assistant_msg, pending_thinking_blocks\n        if current_assistant_msg is None:\n            current_assistant_msg = ChatCompletionAssistantMessageParam(role=\"assistant\")\n            current_assistant_msg[\"content\"] = None\n            current_assistant_msg[\"tool_calls\"] = []\n\n        return current_assistant_msg\n\n    for item in items:\n        # 1) Check easy input message\n        if easy_msg := cls.maybe_easy_input_message(item):\n            role = easy_msg[\"role\"]\n            content = easy_msg[\"content\"]\n\n            if role == \"user\":\n                flush_assistant_message()\n                msg_user: ChatCompletionUserMessageParam = {\n                    \"role\": \"user\",\n                    \"content\": cls.extract_all_content(content),\n                }\n                result.append(msg_user)\n            elif role == \"system\":\n                flush_assistant_message()\n                msg_system: ChatCompletionSystemMessageParam = {\n                    \"role\": \"system\",\n                    \"content\": cls.extract_text_content(content),\n                }\n                result.append(msg_system)\n            elif role == \"developer\":\n                flush_assistant_message()\n                msg_developer: ChatCompletionDeveloperMessageParam = {\n                    \"role\": \"developer\",\n                    \"content\": cls.extract_text_content(content),\n                }\n                result.append(msg_developer)\n            elif role == \"assistant\":\n                flush_assistant_message()\n                msg_assistant: ChatCompletionAssistantMessageParam = {\n                    \"role\": \"assistant\",\n                    \"content\": cls.extract_text_content(content),\n                }\n                result.append(msg_assistant)\n            else:\n                raise UserError(f\"Unexpected role in easy_input_message: {role}\")\n\n        # 2) Check input message\n        elif in_msg := cls.maybe_input_message(item):\n            role = in_msg[\"role\"]\n            content = in_msg[\"content\"]\n            flush_assistant_message()\n\n            if role == \"user\":\n                msg_user = {\n                    \"role\": \"user\",\n                    \"content\": cls.extract_all_content(content),\n                }\n                result.append(msg_user)\n            elif role == \"system\":\n                msg_system = {\n                    \"role\": \"system\",\n                    \"content\": cls.extract_text_content(content),\n                }\n                result.append(msg_system)\n            elif role == \"developer\":\n                msg_developer = {\n                    \"role\": \"developer\",\n                    \"content\": cls.extract_text_content(content),\n                }\n                result.append(msg_developer)\n            else:\n                raise UserError(f\"Unexpected role in input_message: {role}\")\n\n        # 3) response output message =&gt; assistant\n        elif resp_msg := cls.maybe_response_output_message(item):\n            flush_assistant_message()\n            new_asst = ChatCompletionAssistantMessageParam(role=\"assistant\")\n            contents = resp_msg[\"content\"]\n\n            text_segments = []\n            for c in contents:\n                if c[\"type\"] == \"output_text\":\n                    text_segments.append(c[\"text\"])\n                elif c[\"type\"] == \"refusal\":\n                    new_asst[\"refusal\"] = c[\"refusal\"]\n                elif c[\"type\"] == \"output_audio\":\n                    # Can't handle this, b/c chat completions expects an ID which we dont have\n                    raise UserError(\n                        f\"Only audio IDs are supported for chat completions, but got: {c}\"\n                    )\n                else:\n                    raise UserError(f\"Unknown content type in ResponseOutputMessage: {c}\")\n\n            if text_segments:\n                combined = \"\\n\".join(text_segments)\n                new_asst[\"content\"] = combined\n\n            # If we have pending thinking blocks, prepend them to the content\n            # This is required for Anthropic API with interleaved thinking\n            if pending_thinking_blocks:\n                # If there is a text content, convert it to a list to prepend thinking blocks\n                if \"content\" in new_asst and isinstance(new_asst[\"content\"], str):\n                    text_content = ChatCompletionContentPartTextParam(\n                        text=new_asst[\"content\"], type=\"text\"\n                    )\n                    new_asst[\"content\"] = [text_content]\n\n                if \"content\" not in new_asst or new_asst[\"content\"] is None:\n                    new_asst[\"content\"] = []\n\n                # Thinking blocks MUST come before any other content\n                # We ignore type errors because pending_thinking_blocks is not openai standard\n                new_asst[\"content\"] = pending_thinking_blocks + new_asst[\"content\"]  # type: ignore\n                pending_thinking_blocks = None  # Clear after using\n\n            new_asst[\"tool_calls\"] = []\n            current_assistant_msg = new_asst\n\n        # 4) function/file-search calls =&gt; attach to assistant\n        elif file_search := cls.maybe_file_search_call(item):\n            asst = ensure_assistant_message()\n            tool_calls = list(asst.get(\"tool_calls\", []))\n            new_tool_call = ChatCompletionMessageFunctionToolCallParam(\n                id=file_search[\"id\"],\n                type=\"function\",\n                function={\n                    \"name\": \"file_search_call\",\n                    \"arguments\": json.dumps(\n                        {\n                            \"queries\": file_search.get(\"queries\", []),\n                            \"status\": file_search.get(\"status\"),\n                        }\n                    ),\n                },\n            )\n            tool_calls.append(new_tool_call)\n            asst[\"tool_calls\"] = tool_calls\n\n        elif func_call := cls.maybe_function_tool_call(item):\n            asst = ensure_assistant_message()\n\n            # If we have pending reasoning content for DeepSeek, add it to the assistant message\n            if pending_reasoning_content:\n                asst[\"reasoning_content\"] = pending_reasoning_content  # type: ignore[typeddict-unknown-key]\n                pending_reasoning_content = None  # Clear after using\n\n            # If we have pending thinking blocks, use them as the content\n            # This is required for Anthropic API tool calls with interleaved thinking\n            if pending_thinking_blocks:\n                # If there is a text content, save it to append after thinking blocks\n                # content type is Union[str, Iterable[ContentArrayOfContentPart], None]\n                if \"content\" in asst and isinstance(asst[\"content\"], str):\n                    text_content = ChatCompletionContentPartTextParam(\n                        text=asst[\"content\"], type=\"text\"\n                    )\n                    asst[\"content\"] = [text_content]\n\n                if \"content\" not in asst or asst[\"content\"] is None:\n                    asst[\"content\"] = []\n\n                # Thinking blocks MUST come before any other content\n                # We ignore type errors because pending_thinking_blocks is not openai standard\n                asst[\"content\"] = pending_thinking_blocks + asst[\"content\"]  # type: ignore\n                pending_thinking_blocks = None  # Clear after using\n\n            tool_calls = list(asst.get(\"tool_calls\", []))\n            arguments = func_call[\"arguments\"] if func_call[\"arguments\"] else \"{}\"\n            new_tool_call = ChatCompletionMessageFunctionToolCallParam(\n                id=func_call[\"call_id\"],\n                type=\"function\",\n                function={\n                    \"name\": func_call[\"name\"],\n                    \"arguments\": arguments,\n                },\n            )\n\n            # Restore provider_data back to chat completion message for non-OpenAI models\n            if \"provider_data\" in func_call:\n                provider_fields = func_call[\"provider_data\"]  # type: ignore[typeddict-item]\n                if isinstance(provider_fields, dict):\n                    # Restore thought_signature for Gemini in Google's extra_content format\n                    if model and \"gemini\" in model.lower():\n                        thought_sig = provider_fields.get(\"thought_signature\")\n\n                        if thought_sig:\n                            new_tool_call[\"extra_content\"] = {  # type: ignore[typeddict-unknown-key]\n                                \"google\": {\"thought_signature\": thought_sig}\n                            }\n\n            tool_calls.append(new_tool_call)\n            asst[\"tool_calls\"] = tool_calls\n        # 5) function call output =&gt; tool message\n        elif func_output := cls.maybe_function_tool_call_output(item):\n            flush_assistant_message()\n            output_content = cast(\n                Union[str, Iterable[ResponseInputContentWithAudioParam]], func_output[\"output\"]\n            )\n            if preserve_tool_output_all_content:\n                tool_result_content = cls.extract_all_content(output_content)\n            else:\n                tool_result_content = cls.extract_text_content(output_content)  # type: ignore[assignment]\n            msg: ChatCompletionToolMessageParam = {\n                \"role\": \"tool\",\n                \"tool_call_id\": func_output[\"call_id\"],\n                \"content\": tool_result_content,  # type: ignore[typeddict-item]\n            }\n            result.append(msg)\n\n        # 6) item reference =&gt; handle or raise\n        elif item_ref := cls.maybe_item_reference(item):\n            raise UserError(\n                f\"Encountered an item_reference, which is not supported: {item_ref}\"\n            )\n\n        # 7) reasoning message =&gt; extract thinking blocks if present\n        elif reasoning_item := cls.maybe_reasoning_message(item):\n            # Reconstruct thinking blocks from content (text) and encrypted_content (signature)\n            content_items = reasoning_item.get(\"content\", [])\n            encrypted_content = reasoning_item.get(\"encrypted_content\")\n\n            item_provider_data: dict[str, Any] = reasoning_item.get(\"provider_data\", {})  # type: ignore[assignment]\n            item_model = item_provider_data.get(\"model\", \"\")\n\n            if (\n                model\n                and (\"claude\" in model.lower() or \"anthropic\" in model.lower())\n                and content_items\n                and preserve_thinking_blocks\n                # Items may not all originate from Claude, so we need to check for model match.\n                # For backward compatibility, if provider_data is missing, we ignore the check.\n                and (model == item_model or item_provider_data == {})\n            ):\n                signatures = encrypted_content.split(\"\\n\") if encrypted_content else []\n\n                # Reconstruct thinking blocks from content and signature\n                reconstructed_thinking_blocks = []\n                for content_item in content_items:\n                    if (\n                        isinstance(content_item, dict)\n                        and content_item.get(\"type\") == \"reasoning_text\"\n                    ):\n                        thinking_block = {\n                            \"type\": \"thinking\",\n                            \"thinking\": content_item.get(\"text\", \"\"),\n                        }\n                        # Add signatures if available\n                        if signatures:\n                            thinking_block[\"signature\"] = signatures.pop(0)\n                        reconstructed_thinking_blocks.append(thinking_block)\n\n                # Store thinking blocks as pending for the next assistant message\n                # This preserves the original behavior\n                pending_thinking_blocks = reconstructed_thinking_blocks\n\n            # DeepSeek requires reasoning_content field in assistant messages with tool calls\n            # Items may not all originate from DeepSeek, so need to check for model match.\n            # For backward compatibility, if provider_data is missing, ignore the check.\n            elif (\n                model\n                and \"deepseek\" in model.lower()\n                and (\n                    (item_model and \"deepseek\" in item_model.lower())\n                    or item_provider_data == {}\n                )\n            ):\n                summary_items = reasoning_item.get(\"summary\", [])\n                if summary_items:\n                    reasoning_texts = []\n                    for summary_item in summary_items:\n                        if isinstance(summary_item, dict) and summary_item.get(\"text\"):\n                            reasoning_texts.append(summary_item[\"text\"])\n                    if reasoning_texts:\n                        pending_reasoning_content = \"\\n\".join(reasoning_texts)\n\n        # 8) compaction items =&gt; reject for chat completions\n        elif isinstance(item, dict) and item.get(\"type\") == \"compaction\":\n            raise UserError(\n                \"Compaction items are not supported for chat completions. \"\n                \"Please use the Responses API to handle compaction.\"\n            )\n\n        # 9) If we haven't recognized it =&gt; fail or ignore\n        else:\n            raise UserError(f\"Unhandled item type or structure: {item}\")\n\n    flush_assistant_message()\n    return result\n</code></pre>"},{"location":"ref/models/chatcmpl_helpers/","title":"<code>Chatcmpl Helpers</code>","text":""},{"location":"ref/models/chatcmpl_stream_handler/","title":"<code>Chatcmpl Stream Handler</code>","text":""},{"location":"ref/models/chatcmpl_stream_handler/#agents.models.chatcmpl_stream_handler.ChatCmplStreamHandler","title":"ChatCmplStreamHandler","text":"Source code in <code>src/agents/models/chatcmpl_stream_handler.py</code> <pre><code>class ChatCmplStreamHandler:\n    @classmethod\n    async def handle_stream(\n        cls,\n        response: Response,\n        stream: AsyncStream[ChatCompletionChunk],\n        model: str | None = None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        \"\"\"\n        Handle a streaming chat completion response and yield response events.\n\n        Args:\n            response: The initial Response object to populate with streamed data\n            stream: The async stream of chat completion chunks from the model\n            model: The source model that is generating this stream. Used to handle\n                provider-specific stream processing.\n        \"\"\"\n        usage: CompletionUsage | None = None\n        state = StreamingState()\n        sequence_number = SequenceNumber()\n        async for chunk in stream:\n            if not state.started:\n                state.started = True\n                yield ResponseCreatedEvent(\n                    response=response,\n                    type=\"response.created\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n\n            # This is always set by the OpenAI API, but not by others e.g. LiteLLM\n            # Only update when chunk has usage data (not always in the last chunk)\n            if hasattr(chunk, \"usage\") and chunk.usage is not None:\n                usage = chunk.usage\n\n            if not chunk.choices or not chunk.choices[0].delta:\n                continue\n\n            # Build provider_data for non-OpenAI Responses API endpoints format\n            if model:\n                state.provider_data[\"model\"] = model\n            elif hasattr(chunk, \"model\") and chunk.model:\n                state.provider_data[\"model\"] = chunk.model\n\n            if hasattr(chunk, \"id\") and chunk.id:\n                state.provider_data[\"response_id\"] = chunk.id\n\n            delta = chunk.choices[0].delta\n            choice_logprobs = chunk.choices[0].logprobs\n\n            # Handle thinking blocks from Anthropic (for preserving signatures)\n            if hasattr(delta, \"thinking_blocks\") and delta.thinking_blocks:\n                for block in delta.thinking_blocks:\n                    if isinstance(block, dict):\n                        # Accumulate thinking text\n                        thinking_text = block.get(\"thinking\", \"\")\n                        if thinking_text:\n                            state.thinking_text += thinking_text\n                        # Store signature if present\n                        signature = block.get(\"signature\")\n                        if signature:\n                            state.thinking_signature = signature\n\n            # Handle reasoning content for reasoning summaries\n            if hasattr(delta, \"reasoning_content\"):\n                reasoning_content = delta.reasoning_content\n                if reasoning_content and not state.reasoning_content_index_and_output:\n                    reasoning_item = ResponseReasoningItem(\n                        id=FAKE_RESPONSES_ID,\n                        summary=[Summary(text=\"\", type=\"summary_text\")],\n                        type=\"reasoning\",\n                    )\n                    if state.provider_data:\n                        reasoning_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                    state.reasoning_content_index_and_output = (0, reasoning_item)\n                    yield ResponseOutputItemAddedEvent(\n                        item=reasoning_item,\n                        output_index=0,\n                        type=\"response.output_item.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n\n                    yield ResponseReasoningSummaryPartAddedEvent(\n                        item_id=FAKE_RESPONSES_ID,\n                        output_index=0,\n                        summary_index=0,\n                        part=AddedEventPart(text=\"\", type=\"summary_text\"),\n                        type=\"response.reasoning_summary_part.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n\n                if reasoning_content and state.reasoning_content_index_and_output:\n                    # Ensure summary list has at least one element\n                    if not state.reasoning_content_index_and_output[1].summary:\n                        state.reasoning_content_index_and_output[1].summary = [\n                            Summary(text=\"\", type=\"summary_text\")\n                        ]\n\n                    yield ResponseReasoningSummaryTextDeltaEvent(\n                        delta=reasoning_content,\n                        item_id=FAKE_RESPONSES_ID,\n                        output_index=0,\n                        summary_index=0,\n                        type=\"response.reasoning_summary_text.delta\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n\n                    # Create a new summary with updated text\n                    current_content = state.reasoning_content_index_and_output[1].summary[0]\n                    updated_text = current_content.text + reasoning_content\n                    new_content = Summary(text=updated_text, type=\"summary_text\")\n                    state.reasoning_content_index_and_output[1].summary[0] = new_content\n\n            # Handle reasoning content from 3rd party platforms\n            if hasattr(delta, \"reasoning\"):\n                reasoning_text = delta.reasoning\n                if reasoning_text and not state.reasoning_content_index_and_output:\n                    reasoning_item = ResponseReasoningItem(\n                        id=FAKE_RESPONSES_ID,\n                        summary=[],\n                        content=[Content(text=\"\", type=\"reasoning_text\")],\n                        type=\"reasoning\",\n                    )\n                    if state.provider_data:\n                        reasoning_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                    state.reasoning_content_index_and_output = (0, reasoning_item)\n                    yield ResponseOutputItemAddedEvent(\n                        item=reasoning_item,\n                        output_index=0,\n                        type=\"response.output_item.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n\n                if reasoning_text and state.reasoning_content_index_and_output:\n                    yield ResponseReasoningTextDeltaEvent(\n                        delta=reasoning_text,\n                        item_id=FAKE_RESPONSES_ID,\n                        output_index=0,\n                        content_index=0,\n                        type=\"response.reasoning_text.delta\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n\n                    # Create a new summary with updated text\n                    if not state.reasoning_content_index_and_output[1].content:\n                        state.reasoning_content_index_and_output[1].content = [\n                            Content(text=\"\", type=\"reasoning_text\")\n                        ]\n                    current_text = state.reasoning_content_index_and_output[1].content[0]\n                    updated_text = current_text.text + reasoning_text\n                    new_text_content = Content(text=updated_text, type=\"reasoning_text\")\n                    state.reasoning_content_index_and_output[1].content[0] = new_text_content\n\n            # Handle regular content\n            if delta.content is not None:\n                if not state.text_content_index_and_output:\n                    content_index = 0\n                    if state.reasoning_content_index_and_output:\n                        content_index += 1\n                    if state.refusal_content_index_and_output:\n                        content_index += 1\n\n                    state.text_content_index_and_output = (\n                        content_index,\n                        ResponseOutputText(\n                            text=\"\",\n                            type=\"output_text\",\n                            annotations=[],\n                            logprobs=[],\n                        ),\n                    )\n                    # Start a new assistant message stream\n                    assistant_item = ResponseOutputMessage(\n                        id=FAKE_RESPONSES_ID,\n                        content=[],\n                        role=\"assistant\",\n                        type=\"message\",\n                        status=\"in_progress\",\n                    )\n                    if state.provider_data:\n                        assistant_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                    # Notify consumers of the start of a new output message + first content part\n                    yield ResponseOutputItemAddedEvent(\n                        item=assistant_item,\n                        output_index=state.reasoning_content_index_and_output\n                        is not None,  # fixed 0 -&gt; 0 or 1\n                        type=\"response.output_item.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n                    yield ResponseContentPartAddedEvent(\n                        content_index=state.text_content_index_and_output[0],\n                        item_id=FAKE_RESPONSES_ID,\n                        output_index=state.reasoning_content_index_and_output\n                        is not None,  # fixed 0 -&gt; 0 or 1\n                        part=ResponseOutputText(\n                            text=\"\",\n                            type=\"output_text\",\n                            annotations=[],\n                            logprobs=[],\n                        ),\n                        type=\"response.content_part.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n                delta_logprobs = (\n                    ChatCmplHelpers.convert_logprobs_for_text_delta(\n                        choice_logprobs.content if choice_logprobs else None\n                    )\n                    or []\n                )\n                output_logprobs = ChatCmplHelpers.convert_logprobs_for_output_text(\n                    choice_logprobs.content if choice_logprobs else None\n                )\n                # Emit the delta for this segment of content\n                yield ResponseTextDeltaEvent(\n                    content_index=state.text_content_index_and_output[0],\n                    delta=delta.content,\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=state.reasoning_content_index_and_output\n                    is not None,  # fixed 0 -&gt; 0 or 1\n                    type=\"response.output_text.delta\",\n                    sequence_number=sequence_number.get_and_increment(),\n                    logprobs=delta_logprobs,\n                )\n                # Accumulate the text into the response part\n                state.text_content_index_and_output[1].text += delta.content\n                if output_logprobs:\n                    existing_logprobs = state.text_content_index_and_output[1].logprobs or []\n                    state.text_content_index_and_output[1].logprobs = (\n                        existing_logprobs + output_logprobs\n                    )\n\n            # Handle refusals (model declines to answer)\n            # This is always set by the OpenAI API, but not by others e.g. LiteLLM\n            if hasattr(delta, \"refusal\") and delta.refusal:\n                if not state.refusal_content_index_and_output:\n                    refusal_index = 0\n                    if state.reasoning_content_index_and_output:\n                        refusal_index += 1\n                    if state.text_content_index_and_output:\n                        refusal_index += 1\n\n                    state.refusal_content_index_and_output = (\n                        refusal_index,\n                        ResponseOutputRefusal(refusal=\"\", type=\"refusal\"),\n                    )\n                    # Start a new assistant message if one doesn't exist yet (in-progress)\n                    assistant_item = ResponseOutputMessage(\n                        id=FAKE_RESPONSES_ID,\n                        content=[],\n                        role=\"assistant\",\n                        type=\"message\",\n                        status=\"in_progress\",\n                    )\n                    if state.provider_data:\n                        assistant_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                    # Notify downstream that assistant message + first content part are starting\n                    yield ResponseOutputItemAddedEvent(\n                        item=assistant_item,\n                        output_index=state.reasoning_content_index_and_output\n                        is not None,  # fixed 0 -&gt; 0 or 1\n                        type=\"response.output_item.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n                    yield ResponseContentPartAddedEvent(\n                        content_index=state.refusal_content_index_and_output[0],\n                        item_id=FAKE_RESPONSES_ID,\n                        output_index=(1 if state.reasoning_content_index_and_output else 0),\n                        part=ResponseOutputRefusal(\n                            refusal=\"\",\n                            type=\"refusal\",\n                        ),\n                        type=\"response.content_part.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n                # Emit the delta for this segment of refusal\n                yield ResponseRefusalDeltaEvent(\n                    content_index=state.refusal_content_index_and_output[0],\n                    delta=delta.refusal,\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=state.reasoning_content_index_and_output\n                    is not None,  # fixed 0 -&gt; 0 or 1\n                    type=\"response.refusal.delta\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n                # Accumulate the refusal string in the output part\n                state.refusal_content_index_and_output[1].refusal += delta.refusal\n\n            # Handle tool calls with real-time streaming support\n            if delta.tool_calls:\n                for tc_delta in delta.tool_calls:\n                    if tc_delta.index not in state.function_calls:\n                        state.function_calls[tc_delta.index] = ResponseFunctionToolCall(\n                            id=FAKE_RESPONSES_ID,\n                            arguments=\"\",\n                            name=\"\",\n                            type=\"function_call\",\n                            call_id=\"\",\n                        )\n                        state.function_call_streaming[tc_delta.index] = False\n\n                    tc_function = tc_delta.function\n\n                    # Accumulate arguments as they come in\n                    state.function_calls[tc_delta.index].arguments += (\n                        tc_function.arguments if tc_function else \"\"\n                    ) or \"\"\n\n                    # Set function name directly (it's correct from the first function call chunk)\n                    if tc_function and tc_function.name:\n                        state.function_calls[tc_delta.index].name = tc_function.name\n\n                    if tc_delta.id:\n                        # Clean up litellm's addition of __thought__ suffix to tool_call.id for\n                        # Gemini models. See: https://github.com/BerriAI/litellm/pull/16895\n                        # This suffix is redundant since we can get thought_signature from\n                        # provider_specific_fields, and this hack causes validation errors when\n                        # cross-model passing to other models.\n                        tool_call_id = tc_delta.id\n                        if model and \"gemini\" in model.lower() and \"__thought__\" in tool_call_id:\n                            tool_call_id = tool_call_id.split(\"__thought__\")[0]\n\n                        state.function_calls[tc_delta.index].call_id = tool_call_id\n\n                    # Initialize provider_data for this function call from state.provider_data\n                    if not hasattr(state.function_calls[tc_delta.index], \"provider_data\"):\n                        if state.provider_data:\n                            state.function_calls[\n                                tc_delta.index\n                            ].provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n\n                    # Capture provider_specific_fields data from LiteLLM\n                    if (\n                        hasattr(tc_delta, \"provider_specific_fields\")\n                        and tc_delta.provider_specific_fields\n                    ):\n                        # Handle Gemini thought_signatures\n                        if model and \"gemini\" in model.lower():\n                            provider_specific_fields = tc_delta.provider_specific_fields\n                            if isinstance(provider_specific_fields, dict):\n                                thought_sig = provider_specific_fields.get(\"thought_signature\")\n                                if thought_sig:\n                                    # Start with state.provider_data, then add thought_signature\n                                    func_provider_data = (\n                                        state.provider_data.copy() if state.provider_data else {}\n                                    )\n                                    func_provider_data[\"thought_signature\"] = thought_sig\n                                    state.function_calls[\n                                        tc_delta.index\n                                    ].provider_data = func_provider_data  # type: ignore[attr-defined]\n\n                    # Capture extra_content data from Google's chatcmpl endpoint\n                    if hasattr(tc_delta, \"extra_content\") and tc_delta.extra_content:\n                        extra_content = tc_delta.extra_content\n                        if isinstance(extra_content, dict):\n                            google_fields = extra_content.get(\"google\")\n                            if google_fields and isinstance(google_fields, dict):\n                                thought_sig = google_fields.get(\"thought_signature\")\n                                if thought_sig:\n                                    # Start with state.provider_data, then add thought_signature\n                                    func_provider_data = (\n                                        state.provider_data.copy() if state.provider_data else {}\n                                    )\n                                    func_provider_data[\"thought_signature\"] = thought_sig\n                                    state.function_calls[\n                                        tc_delta.index\n                                    ].provider_data = func_provider_data  # type: ignore[attr-defined]\n\n                    function_call = state.function_calls[tc_delta.index]\n\n                    # Start streaming as soon as we have function name and call_id\n                    if (\n                        not state.function_call_streaming[tc_delta.index]\n                        and function_call.name\n                        and function_call.call_id\n                    ):\n                        # Calculate the output index for this function call\n                        function_call_starting_index = 0\n                        if state.reasoning_content_index_and_output:\n                            function_call_starting_index += 1\n                        if state.text_content_index_and_output:\n                            function_call_starting_index += 1\n                        if state.refusal_content_index_and_output:\n                            function_call_starting_index += 1\n\n                        # Add offset for already started function calls\n                        function_call_starting_index += sum(\n                            1 for streaming in state.function_call_streaming.values() if streaming\n                        )\n\n                        # Mark this function call as streaming and store its output index\n                        state.function_call_streaming[tc_delta.index] = True\n                        state.function_call_output_idx[tc_delta.index] = (\n                            function_call_starting_index\n                        )\n\n                        # Send initial function call added event\n                        func_call_item = ResponseFunctionToolCall(\n                            id=FAKE_RESPONSES_ID,\n                            call_id=function_call.call_id,\n                            arguments=\"\",  # Start with empty arguments\n                            name=function_call.name,\n                            type=\"function_call\",\n                        )\n                        # Merge provider_data from state and function_call (e.g. thought_signature)\n                        if state.provider_data or (\n                            hasattr(function_call, \"provider_data\") and function_call.provider_data\n                        ):\n                            merged_provider_data = (\n                                state.provider_data.copy() if state.provider_data else {}\n                            )\n                            if (\n                                hasattr(function_call, \"provider_data\")\n                                and function_call.provider_data\n                            ):\n                                merged_provider_data.update(function_call.provider_data)\n                            func_call_item.provider_data = merged_provider_data  # type: ignore[attr-defined]\n                        yield ResponseOutputItemAddedEvent(\n                            item=func_call_item,\n                            output_index=function_call_starting_index,\n                            type=\"response.output_item.added\",\n                            sequence_number=sequence_number.get_and_increment(),\n                        )\n\n                    # Stream arguments if we've started streaming this function call\n                    if (\n                        state.function_call_streaming.get(tc_delta.index, False)\n                        and tc_function\n                        and tc_function.arguments\n                    ):\n                        output_index = state.function_call_output_idx[tc_delta.index]\n                        yield ResponseFunctionCallArgumentsDeltaEvent(\n                            delta=tc_function.arguments,\n                            item_id=FAKE_RESPONSES_ID,\n                            output_index=output_index,\n                            type=\"response.function_call_arguments.delta\",\n                            sequence_number=sequence_number.get_and_increment(),\n                        )\n\n        if state.reasoning_content_index_and_output:\n            if (\n                state.reasoning_content_index_and_output[1].summary\n                and len(state.reasoning_content_index_and_output[1].summary) &gt; 0\n            ):\n                yield ResponseReasoningSummaryPartDoneEvent(\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=0,\n                    summary_index=0,\n                    part=DoneEventPart(\n                        text=state.reasoning_content_index_and_output[1].summary[0].text,\n                        type=\"summary_text\",\n                    ),\n                    type=\"response.reasoning_summary_part.done\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n            elif state.reasoning_content_index_and_output[1].content is not None:\n                yield ResponseReasoningTextDoneEvent(\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=0,\n                    content_index=0,\n                    text=state.reasoning_content_index_and_output[1].content[0].text,\n                    type=\"response.reasoning_text.done\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n            yield ResponseOutputItemDoneEvent(\n                item=state.reasoning_content_index_and_output[1],\n                output_index=0,\n                type=\"response.output_item.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n\n        function_call_starting_index = 0\n        if state.reasoning_content_index_and_output:\n            function_call_starting_index += 1\n\n        if state.text_content_index_and_output:\n            function_call_starting_index += 1\n            # Send end event for this content part\n            yield ResponseContentPartDoneEvent(\n                content_index=state.text_content_index_and_output[0],\n                item_id=FAKE_RESPONSES_ID,\n                output_index=state.reasoning_content_index_and_output\n                is not None,  # fixed 0 -&gt; 0 or 1\n                part=state.text_content_index_and_output[1],\n                type=\"response.content_part.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n\n        if state.refusal_content_index_and_output:\n            function_call_starting_index += 1\n            # Send end event for this content part\n            yield ResponseContentPartDoneEvent(\n                content_index=state.refusal_content_index_and_output[0],\n                item_id=FAKE_RESPONSES_ID,\n                output_index=state.reasoning_content_index_and_output\n                is not None,  # fixed 0 -&gt; 0 or 1\n                part=state.refusal_content_index_and_output[1],\n                type=\"response.content_part.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n\n        # Send completion events for function calls\n        for index, function_call in state.function_calls.items():\n            if state.function_call_streaming.get(index, False):\n                # Function call was streamed, just send the completion event\n                output_index = state.function_call_output_idx[index]\n\n                # Build function call kwargs, include provider_data if present\n                func_call_kwargs: dict[str, Any] = {\n                    \"id\": FAKE_RESPONSES_ID,\n                    \"call_id\": function_call.call_id,\n                    \"arguments\": function_call.arguments,\n                    \"name\": function_call.name,\n                    \"type\": \"function_call\",\n                }\n\n                # Merge provider_data from state and function_call (e.g. thought_signature)\n                if state.provider_data or (\n                    hasattr(function_call, \"provider_data\") and function_call.provider_data\n                ):\n                    merged_provider_data = state.provider_data.copy() if state.provider_data else {}\n                    if hasattr(function_call, \"provider_data\") and function_call.provider_data:\n                        merged_provider_data.update(function_call.provider_data)\n                    func_call_kwargs[\"provider_data\"] = merged_provider_data\n\n                yield ResponseOutputItemDoneEvent(\n                    item=ResponseFunctionToolCall(**func_call_kwargs),\n                    output_index=output_index,\n                    type=\"response.output_item.done\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n            else:\n                # Function call was not streamed (fallback to old behavior)\n                # This handles edge cases where function name never arrived\n                fallback_starting_index = 0\n                if state.reasoning_content_index_and_output:\n                    fallback_starting_index += 1\n                if state.text_content_index_and_output:\n                    fallback_starting_index += 1\n                if state.refusal_content_index_and_output:\n                    fallback_starting_index += 1\n\n                # Add offset for already started function calls\n                fallback_starting_index += sum(\n                    1 for streaming in state.function_call_streaming.values() if streaming\n                )\n\n                # Build function call kwargs, include provider_data if present\n                fallback_func_call_kwargs: dict[str, Any] = {\n                    \"id\": FAKE_RESPONSES_ID,\n                    \"call_id\": function_call.call_id,\n                    \"arguments\": function_call.arguments,\n                    \"name\": function_call.name,\n                    \"type\": \"function_call\",\n                }\n\n                # Merge provider_data from state and function_call (e.g. thought_signature)\n                if state.provider_data or (\n                    hasattr(function_call, \"provider_data\") and function_call.provider_data\n                ):\n                    merged_provider_data = state.provider_data.copy() if state.provider_data else {}\n                    if hasattr(function_call, \"provider_data\") and function_call.provider_data:\n                        merged_provider_data.update(function_call.provider_data)\n                    fallback_func_call_kwargs[\"provider_data\"] = merged_provider_data\n\n                # Send all events at once (backward compatibility)\n                yield ResponseOutputItemAddedEvent(\n                    item=ResponseFunctionToolCall(**fallback_func_call_kwargs),\n                    output_index=fallback_starting_index,\n                    type=\"response.output_item.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n                yield ResponseFunctionCallArgumentsDeltaEvent(\n                    delta=function_call.arguments,\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=fallback_starting_index,\n                    type=\"response.function_call_arguments.delta\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n                yield ResponseOutputItemDoneEvent(\n                    item=ResponseFunctionToolCall(**fallback_func_call_kwargs),\n                    output_index=fallback_starting_index,\n                    type=\"response.output_item.done\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n\n        # Finally, send the Response completed event\n        outputs: list[ResponseOutputItem] = []\n\n        # include Reasoning item if it exists\n        if state.reasoning_content_index_and_output:\n            reasoning_item = state.reasoning_content_index_and_output[1]\n            # Store thinking text in content and signature in encrypted_content\n            if state.thinking_text:\n                # Add thinking text as a Content object\n                if not reasoning_item.content:\n                    reasoning_item.content = []\n                reasoning_item.content.append(\n                    Content(text=state.thinking_text, type=\"reasoning_text\")\n                )\n            # Store signature in encrypted_content\n            if state.thinking_signature:\n                reasoning_item.encrypted_content = state.thinking_signature\n            outputs.append(reasoning_item)\n\n        # include text or refusal content if they exist\n        if state.text_content_index_and_output or state.refusal_content_index_and_output:\n            assistant_msg = ResponseOutputMessage(\n                id=FAKE_RESPONSES_ID,\n                content=[],\n                role=\"assistant\",\n                type=\"message\",\n                status=\"completed\",\n            )\n            if state.provider_data:\n                assistant_msg.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n            if state.text_content_index_and_output:\n                assistant_msg.content.append(state.text_content_index_and_output[1])\n            if state.refusal_content_index_and_output:\n                assistant_msg.content.append(state.refusal_content_index_and_output[1])\n            outputs.append(assistant_msg)\n\n            # send a ResponseOutputItemDone for the assistant message\n            yield ResponseOutputItemDoneEvent(\n                item=assistant_msg,\n                output_index=state.reasoning_content_index_and_output\n                is not None,  # fixed 0 -&gt; 0 or 1\n                type=\"response.output_item.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n\n        for function_call in state.function_calls.values():\n            outputs.append(function_call)\n\n        final_response = response.model_copy()\n        final_response.output = outputs\n\n        final_response.usage = (\n            ResponseUsage(\n                input_tokens=usage.prompt_tokens or 0,\n                output_tokens=usage.completion_tokens or 0,\n                total_tokens=usage.total_tokens or 0,\n                output_tokens_details=OutputTokensDetails(\n                    reasoning_tokens=usage.completion_tokens_details.reasoning_tokens\n                    if usage.completion_tokens_details\n                    and usage.completion_tokens_details.reasoning_tokens\n                    else 0\n                ),\n                input_tokens_details=InputTokensDetails(\n                    cached_tokens=usage.prompt_tokens_details.cached_tokens\n                    if usage.prompt_tokens_details and usage.prompt_tokens_details.cached_tokens\n                    else 0\n                ),\n            )\n            if usage\n            else None\n        )\n\n        yield ResponseCompletedEvent(\n            response=final_response,\n            type=\"response.completed\",\n            sequence_number=sequence_number.get_and_increment(),\n        )\n</code></pre>"},{"location":"ref/models/chatcmpl_stream_handler/#agents.models.chatcmpl_stream_handler.ChatCmplStreamHandler.handle_stream","title":"handle_stream  <code>async</code> <code>classmethod</code>","text":"<pre><code>handle_stream(\n    response: Response,\n    stream: AsyncStream[ChatCompletionChunk],\n    model: str | None = None,\n) -&gt; AsyncIterator[TResponseStreamEvent]\n</code></pre> <p>Handle a streaming chat completion response and yield response events.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>The initial Response object to populate with streamed data</p> required <code>stream</code> <code>AsyncStream[ChatCompletionChunk]</code> <p>The async stream of chat completion chunks from the model</p> required <code>model</code> <code>str | None</code> <p>The source model that is generating this stream. Used to handle provider-specific stream processing.</p> <code>None</code> Source code in <code>src/agents/models/chatcmpl_stream_handler.py</code> <pre><code>@classmethod\nasync def handle_stream(\n    cls,\n    response: Response,\n    stream: AsyncStream[ChatCompletionChunk],\n    model: str | None = None,\n) -&gt; AsyncIterator[TResponseStreamEvent]:\n    \"\"\"\n    Handle a streaming chat completion response and yield response events.\n\n    Args:\n        response: The initial Response object to populate with streamed data\n        stream: The async stream of chat completion chunks from the model\n        model: The source model that is generating this stream. Used to handle\n            provider-specific stream processing.\n    \"\"\"\n    usage: CompletionUsage | None = None\n    state = StreamingState()\n    sequence_number = SequenceNumber()\n    async for chunk in stream:\n        if not state.started:\n            state.started = True\n            yield ResponseCreatedEvent(\n                response=response,\n                type=\"response.created\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n\n        # This is always set by the OpenAI API, but not by others e.g. LiteLLM\n        # Only update when chunk has usage data (not always in the last chunk)\n        if hasattr(chunk, \"usage\") and chunk.usage is not None:\n            usage = chunk.usage\n\n        if not chunk.choices or not chunk.choices[0].delta:\n            continue\n\n        # Build provider_data for non-OpenAI Responses API endpoints format\n        if model:\n            state.provider_data[\"model\"] = model\n        elif hasattr(chunk, \"model\") and chunk.model:\n            state.provider_data[\"model\"] = chunk.model\n\n        if hasattr(chunk, \"id\") and chunk.id:\n            state.provider_data[\"response_id\"] = chunk.id\n\n        delta = chunk.choices[0].delta\n        choice_logprobs = chunk.choices[0].logprobs\n\n        # Handle thinking blocks from Anthropic (for preserving signatures)\n        if hasattr(delta, \"thinking_blocks\") and delta.thinking_blocks:\n            for block in delta.thinking_blocks:\n                if isinstance(block, dict):\n                    # Accumulate thinking text\n                    thinking_text = block.get(\"thinking\", \"\")\n                    if thinking_text:\n                        state.thinking_text += thinking_text\n                    # Store signature if present\n                    signature = block.get(\"signature\")\n                    if signature:\n                        state.thinking_signature = signature\n\n        # Handle reasoning content for reasoning summaries\n        if hasattr(delta, \"reasoning_content\"):\n            reasoning_content = delta.reasoning_content\n            if reasoning_content and not state.reasoning_content_index_and_output:\n                reasoning_item = ResponseReasoningItem(\n                    id=FAKE_RESPONSES_ID,\n                    summary=[Summary(text=\"\", type=\"summary_text\")],\n                    type=\"reasoning\",\n                )\n                if state.provider_data:\n                    reasoning_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                state.reasoning_content_index_and_output = (0, reasoning_item)\n                yield ResponseOutputItemAddedEvent(\n                    item=reasoning_item,\n                    output_index=0,\n                    type=\"response.output_item.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n\n                yield ResponseReasoningSummaryPartAddedEvent(\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=0,\n                    summary_index=0,\n                    part=AddedEventPart(text=\"\", type=\"summary_text\"),\n                    type=\"response.reasoning_summary_part.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n\n            if reasoning_content and state.reasoning_content_index_and_output:\n                # Ensure summary list has at least one element\n                if not state.reasoning_content_index_and_output[1].summary:\n                    state.reasoning_content_index_and_output[1].summary = [\n                        Summary(text=\"\", type=\"summary_text\")\n                    ]\n\n                yield ResponseReasoningSummaryTextDeltaEvent(\n                    delta=reasoning_content,\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=0,\n                    summary_index=0,\n                    type=\"response.reasoning_summary_text.delta\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n\n                # Create a new summary with updated text\n                current_content = state.reasoning_content_index_and_output[1].summary[0]\n                updated_text = current_content.text + reasoning_content\n                new_content = Summary(text=updated_text, type=\"summary_text\")\n                state.reasoning_content_index_and_output[1].summary[0] = new_content\n\n        # Handle reasoning content from 3rd party platforms\n        if hasattr(delta, \"reasoning\"):\n            reasoning_text = delta.reasoning\n            if reasoning_text and not state.reasoning_content_index_and_output:\n                reasoning_item = ResponseReasoningItem(\n                    id=FAKE_RESPONSES_ID,\n                    summary=[],\n                    content=[Content(text=\"\", type=\"reasoning_text\")],\n                    type=\"reasoning\",\n                )\n                if state.provider_data:\n                    reasoning_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                state.reasoning_content_index_and_output = (0, reasoning_item)\n                yield ResponseOutputItemAddedEvent(\n                    item=reasoning_item,\n                    output_index=0,\n                    type=\"response.output_item.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n\n            if reasoning_text and state.reasoning_content_index_and_output:\n                yield ResponseReasoningTextDeltaEvent(\n                    delta=reasoning_text,\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=0,\n                    content_index=0,\n                    type=\"response.reasoning_text.delta\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n\n                # Create a new summary with updated text\n                if not state.reasoning_content_index_and_output[1].content:\n                    state.reasoning_content_index_and_output[1].content = [\n                        Content(text=\"\", type=\"reasoning_text\")\n                    ]\n                current_text = state.reasoning_content_index_and_output[1].content[0]\n                updated_text = current_text.text + reasoning_text\n                new_text_content = Content(text=updated_text, type=\"reasoning_text\")\n                state.reasoning_content_index_and_output[1].content[0] = new_text_content\n\n        # Handle regular content\n        if delta.content is not None:\n            if not state.text_content_index_and_output:\n                content_index = 0\n                if state.reasoning_content_index_and_output:\n                    content_index += 1\n                if state.refusal_content_index_and_output:\n                    content_index += 1\n\n                state.text_content_index_and_output = (\n                    content_index,\n                    ResponseOutputText(\n                        text=\"\",\n                        type=\"output_text\",\n                        annotations=[],\n                        logprobs=[],\n                    ),\n                )\n                # Start a new assistant message stream\n                assistant_item = ResponseOutputMessage(\n                    id=FAKE_RESPONSES_ID,\n                    content=[],\n                    role=\"assistant\",\n                    type=\"message\",\n                    status=\"in_progress\",\n                )\n                if state.provider_data:\n                    assistant_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                # Notify consumers of the start of a new output message + first content part\n                yield ResponseOutputItemAddedEvent(\n                    item=assistant_item,\n                    output_index=state.reasoning_content_index_and_output\n                    is not None,  # fixed 0 -&gt; 0 or 1\n                    type=\"response.output_item.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n                yield ResponseContentPartAddedEvent(\n                    content_index=state.text_content_index_and_output[0],\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=state.reasoning_content_index_and_output\n                    is not None,  # fixed 0 -&gt; 0 or 1\n                    part=ResponseOutputText(\n                        text=\"\",\n                        type=\"output_text\",\n                        annotations=[],\n                        logprobs=[],\n                    ),\n                    type=\"response.content_part.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n            delta_logprobs = (\n                ChatCmplHelpers.convert_logprobs_for_text_delta(\n                    choice_logprobs.content if choice_logprobs else None\n                )\n                or []\n            )\n            output_logprobs = ChatCmplHelpers.convert_logprobs_for_output_text(\n                choice_logprobs.content if choice_logprobs else None\n            )\n            # Emit the delta for this segment of content\n            yield ResponseTextDeltaEvent(\n                content_index=state.text_content_index_and_output[0],\n                delta=delta.content,\n                item_id=FAKE_RESPONSES_ID,\n                output_index=state.reasoning_content_index_and_output\n                is not None,  # fixed 0 -&gt; 0 or 1\n                type=\"response.output_text.delta\",\n                sequence_number=sequence_number.get_and_increment(),\n                logprobs=delta_logprobs,\n            )\n            # Accumulate the text into the response part\n            state.text_content_index_and_output[1].text += delta.content\n            if output_logprobs:\n                existing_logprobs = state.text_content_index_and_output[1].logprobs or []\n                state.text_content_index_and_output[1].logprobs = (\n                    existing_logprobs + output_logprobs\n                )\n\n        # Handle refusals (model declines to answer)\n        # This is always set by the OpenAI API, but not by others e.g. LiteLLM\n        if hasattr(delta, \"refusal\") and delta.refusal:\n            if not state.refusal_content_index_and_output:\n                refusal_index = 0\n                if state.reasoning_content_index_and_output:\n                    refusal_index += 1\n                if state.text_content_index_and_output:\n                    refusal_index += 1\n\n                state.refusal_content_index_and_output = (\n                    refusal_index,\n                    ResponseOutputRefusal(refusal=\"\", type=\"refusal\"),\n                )\n                # Start a new assistant message if one doesn't exist yet (in-progress)\n                assistant_item = ResponseOutputMessage(\n                    id=FAKE_RESPONSES_ID,\n                    content=[],\n                    role=\"assistant\",\n                    type=\"message\",\n                    status=\"in_progress\",\n                )\n                if state.provider_data:\n                    assistant_item.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n                # Notify downstream that assistant message + first content part are starting\n                yield ResponseOutputItemAddedEvent(\n                    item=assistant_item,\n                    output_index=state.reasoning_content_index_and_output\n                    is not None,  # fixed 0 -&gt; 0 or 1\n                    type=\"response.output_item.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n                yield ResponseContentPartAddedEvent(\n                    content_index=state.refusal_content_index_and_output[0],\n                    item_id=FAKE_RESPONSES_ID,\n                    output_index=(1 if state.reasoning_content_index_and_output else 0),\n                    part=ResponseOutputRefusal(\n                        refusal=\"\",\n                        type=\"refusal\",\n                    ),\n                    type=\"response.content_part.added\",\n                    sequence_number=sequence_number.get_and_increment(),\n                )\n            # Emit the delta for this segment of refusal\n            yield ResponseRefusalDeltaEvent(\n                content_index=state.refusal_content_index_and_output[0],\n                delta=delta.refusal,\n                item_id=FAKE_RESPONSES_ID,\n                output_index=state.reasoning_content_index_and_output\n                is not None,  # fixed 0 -&gt; 0 or 1\n                type=\"response.refusal.delta\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n            # Accumulate the refusal string in the output part\n            state.refusal_content_index_and_output[1].refusal += delta.refusal\n\n        # Handle tool calls with real-time streaming support\n        if delta.tool_calls:\n            for tc_delta in delta.tool_calls:\n                if tc_delta.index not in state.function_calls:\n                    state.function_calls[tc_delta.index] = ResponseFunctionToolCall(\n                        id=FAKE_RESPONSES_ID,\n                        arguments=\"\",\n                        name=\"\",\n                        type=\"function_call\",\n                        call_id=\"\",\n                    )\n                    state.function_call_streaming[tc_delta.index] = False\n\n                tc_function = tc_delta.function\n\n                # Accumulate arguments as they come in\n                state.function_calls[tc_delta.index].arguments += (\n                    tc_function.arguments if tc_function else \"\"\n                ) or \"\"\n\n                # Set function name directly (it's correct from the first function call chunk)\n                if tc_function and tc_function.name:\n                    state.function_calls[tc_delta.index].name = tc_function.name\n\n                if tc_delta.id:\n                    # Clean up litellm's addition of __thought__ suffix to tool_call.id for\n                    # Gemini models. See: https://github.com/BerriAI/litellm/pull/16895\n                    # This suffix is redundant since we can get thought_signature from\n                    # provider_specific_fields, and this hack causes validation errors when\n                    # cross-model passing to other models.\n                    tool_call_id = tc_delta.id\n                    if model and \"gemini\" in model.lower() and \"__thought__\" in tool_call_id:\n                        tool_call_id = tool_call_id.split(\"__thought__\")[0]\n\n                    state.function_calls[tc_delta.index].call_id = tool_call_id\n\n                # Initialize provider_data for this function call from state.provider_data\n                if not hasattr(state.function_calls[tc_delta.index], \"provider_data\"):\n                    if state.provider_data:\n                        state.function_calls[\n                            tc_delta.index\n                        ].provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n\n                # Capture provider_specific_fields data from LiteLLM\n                if (\n                    hasattr(tc_delta, \"provider_specific_fields\")\n                    and tc_delta.provider_specific_fields\n                ):\n                    # Handle Gemini thought_signatures\n                    if model and \"gemini\" in model.lower():\n                        provider_specific_fields = tc_delta.provider_specific_fields\n                        if isinstance(provider_specific_fields, dict):\n                            thought_sig = provider_specific_fields.get(\"thought_signature\")\n                            if thought_sig:\n                                # Start with state.provider_data, then add thought_signature\n                                func_provider_data = (\n                                    state.provider_data.copy() if state.provider_data else {}\n                                )\n                                func_provider_data[\"thought_signature\"] = thought_sig\n                                state.function_calls[\n                                    tc_delta.index\n                                ].provider_data = func_provider_data  # type: ignore[attr-defined]\n\n                # Capture extra_content data from Google's chatcmpl endpoint\n                if hasattr(tc_delta, \"extra_content\") and tc_delta.extra_content:\n                    extra_content = tc_delta.extra_content\n                    if isinstance(extra_content, dict):\n                        google_fields = extra_content.get(\"google\")\n                        if google_fields and isinstance(google_fields, dict):\n                            thought_sig = google_fields.get(\"thought_signature\")\n                            if thought_sig:\n                                # Start with state.provider_data, then add thought_signature\n                                func_provider_data = (\n                                    state.provider_data.copy() if state.provider_data else {}\n                                )\n                                func_provider_data[\"thought_signature\"] = thought_sig\n                                state.function_calls[\n                                    tc_delta.index\n                                ].provider_data = func_provider_data  # type: ignore[attr-defined]\n\n                function_call = state.function_calls[tc_delta.index]\n\n                # Start streaming as soon as we have function name and call_id\n                if (\n                    not state.function_call_streaming[tc_delta.index]\n                    and function_call.name\n                    and function_call.call_id\n                ):\n                    # Calculate the output index for this function call\n                    function_call_starting_index = 0\n                    if state.reasoning_content_index_and_output:\n                        function_call_starting_index += 1\n                    if state.text_content_index_and_output:\n                        function_call_starting_index += 1\n                    if state.refusal_content_index_and_output:\n                        function_call_starting_index += 1\n\n                    # Add offset for already started function calls\n                    function_call_starting_index += sum(\n                        1 for streaming in state.function_call_streaming.values() if streaming\n                    )\n\n                    # Mark this function call as streaming and store its output index\n                    state.function_call_streaming[tc_delta.index] = True\n                    state.function_call_output_idx[tc_delta.index] = (\n                        function_call_starting_index\n                    )\n\n                    # Send initial function call added event\n                    func_call_item = ResponseFunctionToolCall(\n                        id=FAKE_RESPONSES_ID,\n                        call_id=function_call.call_id,\n                        arguments=\"\",  # Start with empty arguments\n                        name=function_call.name,\n                        type=\"function_call\",\n                    )\n                    # Merge provider_data from state and function_call (e.g. thought_signature)\n                    if state.provider_data or (\n                        hasattr(function_call, \"provider_data\") and function_call.provider_data\n                    ):\n                        merged_provider_data = (\n                            state.provider_data.copy() if state.provider_data else {}\n                        )\n                        if (\n                            hasattr(function_call, \"provider_data\")\n                            and function_call.provider_data\n                        ):\n                            merged_provider_data.update(function_call.provider_data)\n                        func_call_item.provider_data = merged_provider_data  # type: ignore[attr-defined]\n                    yield ResponseOutputItemAddedEvent(\n                        item=func_call_item,\n                        output_index=function_call_starting_index,\n                        type=\"response.output_item.added\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n\n                # Stream arguments if we've started streaming this function call\n                if (\n                    state.function_call_streaming.get(tc_delta.index, False)\n                    and tc_function\n                    and tc_function.arguments\n                ):\n                    output_index = state.function_call_output_idx[tc_delta.index]\n                    yield ResponseFunctionCallArgumentsDeltaEvent(\n                        delta=tc_function.arguments,\n                        item_id=FAKE_RESPONSES_ID,\n                        output_index=output_index,\n                        type=\"response.function_call_arguments.delta\",\n                        sequence_number=sequence_number.get_and_increment(),\n                    )\n\n    if state.reasoning_content_index_and_output:\n        if (\n            state.reasoning_content_index_and_output[1].summary\n            and len(state.reasoning_content_index_and_output[1].summary) &gt; 0\n        ):\n            yield ResponseReasoningSummaryPartDoneEvent(\n                item_id=FAKE_RESPONSES_ID,\n                output_index=0,\n                summary_index=0,\n                part=DoneEventPart(\n                    text=state.reasoning_content_index_and_output[1].summary[0].text,\n                    type=\"summary_text\",\n                ),\n                type=\"response.reasoning_summary_part.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n        elif state.reasoning_content_index_and_output[1].content is not None:\n            yield ResponseReasoningTextDoneEvent(\n                item_id=FAKE_RESPONSES_ID,\n                output_index=0,\n                content_index=0,\n                text=state.reasoning_content_index_and_output[1].content[0].text,\n                type=\"response.reasoning_text.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n        yield ResponseOutputItemDoneEvent(\n            item=state.reasoning_content_index_and_output[1],\n            output_index=0,\n            type=\"response.output_item.done\",\n            sequence_number=sequence_number.get_and_increment(),\n        )\n\n    function_call_starting_index = 0\n    if state.reasoning_content_index_and_output:\n        function_call_starting_index += 1\n\n    if state.text_content_index_and_output:\n        function_call_starting_index += 1\n        # Send end event for this content part\n        yield ResponseContentPartDoneEvent(\n            content_index=state.text_content_index_and_output[0],\n            item_id=FAKE_RESPONSES_ID,\n            output_index=state.reasoning_content_index_and_output\n            is not None,  # fixed 0 -&gt; 0 or 1\n            part=state.text_content_index_and_output[1],\n            type=\"response.content_part.done\",\n            sequence_number=sequence_number.get_and_increment(),\n        )\n\n    if state.refusal_content_index_and_output:\n        function_call_starting_index += 1\n        # Send end event for this content part\n        yield ResponseContentPartDoneEvent(\n            content_index=state.refusal_content_index_and_output[0],\n            item_id=FAKE_RESPONSES_ID,\n            output_index=state.reasoning_content_index_and_output\n            is not None,  # fixed 0 -&gt; 0 or 1\n            part=state.refusal_content_index_and_output[1],\n            type=\"response.content_part.done\",\n            sequence_number=sequence_number.get_and_increment(),\n        )\n\n    # Send completion events for function calls\n    for index, function_call in state.function_calls.items():\n        if state.function_call_streaming.get(index, False):\n            # Function call was streamed, just send the completion event\n            output_index = state.function_call_output_idx[index]\n\n            # Build function call kwargs, include provider_data if present\n            func_call_kwargs: dict[str, Any] = {\n                \"id\": FAKE_RESPONSES_ID,\n                \"call_id\": function_call.call_id,\n                \"arguments\": function_call.arguments,\n                \"name\": function_call.name,\n                \"type\": \"function_call\",\n            }\n\n            # Merge provider_data from state and function_call (e.g. thought_signature)\n            if state.provider_data or (\n                hasattr(function_call, \"provider_data\") and function_call.provider_data\n            ):\n                merged_provider_data = state.provider_data.copy() if state.provider_data else {}\n                if hasattr(function_call, \"provider_data\") and function_call.provider_data:\n                    merged_provider_data.update(function_call.provider_data)\n                func_call_kwargs[\"provider_data\"] = merged_provider_data\n\n            yield ResponseOutputItemDoneEvent(\n                item=ResponseFunctionToolCall(**func_call_kwargs),\n                output_index=output_index,\n                type=\"response.output_item.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n        else:\n            # Function call was not streamed (fallback to old behavior)\n            # This handles edge cases where function name never arrived\n            fallback_starting_index = 0\n            if state.reasoning_content_index_and_output:\n                fallback_starting_index += 1\n            if state.text_content_index_and_output:\n                fallback_starting_index += 1\n            if state.refusal_content_index_and_output:\n                fallback_starting_index += 1\n\n            # Add offset for already started function calls\n            fallback_starting_index += sum(\n                1 for streaming in state.function_call_streaming.values() if streaming\n            )\n\n            # Build function call kwargs, include provider_data if present\n            fallback_func_call_kwargs: dict[str, Any] = {\n                \"id\": FAKE_RESPONSES_ID,\n                \"call_id\": function_call.call_id,\n                \"arguments\": function_call.arguments,\n                \"name\": function_call.name,\n                \"type\": \"function_call\",\n            }\n\n            # Merge provider_data from state and function_call (e.g. thought_signature)\n            if state.provider_data or (\n                hasattr(function_call, \"provider_data\") and function_call.provider_data\n            ):\n                merged_provider_data = state.provider_data.copy() if state.provider_data else {}\n                if hasattr(function_call, \"provider_data\") and function_call.provider_data:\n                    merged_provider_data.update(function_call.provider_data)\n                fallback_func_call_kwargs[\"provider_data\"] = merged_provider_data\n\n            # Send all events at once (backward compatibility)\n            yield ResponseOutputItemAddedEvent(\n                item=ResponseFunctionToolCall(**fallback_func_call_kwargs),\n                output_index=fallback_starting_index,\n                type=\"response.output_item.added\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n            yield ResponseFunctionCallArgumentsDeltaEvent(\n                delta=function_call.arguments,\n                item_id=FAKE_RESPONSES_ID,\n                output_index=fallback_starting_index,\n                type=\"response.function_call_arguments.delta\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n            yield ResponseOutputItemDoneEvent(\n                item=ResponseFunctionToolCall(**fallback_func_call_kwargs),\n                output_index=fallback_starting_index,\n                type=\"response.output_item.done\",\n                sequence_number=sequence_number.get_and_increment(),\n            )\n\n    # Finally, send the Response completed event\n    outputs: list[ResponseOutputItem] = []\n\n    # include Reasoning item if it exists\n    if state.reasoning_content_index_and_output:\n        reasoning_item = state.reasoning_content_index_and_output[1]\n        # Store thinking text in content and signature in encrypted_content\n        if state.thinking_text:\n            # Add thinking text as a Content object\n            if not reasoning_item.content:\n                reasoning_item.content = []\n            reasoning_item.content.append(\n                Content(text=state.thinking_text, type=\"reasoning_text\")\n            )\n        # Store signature in encrypted_content\n        if state.thinking_signature:\n            reasoning_item.encrypted_content = state.thinking_signature\n        outputs.append(reasoning_item)\n\n    # include text or refusal content if they exist\n    if state.text_content_index_and_output or state.refusal_content_index_and_output:\n        assistant_msg = ResponseOutputMessage(\n            id=FAKE_RESPONSES_ID,\n            content=[],\n            role=\"assistant\",\n            type=\"message\",\n            status=\"completed\",\n        )\n        if state.provider_data:\n            assistant_msg.provider_data = state.provider_data.copy()  # type: ignore[attr-defined]\n        if state.text_content_index_and_output:\n            assistant_msg.content.append(state.text_content_index_and_output[1])\n        if state.refusal_content_index_and_output:\n            assistant_msg.content.append(state.refusal_content_index_and_output[1])\n        outputs.append(assistant_msg)\n\n        # send a ResponseOutputItemDone for the assistant message\n        yield ResponseOutputItemDoneEvent(\n            item=assistant_msg,\n            output_index=state.reasoning_content_index_and_output\n            is not None,  # fixed 0 -&gt; 0 or 1\n            type=\"response.output_item.done\",\n            sequence_number=sequence_number.get_and_increment(),\n        )\n\n    for function_call in state.function_calls.values():\n        outputs.append(function_call)\n\n    final_response = response.model_copy()\n    final_response.output = outputs\n\n    final_response.usage = (\n        ResponseUsage(\n            input_tokens=usage.prompt_tokens or 0,\n            output_tokens=usage.completion_tokens or 0,\n            total_tokens=usage.total_tokens or 0,\n            output_tokens_details=OutputTokensDetails(\n                reasoning_tokens=usage.completion_tokens_details.reasoning_tokens\n                if usage.completion_tokens_details\n                and usage.completion_tokens_details.reasoning_tokens\n                else 0\n            ),\n            input_tokens_details=InputTokensDetails(\n                cached_tokens=usage.prompt_tokens_details.cached_tokens\n                if usage.prompt_tokens_details and usage.prompt_tokens_details.cached_tokens\n                else 0\n            ),\n        )\n        if usage\n        else None\n    )\n\n    yield ResponseCompletedEvent(\n        response=final_response,\n        type=\"response.completed\",\n        sequence_number=sequence_number.get_and_increment(),\n    )\n</code></pre>"},{"location":"ref/models/default_models/","title":"<code>Default Models</code>","text":""},{"location":"ref/models/default_models/#agents.models.default_models.gpt_5_reasoning_settings_required","title":"gpt_5_reasoning_settings_required","text":"<pre><code>gpt_5_reasoning_settings_required(model_name: str) -&gt; bool\n</code></pre> <p>Returns True if the model name is a GPT-5 model and reasoning settings are required.</p> Source code in <code>src/agents/models/default_models.py</code> <pre><code>def gpt_5_reasoning_settings_required(model_name: str) -&gt; bool:\n    \"\"\"\n    Returns True if the model name is a GPT-5 model and reasoning settings are required.\n    \"\"\"\n    if model_name.startswith(\"gpt-5-chat\"):\n        # gpt-5-chat-latest does not require reasoning settings\n        return False\n    # matches any of gpt-5 models\n    return model_name.startswith(\"gpt-5\")\n</code></pre>"},{"location":"ref/models/default_models/#agents.models.default_models.is_gpt_5_default","title":"is_gpt_5_default","text":"<pre><code>is_gpt_5_default() -&gt; bool\n</code></pre> <p>Returns True if the default model is a GPT-5 model. This is used to determine if the default model settings are compatible with GPT-5 models. If the default model is not a GPT-5 model, the model settings are compatible with other models.</p> Source code in <code>src/agents/models/default_models.py</code> <pre><code>def is_gpt_5_default() -&gt; bool:\n    \"\"\"\n    Returns True if the default model is a GPT-5 model.\n    This is used to determine if the default model settings are compatible with GPT-5 models.\n    If the default model is not a GPT-5 model, the model settings are compatible with other models.\n    \"\"\"\n    return gpt_5_reasoning_settings_required(get_default_model())\n</code></pre>"},{"location":"ref/models/default_models/#agents.models.default_models.get_default_model","title":"get_default_model","text":"<pre><code>get_default_model() -&gt; str\n</code></pre> <p>Returns the default model name.</p> Source code in <code>src/agents/models/default_models.py</code> <pre><code>def get_default_model() -&gt; str:\n    \"\"\"\n    Returns the default model name.\n    \"\"\"\n    return os.getenv(OPENAI_DEFAULT_MODEL_ENV_VARIABLE_NAME, \"gpt-4.1\").lower()\n</code></pre>"},{"location":"ref/models/default_models/#agents.models.default_models.get_default_model_settings","title":"get_default_model_settings","text":"<pre><code>get_default_model_settings(\n    model: Optional[str] = None,\n) -&gt; ModelSettings\n</code></pre> <p>Returns the default model settings. If the default model is a GPT-5 model, returns the GPT-5 default model settings. Otherwise, returns the legacy default model settings.</p> Source code in <code>src/agents/models/default_models.py</code> <pre><code>def get_default_model_settings(model: Optional[str] = None) -&gt; ModelSettings:\n    \"\"\"\n    Returns the default model settings.\n    If the default model is a GPT-5 model, returns the GPT-5 default model settings.\n    Otherwise, returns the legacy default model settings.\n    \"\"\"\n    _model = model if model is not None else get_default_model()\n    if gpt_5_reasoning_settings_required(_model):\n        if _is_gpt_5_none_effort_model(_model):\n            return copy.deepcopy(_GPT_5_NONE_DEFAULT_MODEL_SETTINGS)\n        return copy.deepcopy(_GPT_5_DEFAULT_MODEL_SETTINGS)\n    return ModelSettings()\n</code></pre>"},{"location":"ref/models/fake_id/","title":"<code>Fake Id</code>","text":""},{"location":"ref/models/fake_id/#agents.models.fake_id.FAKE_RESPONSES_ID","title":"FAKE_RESPONSES_ID  <code>module-attribute</code>","text":"<pre><code>FAKE_RESPONSES_ID = '__fake_id__'\n</code></pre> <p>This is a placeholder ID used to fill in the <code>id</code> field in Responses API related objects. It's useful when you're creating Responses objects from non-Responses APIs, e.g. the OpenAI Chat Completions API or other LLM providers.</p>"},{"location":"ref/models/interface/","title":"<code>Model interface</code>","text":""},{"location":"ref/models/interface/#agents.models.interface.ModelTracing","title":"ModelTracing","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>src/agents/models/interface.py</code> <pre><code>class ModelTracing(enum.Enum):\n    DISABLED = 0\n    \"\"\"Tracing is disabled entirely.\"\"\"\n\n    ENABLED = 1\n    \"\"\"Tracing is enabled, and all data is included.\"\"\"\n\n    ENABLED_WITHOUT_DATA = 2\n    \"\"\"Tracing is enabled, but inputs/outputs are not included.\"\"\"\n\n    def is_disabled(self) -&gt; bool:\n        return self == ModelTracing.DISABLED\n\n    def include_data(self) -&gt; bool:\n        return self == ModelTracing.ENABLED\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.ModelTracing.DISABLED","title":"DISABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED = 0\n</code></pre> <p>Tracing is disabled entirely.</p>"},{"location":"ref/models/interface/#agents.models.interface.ModelTracing.ENABLED","title":"ENABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENABLED = 1\n</code></pre> <p>Tracing is enabled, and all data is included.</p>"},{"location":"ref/models/interface/#agents.models.interface.ModelTracing.ENABLED_WITHOUT_DATA","title":"ENABLED_WITHOUT_DATA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENABLED_WITHOUT_DATA = 2\n</code></pre> <p>Tracing is enabled, but inputs/outputs are not included.</p>"},{"location":"ref/models/interface/#agents.models.interface.Model","title":"Model","text":"<p>               Bases: <code>ABC</code></p> <p>The base interface for calling an LLM.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>class Model(abc.ABC):\n    \"\"\"The base interface for calling an LLM.\"\"\"\n\n    @abc.abstractmethod\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        *,\n        previous_response_id: str | None,\n        conversation_id: str | None,\n        prompt: ResponsePromptParam | None,\n    ) -&gt; ModelResponse:\n        \"\"\"Get a response from the model.\n\n        Args:\n            system_instructions: The system instructions to use.\n            input: The input items to the model, in OpenAI Responses format.\n            model_settings: The model settings to use.\n            tools: The tools available to the model.\n            output_schema: The output schema to use.\n            handoffs: The handoffs available to the model.\n            tracing: Tracing configuration.\n            previous_response_id: the ID of the previous response. Generally not used by the model,\n                except for the OpenAI Responses API.\n            conversation_id: The ID of the stored conversation, if any.\n            prompt: The prompt config to use for the model.\n\n        Returns:\n            The full model response.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        *,\n        previous_response_id: str | None,\n        conversation_id: str | None,\n        prompt: ResponsePromptParam | None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        \"\"\"Stream a response from the model.\n\n        Args:\n            system_instructions: The system instructions to use.\n            input: The input items to the model, in OpenAI Responses format.\n            model_settings: The model settings to use.\n            tools: The tools available to the model.\n            output_schema: The output schema to use.\n            handoffs: The handoffs available to the model.\n            tracing: Tracing configuration.\n            previous_response_id: the ID of the previous response. Generally not used by the model,\n                except for the OpenAI Responses API.\n            conversation_id: The ID of the stored conversation, if any.\n            prompt: The prompt config to use for the model.\n\n        Returns:\n            An iterator of response stream events, in OpenAI Responses format.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.Model.get_response","title":"get_response  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    conversation_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; ModelResponse\n</code></pre> <p>Get a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>system_instructions</code> <code>str | None</code> <p>The system instructions to use.</p> required <code>input</code> <code>str | list[TResponseInputItem]</code> <p>The input items to the model, in OpenAI Responses format.</p> required <code>model_settings</code> <code>ModelSettings</code> <p>The model settings to use.</p> required <code>tools</code> <code>list[Tool]</code> <p>The tools available to the model.</p> required <code>output_schema</code> <code>AgentOutputSchemaBase | None</code> <p>The output schema to use.</p> required <code>handoffs</code> <code>list[Handoff]</code> <p>The handoffs available to the model.</p> required <code>tracing</code> <code>ModelTracing</code> <p>Tracing configuration.</p> required <code>previous_response_id</code> <code>str | None</code> <p>the ID of the previous response. Generally not used by the model, except for the OpenAI Responses API.</p> required <code>conversation_id</code> <code>str | None</code> <p>The ID of the stored conversation, if any.</p> required <code>prompt</code> <code>ResponsePromptParam | None</code> <p>The prompt config to use for the model.</p> required <p>Returns:</p> Type Description <code>ModelResponse</code> <p>The full model response.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>@abc.abstractmethod\nasync def get_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    conversation_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; ModelResponse:\n    \"\"\"Get a response from the model.\n\n    Args:\n        system_instructions: The system instructions to use.\n        input: The input items to the model, in OpenAI Responses format.\n        model_settings: The model settings to use.\n        tools: The tools available to the model.\n        output_schema: The output schema to use.\n        handoffs: The handoffs available to the model.\n        tracing: Tracing configuration.\n        previous_response_id: the ID of the previous response. Generally not used by the model,\n            except for the OpenAI Responses API.\n        conversation_id: The ID of the stored conversation, if any.\n        prompt: The prompt config to use for the model.\n\n    Returns:\n        The full model response.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.Model.stream_response","title":"stream_response  <code>abstractmethod</code>","text":"<pre><code>stream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    conversation_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; AsyncIterator[TResponseStreamEvent]\n</code></pre> <p>Stream a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>system_instructions</code> <code>str | None</code> <p>The system instructions to use.</p> required <code>input</code> <code>str | list[TResponseInputItem]</code> <p>The input items to the model, in OpenAI Responses format.</p> required <code>model_settings</code> <code>ModelSettings</code> <p>The model settings to use.</p> required <code>tools</code> <code>list[Tool]</code> <p>The tools available to the model.</p> required <code>output_schema</code> <code>AgentOutputSchemaBase | None</code> <p>The output schema to use.</p> required <code>handoffs</code> <code>list[Handoff]</code> <p>The handoffs available to the model.</p> required <code>tracing</code> <code>ModelTracing</code> <p>Tracing configuration.</p> required <code>previous_response_id</code> <code>str | None</code> <p>the ID of the previous response. Generally not used by the model, except for the OpenAI Responses API.</p> required <code>conversation_id</code> <code>str | None</code> <p>The ID of the stored conversation, if any.</p> required <code>prompt</code> <code>ResponsePromptParam | None</code> <p>The prompt config to use for the model.</p> required <p>Returns:</p> Type Description <code>AsyncIterator[TResponseStreamEvent]</code> <p>An iterator of response stream events, in OpenAI Responses format.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>@abc.abstractmethod\ndef stream_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    conversation_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; AsyncIterator[TResponseStreamEvent]:\n    \"\"\"Stream a response from the model.\n\n    Args:\n        system_instructions: The system instructions to use.\n        input: The input items to the model, in OpenAI Responses format.\n        model_settings: The model settings to use.\n        tools: The tools available to the model.\n        output_schema: The output schema to use.\n        handoffs: The handoffs available to the model.\n        tracing: Tracing configuration.\n        previous_response_id: the ID of the previous response. Generally not used by the model,\n            except for the OpenAI Responses API.\n        conversation_id: The ID of the stored conversation, if any.\n        prompt: The prompt config to use for the model.\n\n    Returns:\n        An iterator of response stream events, in OpenAI Responses format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.ModelProvider","title":"ModelProvider","text":"<p>               Bases: <code>ABC</code></p> <p>The base interface for a model provider.</p> <p>Model provider is responsible for looking up Models by name.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>class ModelProvider(abc.ABC):\n    \"\"\"The base interface for a model provider.\n\n    Model provider is responsible for looking up Models by name.\n    \"\"\"\n\n    @abc.abstractmethod\n    def get_model(self, model_name: str | None) -&gt; Model:\n        \"\"\"Get a model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The model.\n        \"\"\"\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.ModelProvider.get_model","title":"get_model  <code>abstractmethod</code>","text":"<pre><code>get_model(model_name: str | None) -&gt; Model\n</code></pre> <p>Get a model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The model.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>@abc.abstractmethod\ndef get_model(self, model_name: str | None) -&gt; Model:\n    \"\"\"Get a model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The model.\n    \"\"\"\n</code></pre>"},{"location":"ref/models/multi_provider/","title":"<code>Multi Provider</code>","text":""},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProviderMap","title":"MultiProviderMap","text":"<p>A map of model name prefixes to ModelProviders.</p> Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>class MultiProviderMap:\n    \"\"\"A map of model name prefixes to ModelProviders.\"\"\"\n\n    def __init__(self):\n        self._mapping: dict[str, ModelProvider] = {}\n\n    def has_prefix(self, prefix: str) -&gt; bool:\n        \"\"\"Returns True if the given prefix is in the mapping.\"\"\"\n        return prefix in self._mapping\n\n    def get_mapping(self) -&gt; dict[str, ModelProvider]:\n        \"\"\"Returns a copy of the current prefix -&gt; ModelProvider mapping.\"\"\"\n        return self._mapping.copy()\n\n    def set_mapping(self, mapping: dict[str, ModelProvider]):\n        \"\"\"Overwrites the current mapping with a new one.\"\"\"\n        self._mapping = mapping\n\n    def get_provider(self, prefix: str) -&gt; ModelProvider | None:\n        \"\"\"Returns the ModelProvider for the given prefix.\n\n        Args:\n            prefix: The prefix of the model name e.g. \"openai\" or \"my_prefix\".\n        \"\"\"\n        return self._mapping.get(prefix)\n\n    def add_provider(self, prefix: str, provider: ModelProvider):\n        \"\"\"Adds a new prefix -&gt; ModelProvider mapping.\n\n        Args:\n            prefix: The prefix of the model name e.g. \"openai\" or \"my_prefix\".\n            provider: The ModelProvider to use for the given prefix.\n        \"\"\"\n        self._mapping[prefix] = provider\n\n    def remove_provider(self, prefix: str):\n        \"\"\"Removes the mapping for the given prefix.\n\n        Args:\n            prefix: The prefix of the model name e.g. \"openai\" or \"my_prefix\".\n        \"\"\"\n        del self._mapping[prefix]\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProviderMap.has_prefix","title":"has_prefix","text":"<pre><code>has_prefix(prefix: str) -&gt; bool\n</code></pre> <p>Returns True if the given prefix is in the mapping.</p> Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def has_prefix(self, prefix: str) -&gt; bool:\n    \"\"\"Returns True if the given prefix is in the mapping.\"\"\"\n    return prefix in self._mapping\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProviderMap.get_mapping","title":"get_mapping","text":"<pre><code>get_mapping() -&gt; dict[str, ModelProvider]\n</code></pre> <p>Returns a copy of the current prefix -&gt; ModelProvider mapping.</p> Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def get_mapping(self) -&gt; dict[str, ModelProvider]:\n    \"\"\"Returns a copy of the current prefix -&gt; ModelProvider mapping.\"\"\"\n    return self._mapping.copy()\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProviderMap.set_mapping","title":"set_mapping","text":"<pre><code>set_mapping(mapping: dict[str, ModelProvider])\n</code></pre> <p>Overwrites the current mapping with a new one.</p> Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def set_mapping(self, mapping: dict[str, ModelProvider]):\n    \"\"\"Overwrites the current mapping with a new one.\"\"\"\n    self._mapping = mapping\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProviderMap.get_provider","title":"get_provider","text":"<pre><code>get_provider(prefix: str) -&gt; ModelProvider | None\n</code></pre> <p>Returns the ModelProvider for the given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix of the model name e.g. \"openai\" or \"my_prefix\".</p> required Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def get_provider(self, prefix: str) -&gt; ModelProvider | None:\n    \"\"\"Returns the ModelProvider for the given prefix.\n\n    Args:\n        prefix: The prefix of the model name e.g. \"openai\" or \"my_prefix\".\n    \"\"\"\n    return self._mapping.get(prefix)\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProviderMap.add_provider","title":"add_provider","text":"<pre><code>add_provider(prefix: str, provider: ModelProvider)\n</code></pre> <p>Adds a new prefix -&gt; ModelProvider mapping.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix of the model name e.g. \"openai\" or \"my_prefix\".</p> required <code>provider</code> <code>ModelProvider</code> <p>The ModelProvider to use for the given prefix.</p> required Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def add_provider(self, prefix: str, provider: ModelProvider):\n    \"\"\"Adds a new prefix -&gt; ModelProvider mapping.\n\n    Args:\n        prefix: The prefix of the model name e.g. \"openai\" or \"my_prefix\".\n        provider: The ModelProvider to use for the given prefix.\n    \"\"\"\n    self._mapping[prefix] = provider\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProviderMap.remove_provider","title":"remove_provider","text":"<pre><code>remove_provider(prefix: str)\n</code></pre> <p>Removes the mapping for the given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix of the model name e.g. \"openai\" or \"my_prefix\".</p> required Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def remove_provider(self, prefix: str):\n    \"\"\"Removes the mapping for the given prefix.\n\n    Args:\n        prefix: The prefix of the model name e.g. \"openai\" or \"my_prefix\".\n    \"\"\"\n    del self._mapping[prefix]\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProvider","title":"MultiProvider","text":"<p>               Bases: <code>ModelProvider</code></p> <p>This ModelProvider maps to a Model based on the prefix of the model name. By default, the mapping is: - \"openai/\" prefix or no prefix -&gt; OpenAIProvider. e.g. \"openai/gpt-4.1\", \"gpt-4.1\" - \"litellm/\" prefix -&gt; LitellmProvider. e.g. \"litellm/openai/gpt-4.1\"</p> <p>You can override or customize this mapping.</p> Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>class MultiProvider(ModelProvider):\n    \"\"\"This ModelProvider maps to a Model based on the prefix of the model name. By default, the\n    mapping is:\n    - \"openai/\" prefix or no prefix -&gt; OpenAIProvider. e.g. \"openai/gpt-4.1\", \"gpt-4.1\"\n    - \"litellm/\" prefix -&gt; LitellmProvider. e.g. \"litellm/openai/gpt-4.1\"\n\n    You can override or customize this mapping.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        provider_map: MultiProviderMap | None = None,\n        openai_api_key: str | None = None,\n        openai_base_url: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        openai_organization: str | None = None,\n        openai_project: str | None = None,\n        openai_use_responses: bool | None = None,\n    ) -&gt; None:\n        \"\"\"Create a new OpenAI provider.\n\n        Args:\n            provider_map: A MultiProviderMap that maps prefixes to ModelProviders. If not provided,\n                we will use a default mapping. See the documentation for this class to see the\n                default mapping.\n            openai_api_key: The API key to use for the OpenAI provider. If not provided, we will use\n                the default API key.\n            openai_base_url: The base URL to use for the OpenAI provider. If not provided, we will\n                use the default base URL.\n            openai_client: An optional OpenAI client to use. If not provided, we will create a new\n                OpenAI client using the api_key and base_url.\n            openai_organization: The organization to use for the OpenAI provider.\n            openai_project: The project to use for the OpenAI provider.\n            openai_use_responses: Whether to use the OpenAI responses API.\n        \"\"\"\n        self.provider_map = provider_map\n        self.openai_provider = OpenAIProvider(\n            api_key=openai_api_key,\n            base_url=openai_base_url,\n            openai_client=openai_client,\n            organization=openai_organization,\n            project=openai_project,\n            use_responses=openai_use_responses,\n        )\n\n        self._fallback_providers: dict[str, ModelProvider] = {}\n\n    def _get_prefix_and_model_name(self, model_name: str | None) -&gt; tuple[str | None, str | None]:\n        if model_name is None:\n            return None, None\n        elif \"/\" in model_name:\n            prefix, model_name = model_name.split(\"/\", 1)\n            return prefix, model_name\n        else:\n            return None, model_name\n\n    def _create_fallback_provider(self, prefix: str) -&gt; ModelProvider:\n        if prefix == \"litellm\":\n            from ..extensions.models.litellm_provider import LitellmProvider\n\n            return LitellmProvider()\n        else:\n            raise UserError(f\"Unknown prefix: {prefix}\")\n\n    def _get_fallback_provider(self, prefix: str | None) -&gt; ModelProvider:\n        if prefix is None or prefix == \"openai\":\n            return self.openai_provider\n        elif prefix in self._fallback_providers:\n            return self._fallback_providers[prefix]\n        else:\n            self._fallback_providers[prefix] = self._create_fallback_provider(prefix)\n            return self._fallback_providers[prefix]\n\n    def get_model(self, model_name: str | None) -&gt; Model:\n        \"\"\"Returns a Model based on the model name. The model name can have a prefix, ending with\n        a \"/\", which will be used to look up the ModelProvider. If there is no prefix, we will use\n        the OpenAI provider.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            A Model.\n        \"\"\"\n        prefix, model_name = self._get_prefix_and_model_name(model_name)\n\n        if prefix and self.provider_map and (provider := self.provider_map.get_provider(prefix)):\n            return provider.get_model(model_name)\n        else:\n            return self._get_fallback_provider(prefix).get_model(model_name)\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    provider_map: MultiProviderMap | None = None,\n    openai_api_key: str | None = None,\n    openai_base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    openai_organization: str | None = None,\n    openai_project: str | None = None,\n    openai_use_responses: bool | None = None,\n) -&gt; None\n</code></pre> <p>Create a new OpenAI provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_map</code> <code>MultiProviderMap | None</code> <p>A MultiProviderMap that maps prefixes to ModelProviders. If not provided, we will use a default mapping. See the documentation for this class to see the default mapping.</p> <code>None</code> <code>openai_api_key</code> <code>str | None</code> <p>The API key to use for the OpenAI provider. If not provided, we will use the default API key.</p> <code>None</code> <code>openai_base_url</code> <code>str | None</code> <p>The base URL to use for the OpenAI provider. If not provided, we will use the default base URL.</p> <code>None</code> <code>openai_client</code> <code>AsyncOpenAI | None</code> <p>An optional OpenAI client to use. If not provided, we will create a new OpenAI client using the api_key and base_url.</p> <code>None</code> <code>openai_organization</code> <code>str | None</code> <p>The organization to use for the OpenAI provider.</p> <code>None</code> <code>openai_project</code> <code>str | None</code> <p>The project to use for the OpenAI provider.</p> <code>None</code> <code>openai_use_responses</code> <code>bool | None</code> <p>Whether to use the OpenAI responses API.</p> <code>None</code> Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def __init__(\n    self,\n    *,\n    provider_map: MultiProviderMap | None = None,\n    openai_api_key: str | None = None,\n    openai_base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    openai_organization: str | None = None,\n    openai_project: str | None = None,\n    openai_use_responses: bool | None = None,\n) -&gt; None:\n    \"\"\"Create a new OpenAI provider.\n\n    Args:\n        provider_map: A MultiProviderMap that maps prefixes to ModelProviders. If not provided,\n            we will use a default mapping. See the documentation for this class to see the\n            default mapping.\n        openai_api_key: The API key to use for the OpenAI provider. If not provided, we will use\n            the default API key.\n        openai_base_url: The base URL to use for the OpenAI provider. If not provided, we will\n            use the default base URL.\n        openai_client: An optional OpenAI client to use. If not provided, we will create a new\n            OpenAI client using the api_key and base_url.\n        openai_organization: The organization to use for the OpenAI provider.\n        openai_project: The project to use for the OpenAI provider.\n        openai_use_responses: Whether to use the OpenAI responses API.\n    \"\"\"\n    self.provider_map = provider_map\n    self.openai_provider = OpenAIProvider(\n        api_key=openai_api_key,\n        base_url=openai_base_url,\n        openai_client=openai_client,\n        organization=openai_organization,\n        project=openai_project,\n        use_responses=openai_use_responses,\n    )\n\n    self._fallback_providers: dict[str, ModelProvider] = {}\n</code></pre>"},{"location":"ref/models/multi_provider/#agents.models.multi_provider.MultiProvider.get_model","title":"get_model","text":"<pre><code>get_model(model_name: str | None) -&gt; Model\n</code></pre> <p>Returns a Model based on the model name. The model name can have a prefix, ending with a \"/\", which will be used to look up the ModelProvider. If there is no prefix, we will use the OpenAI provider.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>A Model.</p> Source code in <code>src/agents/models/multi_provider.py</code> <pre><code>def get_model(self, model_name: str | None) -&gt; Model:\n    \"\"\"Returns a Model based on the model name. The model name can have a prefix, ending with\n    a \"/\", which will be used to look up the ModelProvider. If there is no prefix, we will use\n    the OpenAI provider.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        A Model.\n    \"\"\"\n    prefix, model_name = self._get_prefix_and_model_name(model_name)\n\n    if prefix and self.provider_map and (provider := self.provider_map.get_provider(prefix)):\n        return provider.get_model(model_name)\n    else:\n        return self._get_fallback_provider(prefix).get_model(model_name)\n</code></pre>"},{"location":"ref/models/openai_chatcompletions/","title":"<code>OpenAI Chat Completions model</code>","text":""},{"location":"ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel","title":"OpenAIChatCompletionsModel","text":"<p>               Bases: <code>Model</code></p> Source code in <code>src/agents/models/openai_chatcompletions.py</code> <pre><code>class OpenAIChatCompletionsModel(Model):\n    def __init__(\n        self,\n        model: str | ChatModel,\n        openai_client: AsyncOpenAI,\n    ) -&gt; None:\n        self.model = model\n        self._client = openai_client\n\n    def _non_null_or_omit(self, value: Any) -&gt; Any:\n        return value if value is not None else omit\n\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,  # unused\n        conversation_id: str | None = None,  # unused\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ModelResponse:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=False,\n                prompt=prompt,\n            )\n\n            message: ChatCompletionMessage | None = None\n            first_choice: Choice | None = None\n            if response.choices and len(response.choices) &gt; 0:\n                first_choice = response.choices[0]\n                message = first_choice.message\n\n            if _debug.DONT_LOG_MODEL_DATA:\n                logger.debug(\"Received model response\")\n            else:\n                if message is not None:\n                    logger.debug(\n                        \"LLM resp:\\n%s\\n\",\n                        json.dumps(message.model_dump(), indent=2, ensure_ascii=False),\n                    )\n                else:\n                    finish_reason = first_choice.finish_reason if first_choice else \"-\"\n                    logger.debug(f\"LLM resp had no message. finish_reason: {finish_reason}\")\n\n            usage = (\n                Usage(\n                    requests=1,\n                    input_tokens=response.usage.prompt_tokens,\n                    output_tokens=response.usage.completion_tokens,\n                    total_tokens=response.usage.total_tokens,\n                    # BeforeValidator in Usage normalizes these from Chat Completions types\n                    input_tokens_details=response.usage.prompt_tokens_details,  # type: ignore[arg-type]\n                    output_tokens_details=response.usage.completion_tokens_details,  # type: ignore[arg-type]\n                )\n                if response.usage\n                else Usage()\n            )\n            if tracing.include_data():\n                span_generation.span_data.output = (\n                    [message.model_dump()] if message is not None else []\n                )\n            span_generation.span_data.usage = {\n                \"requests\": usage.requests,\n                \"input_tokens\": usage.input_tokens,\n                \"output_tokens\": usage.output_tokens,\n                \"total_tokens\": usage.total_tokens,\n                \"input_tokens_details\": usage.input_tokens_details.model_dump(),\n                \"output_tokens_details\": usage.output_tokens_details.model_dump(),\n            }\n\n            # Build provider_data for provider_specific_fields\n            provider_data = {\"model\": self.model}\n            if message is not None and hasattr(response, \"id\"):\n                provider_data[\"response_id\"] = response.id\n\n            items = (\n                Converter.message_to_output_items(message, provider_data=provider_data)\n                if message is not None\n                else []\n            )\n\n            logprob_models = None\n            if first_choice and first_choice.logprobs and first_choice.logprobs.content:\n                logprob_models = ChatCmplHelpers.convert_logprobs_for_output_text(\n                    first_choice.logprobs.content\n                )\n\n            if logprob_models:\n                self._attach_logprobs_to_output(items, logprob_models)\n\n            return ModelResponse(\n                output=items,\n                usage=usage,\n                response_id=None,\n            )\n\n    def _attach_logprobs_to_output(\n        self, output_items: list[ResponseOutputItem], logprobs: list[Logprob]\n    ) -&gt; None:\n        for output_item in output_items:\n            if not isinstance(output_item, ResponseOutputMessage):\n                continue\n\n            for content in output_item.content:\n                if isinstance(content, ResponseOutputText):\n                    content.logprobs = logprobs\n                    return\n\n    async def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,  # unused\n        conversation_id: str | None = None,  # unused\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        \"\"\"\n        Yields a partial message as it is generated, as well as the usage information.\n        \"\"\"\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response, stream = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=True,\n                prompt=prompt,\n            )\n\n            final_response: Response | None = None\n            async for chunk in ChatCmplStreamHandler.handle_stream(\n                response, stream, model=self.model\n            ):\n                yield chunk\n\n                if chunk.type == \"response.completed\":\n                    final_response = chunk.response\n\n            if tracing.include_data() and final_response:\n                span_generation.span_data.output = [final_response.model_dump()]\n\n            if final_response and final_response.usage:\n                span_generation.span_data.usage = {\n                    \"requests\": 1,\n                    \"input_tokens\": final_response.usage.input_tokens,\n                    \"output_tokens\": final_response.usage.output_tokens,\n                    \"total_tokens\": final_response.usage.total_tokens,\n                    \"input_tokens_details\": (\n                        final_response.usage.input_tokens_details.model_dump()\n                        if final_response.usage.input_tokens_details\n                        else {\"cached_tokens\": 0}\n                    ),\n                    \"output_tokens_details\": (\n                        final_response.usage.output_tokens_details.model_dump()\n                        if final_response.usage.output_tokens_details\n                        else {\"reasoning_tokens\": 0}\n                    ),\n                }\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[True],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; tuple[Response, AsyncStream[ChatCompletionChunk]]: ...\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[False],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ChatCompletion: ...\n\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: bool = False,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ChatCompletion | tuple[Response, AsyncStream[ChatCompletionChunk]]:\n        converted_messages = Converter.items_to_messages(input, model=self.model)\n\n        if system_instructions:\n            converted_messages.insert(\n                0,\n                {\n                    \"content\": system_instructions,\n                    \"role\": \"system\",\n                },\n            )\n        converted_messages = _to_dump_compatible(converted_messages)\n\n        if tracing.include_data():\n            span.span_data.input = converted_messages\n\n        if model_settings.parallel_tool_calls and tools:\n            parallel_tool_calls: bool | Omit = True\n        elif model_settings.parallel_tool_calls is False:\n            parallel_tool_calls = False\n        else:\n            parallel_tool_calls = omit\n        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)\n        response_format = Converter.convert_response_format(output_schema)\n\n        converted_tools = [Converter.tool_to_openai(tool) for tool in tools] if tools else []\n\n        for handoff in handoffs:\n            converted_tools.append(Converter.convert_handoff_tool(handoff))\n\n        converted_tools = _to_dump_compatible(converted_tools)\n        tools_param = converted_tools if converted_tools else omit\n\n        if _debug.DONT_LOG_MODEL_DATA:\n            logger.debug(\"Calling LLM\")\n        else:\n            messages_json = json.dumps(\n                converted_messages,\n                indent=2,\n                ensure_ascii=False,\n            )\n            tools_json = json.dumps(\n                converted_tools,\n                indent=2,\n                ensure_ascii=False,\n            )\n            logger.debug(\n                f\"{messages_json}\\n\"\n                f\"Tools:\\n{tools_json}\\n\"\n                f\"Stream: {stream}\\n\"\n                f\"Tool choice: {tool_choice}\\n\"\n                f\"Response format: {response_format}\\n\"\n            )\n\n        reasoning_effort = model_settings.reasoning.effort if model_settings.reasoning else None\n        store = ChatCmplHelpers.get_store_param(self._get_client(), model_settings)\n\n        stream_options = ChatCmplHelpers.get_stream_options_param(\n            self._get_client(), model_settings, stream=stream\n        )\n\n        stream_param: Literal[True] | Omit = True if stream else omit\n\n        ret = await self._get_client().chat.completions.create(\n            model=self.model,\n            messages=converted_messages,\n            tools=tools_param,\n            temperature=self._non_null_or_omit(model_settings.temperature),\n            top_p=self._non_null_or_omit(model_settings.top_p),\n            frequency_penalty=self._non_null_or_omit(model_settings.frequency_penalty),\n            presence_penalty=self._non_null_or_omit(model_settings.presence_penalty),\n            max_tokens=self._non_null_or_omit(model_settings.max_tokens),\n            tool_choice=tool_choice,\n            response_format=response_format,\n            parallel_tool_calls=parallel_tool_calls,\n            stream=cast(Any, stream_param),\n            stream_options=self._non_null_or_omit(stream_options),\n            store=self._non_null_or_omit(store),\n            reasoning_effort=self._non_null_or_omit(reasoning_effort),\n            verbosity=self._non_null_or_omit(model_settings.verbosity),\n            top_logprobs=self._non_null_or_omit(model_settings.top_logprobs),\n            prompt_cache_retention=self._non_null_or_omit(model_settings.prompt_cache_retention),\n            extra_headers=self._merge_headers(model_settings),\n            extra_query=model_settings.extra_query,\n            extra_body=model_settings.extra_body,\n            metadata=self._non_null_or_omit(model_settings.metadata),\n            **(model_settings.extra_args or {}),\n        )\n\n        if isinstance(ret, ChatCompletion):\n            return ret\n\n        responses_tool_choice = OpenAIResponsesConverter.convert_tool_choice(\n            model_settings.tool_choice\n        )\n        if responses_tool_choice is None or responses_tool_choice is omit:\n            # For Responses API data compatibility with Chat Completions patterns,\n            # we need to set \"none\" if tool_choice is absent.\n            # Without this fix, you'll get the following error:\n            # pydantic_core._pydantic_core.ValidationError: 4 validation errors for Response\n            # tool_choice.literal['none','auto','required']\n            #   Input should be 'none', 'auto' or 'required'\n            # see also: https://github.com/openai/openai-agents-python/issues/980\n            responses_tool_choice = \"auto\"\n\n        response = Response(\n            id=FAKE_RESPONSES_ID,\n            created_at=time.time(),\n            model=self.model,\n            object=\"response\",\n            output=[],\n            tool_choice=responses_tool_choice,  # type: ignore[arg-type]\n            top_p=model_settings.top_p,\n            temperature=model_settings.temperature,\n            tools=[],\n            parallel_tool_calls=parallel_tool_calls or False,\n            reasoning=model_settings.reasoning,\n        )\n        return response, ret\n\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = AsyncOpenAI()\n        return self._client\n\n    def _merge_headers(self, model_settings: ModelSettings):\n        return {\n            **HEADERS,\n            **(model_settings.extra_headers or {}),\n            **(HEADERS_OVERRIDE.get() or {}),\n        }\n</code></pre>"},{"location":"ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel.stream_response","title":"stream_response  <code>async</code>","text":"<pre><code>stream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None = None,\n    conversation_id: str | None = None,\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[TResponseStreamEvent]\n</code></pre> <p>Yields a partial message as it is generated, as well as the usage information.</p> Source code in <code>src/agents/models/openai_chatcompletions.py</code> <pre><code>async def stream_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None = None,  # unused\n    conversation_id: str | None = None,  # unused\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[TResponseStreamEvent]:\n    \"\"\"\n    Yields a partial message as it is generated, as well as the usage information.\n    \"\"\"\n    with generation_span(\n        model=str(self.model),\n        model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},\n        disabled=tracing.is_disabled(),\n    ) as span_generation:\n        response, stream = await self._fetch_response(\n            system_instructions,\n            input,\n            model_settings,\n            tools,\n            output_schema,\n            handoffs,\n            span_generation,\n            tracing,\n            stream=True,\n            prompt=prompt,\n        )\n\n        final_response: Response | None = None\n        async for chunk in ChatCmplStreamHandler.handle_stream(\n            response, stream, model=self.model\n        ):\n            yield chunk\n\n            if chunk.type == \"response.completed\":\n                final_response = chunk.response\n\n        if tracing.include_data() and final_response:\n            span_generation.span_data.output = [final_response.model_dump()]\n\n        if final_response and final_response.usage:\n            span_generation.span_data.usage = {\n                \"requests\": 1,\n                \"input_tokens\": final_response.usage.input_tokens,\n                \"output_tokens\": final_response.usage.output_tokens,\n                \"total_tokens\": final_response.usage.total_tokens,\n                \"input_tokens_details\": (\n                    final_response.usage.input_tokens_details.model_dump()\n                    if final_response.usage.input_tokens_details\n                    else {\"cached_tokens\": 0}\n                ),\n                \"output_tokens_details\": (\n                    final_response.usage.output_tokens_details.model_dump()\n                    if final_response.usage.output_tokens_details\n                    else {\"reasoning_tokens\": 0}\n                ),\n            }\n</code></pre>"},{"location":"ref/models/openai_provider/","title":"<code>OpenAI Provider</code>","text":""},{"location":"ref/models/openai_provider/#agents.models.openai_provider.OpenAIProvider","title":"OpenAIProvider","text":"<p>               Bases: <code>ModelProvider</code></p> Source code in <code>src/agents/models/openai_provider.py</code> <pre><code>class OpenAIProvider(ModelProvider):\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        base_url: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        use_responses: bool | None = None,\n    ) -&gt; None:\n        \"\"\"Create a new OpenAI provider.\n\n        Args:\n            api_key: The API key to use for the OpenAI client. If not provided, we will use the\n                default API key.\n            base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n                default base URL.\n            openai_client: An optional OpenAI client to use. If not provided, we will create a new\n                OpenAI client using the api_key and base_url.\n            organization: The organization to use for the OpenAI client.\n            project: The project to use for the OpenAI client.\n            use_responses: Whether to use the OpenAI responses API.\n        \"\"\"\n        if openai_client is not None:\n            assert api_key is None and base_url is None, (\n                \"Don't provide api_key or base_url if you provide openai_client\"\n            )\n            self._client: AsyncOpenAI | None = openai_client\n        else:\n            self._client = None\n            self._stored_api_key = api_key\n            self._stored_base_url = base_url\n            self._stored_organization = organization\n            self._stored_project = project\n\n        if use_responses is not None:\n            self._use_responses = use_responses\n        else:\n            self._use_responses = _openai_shared.get_use_responses_by_default()\n\n    # We lazy load the client in case you never actually use OpenAIProvider(). Otherwise\n    # AsyncOpenAI() raises an error if you don't have an API key set.\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = _openai_shared.get_default_openai_client() or AsyncOpenAI(\n                api_key=self._stored_api_key or _openai_shared.get_default_openai_key(),\n                base_url=self._stored_base_url,\n                organization=self._stored_organization,\n                project=self._stored_project,\n                http_client=shared_http_client(),\n            )\n\n        return self._client\n\n    def get_model(self, model_name: str | None) -&gt; Model:\n        model_is_explicit = model_name is not None\n        resolved_model_name = model_name if model_name is not None else get_default_model()\n\n        client = self._get_client()\n\n        return (\n            OpenAIResponsesModel(\n                model=resolved_model_name,\n                openai_client=client,\n                model_is_explicit=model_is_explicit,\n            )\n            if self._use_responses\n            else OpenAIChatCompletionsModel(model=resolved_model_name, openai_client=client)\n        )\n</code></pre>"},{"location":"ref/models/openai_provider/#agents.models.openai_provider.OpenAIProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n    use_responses: bool | None = None,\n) -&gt; None\n</code></pre> <p>Create a new OpenAI provider.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>The API key to use for the OpenAI client. If not provided, we will use the default API key.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>The base URL to use for the OpenAI client. If not provided, we will use the default base URL.</p> <code>None</code> <code>openai_client</code> <code>AsyncOpenAI | None</code> <p>An optional OpenAI client to use. If not provided, we will create a new OpenAI client using the api_key and base_url.</p> <code>None</code> <code>organization</code> <code>str | None</code> <p>The organization to use for the OpenAI client.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>The project to use for the OpenAI client.</p> <code>None</code> <code>use_responses</code> <code>bool | None</code> <p>Whether to use the OpenAI responses API.</p> <code>None</code> Source code in <code>src/agents/models/openai_provider.py</code> <pre><code>def __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n    use_responses: bool | None = None,\n) -&gt; None:\n    \"\"\"Create a new OpenAI provider.\n\n    Args:\n        api_key: The API key to use for the OpenAI client. If not provided, we will use the\n            default API key.\n        base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n            default base URL.\n        openai_client: An optional OpenAI client to use. If not provided, we will create a new\n            OpenAI client using the api_key and base_url.\n        organization: The organization to use for the OpenAI client.\n        project: The project to use for the OpenAI client.\n        use_responses: Whether to use the OpenAI responses API.\n    \"\"\"\n    if openai_client is not None:\n        assert api_key is None and base_url is None, (\n            \"Don't provide api_key or base_url if you provide openai_client\"\n        )\n        self._client: AsyncOpenAI | None = openai_client\n    else:\n        self._client = None\n        self._stored_api_key = api_key\n        self._stored_base_url = base_url\n        self._stored_organization = organization\n        self._stored_project = project\n\n    if use_responses is not None:\n        self._use_responses = use_responses\n    else:\n        self._use_responses = _openai_shared.get_use_responses_by_default()\n</code></pre>"},{"location":"ref/models/openai_responses/","title":"<code>OpenAI Responses model</code>","text":""},{"location":"ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel","title":"OpenAIResponsesModel","text":"<p>               Bases: <code>Model</code></p> <p>Implementation of <code>Model</code> that uses the OpenAI Responses API.</p> Source code in <code>src/agents/models/openai_responses.py</code> <pre><code>class OpenAIResponsesModel(Model):\n    \"\"\"\n    Implementation of `Model` that uses the OpenAI Responses API.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str | ChatModel,\n        openai_client: AsyncOpenAI,\n        *,\n        model_is_explicit: bool = True,\n    ) -&gt; None:\n        self.model = model\n        self._model_is_explicit = model_is_explicit\n        self._client = openai_client\n\n    def _non_null_or_omit(self, value: Any) -&gt; Any:\n        return value if value is not None else omit\n\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,\n        conversation_id: str | None = None,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ModelResponse:\n        with response_span(disabled=tracing.is_disabled()) as span_response:\n            try:\n                response = await self._fetch_response(\n                    system_instructions,\n                    input,\n                    model_settings,\n                    tools,\n                    output_schema,\n                    handoffs,\n                    previous_response_id=previous_response_id,\n                    conversation_id=conversation_id,\n                    stream=False,\n                    prompt=prompt,\n                )\n\n                if _debug.DONT_LOG_MODEL_DATA:\n                    logger.debug(\"LLM responded\")\n                else:\n                    logger.debug(\n                        \"LLM resp:\\n\"\n                        f\"\"\"{\n                            json.dumps(\n                                [x.model_dump() for x in response.output],\n                                indent=2,\n                                ensure_ascii=False,\n                            )\n                        }\\n\"\"\"\n                    )\n\n                usage = (\n                    Usage(\n                        requests=1,\n                        input_tokens=response.usage.input_tokens,\n                        output_tokens=response.usage.output_tokens,\n                        total_tokens=response.usage.total_tokens,\n                        input_tokens_details=response.usage.input_tokens_details,\n                        output_tokens_details=response.usage.output_tokens_details,\n                    )\n                    if response.usage\n                    else Usage()\n                )\n\n                if tracing.include_data():\n                    span_response.span_data.response = response\n                    span_response.span_data.input = input\n            except Exception as e:\n                span_response.set_error(\n                    SpanError(\n                        message=\"Error getting response\",\n                        data={\n                            \"error\": str(e) if tracing.include_data() else e.__class__.__name__,\n                        },\n                    )\n                )\n                request_id = e.request_id if isinstance(e, APIStatusError) else None\n                logger.error(f\"Error getting response: {e}. (request_id: {request_id})\")\n                raise\n\n        return ModelResponse(\n            output=response.output,\n            usage=usage,\n            response_id=response.id,\n        )\n\n    async def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None = None,\n        conversation_id: str | None = None,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; AsyncIterator[ResponseStreamEvent]:\n        \"\"\"\n        Yields a partial message as it is generated, as well as the usage information.\n        \"\"\"\n        with response_span(disabled=tracing.is_disabled()) as span_response:\n            try:\n                stream = await self._fetch_response(\n                    system_instructions,\n                    input,\n                    model_settings,\n                    tools,\n                    output_schema,\n                    handoffs,\n                    previous_response_id=previous_response_id,\n                    conversation_id=conversation_id,\n                    stream=True,\n                    prompt=prompt,\n                )\n\n                final_response: Response | None = None\n\n                async for chunk in stream:\n                    if isinstance(chunk, ResponseCompletedEvent):\n                        final_response = chunk.response\n                    yield chunk\n\n                if final_response and tracing.include_data():\n                    span_response.span_data.response = final_response\n                    span_response.span_data.input = input\n\n            except Exception as e:\n                span_response.set_error(\n                    SpanError(\n                        message=\"Error streaming response\",\n                        data={\n                            \"error\": str(e) if tracing.include_data() else e.__class__.__name__,\n                        },\n                    )\n                )\n                logger.error(f\"Error streaming response: {e}\")\n                raise\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        previous_response_id: str | None,\n        conversation_id: str | None,\n        stream: Literal[True],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; AsyncStream[ResponseStreamEvent]: ...\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        previous_response_id: str | None,\n        conversation_id: str | None,\n        stream: Literal[False],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; Response: ...\n\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        previous_response_id: str | None = None,\n        conversation_id: str | None = None,\n        stream: Literal[True] | Literal[False] = False,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; Response | AsyncStream[ResponseStreamEvent]:\n        list_input = ItemHelpers.input_to_new_input_list(input)\n        list_input = _to_dump_compatible(list_input)\n        list_input = self._remove_openai_responses_api_incompatible_fields(list_input)\n\n        if model_settings.parallel_tool_calls and tools:\n            parallel_tool_calls: bool | Omit = True\n        elif model_settings.parallel_tool_calls is False:\n            parallel_tool_calls = False\n        else:\n            parallel_tool_calls = omit\n\n        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)\n        converted_tools = Converter.convert_tools(tools, handoffs)\n        converted_tools_payload = _to_dump_compatible(converted_tools.tools)\n        response_format = Converter.get_response_format(output_schema)\n        should_omit_model = prompt is not None and not self._model_is_explicit\n        model_param: str | ChatModel | Omit = self.model if not should_omit_model else omit\n        should_omit_tools = prompt is not None and len(converted_tools_payload) == 0\n        tools_param: list[ToolParam] | Omit = (\n            converted_tools_payload if not should_omit_tools else omit\n        )\n\n        include_set: set[str] = set(converted_tools.includes)\n        if model_settings.response_include is not None:\n            include_set.update(model_settings.response_include)\n        if model_settings.top_logprobs is not None:\n            include_set.add(\"message.output_text.logprobs\")\n        include = cast(list[ResponseIncludable], list(include_set))\n\n        if _debug.DONT_LOG_MODEL_DATA:\n            logger.debug(\"Calling LLM\")\n        else:\n            input_json = json.dumps(\n                list_input,\n                indent=2,\n                ensure_ascii=False,\n            )\n            tools_json = json.dumps(\n                converted_tools_payload,\n                indent=2,\n                ensure_ascii=False,\n            )\n            logger.debug(\n                f\"Calling LLM {self.model} with input:\\n\"\n                f\"{input_json}\\n\"\n                f\"Tools:\\n{tools_json}\\n\"\n                f\"Stream: {stream}\\n\"\n                f\"Tool choice: {tool_choice}\\n\"\n                f\"Response format: {response_format}\\n\"\n                f\"Previous response id: {previous_response_id}\\n\"\n                f\"Conversation id: {conversation_id}\\n\"\n            )\n\n        extra_args = dict(model_settings.extra_args or {})\n        if model_settings.top_logprobs is not None:\n            extra_args[\"top_logprobs\"] = model_settings.top_logprobs\n        if model_settings.verbosity is not None:\n            if response_format is not omit:\n                response_format[\"verbosity\"] = model_settings.verbosity  # type: ignore [index]\n            else:\n                response_format = {\"verbosity\": model_settings.verbosity}\n\n        stream_param: Literal[True] | Omit = True if stream else omit\n\n        response = await self._client.responses.create(\n            previous_response_id=self._non_null_or_omit(previous_response_id),\n            conversation=self._non_null_or_omit(conversation_id),\n            instructions=self._non_null_or_omit(system_instructions),\n            model=model_param,\n            input=list_input,\n            include=include,\n            tools=tools_param,\n            prompt=self._non_null_or_omit(prompt),\n            temperature=self._non_null_or_omit(model_settings.temperature),\n            top_p=self._non_null_or_omit(model_settings.top_p),\n            truncation=self._non_null_or_omit(model_settings.truncation),\n            max_output_tokens=self._non_null_or_omit(model_settings.max_tokens),\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            stream=cast(Any, stream_param),\n            extra_headers=self._merge_headers(model_settings),\n            extra_query=model_settings.extra_query,\n            extra_body=model_settings.extra_body,\n            text=response_format,\n            store=self._non_null_or_omit(model_settings.store),\n            prompt_cache_retention=self._non_null_or_omit(model_settings.prompt_cache_retention),\n            reasoning=self._non_null_or_omit(model_settings.reasoning),\n            metadata=self._non_null_or_omit(model_settings.metadata),\n            **extra_args,\n        )\n        return cast(Union[Response, AsyncStream[ResponseStreamEvent]], response)\n\n    def _remove_openai_responses_api_incompatible_fields(self, list_input: list[Any]) -&gt; list[Any]:\n        \"\"\"\n        Remove or transform input items that are incompatible with the OpenAI Responses API.\n\n        This data transformation does not always guarantee that items from other provider\n        interactions are accepted by the OpenAI Responses API.\n\n        Only items with truthy provider_data are processed.\n        This function handles the following incompatibilities:\n        - provider_data: Removes fields specific to other providers (e.g., Gemini, Claude).\n        - Fake IDs: Removes temporary IDs (FAKE_RESPONSES_ID) that should not be sent to OpenAI.\n        - Reasoning items: Filters out provider-specific reasoning items entirely.\n        \"\"\"\n        # Early return optimization: if no item has provider_data, return unchanged.\n        has_provider_data = any(\n            isinstance(item, dict) and item.get(\"provider_data\") for item in list_input\n        )\n        if not has_provider_data:\n            return list_input\n\n        result = []\n        for item in list_input:\n            cleaned = self._clean_item_for_openai(item)\n            if cleaned is not None:\n                result.append(cleaned)\n        return result\n\n    def _clean_item_for_openai(self, item: Any) -&gt; Any | None:\n        # Only process dict items\n        if not isinstance(item, dict):\n            return item\n\n        # Filter out reasoning items with provider_data (provider-specific reasoning).\n        if item.get(\"type\") == \"reasoning\" and item.get(\"provider_data\"):\n            return None\n\n        # Remove fake response ID.\n        if item.get(\"id\") == FAKE_RESPONSES_ID:\n            del item[\"id\"]\n\n        # Remove provider_data field.\n        if \"provider_data\" in item:\n            del item[\"provider_data\"]\n\n        return item\n\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = AsyncOpenAI()\n        return self._client\n\n    def _merge_headers(self, model_settings: ModelSettings):\n        return {\n            **_HEADERS,\n            **(model_settings.extra_headers or {}),\n            **(_HEADERS_OVERRIDE.get() or {}),\n        }\n</code></pre>"},{"location":"ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel.stream_response","title":"stream_response  <code>async</code>","text":"<pre><code>stream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None = None,\n    conversation_id: str | None = None,\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[ResponseStreamEvent]\n</code></pre> <p>Yields a partial message as it is generated, as well as the usage information.</p> Source code in <code>src/agents/models/openai_responses.py</code> <pre><code>async def stream_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None = None,\n    conversation_id: str | None = None,\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[ResponseStreamEvent]:\n    \"\"\"\n    Yields a partial message as it is generated, as well as the usage information.\n    \"\"\"\n    with response_span(disabled=tracing.is_disabled()) as span_response:\n        try:\n            stream = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                previous_response_id=previous_response_id,\n                conversation_id=conversation_id,\n                stream=True,\n                prompt=prompt,\n            )\n\n            final_response: Response | None = None\n\n            async for chunk in stream:\n                if isinstance(chunk, ResponseCompletedEvent):\n                    final_response = chunk.response\n                yield chunk\n\n            if final_response and tracing.include_data():\n                span_response.span_data.response = final_response\n                span_response.span_data.input = input\n\n        except Exception as e:\n            span_response.set_error(\n                SpanError(\n                    message=\"Error streaming response\",\n                    data={\n                        \"error\": str(e) if tracing.include_data() else e.__class__.__name__,\n                    },\n                )\n            )\n            logger.error(f\"Error streaming response: {e}\")\n            raise\n</code></pre>"},{"location":"ref/models/openai_responses/#agents.models.openai_responses.Converter","title":"Converter","text":"Source code in <code>src/agents/models/openai_responses.py</code> <pre><code>class Converter:\n    @classmethod\n    def convert_tool_choice(\n        cls, tool_choice: Literal[\"auto\", \"required\", \"none\"] | str | MCPToolChoice | None\n    ) -&gt; response_create_params.ToolChoice | Omit:\n        if tool_choice is None:\n            return omit\n        elif isinstance(tool_choice, MCPToolChoice):\n            return {\n                \"server_label\": tool_choice.server_label,\n                \"type\": \"mcp\",\n                \"name\": tool_choice.name,\n            }\n        elif tool_choice == \"required\":\n            return \"required\"\n        elif tool_choice == \"auto\":\n            return \"auto\"\n        elif tool_choice == \"none\":\n            return \"none\"\n        elif tool_choice == \"file_search\":\n            return {\n                \"type\": \"file_search\",\n            }\n        elif tool_choice == \"web_search\":\n            return {\n                # TODO: revisit the type: ignore comment when ToolChoice is updated in the future\n                \"type\": \"web_search\",  # type: ignore[misc, return-value]\n            }\n        elif tool_choice == \"web_search_preview\":\n            return {\n                \"type\": \"web_search_preview\",\n            }\n        elif tool_choice == \"computer_use_preview\":\n            return {\n                \"type\": \"computer_use_preview\",\n            }\n        elif tool_choice == \"image_generation\":\n            return {\n                \"type\": \"image_generation\",\n            }\n        elif tool_choice == \"code_interpreter\":\n            return {\n                \"type\": \"code_interpreter\",\n            }\n        elif tool_choice == \"mcp\":\n            # Note that this is still here for backwards compatibility,\n            # but migrating to MCPToolChoice is recommended.\n            return {\"type\": \"mcp\"}  # type: ignore[misc, return-value]\n        else:\n            return {\n                \"type\": \"function\",\n                \"name\": tool_choice,\n            }\n\n    @classmethod\n    def get_response_format(\n        cls, output_schema: AgentOutputSchemaBase | None\n    ) -&gt; ResponseTextConfigParam | Omit:\n        if output_schema is None or output_schema.is_plain_text():\n            return omit\n        else:\n            return {\n                \"format\": {\n                    \"type\": \"json_schema\",\n                    \"name\": \"final_output\",\n                    \"schema\": output_schema.json_schema(),\n                    \"strict\": output_schema.is_strict_json_schema(),\n                }\n            }\n\n    @classmethod\n    def convert_tools(\n        cls,\n        tools: list[Tool],\n        handoffs: list[Handoff[Any, Any]],\n    ) -&gt; ConvertedTools:\n        converted_tools: list[ToolParam] = []\n        includes: list[ResponseIncludable] = []\n\n        computer_tools = [tool for tool in tools if isinstance(tool, ComputerTool)]\n        if len(computer_tools) &gt; 1:\n            raise UserError(f\"You can only provide one computer tool. Got {len(computer_tools)}\")\n\n        for tool in tools:\n            converted_tool, include = cls._convert_tool(tool)\n            converted_tools.append(converted_tool)\n            if include:\n                includes.append(include)\n\n        for handoff in handoffs:\n            converted_tools.append(cls._convert_handoff_tool(handoff))\n\n        return ConvertedTools(tools=converted_tools, includes=includes)\n\n    @classmethod\n    def _convert_tool(cls, tool: Tool) -&gt; tuple[ToolParam, ResponseIncludable | None]:\n        \"\"\"Returns converted tool and includes\"\"\"\n\n        if isinstance(tool, FunctionTool):\n            converted_tool: ToolParam = {\n                \"name\": tool.name,\n                \"parameters\": tool.params_json_schema,\n                \"strict\": tool.strict_json_schema,\n                \"type\": \"function\",\n                \"description\": tool.description,\n            }\n            includes: ResponseIncludable | None = None\n        elif isinstance(tool, WebSearchTool):\n            # TODO: revisit the type: ignore comment when ToolParam is updated in the future\n            converted_tool = {\n                \"type\": \"web_search\",\n                \"filters\": tool.filters.model_dump() if tool.filters is not None else None,  # type: ignore [typeddict-item]\n                \"user_location\": tool.user_location,\n                \"search_context_size\": tool.search_context_size,\n            }\n            includes = None\n        elif isinstance(tool, FileSearchTool):\n            converted_tool = {\n                \"type\": \"file_search\",\n                \"vector_store_ids\": tool.vector_store_ids,\n            }\n            if tool.max_num_results:\n                converted_tool[\"max_num_results\"] = tool.max_num_results\n            if tool.ranking_options:\n                converted_tool[\"ranking_options\"] = tool.ranking_options\n            if tool.filters:\n                converted_tool[\"filters\"] = tool.filters\n\n            includes = \"file_search_call.results\" if tool.include_search_results else None\n        elif isinstance(tool, ComputerTool):\n            computer = tool.computer\n            if not isinstance(computer, (Computer, AsyncComputer)):\n                raise UserError(\n                    \"Computer tool is not initialized for serialization. Call \"\n                    \"resolve_computer({ tool, run_context }) with a run context first \"\n                    \"when building payloads manually.\"\n                )\n            converted_tool = {\n                \"type\": \"computer_use_preview\",\n                \"environment\": computer.environment,\n                \"display_width\": computer.dimensions[0],\n                \"display_height\": computer.dimensions[1],\n            }\n            includes = None\n        elif isinstance(tool, HostedMCPTool):\n            converted_tool = tool.tool_config\n            includes = None\n        elif isinstance(tool, ApplyPatchTool):\n            converted_tool = cast(ToolParam, {\"type\": \"apply_patch\"})\n            includes = None\n        elif isinstance(tool, ShellTool):\n            converted_tool = cast(ToolParam, {\"type\": \"shell\"})\n            includes = None\n        elif isinstance(tool, ImageGenerationTool):\n            converted_tool = tool.tool_config\n            includes = None\n        elif isinstance(tool, CodeInterpreterTool):\n            converted_tool = tool.tool_config\n            includes = None\n        elif isinstance(tool, LocalShellTool):\n            converted_tool = {\n                \"type\": \"local_shell\",\n            }\n            includes = None\n        else:\n            raise UserError(f\"Unknown tool type: {type(tool)}, tool\")\n\n        return converted_tool, includes\n\n    @classmethod\n    def _convert_handoff_tool(cls, handoff: Handoff) -&gt; ToolParam:\n        return {\n            \"name\": handoff.tool_name,\n            \"parameters\": handoff.input_json_schema,\n            \"strict\": handoff.strict_json_schema,\n            \"type\": \"function\",\n            \"description\": handoff.tool_description,\n        }\n</code></pre>"},{"location":"ref/realtime/agent/","title":"<code>RealtimeAgent</code>","text":"<p>               Bases: <code>AgentBase</code>, <code>Generic[TContext]</code></p> <p>A specialized agent instance that is meant to be used within a <code>RealtimeSession</code> to build voice agents. Due to the nature of this agent, some configuration options are not supported that are supported by regular <code>Agent</code> instances. For example: - <code>model</code> choice is not supported, as all RealtimeAgents will be handled by the same model   within a <code>RealtimeSession</code>. - <code>modelSettings</code> is not supported, as all RealtimeAgents will be handled by the same model   within a <code>RealtimeSession</code>. - <code>outputType</code> is not supported, as RealtimeAgents do not support structured outputs. - <code>toolUseBehavior</code> is not supported, as all RealtimeAgents will be handled by the same model   within a <code>RealtimeSession</code>. - <code>voice</code> can be configured on an <code>Agent</code> level; however, it cannot be changed after the first   agent within a <code>RealtimeSession</code> has spoken.</p> <p>See <code>AgentBase</code> for base parameters that are shared with <code>Agent</code>s.</p> Source code in <code>src/agents/realtime/agent.py</code> <pre><code>@dataclass\nclass RealtimeAgent(AgentBase, Generic[TContext]):\n    \"\"\"A specialized agent instance that is meant to be used within a `RealtimeSession` to build\n    voice agents. Due to the nature of this agent, some configuration options are not supported\n    that are supported by regular `Agent` instances. For example:\n    - `model` choice is not supported, as all RealtimeAgents will be handled by the same model\n      within a `RealtimeSession`.\n    - `modelSettings` is not supported, as all RealtimeAgents will be handled by the same model\n      within a `RealtimeSession`.\n    - `outputType` is not supported, as RealtimeAgents do not support structured outputs.\n    - `toolUseBehavior` is not supported, as all RealtimeAgents will be handled by the same model\n      within a `RealtimeSession`.\n    - `voice` can be configured on an `Agent` level; however, it cannot be changed after the first\n      agent within a `RealtimeSession` has spoken.\n\n    See `AgentBase` for base parameters that are shared with `Agent`s.\n    \"\"\"\n\n    instructions: (\n        str\n        | Callable[\n            [RunContextWrapper[TContext], RealtimeAgent[TContext]],\n            MaybeAwaitable[str],\n        ]\n        | None\n    ) = None\n    \"\"\"The instructions for the agent. Will be used as the \"system prompt\" when this agent is\n    invoked. Describes what the agent should do, and how it responds.\n\n    Can either be a string, or a function that dynamically generates instructions for the agent. If\n    you provide a function, it will be called with the context and the agent instance. It must\n    return a string.\n    \"\"\"\n\n    prompt: Prompt | None = None\n    \"\"\"A prompt object. Prompts allow you to dynamically configure the instructions, tools\n    and other config for an agent outside of your code. Only usable with OpenAI models.\n    \"\"\"\n\n    handoffs: list[RealtimeAgent[Any] | Handoff[TContext, RealtimeAgent[Any]]] = field(\n        default_factory=list\n    )\n    \"\"\"Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs,\n    and the agent can choose to delegate to them if relevant. Allows for separation of concerns and\n    modularity.\n    \"\"\"\n\n    output_guardrails: list[OutputGuardrail[TContext]] = field(default_factory=list)\n    \"\"\"A list of checks that run on the final output of the agent, after generating a response.\n    Runs only if the agent produces a final output.\n    \"\"\"\n\n    hooks: RealtimeAgentHooks | None = None\n    \"\"\"A class that receives callbacks on various lifecycle events for this agent.\n    \"\"\"\n\n    def clone(self, **kwargs: Any) -&gt; RealtimeAgent[TContext]:\n        \"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n        ```\n        new_agent = agent.clone(instructions=\"New instructions\")\n        ```\n        \"\"\"\n        return dataclasses.replace(self, **kwargs)\n\n    async def get_system_prompt(self, run_context: RunContextWrapper[TContext]) -&gt; str | None:\n        \"\"\"Get the system prompt for the agent.\"\"\"\n        if isinstance(self.instructions, str):\n            return self.instructions\n        elif callable(self.instructions):\n            if inspect.iscoroutinefunction(self.instructions):\n                return await cast(Awaitable[str], self.instructions(run_context, self))\n            else:\n                return cast(str, self.instructions(run_context, self))\n        elif self.instructions is not None:\n            logger.error(f\"Instructions must be a string or a function, got {self.instructions}\")\n\n        return None\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.instructions","title":"instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instructions: (\n    str\n    | Callable[\n        [\n            RunContextWrapper[TContext],\n            RealtimeAgent[TContext],\n        ],\n        MaybeAwaitable[str],\n    ]\n    | None\n) = None\n</code></pre> <p>The instructions for the agent. Will be used as the \"system prompt\" when this agent is invoked. Describes what the agent should do, and how it responds.</p> <p>Can either be a string, or a function that dynamically generates instructions for the agent. If you provide a function, it will be called with the context and the agent instance. It must return a string.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.prompt","title":"prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt: Prompt | None = None\n</code></pre> <p>A prompt object. Prompts allow you to dynamically configure the instructions, tools and other config for an agent outside of your code. Only usable with OpenAI models.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.handoffs","title":"handoffs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoffs: list[\n    RealtimeAgent[Any]\n    | Handoff[TContext, RealtimeAgent[Any]]\n] = field(default_factory=list)\n</code></pre> <p>Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs, and the agent can choose to delegate to them if relevant. Allows for separation of concerns and modularity.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.output_guardrails","title":"output_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrails: list[OutputGuardrail[TContext]] = field(\n    default_factory=list\n)\n</code></pre> <p>A list of checks that run on the final output of the agent, after generating a response. Runs only if the agent produces a final output.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.hooks","title":"hooks  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hooks: RealtimeAgentHooks | None = None\n</code></pre> <p>A class that receives callbacks on various lifecycle events for this agent.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the agent.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.handoff_description","title":"handoff_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_description: str | None = None\n</code></pre> <p>A description of the agent. This is used when the agent is used as a handoff, so that an LLM knows what it does and when to invoke it.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.tools","title":"tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools: list[Tool] = field(default_factory=list)\n</code></pre> <p>A list of tools that the agent can use.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.mcp_servers","title":"mcp_servers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_servers: list[MCPServer] = field(default_factory=list)\n</code></pre> <p>A list of Model Context Protocol servers that the agent can use. Every time the agent runs, it will include tools from these servers in the list of available tools.</p> <p>NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call <code>server.connect()</code> before passing it to the agent, and <code>server.cleanup()</code> when the server is no longer needed. Consider using <code>MCPServerManager</code> from <code>agents.mcp</code> to keep connect/cleanup in the same task.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.mcp_config","title":"mcp_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_config: MCPConfig = field(\n    default_factory=lambda: MCPConfig()\n)\n</code></pre> <p>Configuration for MCP servers.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.clone","title":"clone","text":"<pre><code>clone(**kwargs: Any) -&gt; RealtimeAgent[TContext]\n</code></pre> <p>Make a copy of the agent, with the given arguments changed. For example, you could do: <pre><code>new_agent = agent.clone(instructions=\"New instructions\")\n</code></pre></p> Source code in <code>src/agents/realtime/agent.py</code> <pre><code>def clone(self, **kwargs: Any) -&gt; RealtimeAgent[TContext]:\n    \"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n    ```\n    new_agent = agent.clone(instructions=\"New instructions\")\n    ```\n    \"\"\"\n    return dataclasses.replace(self, **kwargs)\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.get_system_prompt","title":"get_system_prompt  <code>async</code>","text":"<pre><code>get_system_prompt(\n    run_context: RunContextWrapper[TContext],\n) -&gt; str | None\n</code></pre> <p>Get the system prompt for the agent.</p> Source code in <code>src/agents/realtime/agent.py</code> <pre><code>async def get_system_prompt(self, run_context: RunContextWrapper[TContext]) -&gt; str | None:\n    \"\"\"Get the system prompt for the agent.\"\"\"\n    if isinstance(self.instructions, str):\n        return self.instructions\n    elif callable(self.instructions):\n        if inspect.iscoroutinefunction(self.instructions):\n            return await cast(Awaitable[str], self.instructions(run_context, self))\n        else:\n            return cast(str, self.instructions(run_context, self))\n    elif self.instructions is not None:\n        logger.error(f\"Instructions must be a string or a function, got {self.instructions}\")\n\n    return None\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.get_mcp_tools","title":"get_mcp_tools  <code>async</code>","text":"<pre><code>get_mcp_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>Fetches the available tools from the MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n    convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n    failure_error_function = self.mcp_config.get(\n        \"failure_error_function\", default_tool_error_function\n    )\n    return await MCPUtil.get_all_function_tools(\n        self.mcp_servers,\n        convert_schemas_to_strict,\n        run_context,\n        self,\n        failure_error_function=failure_error_function,\n    )\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>All agent tools, including MCP tools and function tools.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_all_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n    mcp_tools = await self.get_mcp_tools(run_context)\n\n    async def _check_tool_enabled(tool: Tool) -&gt; bool:\n        if not isinstance(tool, FunctionTool):\n            return True\n\n        attr = tool.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(run_context, self)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n    enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n    return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/realtime/audio_formats/","title":"<code>Audio Formats</code>","text":""},{"location":"ref/realtime/config/","title":"Realtime Configuration","text":""},{"location":"ref/realtime/config/#run-configuration","title":"Run Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for running a realtime agent session.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeRunConfig(TypedDict):\n    \"\"\"Configuration for running a realtime agent session.\"\"\"\n\n    model_settings: NotRequired[RealtimeSessionModelSettings]\n    \"\"\"Settings for the realtime model session.\"\"\"\n\n    output_guardrails: NotRequired[list[OutputGuardrail[Any]]]\n    \"\"\"List of output guardrails to run on the agent's responses.\"\"\"\n\n    guardrails_settings: NotRequired[RealtimeGuardrailsSettings]\n    \"\"\"Settings for guardrail execution.\"\"\"\n\n    tracing_disabled: NotRequired[bool]\n    \"\"\"Whether tracing is disabled for this run.\"\"\"\n\n    async_tool_calls: NotRequired[bool]\n    \"\"\"Whether function tool calls should run asynchronously. Defaults to True.\"\"\"\n\n    tool_error_formatter: NotRequired[ToolErrorFormatter]\n    \"\"\"Optional callback that formats tool error messages returned to the model.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.model_settings","title":"model_settings  <code>instance-attribute</code>","text":"<pre><code>model_settings: NotRequired[RealtimeSessionModelSettings]\n</code></pre> <p>Settings for the realtime model session.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.output_guardrails","title":"output_guardrails  <code>instance-attribute</code>","text":"<pre><code>output_guardrails: NotRequired[list[OutputGuardrail[Any]]]\n</code></pre> <p>List of output guardrails to run on the agent's responses.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.guardrails_settings","title":"guardrails_settings  <code>instance-attribute</code>","text":"<pre><code>guardrails_settings: NotRequired[RealtimeGuardrailsSettings]\n</code></pre> <p>Settings for guardrail execution.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.tracing_disabled","title":"tracing_disabled  <code>instance-attribute</code>","text":"<pre><code>tracing_disabled: NotRequired[bool]\n</code></pre> <p>Whether tracing is disabled for this run.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.async_tool_calls","title":"async_tool_calls  <code>instance-attribute</code>","text":"<pre><code>async_tool_calls: NotRequired[bool]\n</code></pre> <p>Whether function tool calls should run asynchronously. Defaults to True.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.tool_error_formatter","title":"tool_error_formatter  <code>instance-attribute</code>","text":"<pre><code>tool_error_formatter: NotRequired[ToolErrorFormatter]\n</code></pre> <p>Optional callback that formats tool error messages returned to the model.</p>"},{"location":"ref/realtime/config/#model-settings","title":"Model Settings","text":"<p>               Bases: <code>TypedDict</code></p> <p>Model settings for a realtime model session.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeSessionModelSettings(TypedDict):\n    \"\"\"Model settings for a realtime model session.\"\"\"\n\n    model_name: NotRequired[RealtimeModelName]\n    \"\"\"The name of the realtime model to use.\"\"\"\n\n    instructions: NotRequired[str]\n    \"\"\"System instructions for the model.\"\"\"\n\n    prompt: NotRequired[Prompt]\n    \"\"\"The prompt to use for the model.\"\"\"\n\n    modalities: NotRequired[list[Literal[\"text\", \"audio\"]]]\n    \"\"\"The modalities the model should support.\"\"\"\n\n    output_modalities: NotRequired[list[Literal[\"text\", \"audio\"]]]\n    \"\"\"The output modalities the model should support.\"\"\"\n\n    audio: NotRequired[RealtimeAudioConfig]\n    \"\"\"The audio configuration for the session.\"\"\"\n\n    voice: NotRequired[str]\n    \"\"\"The voice to use for audio output.\"\"\"\n\n    speed: NotRequired[float]\n    \"\"\"The speed of the model's responses.\"\"\"\n\n    input_audio_format: NotRequired[RealtimeAudioFormat | OpenAIRealtimeAudioFormats]\n    \"\"\"The format for input audio streams.\"\"\"\n\n    output_audio_format: NotRequired[RealtimeAudioFormat | OpenAIRealtimeAudioFormats]\n    \"\"\"The format for output audio streams.\"\"\"\n\n    input_audio_transcription: NotRequired[RealtimeInputAudioTranscriptionConfig]\n    \"\"\"Configuration for transcribing input audio.\"\"\"\n\n    input_audio_noise_reduction: NotRequired[RealtimeInputAudioNoiseReductionConfig | None]\n    \"\"\"Noise reduction configuration for input audio.\"\"\"\n\n    turn_detection: NotRequired[RealtimeTurnDetectionConfig]\n    \"\"\"Configuration for detecting conversation turns.\"\"\"\n\n    tool_choice: NotRequired[ToolChoice]\n    \"\"\"How the model should choose which tools to call.\"\"\"\n\n    tools: NotRequired[list[Tool]]\n    \"\"\"List of tools available to the model.\"\"\"\n\n    handoffs: NotRequired[list[Handoff]]\n    \"\"\"List of handoff configurations.\"\"\"\n\n    tracing: NotRequired[RealtimeModelTracingConfig | None]\n    \"\"\"Configuration for request tracing.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: NotRequired[RealtimeModelName]\n</code></pre> <p>The name of the realtime model to use.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.instructions","title":"instructions  <code>instance-attribute</code>","text":"<pre><code>instructions: NotRequired[str]\n</code></pre> <p>System instructions for the model.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: NotRequired[Prompt]\n</code></pre> <p>The prompt to use for the model.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.modalities","title":"modalities  <code>instance-attribute</code>","text":"<pre><code>modalities: NotRequired[list[Literal['text', 'audio']]]\n</code></pre> <p>The modalities the model should support.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.output_modalities","title":"output_modalities  <code>instance-attribute</code>","text":"<pre><code>output_modalities: NotRequired[\n    list[Literal[\"text\", \"audio\"]]\n]\n</code></pre> <p>The output modalities the model should support.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: NotRequired[RealtimeAudioConfig]\n</code></pre> <p>The audio configuration for the session.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.voice","title":"voice  <code>instance-attribute</code>","text":"<pre><code>voice: NotRequired[str]\n</code></pre> <p>The voice to use for audio output.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.speed","title":"speed  <code>instance-attribute</code>","text":"<pre><code>speed: NotRequired[float]\n</code></pre> <p>The speed of the model's responses.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.input_audio_format","title":"input_audio_format  <code>instance-attribute</code>","text":"<pre><code>input_audio_format: NotRequired[\n    RealtimeAudioFormat | RealtimeAudioFormats\n]\n</code></pre> <p>The format for input audio streams.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.output_audio_format","title":"output_audio_format  <code>instance-attribute</code>","text":"<pre><code>output_audio_format: NotRequired[\n    RealtimeAudioFormat | RealtimeAudioFormats\n]\n</code></pre> <p>The format for output audio streams.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.input_audio_transcription","title":"input_audio_transcription  <code>instance-attribute</code>","text":"<pre><code>input_audio_transcription: NotRequired[\n    RealtimeInputAudioTranscriptionConfig\n]\n</code></pre> <p>Configuration for transcribing input audio.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.input_audio_noise_reduction","title":"input_audio_noise_reduction  <code>instance-attribute</code>","text":"<pre><code>input_audio_noise_reduction: NotRequired[\n    RealtimeInputAudioNoiseReductionConfig | None\n]\n</code></pre> <p>Noise reduction configuration for input audio.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.turn_detection","title":"turn_detection  <code>instance-attribute</code>","text":"<pre><code>turn_detection: NotRequired[RealtimeTurnDetectionConfig]\n</code></pre> <p>Configuration for detecting conversation turns.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.tool_choice","title":"tool_choice  <code>instance-attribute</code>","text":"<pre><code>tool_choice: NotRequired[ToolChoice]\n</code></pre> <p>How the model should choose which tools to call.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: NotRequired[list[Tool]]\n</code></pre> <p>List of tools available to the model.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.handoffs","title":"handoffs  <code>instance-attribute</code>","text":"<pre><code>handoffs: NotRequired[list[Handoff]]\n</code></pre> <p>List of handoff configurations.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.tracing","title":"tracing  <code>instance-attribute</code>","text":"<pre><code>tracing: NotRequired[RealtimeModelTracingConfig | None]\n</code></pre> <p>Configuration for request tracing.</p>"},{"location":"ref/realtime/config/#audio-configuration","title":"Audio Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for audio transcription in realtime sessions.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeInputAudioTranscriptionConfig(TypedDict):\n    \"\"\"Configuration for audio transcription in realtime sessions.\"\"\"\n\n    language: NotRequired[str]\n    \"\"\"The language code for transcription.\"\"\"\n\n    model: NotRequired[Literal[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"] | str]\n    \"\"\"The transcription model to use.\"\"\"\n\n    prompt: NotRequired[str]\n    \"\"\"An optional prompt to guide transcription.\"\"\"\n</code></pre> <p>               Bases: <code>TypedDict</code></p> <p>Noise reduction configuration for input audio.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeInputAudioNoiseReductionConfig(TypedDict):\n    \"\"\"Noise reduction configuration for input audio.\"\"\"\n\n    type: NotRequired[Literal[\"near_field\", \"far_field\"]]\n    \"\"\"Noise reduction mode to apply to input audio.\"\"\"\n</code></pre> <p>               Bases: <code>TypedDict</code></p> <p>Turn detection config. Allows extra vendor keys if needed.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeTurnDetectionConfig(TypedDict):\n    \"\"\"Turn detection config. Allows extra vendor keys if needed.\"\"\"\n\n    type: NotRequired[Literal[\"semantic_vad\", \"server_vad\"]]\n    \"\"\"The type of voice activity detection to use.\"\"\"\n\n    create_response: NotRequired[bool]\n    \"\"\"Whether to create a response when a turn is detected.\"\"\"\n\n    eagerness: NotRequired[Literal[\"auto\", \"low\", \"medium\", \"high\"]]\n    \"\"\"How eagerly to detect turn boundaries.\"\"\"\n\n    interrupt_response: NotRequired[bool]\n    \"\"\"Whether to allow interrupting the assistant's response.\"\"\"\n\n    prefix_padding_ms: NotRequired[int]\n    \"\"\"Padding time in milliseconds before turn detection.\"\"\"\n\n    silence_duration_ms: NotRequired[int]\n    \"\"\"Duration of silence in milliseconds to trigger turn detection.\"\"\"\n\n    threshold: NotRequired[float]\n    \"\"\"The threshold for voice activity detection.\"\"\"\n\n    idle_timeout_ms: NotRequired[int]\n    \"\"\"Threshold for server-vad to trigger a response if the user is idle for this duration.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeInputAudioTranscriptionConfig.language","title":"language  <code>instance-attribute</code>","text":"<pre><code>language: NotRequired[str]\n</code></pre> <p>The language code for transcription.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeInputAudioTranscriptionConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: NotRequired[\n    Literal[\n        \"gpt-4o-transcribe\",\n        \"gpt-4o-mini-transcribe\",\n        \"whisper-1\",\n    ]\n    | str\n]\n</code></pre> <p>The transcription model to use.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeInputAudioTranscriptionConfig.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: NotRequired[str]\n</code></pre> <p>An optional prompt to guide transcription.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeInputAudioNoiseReductionConfig.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: NotRequired[Literal['near_field', 'far_field']]\n</code></pre> <p>Noise reduction mode to apply to input audio.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: NotRequired[Literal['semantic_vad', 'server_vad']]\n</code></pre> <p>The type of voice activity detection to use.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.create_response","title":"create_response  <code>instance-attribute</code>","text":"<pre><code>create_response: NotRequired[bool]\n</code></pre> <p>Whether to create a response when a turn is detected.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.eagerness","title":"eagerness  <code>instance-attribute</code>","text":"<pre><code>eagerness: NotRequired[\n    Literal[\"auto\", \"low\", \"medium\", \"high\"]\n]\n</code></pre> <p>How eagerly to detect turn boundaries.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.interrupt_response","title":"interrupt_response  <code>instance-attribute</code>","text":"<pre><code>interrupt_response: NotRequired[bool]\n</code></pre> <p>Whether to allow interrupting the assistant's response.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.prefix_padding_ms","title":"prefix_padding_ms  <code>instance-attribute</code>","text":"<pre><code>prefix_padding_ms: NotRequired[int]\n</code></pre> <p>Padding time in milliseconds before turn detection.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.silence_duration_ms","title":"silence_duration_ms  <code>instance-attribute</code>","text":"<pre><code>silence_duration_ms: NotRequired[int]\n</code></pre> <p>Duration of silence in milliseconds to trigger turn detection.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold: NotRequired[float]\n</code></pre> <p>The threshold for voice activity detection.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.idle_timeout_ms","title":"idle_timeout_ms  <code>instance-attribute</code>","text":"<pre><code>idle_timeout_ms: NotRequired[int]\n</code></pre> <p>Threshold for server-vad to trigger a response if the user is idle for this duration.</p>"},{"location":"ref/realtime/config/#guardrails-settings","title":"Guardrails Settings","text":"<p>               Bases: <code>TypedDict</code></p> <p>Settings for output guardrails in realtime sessions.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeGuardrailsSettings(TypedDict):\n    \"\"\"Settings for output guardrails in realtime sessions.\"\"\"\n\n    debounce_text_length: NotRequired[int]\n    \"\"\"\n    The minimum number of characters to accumulate before running guardrails on transcript\n    deltas. Defaults to 100. Guardrails run every time the accumulated text reaches\n    1x, 2x, 3x, etc. times this threshold.\n    \"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeGuardrailsSettings.debounce_text_length","title":"debounce_text_length  <code>instance-attribute</code>","text":"<pre><code>debounce_text_length: NotRequired[int]\n</code></pre> <p>The minimum number of characters to accumulate before running guardrails on transcript deltas. Defaults to 100. Guardrails run every time the accumulated text reaches 1x, 2x, 3x, etc. times this threshold.</p>"},{"location":"ref/realtime/config/#model-configuration","title":"Model Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Options for connecting to a realtime model.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>class RealtimeModelConfig(TypedDict):\n    \"\"\"Options for connecting to a realtime model.\"\"\"\n\n    api_key: NotRequired[str | Callable[[], MaybeAwaitable[str]]]\n    \"\"\"The API key (or function that returns a key) to use when connecting. If unset, the model will\n    try to use a sane default. For example, the OpenAI Realtime model will try to use the\n    `OPENAI_API_KEY`  environment variable.\n    \"\"\"\n\n    url: NotRequired[str]\n    \"\"\"The URL to use when connecting. If unset, the model will use a sane default. For example,\n    the OpenAI Realtime model will use the default OpenAI WebSocket URL.\n    \"\"\"\n\n    headers: NotRequired[dict[str, str]]\n    \"\"\"The headers to use when connecting. If unset, the model will use a sane default.\n    Note that, when you set this, authorization header won't be set under the hood.\n    e.g., {\"api-key\": \"your api key here\"} for Azure OpenAI Realtime WebSocket connections.\n    \"\"\"\n\n    initial_model_settings: NotRequired[RealtimeSessionModelSettings]\n    \"\"\"The initial model settings to use when connecting.\"\"\"\n\n    playback_tracker: NotRequired[RealtimePlaybackTracker]\n    \"\"\"The playback tracker to use when tracking audio playback progress. If not set, the model will\n    use a default implementation that assumes audio is played immediately, at realtime speed.\n\n    A playback tracker is useful for interruptions. The model generates audio much faster than\n    realtime playback speed. So if there's an interruption, its useful for the model to know how\n    much of the audio has been played by the user. In low-latency scenarios, it's fine to assume\n    that audio is played back immediately at realtime speed. But in scenarios like phone calls or\n    other remote interactions, you can set a playback tracker that lets the model know when audio\n    is played to the user.\n    \"\"\"\n\n    call_id: NotRequired[str]\n    \"\"\"Attach to an existing realtime call instead of creating a new session.\n\n    When provided, the transport connects using the `call_id` query string parameter rather than a\n    model name. This is used for SIP-originated calls that are accepted via the Realtime Calls API.\n    \"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: NotRequired[\n    str | Callable[[], MaybeAwaitable[str]]\n]\n</code></pre> <p>The API key (or function that returns a key) to use when connecting. If unset, the model will try to use a sane default. For example, the OpenAI Realtime model will try to use the <code>OPENAI_API_KEY</code>  environment variable.</p>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: NotRequired[str]\n</code></pre> <p>The URL to use when connecting. If unset, the model will use a sane default. For example, the OpenAI Realtime model will use the default OpenAI WebSocket URL.</p>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: NotRequired[dict[str, str]]\n</code></pre> <p>The headers to use when connecting. If unset, the model will use a sane default. Note that, when you set this, authorization header won't be set under the hood. e.g., {\"api-key\": \"your api key here\"} for Azure OpenAI Realtime WebSocket connections.</p>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.initial_model_settings","title":"initial_model_settings  <code>instance-attribute</code>","text":"<pre><code>initial_model_settings: NotRequired[\n    RealtimeSessionModelSettings\n]\n</code></pre> <p>The initial model settings to use when connecting.</p>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.playback_tracker","title":"playback_tracker  <code>instance-attribute</code>","text":"<pre><code>playback_tracker: NotRequired[RealtimePlaybackTracker]\n</code></pre> <p>The playback tracker to use when tracking audio playback progress. If not set, the model will use a default implementation that assumes audio is played immediately, at realtime speed.</p> <p>A playback tracker is useful for interruptions. The model generates audio much faster than realtime playback speed. So if there's an interruption, its useful for the model to know how much of the audio has been played by the user. In low-latency scenarios, it's fine to assume that audio is played back immediately at realtime speed. But in scenarios like phone calls or other remote interactions, you can set a playback tracker that lets the model know when audio is played to the user.</p>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.call_id","title":"call_id  <code>instance-attribute</code>","text":"<pre><code>call_id: NotRequired[str]\n</code></pre> <p>Attach to an existing realtime call instead of creating a new session.</p> <p>When provided, the transport connects using the <code>call_id</code> query string parameter rather than a model name. This is used for SIP-originated calls that are accepted via the Realtime Calls API.</p>"},{"location":"ref/realtime/config/#tracing-configuration","title":"Tracing Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for tracing in realtime model sessions.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeModelTracingConfig(TypedDict):\n    \"\"\"Configuration for tracing in realtime model sessions.\"\"\"\n\n    workflow_name: NotRequired[str]\n    \"\"\"The workflow name to use for tracing.\"\"\"\n\n    group_id: NotRequired[str]\n    \"\"\"A group identifier to use for tracing, to link multiple traces together.\"\"\"\n\n    metadata: NotRequired[dict[str, Any]]\n    \"\"\"Additional metadata to include with the trace.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeModelTracingConfig.workflow_name","title":"workflow_name  <code>instance-attribute</code>","text":"<pre><code>workflow_name: NotRequired[str]\n</code></pre> <p>The workflow name to use for tracing.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeModelTracingConfig.group_id","title":"group_id  <code>instance-attribute</code>","text":"<pre><code>group_id: NotRequired[str]\n</code></pre> <p>A group identifier to use for tracing, to link multiple traces together.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeModelTracingConfig.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: NotRequired[dict[str, Any]]\n</code></pre> <p>Additional metadata to include with the trace.</p>"},{"location":"ref/realtime/config/#user-input-types","title":"User Input Types","text":"<p>User input that can be a string or structured message.</p> <p>               Bases: <code>TypedDict</code></p> <p>A text input from the user.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeUserInputText(TypedDict):\n    \"\"\"A text input from the user.\"\"\"\n\n    type: Literal[\"input_text\"]\n    \"\"\"The type identifier for text input.\"\"\"\n\n    text: str\n    \"\"\"The text content from the user.\"\"\"\n</code></pre> <p>               Bases: <code>TypedDict</code></p> <p>A message input from the user.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeUserInputMessage(TypedDict):\n    \"\"\"A message input from the user.\"\"\"\n\n    type: Literal[\"message\"]\n    \"\"\"The type identifier for message inputs.\"\"\"\n\n    role: Literal[\"user\"]\n    \"\"\"The role identifier for user messages.\"\"\"\n\n    content: list[RealtimeUserInputText | RealtimeUserInputImage]\n    \"\"\"List of content items (text and image) in the message.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputText.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['input_text']\n</code></pre> <p>The type identifier for text input.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputText.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text content from the user.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputMessage.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['message']\n</code></pre> <p>The type identifier for message inputs.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Literal['user']\n</code></pre> <p>The role identifier for user messages.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: list[\n    RealtimeUserInputText | RealtimeUserInputImage\n]\n</code></pre> <p>List of content items (text and image) in the message.</p>"},{"location":"ref/realtime/config/#client-messages","title":"Client Messages","text":"<p>               Bases: <code>TypedDict</code></p> <p>A raw message to be sent to the model.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeClientMessage(TypedDict):\n    \"\"\"A raw message to be sent to the model.\"\"\"\n\n    type: str  # explicitly required\n    \"\"\"The type of the message.\"\"\"\n\n    other_data: NotRequired[dict[str, Any]]\n    \"\"\"Merged into the message body.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeClientMessage.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: str\n</code></pre> <p>The type of the message.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeClientMessage.other_data","title":"other_data  <code>instance-attribute</code>","text":"<pre><code>other_data: NotRequired[dict[str, Any]]\n</code></pre> <p>Merged into the message body.</p>"},{"location":"ref/realtime/config/#type-aliases","title":"Type Aliases","text":"<p>The name of a realtime model.</p> <p>The audio format for realtime audio streams.</p>"},{"location":"ref/realtime/events/","title":"Realtime Events","text":""},{"location":"ref/realtime/events/#session-events","title":"Session Events","text":"<p>An event emitted by the realtime session.</p>"},{"location":"ref/realtime/events/#event-types","title":"Event Types","text":""},{"location":"ref/realtime/events/#agent-events","title":"Agent Events","text":"<p>A new agent has started.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAgentStartEvent:\n    \"\"\"A new agent has started.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The new agent.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"agent_start\"] = \"agent_start\"\n</code></pre> <p>An agent has ended.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAgentEndEvent:\n    \"\"\"An agent has ended.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The agent that ended.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"agent_end\"] = \"agent_end\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentStartEvent.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The new agent.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentStartEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentEndEvent.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The agent that ended.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentEndEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#audio-events","title":"Audio Events","text":"<p>Triggered when the agent generates new audio to be played.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAudio:\n    \"\"\"Triggered when the agent generates new audio to be played.\"\"\"\n\n    audio: RealtimeModelAudioEvent\n    \"\"\"The audio event from the model layer.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the item containing audio.\"\"\"\n\n    content_index: int\n    \"\"\"The index of the audio content in `item.content`\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"audio\"] = \"audio\"\n</code></pre> <p>Triggered when the agent stops generating audio.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAudioEnd:\n    \"\"\"Triggered when the agent stops generating audio.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the item containing audio.\"\"\"\n\n    content_index: int\n    \"\"\"The index of the audio content in `item.content`\"\"\"\n\n    type: Literal[\"audio_end\"] = \"audio_end\"\n</code></pre> <p>Triggered when the agent is interrupted. Can be listened to by the user to stop audio playback or give visual indicators to the user.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAudioInterrupted:\n    \"\"\"Triggered when the agent is interrupted. Can be listened to by the user to stop audio\n    playback or give visual indicators to the user.\n    \"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the item containing audio.\"\"\"\n\n    content_index: int\n    \"\"\"The index of the audio content in `item.content`\"\"\"\n\n    type: Literal[\"audio_interrupted\"] = \"audio_interrupted\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudio.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: RealtimeModelAudioEvent\n</code></pre> <p>The audio event from the model layer.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudio.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>The ID of the item containing audio.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudio.content_index","title":"content_index  <code>instance-attribute</code>","text":"<pre><code>content_index: int\n</code></pre> <p>The index of the audio content in <code>item.content</code></p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudio.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioEnd.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioEnd.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>The ID of the item containing audio.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioEnd.content_index","title":"content_index  <code>instance-attribute</code>","text":"<pre><code>content_index: int\n</code></pre> <p>The index of the audio content in <code>item.content</code></p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioInterrupted.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioInterrupted.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>The ID of the item containing audio.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioInterrupted.content_index","title":"content_index  <code>instance-attribute</code>","text":"<pre><code>content_index: int\n</code></pre> <p>The index of the audio content in <code>item.content</code></p>"},{"location":"ref/realtime/events/#tool-events","title":"Tool Events","text":"<p>An agent is starting a tool call.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeToolStart:\n    \"\"\"An agent is starting a tool call.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The agent that updated.\"\"\"\n\n    tool: Tool\n    \"\"\"The tool being called.\"\"\"\n\n    arguments: str\n    \"\"\"The arguments passed to the tool as a JSON string.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"tool_start\"] = \"tool_start\"\n</code></pre> <p>An agent has ended a tool call.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeToolEnd:\n    \"\"\"An agent has ended a tool call.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The agent that ended the tool call.\"\"\"\n\n    tool: Tool\n    \"\"\"The tool that was called.\"\"\"\n\n    arguments: str\n    \"\"\"The arguments passed to the tool as a JSON string.\"\"\"\n\n    output: Any\n    \"\"\"The output of the tool call.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"tool_end\"] = \"tool_end\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolStart.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The agent that updated.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolStart.tool","title":"tool  <code>instance-attribute</code>","text":"<pre><code>tool: Tool\n</code></pre> <p>The tool being called.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolStart.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre> <p>The arguments passed to the tool as a JSON string.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolStart.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The agent that ended the tool call.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.tool","title":"tool  <code>instance-attribute</code>","text":"<pre><code>tool: Tool\n</code></pre> <p>The tool that was called.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre> <p>The arguments passed to the tool as a JSON string.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: Any\n</code></pre> <p>The output of the tool call.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#handoff-events","title":"Handoff Events","text":"<p>An agent has handed off to another agent.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeHandoffEvent:\n    \"\"\"An agent has handed off to another agent.\"\"\"\n\n    from_agent: RealtimeAgent\n    \"\"\"The agent that handed off.\"\"\"\n\n    to_agent: RealtimeAgent\n    \"\"\"The agent that was handed off to.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"handoff\"] = \"handoff\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHandoffEvent.from_agent","title":"from_agent  <code>instance-attribute</code>","text":"<pre><code>from_agent: RealtimeAgent\n</code></pre> <p>The agent that handed off.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHandoffEvent.to_agent","title":"to_agent  <code>instance-attribute</code>","text":"<pre><code>to_agent: RealtimeAgent\n</code></pre> <p>The agent that was handed off to.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHandoffEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#guardrail-events","title":"Guardrail Events","text":"<p>A guardrail has been tripped and the agent has been interrupted.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeGuardrailTripped:\n    \"\"\"A guardrail has been tripped and the agent has been interrupted.\"\"\"\n\n    guardrail_results: list[OutputGuardrailResult]\n    \"\"\"The results from all triggered guardrails.\"\"\"\n\n    message: str\n    \"\"\"The message that was being generated when the guardrail was triggered.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"guardrail_tripped\"] = \"guardrail_tripped\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeGuardrailTripped.guardrail_results","title":"guardrail_results  <code>instance-attribute</code>","text":"<pre><code>guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>The results from all triggered guardrails.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeGuardrailTripped.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>The message that was being generated when the guardrail was triggered.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeGuardrailTripped.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#history-events","title":"History Events","text":"<p>A new item has been added to the history.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeHistoryAdded:\n    \"\"\"A new item has been added to the history.\"\"\"\n\n    item: RealtimeItem\n    \"\"\"The new item that was added to the history.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"history_added\"] = \"history_added\"\n</code></pre> <p>The history has been updated. Contains the full history of the session.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeHistoryUpdated:\n    \"\"\"The history has been updated. Contains the full history of the session.\"\"\"\n\n    history: list[RealtimeItem]\n    \"\"\"The full history of the session.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"history_updated\"] = \"history_updated\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryAdded.item","title":"item  <code>instance-attribute</code>","text":"<pre><code>item: RealtimeItem\n</code></pre> <p>The new item that was added to the history.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryAdded.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryUpdated.history","title":"history  <code>instance-attribute</code>","text":"<pre><code>history: list[RealtimeItem]\n</code></pre> <p>The full history of the session.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryUpdated.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#error-events","title":"Error Events","text":"<p>An error has occurred.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeError:\n    \"\"\"An error has occurred.\"\"\"\n\n    error: Any\n    \"\"\"The error that occurred.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"error\"] = \"error\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeError.error","title":"error  <code>instance-attribute</code>","text":"<pre><code>error: Any\n</code></pre> <p>The error that occurred.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeError.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#raw-model-events","title":"Raw Model Events","text":"<p>Forwards raw events from the model layer.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeRawModelEvent:\n    \"\"\"Forwards raw events from the model layer.\"\"\"\n\n    data: RealtimeModelEvent\n    \"\"\"The raw data from the model layer.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"raw_model_event\"] = \"raw_model_event\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeRawModelEvent.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: RealtimeModelEvent\n</code></pre> <p>The raw data from the model layer.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeRawModelEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/handoffs/","title":"<code>Handoffs</code>","text":""},{"location":"ref/realtime/handoffs/#agents.realtime.handoffs.realtime_handoff","title":"realtime_handoff","text":"<pre><code>realtime_handoff(\n    agent: RealtimeAgent[TContext],\n    *,\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], RealtimeAgent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, RealtimeAgent[TContext]]\n</code></pre><pre><code>realtime_handoff(\n    agent: RealtimeAgent[TContext],\n    *,\n    on_handoff: OnHandoffWithInput[THandoffInput],\n    input_type: type[THandoffInput],\n    tool_description_override: str | None = None,\n    tool_name_override: str | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], RealtimeAgent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, RealtimeAgent[TContext]]\n</code></pre><pre><code>realtime_handoff(\n    agent: RealtimeAgent[TContext],\n    *,\n    on_handoff: OnHandoffWithoutInput,\n    tool_description_override: str | None = None,\n    tool_name_override: str | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], RealtimeAgent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, RealtimeAgent[TContext]]\n</code></pre> <pre><code>realtime_handoff(\n    agent: RealtimeAgent[TContext],\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    on_handoff: OnHandoffWithInput[THandoffInput]\n    | OnHandoffWithoutInput\n    | None = None,\n    input_type: type[THandoffInput] | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], RealtimeAgent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, RealtimeAgent[TContext]]\n</code></pre> <p>Create a handoff from a RealtimeAgent.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>RealtimeAgent[TContext]</code> <p>The RealtimeAgent to handoff to.</p> required <code>tool_name_override</code> <code>str | None</code> <p>Optional override for the name of the tool that represents the handoff.</p> <code>None</code> <code>tool_description_override</code> <code>str | None</code> <p>Optional override for the description of the tool that represents the handoff.</p> <code>None</code> <code>on_handoff</code> <code>OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None</code> <p>A function that runs when the handoff is invoked.</p> <code>None</code> <code>input_type</code> <code>type[THandoffInput] | None</code> <p>the type of the input to the handoff. If provided, the input will be validated against this type. Only relevant if you pass a function that takes an input.</p> <code>None</code> <code>is_enabled</code> <code>bool | Callable[[RunContextWrapper[Any], RealtimeAgent[Any]], MaybeAwaitable[bool]]</code> <p>Whether the handoff is enabled. Can be a bool or a callable that takes the run context and agent and returns whether the handoff is enabled. Disabled handoffs are hidden from the LLM at runtime.</p> <code>True</code> <p>Note: input_filter is not supported for RealtimeAgent handoffs.</p> Source code in <code>src/agents/realtime/handoffs.py</code> <pre><code>def realtime_handoff(\n    agent: RealtimeAgent[TContext],\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    on_handoff: OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None = None,\n    input_type: type[THandoffInput] | None = None,\n    is_enabled: bool\n    | Callable[[RunContextWrapper[Any], RealtimeAgent[Any]], MaybeAwaitable[bool]] = True,\n) -&gt; Handoff[TContext, RealtimeAgent[TContext]]:\n    \"\"\"Create a handoff from a RealtimeAgent.\n\n    Args:\n        agent: The RealtimeAgent to handoff to.\n        tool_name_override: Optional override for the name of the tool that represents the handoff.\n        tool_description_override: Optional override for the description of the tool that\n            represents the handoff.\n        on_handoff: A function that runs when the handoff is invoked.\n        input_type: the type of the input to the handoff. If provided, the input will be validated\n            against this type. Only relevant if you pass a function that takes an input.\n        is_enabled: Whether the handoff is enabled. Can be a bool or a callable that takes the run\n            context and agent and returns whether the handoff is enabled. Disabled handoffs are\n            hidden from the LLM at runtime.\n\n    Note: input_filter is not supported for RealtimeAgent handoffs.\n    \"\"\"\n    assert (on_handoff and input_type) or not (on_handoff and input_type), (\n        \"You must provide either both on_handoff and input_type, or neither\"\n    )\n    type_adapter: TypeAdapter[Any] | None\n    if input_type is not None:\n        assert callable(on_handoff), \"on_handoff must be callable\"\n        sig = inspect.signature(on_handoff)\n        if len(sig.parameters) != 2:\n            raise UserError(\"on_handoff must take two arguments: context and input\")\n\n        type_adapter = TypeAdapter(input_type)\n        input_json_schema = type_adapter.json_schema()\n    else:\n        type_adapter = None\n        input_json_schema = {}\n        if on_handoff is not None:\n            sig = inspect.signature(on_handoff)\n            if len(sig.parameters) != 1:\n                raise UserError(\"on_handoff must take one argument: context\")\n\n    async def _invoke_handoff(\n        ctx: RunContextWrapper[Any], input_json: str | None = None\n    ) -&gt; RealtimeAgent[TContext]:\n        if input_type is not None and type_adapter is not None:\n            if input_json is None:\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Handoff function expected non-null input, but got None\",\n                        data={\"details\": \"input_json is None\"},\n                    )\n                )\n                raise ModelBehaviorError(\"Handoff function expected non-null input, but got None\")\n\n            validated_input = _json.validate_json(\n                json_str=input_json,\n                type_adapter=type_adapter,\n                partial=False,\n            )\n            input_func = cast(OnHandoffWithInput[THandoffInput], on_handoff)\n            if inspect.iscoroutinefunction(input_func):\n                await input_func(ctx, validated_input)\n            else:\n                input_func(ctx, validated_input)\n        elif on_handoff is not None:\n            no_input_func = cast(OnHandoffWithoutInput, on_handoff)\n            if inspect.iscoroutinefunction(no_input_func):\n                await no_input_func(ctx)\n            else:\n                no_input_func(ctx)\n\n        return agent\n\n    tool_name = tool_name_override or Handoff.default_tool_name(agent)\n    tool_description = tool_description_override or Handoff.default_tool_description(agent)\n\n    # Always ensure the input JSON schema is in strict mode\n    # If there is a need, we can make this configurable in the future\n    input_json_schema = ensure_strict_json_schema(input_json_schema)\n\n    async def _is_enabled(ctx: RunContextWrapper[Any], agent_base: AgentBase[Any]) -&gt; bool:\n        assert callable(is_enabled), \"is_enabled must be non-null here\"\n        assert isinstance(agent_base, RealtimeAgent), \"Can't handoff to a non-RealtimeAgent\"\n        result = is_enabled(ctx, agent_base)\n        if inspect.isawaitable(result):\n            return await result\n        return result\n\n    return Handoff(\n        tool_name=tool_name,\n        tool_description=tool_description,\n        input_json_schema=input_json_schema,\n        on_invoke_handoff=_invoke_handoff,\n        input_filter=None,  # Not supported for RealtimeAgent handoffs\n        agent_name=agent.name,\n        is_enabled=_is_enabled if callable(is_enabled) else is_enabled,\n    )\n</code></pre>"},{"location":"ref/realtime/items/","title":"<code>Items</code>","text":""},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeMessageItem","title":"RealtimeMessageItem  <code>module-attribute</code>","text":"<pre><code>RealtimeMessageItem = Annotated[\n    Union[\n        SystemMessageItem,\n        UserMessageItem,\n        AssistantMessageItem,\n    ],\n    Field(discriminator=\"role\"),\n]\n</code></pre> <p>A message item that can be from system, user, or assistant.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeItem","title":"RealtimeItem  <code>module-attribute</code>","text":"<pre><code>RealtimeItem = Union[\n    RealtimeMessageItem, RealtimeToolCallItem\n]\n</code></pre> <p>A realtime item that can be a message or tool call.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputText","title":"InputText","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text input content for realtime messages.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class InputText(BaseModel):\n    \"\"\"Text input content for realtime messages.\"\"\"\n\n    type: Literal[\"input_text\"] = \"input_text\"\n    \"\"\"The type identifier for text input.\"\"\"\n\n    text: str | None = None\n    \"\"\"The text content.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.InputText.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['input_text'] = 'input_text'\n</code></pre> <p>The type identifier for text input.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputText.text","title":"text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text: str | None = None\n</code></pre> <p>The text content.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputAudio","title":"InputAudio","text":"<p>               Bases: <code>BaseModel</code></p> <p>Audio input content for realtime messages.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class InputAudio(BaseModel):\n    \"\"\"Audio input content for realtime messages.\"\"\"\n\n    type: Literal[\"input_audio\"] = \"input_audio\"\n    \"\"\"The type identifier for audio input.\"\"\"\n\n    audio: str | None = None\n    \"\"\"The base64-encoded audio data.\"\"\"\n\n    transcript: str | None = None\n    \"\"\"The transcript of the audio, if available.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.InputAudio.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['input_audio'] = 'input_audio'\n</code></pre> <p>The type identifier for audio input.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputAudio.audio","title":"audio  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>audio: str | None = None\n</code></pre> <p>The base64-encoded audio data.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputAudio.transcript","title":"transcript  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transcript: str | None = None\n</code></pre> <p>The transcript of the audio, if available.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputImage","title":"InputImage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Image input content for realtime messages.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class InputImage(BaseModel):\n    \"\"\"Image input content for realtime messages.\"\"\"\n\n    type: Literal[\"input_image\"] = \"input_image\"\n    \"\"\"The type identifier for image input.\"\"\"\n\n    image_url: str | None = None\n    \"\"\"Data/remote URL string (data:... or https:...).\"\"\"\n\n    detail: str | None = None\n    \"\"\"Optional detail hint (e.g., 'auto', 'high', 'low').\"\"\"\n\n    # Allow extra data (e.g., `detail`)\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.InputImage.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['input_image'] = 'input_image'\n</code></pre> <p>The type identifier for image input.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputImage.image_url","title":"image_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_url: str | None = None\n</code></pre> <p>Data/remote URL string (data:... or https:...).</p>"},{"location":"ref/realtime/items/#agents.realtime.items.InputImage.detail","title":"detail  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>detail: str | None = None\n</code></pre> <p>Optional detail hint (e.g., 'auto', 'high', 'low').</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantText","title":"AssistantText","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text content from the assistant in realtime responses.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class AssistantText(BaseModel):\n    \"\"\"Text content from the assistant in realtime responses.\"\"\"\n\n    type: Literal[\"text\"] = \"text\"\n    \"\"\"The type identifier for text content.\"\"\"\n\n    text: str | None = None\n    \"\"\"The text content from the assistant.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantText.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['text'] = 'text'\n</code></pre> <p>The type identifier for text content.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantText.text","title":"text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text: str | None = None\n</code></pre> <p>The text content from the assistant.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantAudio","title":"AssistantAudio","text":"<p>               Bases: <code>BaseModel</code></p> <p>Audio content from the assistant in realtime responses.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class AssistantAudio(BaseModel):\n    \"\"\"Audio content from the assistant in realtime responses.\"\"\"\n\n    type: Literal[\"audio\"] = \"audio\"\n    \"\"\"The type identifier for audio content.\"\"\"\n\n    audio: str | None = None\n    \"\"\"The base64-encoded audio data from the assistant.\"\"\"\n\n    transcript: str | None = None\n    \"\"\"The transcript of the audio response.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantAudio.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['audio'] = 'audio'\n</code></pre> <p>The type identifier for audio content.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantAudio.audio","title":"audio  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>audio: str | None = None\n</code></pre> <p>The base64-encoded audio data from the assistant.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantAudio.transcript","title":"transcript  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transcript: str | None = None\n</code></pre> <p>The transcript of the audio response.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.SystemMessageItem","title":"SystemMessageItem","text":"<p>               Bases: <code>BaseModel</code></p> <p>A system message item in realtime conversations.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class SystemMessageItem(BaseModel):\n    \"\"\"A system message item in realtime conversations.\"\"\"\n\n    item_id: str\n    \"\"\"Unique identifier for this message item.\"\"\"\n\n    previous_item_id: str | None = None\n    \"\"\"ID of the previous item in the conversation.\"\"\"\n\n    type: Literal[\"message\"] = \"message\"\n    \"\"\"The type identifier for message items.\"\"\"\n\n    role: Literal[\"system\"] = \"system\"\n    \"\"\"The role identifier for system messages.\"\"\"\n\n    content: list[InputText]\n    \"\"\"List of text content for the system message.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.SystemMessageItem.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>Unique identifier for this message item.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.SystemMessageItem.previous_item_id","title":"previous_item_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>previous_item_id: str | None = None\n</code></pre> <p>ID of the previous item in the conversation.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.SystemMessageItem.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['message'] = 'message'\n</code></pre> <p>The type identifier for message items.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.SystemMessageItem.role","title":"role  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>role: Literal['system'] = 'system'\n</code></pre> <p>The role identifier for system messages.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.SystemMessageItem.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: list[InputText]\n</code></pre> <p>List of text content for the system message.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.UserMessageItem","title":"UserMessageItem","text":"<p>               Bases: <code>BaseModel</code></p> <p>A user message item in realtime conversations.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class UserMessageItem(BaseModel):\n    \"\"\"A user message item in realtime conversations.\"\"\"\n\n    item_id: str\n    \"\"\"Unique identifier for this message item.\"\"\"\n\n    previous_item_id: str | None = None\n    \"\"\"ID of the previous item in the conversation.\"\"\"\n\n    type: Literal[\"message\"] = \"message\"\n    \"\"\"The type identifier for message items.\"\"\"\n\n    role: Literal[\"user\"] = \"user\"\n    \"\"\"The role identifier for user messages.\"\"\"\n\n    content: list[Annotated[InputText | InputAudio | InputImage, Field(discriminator=\"type\")]]\n    \"\"\"List of content items, can be text or audio.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.UserMessageItem.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>Unique identifier for this message item.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.UserMessageItem.previous_item_id","title":"previous_item_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>previous_item_id: str | None = None\n</code></pre> <p>ID of the previous item in the conversation.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.UserMessageItem.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['message'] = 'message'\n</code></pre> <p>The type identifier for message items.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.UserMessageItem.role","title":"role  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>role: Literal['user'] = 'user'\n</code></pre> <p>The role identifier for user messages.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.UserMessageItem.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: list[\n    Annotated[\n        InputText | InputAudio | InputImage,\n        Field(discriminator=\"type\"),\n    ]\n]\n</code></pre> <p>List of content items, can be text or audio.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantMessageItem","title":"AssistantMessageItem","text":"<p>               Bases: <code>BaseModel</code></p> <p>An assistant message item in realtime conversations.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class AssistantMessageItem(BaseModel):\n    \"\"\"An assistant message item in realtime conversations.\"\"\"\n\n    item_id: str\n    \"\"\"Unique identifier for this message item.\"\"\"\n\n    previous_item_id: str | None = None\n    \"\"\"ID of the previous item in the conversation.\"\"\"\n\n    type: Literal[\"message\"] = \"message\"\n    \"\"\"The type identifier for message items.\"\"\"\n\n    role: Literal[\"assistant\"] = \"assistant\"\n    \"\"\"The role identifier for assistant messages.\"\"\"\n\n    status: Literal[\"in_progress\", \"completed\", \"incomplete\"] | None = None\n    \"\"\"The status of the assistant's response.\"\"\"\n\n    content: list[Annotated[AssistantText | AssistantAudio, Field(discriminator=\"type\")]]\n    \"\"\"List of content items from the assistant, can be text or audio.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantMessageItem.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>Unique identifier for this message item.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantMessageItem.previous_item_id","title":"previous_item_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>previous_item_id: str | None = None\n</code></pre> <p>ID of the previous item in the conversation.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantMessageItem.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['message'] = 'message'\n</code></pre> <p>The type identifier for message items.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantMessageItem.role","title":"role  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>role: Literal['assistant'] = 'assistant'\n</code></pre> <p>The role identifier for assistant messages.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantMessageItem.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: (\n    Literal[\"in_progress\", \"completed\", \"incomplete\"] | None\n) = None\n</code></pre> <p>The status of the assistant's response.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.AssistantMessageItem.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: list[\n    Annotated[\n        AssistantText | AssistantAudio,\n        Field(discriminator=\"type\"),\n    ]\n]\n</code></pre> <p>List of content items from the assistant, can be text or audio.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem","title":"RealtimeToolCallItem","text":"<p>               Bases: <code>BaseModel</code></p> <p>A tool call item in realtime conversations.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class RealtimeToolCallItem(BaseModel):\n    \"\"\"A tool call item in realtime conversations.\"\"\"\n\n    item_id: str\n    \"\"\"Unique identifier for this tool call item.\"\"\"\n\n    previous_item_id: str | None = None\n    \"\"\"ID of the previous item in the conversation.\"\"\"\n\n    call_id: str | None\n    \"\"\"The call ID for this tool invocation.\"\"\"\n\n    type: Literal[\"function_call\"] = \"function_call\"\n    \"\"\"The type identifier for function call items.\"\"\"\n\n    status: Literal[\"in_progress\", \"completed\"]\n    \"\"\"The status of the tool call execution.\"\"\"\n\n    arguments: str\n    \"\"\"The JSON string arguments passed to the tool.\"\"\"\n\n    name: str\n    \"\"\"The name of the tool being called.\"\"\"\n\n    output: str | None = None\n    \"\"\"The output result from the tool execution.\"\"\"\n\n    # Allow extra data\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>Unique identifier for this tool call item.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.previous_item_id","title":"previous_item_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>previous_item_id: str | None = None\n</code></pre> <p>ID of the previous item in the conversation.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.call_id","title":"call_id  <code>instance-attribute</code>","text":"<pre><code>call_id: str | None\n</code></pre> <p>The call ID for this tool invocation.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['function_call'] = 'function_call'\n</code></pre> <p>The type identifier for function call items.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: Literal['in_progress', 'completed']\n</code></pre> <p>The status of the tool call execution.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre> <p>The JSON string arguments passed to the tool.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the tool being called.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeToolCallItem.output","title":"output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output: str | None = None\n</code></pre> <p>The output result from the tool execution.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeResponse","title":"RealtimeResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response from the realtime model.</p> Source code in <code>src/agents/realtime/items.py</code> <pre><code>class RealtimeResponse(BaseModel):\n    \"\"\"A response from the realtime model.\"\"\"\n\n    id: str\n    \"\"\"Unique identifier for this response.\"\"\"\n\n    output: list[RealtimeMessageItem]\n    \"\"\"List of message items in the response.\"\"\"\n</code></pre>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeResponse.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>Unique identifier for this response.</p>"},{"location":"ref/realtime/items/#agents.realtime.items.RealtimeResponse.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: list[RealtimeMessageItem]\n</code></pre> <p>List of message items in the response.</p>"},{"location":"ref/realtime/model/","title":"<code>Model</code>","text":""},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackState","title":"RealtimePlaybackState","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>class RealtimePlaybackState(TypedDict):\n    current_item_id: str | None\n    \"\"\"The item ID of the current item being played.\"\"\"\n\n    current_item_content_index: int | None\n    \"\"\"The index of the current item content being played.\"\"\"\n\n    elapsed_ms: float | None\n    \"\"\"The number of milliseconds of audio that have been played.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackState.current_item_id","title":"current_item_id  <code>instance-attribute</code>","text":"<pre><code>current_item_id: str | None\n</code></pre> <p>The item ID of the current item being played.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackState.current_item_content_index","title":"current_item_content_index  <code>instance-attribute</code>","text":"<pre><code>current_item_content_index: int | None\n</code></pre> <p>The index of the current item content being played.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackState.elapsed_ms","title":"elapsed_ms  <code>instance-attribute</code>","text":"<pre><code>elapsed_ms: float | None\n</code></pre> <p>The number of milliseconds of audio that have been played.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackTracker","title":"RealtimePlaybackTracker","text":"<p>If you have custom playback logic or expect that audio is played with delays or at different speeds, create an instance of RealtimePlaybackTracker and pass it to the session. You are responsible for tracking the audio playback progress and calling <code>on_play_bytes</code> or <code>on_play_ms</code> when the user has played some audio.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>class RealtimePlaybackTracker:\n    \"\"\"If you have custom playback logic or expect that audio is played with delays or at different\n    speeds, create an instance of RealtimePlaybackTracker and pass it to the session. You are\n    responsible for tracking the audio playback progress and calling `on_play_bytes` or\n    `on_play_ms` when the user has played some audio.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._format: RealtimeAudioFormat | None = None\n        # (item_id, item_content_index)\n        self._current_item: tuple[str, int] | None = None\n        self._elapsed_ms: float | None = None\n\n    def on_play_bytes(self, item_id: str, item_content_index: int, bytes: bytes) -&gt; None:\n        \"\"\"Called by you when you have played some audio.\n\n        Args:\n            item_id: The item ID of the audio being played.\n            item_content_index: The index of the audio content in `item.content`\n            bytes: The audio bytes that have been fully played.\n        \"\"\"\n        ms = calculate_audio_length_ms(self._format, bytes)\n        self.on_play_ms(item_id, item_content_index, ms)\n\n    def on_play_ms(self, item_id: str, item_content_index: int, ms: float) -&gt; None:\n        \"\"\"Called by you when you have played some audio.\n\n        Args:\n            item_id: The item ID of the audio being played.\n            item_content_index: The index of the audio content in `item.content`\n            ms: The number of milliseconds of audio that have been played.\n        \"\"\"\n        if self._current_item != (item_id, item_content_index):\n            self._current_item = (item_id, item_content_index)\n            self._elapsed_ms = ms\n        else:\n            assert self._elapsed_ms is not None\n            self._elapsed_ms += ms\n\n    def on_interrupted(self) -&gt; None:\n        \"\"\"Called by the model when the audio playback has been interrupted.\"\"\"\n        self._current_item = None\n        self._elapsed_ms = None\n\n    def set_audio_format(self, format: RealtimeAudioFormat) -&gt; None:\n        \"\"\"Will be called by the model to set the audio format.\n\n        Args:\n            format: The audio format to use.\n        \"\"\"\n        self._format = format\n\n    def get_state(self) -&gt; RealtimePlaybackState:\n        \"\"\"Will be called by the model to get the current playback state.\"\"\"\n        if self._current_item is None:\n            return {\n                \"current_item_id\": None,\n                \"current_item_content_index\": None,\n                \"elapsed_ms\": None,\n            }\n        assert self._elapsed_ms is not None\n\n        item_id, item_content_index = self._current_item\n        return {\n            \"current_item_id\": item_id,\n            \"current_item_content_index\": item_content_index,\n            \"elapsed_ms\": self._elapsed_ms,\n        }\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackTracker.on_play_bytes","title":"on_play_bytes","text":"<pre><code>on_play_bytes(\n    item_id: str, item_content_index: int, bytes: bytes\n) -&gt; None\n</code></pre> <p>Called by you when you have played some audio.</p> <p>Parameters:</p> Name Type Description Default <code>item_id</code> <code>str</code> <p>The item ID of the audio being played.</p> required <code>item_content_index</code> <code>int</code> <p>The index of the audio content in <code>item.content</code></p> required <code>bytes</code> <code>bytes</code> <p>The audio bytes that have been fully played.</p> required Source code in <code>src/agents/realtime/model.py</code> <pre><code>def on_play_bytes(self, item_id: str, item_content_index: int, bytes: bytes) -&gt; None:\n    \"\"\"Called by you when you have played some audio.\n\n    Args:\n        item_id: The item ID of the audio being played.\n        item_content_index: The index of the audio content in `item.content`\n        bytes: The audio bytes that have been fully played.\n    \"\"\"\n    ms = calculate_audio_length_ms(self._format, bytes)\n    self.on_play_ms(item_id, item_content_index, ms)\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackTracker.on_play_ms","title":"on_play_ms","text":"<pre><code>on_play_ms(\n    item_id: str, item_content_index: int, ms: float\n) -&gt; None\n</code></pre> <p>Called by you when you have played some audio.</p> <p>Parameters:</p> Name Type Description Default <code>item_id</code> <code>str</code> <p>The item ID of the audio being played.</p> required <code>item_content_index</code> <code>int</code> <p>The index of the audio content in <code>item.content</code></p> required <code>ms</code> <code>float</code> <p>The number of milliseconds of audio that have been played.</p> required Source code in <code>src/agents/realtime/model.py</code> <pre><code>def on_play_ms(self, item_id: str, item_content_index: int, ms: float) -&gt; None:\n    \"\"\"Called by you when you have played some audio.\n\n    Args:\n        item_id: The item ID of the audio being played.\n        item_content_index: The index of the audio content in `item.content`\n        ms: The number of milliseconds of audio that have been played.\n    \"\"\"\n    if self._current_item != (item_id, item_content_index):\n        self._current_item = (item_id, item_content_index)\n        self._elapsed_ms = ms\n    else:\n        assert self._elapsed_ms is not None\n        self._elapsed_ms += ms\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackTracker.on_interrupted","title":"on_interrupted","text":"<pre><code>on_interrupted() -&gt; None\n</code></pre> <p>Called by the model when the audio playback has been interrupted.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>def on_interrupted(self) -&gt; None:\n    \"\"\"Called by the model when the audio playback has been interrupted.\"\"\"\n    self._current_item = None\n    self._elapsed_ms = None\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackTracker.set_audio_format","title":"set_audio_format","text":"<pre><code>set_audio_format(format: RealtimeAudioFormat) -&gt; None\n</code></pre> <p>Will be called by the model to set the audio format.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>RealtimeAudioFormat</code> <p>The audio format to use.</p> required Source code in <code>src/agents/realtime/model.py</code> <pre><code>def set_audio_format(self, format: RealtimeAudioFormat) -&gt; None:\n    \"\"\"Will be called by the model to set the audio format.\n\n    Args:\n        format: The audio format to use.\n    \"\"\"\n    self._format = format\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimePlaybackTracker.get_state","title":"get_state","text":"<pre><code>get_state() -&gt; RealtimePlaybackState\n</code></pre> <p>Will be called by the model to get the current playback state.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>def get_state(self) -&gt; RealtimePlaybackState:\n    \"\"\"Will be called by the model to get the current playback state.\"\"\"\n    if self._current_item is None:\n        return {\n            \"current_item_id\": None,\n            \"current_item_content_index\": None,\n            \"elapsed_ms\": None,\n        }\n    assert self._elapsed_ms is not None\n\n    item_id, item_content_index = self._current_item\n    return {\n        \"current_item_id\": item_id,\n        \"current_item_content_index\": item_content_index,\n        \"elapsed_ms\": self._elapsed_ms,\n    }\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelListener","title":"RealtimeModelListener","text":"<p>               Bases: <code>ABC</code></p> <p>A listener for realtime transport events.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>class RealtimeModelListener(abc.ABC):\n    \"\"\"A listener for realtime transport events.\"\"\"\n\n    @abc.abstractmethod\n    async def on_event(self, event: RealtimeModelEvent) -&gt; None:\n        \"\"\"Called when an event is emitted by the realtime transport.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelListener.on_event","title":"on_event  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>on_event(event: RealtimeModelEvent) -&gt; None\n</code></pre> <p>Called when an event is emitted by the realtime transport.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>@abc.abstractmethod\nasync def on_event(self, event: RealtimeModelEvent) -&gt; None:\n    \"\"\"Called when an event is emitted by the realtime transport.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelConfig","title":"RealtimeModelConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Options for connecting to a realtime model.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>class RealtimeModelConfig(TypedDict):\n    \"\"\"Options for connecting to a realtime model.\"\"\"\n\n    api_key: NotRequired[str | Callable[[], MaybeAwaitable[str]]]\n    \"\"\"The API key (or function that returns a key) to use when connecting. If unset, the model will\n    try to use a sane default. For example, the OpenAI Realtime model will try to use the\n    `OPENAI_API_KEY`  environment variable.\n    \"\"\"\n\n    url: NotRequired[str]\n    \"\"\"The URL to use when connecting. If unset, the model will use a sane default. For example,\n    the OpenAI Realtime model will use the default OpenAI WebSocket URL.\n    \"\"\"\n\n    headers: NotRequired[dict[str, str]]\n    \"\"\"The headers to use when connecting. If unset, the model will use a sane default.\n    Note that, when you set this, authorization header won't be set under the hood.\n    e.g., {\"api-key\": \"your api key here\"} for Azure OpenAI Realtime WebSocket connections.\n    \"\"\"\n\n    initial_model_settings: NotRequired[RealtimeSessionModelSettings]\n    \"\"\"The initial model settings to use when connecting.\"\"\"\n\n    playback_tracker: NotRequired[RealtimePlaybackTracker]\n    \"\"\"The playback tracker to use when tracking audio playback progress. If not set, the model will\n    use a default implementation that assumes audio is played immediately, at realtime speed.\n\n    A playback tracker is useful for interruptions. The model generates audio much faster than\n    realtime playback speed. So if there's an interruption, its useful for the model to know how\n    much of the audio has been played by the user. In low-latency scenarios, it's fine to assume\n    that audio is played back immediately at realtime speed. But in scenarios like phone calls or\n    other remote interactions, you can set a playback tracker that lets the model know when audio\n    is played to the user.\n    \"\"\"\n\n    call_id: NotRequired[str]\n    \"\"\"Attach to an existing realtime call instead of creating a new session.\n\n    When provided, the transport connects using the `call_id` query string parameter rather than a\n    model name. This is used for SIP-originated calls that are accepted via the Realtime Calls API.\n    \"\"\"\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelConfig.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: NotRequired[\n    str | Callable[[], MaybeAwaitable[str]]\n]\n</code></pre> <p>The API key (or function that returns a key) to use when connecting. If unset, the model will try to use a sane default. For example, the OpenAI Realtime model will try to use the <code>OPENAI_API_KEY</code>  environment variable.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelConfig.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: NotRequired[str]\n</code></pre> <p>The URL to use when connecting. If unset, the model will use a sane default. For example, the OpenAI Realtime model will use the default OpenAI WebSocket URL.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelConfig.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: NotRequired[dict[str, str]]\n</code></pre> <p>The headers to use when connecting. If unset, the model will use a sane default. Note that, when you set this, authorization header won't be set under the hood. e.g., {\"api-key\": \"your api key here\"} for Azure OpenAI Realtime WebSocket connections.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelConfig.initial_model_settings","title":"initial_model_settings  <code>instance-attribute</code>","text":"<pre><code>initial_model_settings: NotRequired[\n    RealtimeSessionModelSettings\n]\n</code></pre> <p>The initial model settings to use when connecting.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelConfig.playback_tracker","title":"playback_tracker  <code>instance-attribute</code>","text":"<pre><code>playback_tracker: NotRequired[RealtimePlaybackTracker]\n</code></pre> <p>The playback tracker to use when tracking audio playback progress. If not set, the model will use a default implementation that assumes audio is played immediately, at realtime speed.</p> <p>A playback tracker is useful for interruptions. The model generates audio much faster than realtime playback speed. So if there's an interruption, its useful for the model to know how much of the audio has been played by the user. In low-latency scenarios, it's fine to assume that audio is played back immediately at realtime speed. But in scenarios like phone calls or other remote interactions, you can set a playback tracker that lets the model know when audio is played to the user.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModelConfig.call_id","title":"call_id  <code>instance-attribute</code>","text":"<pre><code>call_id: NotRequired[str]\n</code></pre> <p>Attach to an existing realtime call instead of creating a new session.</p> <p>When provided, the transport connects using the <code>call_id</code> query string parameter rather than a model name. This is used for SIP-originated calls that are accepted via the Realtime Calls API.</p>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModel","title":"RealtimeModel","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for connecting to a realtime model and sending/receiving events.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>class RealtimeModel(abc.ABC):\n    \"\"\"Interface for connecting to a realtime model and sending/receiving events.\"\"\"\n\n    @abc.abstractmethod\n    async def connect(self, options: RealtimeModelConfig) -&gt; None:\n        \"\"\"Establish a connection to the model and keep it alive.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def add_listener(self, listener: RealtimeModelListener) -&gt; None:\n        \"\"\"Add a listener to the model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def remove_listener(self, listener: RealtimeModelListener) -&gt; None:\n        \"\"\"Remove a listener from the model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def send_event(self, event: RealtimeModelSendEvent) -&gt; None:\n        \"\"\"Send an event to the model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def close(self) -&gt; None:\n        \"\"\"Close the session.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModel.connect","title":"connect  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>connect(options: RealtimeModelConfig) -&gt; None\n</code></pre> <p>Establish a connection to the model and keep it alive.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>@abc.abstractmethod\nasync def connect(self, options: RealtimeModelConfig) -&gt; None:\n    \"\"\"Establish a connection to the model and keep it alive.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModel.add_listener","title":"add_listener  <code>abstractmethod</code>","text":"<pre><code>add_listener(listener: RealtimeModelListener) -&gt; None\n</code></pre> <p>Add a listener to the model.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>@abc.abstractmethod\ndef add_listener(self, listener: RealtimeModelListener) -&gt; None:\n    \"\"\"Add a listener to the model.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModel.remove_listener","title":"remove_listener  <code>abstractmethod</code>","text":"<pre><code>remove_listener(listener: RealtimeModelListener) -&gt; None\n</code></pre> <p>Remove a listener from the model.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>@abc.abstractmethod\ndef remove_listener(self, listener: RealtimeModelListener) -&gt; None:\n    \"\"\"Remove a listener from the model.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModel.send_event","title":"send_event  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>send_event(event: RealtimeModelSendEvent) -&gt; None\n</code></pre> <p>Send an event to the model.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>@abc.abstractmethod\nasync def send_event(self, event: RealtimeModelSendEvent) -&gt; None:\n    \"\"\"Send an event to the model.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/realtime/model/#agents.realtime.model.RealtimeModel.close","title":"close  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the session.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>@abc.abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Close the session.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/realtime/model_events/","title":"<code>Model Events</code>","text":""},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelErrorEvent","title":"RealtimeModelErrorEvent  <code>dataclass</code>","text":"<p>Represents a transport\u2011layer error.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelErrorEvent:\n    \"\"\"Represents a transport\u2011layer error.\"\"\"\n\n    error: Any\n\n    type: Literal[\"error\"] = \"error\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelToolCallEvent","title":"RealtimeModelToolCallEvent  <code>dataclass</code>","text":"<p>Model attempted a tool/function call.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelToolCallEvent:\n    \"\"\"Model attempted a tool/function call.\"\"\"\n\n    name: str\n    call_id: str\n    arguments: str\n\n    id: str | None = None\n    previous_item_id: str | None = None\n\n    type: Literal[\"function_call\"] = \"function_call\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioEvent","title":"RealtimeModelAudioEvent  <code>dataclass</code>","text":"<p>Raw audio bytes emitted by the model.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelAudioEvent:\n    \"\"\"Raw audio bytes emitted by the model.\"\"\"\n\n    data: bytes\n    response_id: str\n\n    item_id: str\n    \"\"\"The ID of the item containing audio.\"\"\"\n\n    content_index: int\n    \"\"\"The index of the audio content in `item.content`\"\"\"\n\n    type: Literal[\"audio\"] = \"audio\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioEvent.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>The ID of the item containing audio.</p>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioEvent.content_index","title":"content_index  <code>instance-attribute</code>","text":"<pre><code>content_index: int\n</code></pre> <p>The index of the audio content in <code>item.content</code></p>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioInterruptedEvent","title":"RealtimeModelAudioInterruptedEvent  <code>dataclass</code>","text":"<p>Audio interrupted.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelAudioInterruptedEvent:\n    \"\"\"Audio interrupted.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the item containing audio.\"\"\"\n\n    content_index: int\n    \"\"\"The index of the audio content in `item.content`\"\"\"\n\n    type: Literal[\"audio_interrupted\"] = \"audio_interrupted\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioInterruptedEvent.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>The ID of the item containing audio.</p>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioInterruptedEvent.content_index","title":"content_index  <code>instance-attribute</code>","text":"<pre><code>content_index: int\n</code></pre> <p>The index of the audio content in <code>item.content</code></p>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioDoneEvent","title":"RealtimeModelAudioDoneEvent  <code>dataclass</code>","text":"<p>Audio done.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelAudioDoneEvent:\n    \"\"\"Audio done.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the item containing audio.\"\"\"\n\n    content_index: int\n    \"\"\"The index of the audio content in `item.content`\"\"\"\n\n    type: Literal[\"audio_done\"] = \"audio_done\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioDoneEvent.item_id","title":"item_id  <code>instance-attribute</code>","text":"<pre><code>item_id: str\n</code></pre> <p>The ID of the item containing audio.</p>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelAudioDoneEvent.content_index","title":"content_index  <code>instance-attribute</code>","text":"<pre><code>content_index: int\n</code></pre> <p>The index of the audio content in <code>item.content</code></p>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelInputAudioTranscriptionCompletedEvent","title":"RealtimeModelInputAudioTranscriptionCompletedEvent  <code>dataclass</code>","text":"<p>Input audio transcription completed.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelInputAudioTranscriptionCompletedEvent:\n    \"\"\"Input audio transcription completed.\"\"\"\n\n    item_id: str\n    transcript: str\n\n    type: Literal[\"input_audio_transcription_completed\"] = \"input_audio_transcription_completed\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelInputAudioTimeoutTriggeredEvent","title":"RealtimeModelInputAudioTimeoutTriggeredEvent  <code>dataclass</code>","text":"<p>Input audio timeout triggered.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelInputAudioTimeoutTriggeredEvent:\n    \"\"\"Input audio timeout triggered.\"\"\"\n\n    item_id: str\n    audio_start_ms: int\n    audio_end_ms: int\n\n    type: Literal[\"input_audio_timeout_triggered\"] = \"input_audio_timeout_triggered\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelTranscriptDeltaEvent","title":"RealtimeModelTranscriptDeltaEvent  <code>dataclass</code>","text":"<p>Partial transcript update.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelTranscriptDeltaEvent:\n    \"\"\"Partial transcript update.\"\"\"\n\n    item_id: str\n    delta: str\n    response_id: str\n\n    type: Literal[\"transcript_delta\"] = \"transcript_delta\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelItemUpdatedEvent","title":"RealtimeModelItemUpdatedEvent  <code>dataclass</code>","text":"<p>Item added to the history or updated.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelItemUpdatedEvent:\n    \"\"\"Item added to the history or updated.\"\"\"\n\n    item: RealtimeItem\n\n    type: Literal[\"item_updated\"] = \"item_updated\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelItemDeletedEvent","title":"RealtimeModelItemDeletedEvent  <code>dataclass</code>","text":"<p>Item deleted from the history.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelItemDeletedEvent:\n    \"\"\"Item deleted from the history.\"\"\"\n\n    item_id: str\n\n    type: Literal[\"item_deleted\"] = \"item_deleted\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelConnectionStatusEvent","title":"RealtimeModelConnectionStatusEvent  <code>dataclass</code>","text":"<p>Connection status changed.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelConnectionStatusEvent:\n    \"\"\"Connection status changed.\"\"\"\n\n    status: RealtimeConnectionStatus\n\n    type: Literal[\"connection_status\"] = \"connection_status\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelTurnStartedEvent","title":"RealtimeModelTurnStartedEvent  <code>dataclass</code>","text":"<p>Triggered when the model starts generating a response for a turn.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelTurnStartedEvent:\n    \"\"\"Triggered when the model starts generating a response for a turn.\"\"\"\n\n    type: Literal[\"turn_started\"] = \"turn_started\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelTurnEndedEvent","title":"RealtimeModelTurnEndedEvent  <code>dataclass</code>","text":"<p>Triggered when the model finishes generating a response for a turn.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelTurnEndedEvent:\n    \"\"\"Triggered when the model finishes generating a response for a turn.\"\"\"\n\n    type: Literal[\"turn_ended\"] = \"turn_ended\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelOtherEvent","title":"RealtimeModelOtherEvent  <code>dataclass</code>","text":"<p>Used as a catchall for vendor-specific events.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelOtherEvent:\n    \"\"\"Used as a catchall for vendor-specific events.\"\"\"\n\n    data: Any\n\n    type: Literal[\"other\"] = \"other\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelExceptionEvent","title":"RealtimeModelExceptionEvent  <code>dataclass</code>","text":"<p>Exception occurred during model operation.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelExceptionEvent:\n    \"\"\"Exception occurred during model operation.\"\"\"\n\n    exception: Exception\n    context: str | None = None\n\n    type: Literal[\"exception\"] = \"exception\"\n</code></pre>"},{"location":"ref/realtime/model_events/#agents.realtime.model_events.RealtimeModelRawServerEvent","title":"RealtimeModelRawServerEvent  <code>dataclass</code>","text":"<p>Raw events forwarded from the server.</p> Source code in <code>src/agents/realtime/model_events.py</code> <pre><code>@dataclass\nclass RealtimeModelRawServerEvent:\n    \"\"\"Raw events forwarded from the server.\"\"\"\n\n    data: Any\n\n    type: Literal[\"raw_server_event\"] = \"raw_server_event\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/","title":"<code>Model Inputs</code>","text":""},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelUserInput","title":"RealtimeModelUserInput  <code>module-attribute</code>","text":"<pre><code>RealtimeModelUserInput: TypeAlias = Union[\n    str, RealtimeModelUserInputMessage\n]\n</code></pre> <p>A user input to be sent to the model.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelRawClientMessage","title":"RealtimeModelRawClientMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>A raw message to be sent to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>class RealtimeModelRawClientMessage(TypedDict):\n    \"\"\"A raw message to be sent to the model.\"\"\"\n\n    type: str  # explicitly required\n    other_data: NotRequired[dict[str, Any]]\n    \"\"\"Merged into the message body.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelRawClientMessage.other_data","title":"other_data  <code>instance-attribute</code>","text":"<pre><code>other_data: NotRequired[dict[str, Any]]\n</code></pre> <p>Merged into the message body.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelInputTextContent","title":"RealtimeModelInputTextContent","text":"<p>               Bases: <code>TypedDict</code></p> <p>A piece of text to be sent to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>class RealtimeModelInputTextContent(TypedDict):\n    \"\"\"A piece of text to be sent to the model.\"\"\"\n\n    type: Literal[\"input_text\"]\n    text: str\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelInputImageContent","title":"RealtimeModelInputImageContent","text":"<p>               Bases: <code>TypedDict</code></p> <p>An image to be sent to the model.</p> <p>The Realtime API expects <code>image_url</code> to be a string data/remote URL.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>class RealtimeModelInputImageContent(TypedDict, total=False):\n    \"\"\"An image to be sent to the model.\n\n    The Realtime API expects `image_url` to be a string data/remote URL.\n    \"\"\"\n\n    type: Literal[\"input_image\"]\n    image_url: str\n    \"\"\"String URL (data:... or https:...).\"\"\"\n\n    detail: NotRequired[str]\n    \"\"\"Optional detail hint such as 'high', 'low', or 'auto'.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelInputImageContent.image_url","title":"image_url  <code>instance-attribute</code>","text":"<pre><code>image_url: str\n</code></pre> <p>String URL (data:... or https:...).</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelInputImageContent.detail","title":"detail  <code>instance-attribute</code>","text":"<pre><code>detail: NotRequired[str]\n</code></pre> <p>Optional detail hint such as 'high', 'low', or 'auto'.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelUserInputMessage","title":"RealtimeModelUserInputMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>A message to be sent to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>class RealtimeModelUserInputMessage(TypedDict):\n    \"\"\"A message to be sent to the model.\"\"\"\n\n    type: Literal[\"message\"]\n    role: Literal[\"user\"]\n    content: list[RealtimeModelInputTextContent | RealtimeModelInputImageContent]\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendRawMessage","title":"RealtimeModelSendRawMessage  <code>dataclass</code>","text":"<p>Send a raw message to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>@dataclass\nclass RealtimeModelSendRawMessage:\n    \"\"\"Send a raw message to the model.\"\"\"\n\n    message: RealtimeModelRawClientMessage\n    \"\"\"The message to send.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendRawMessage.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: RealtimeModelRawClientMessage\n</code></pre> <p>The message to send.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendUserInput","title":"RealtimeModelSendUserInput  <code>dataclass</code>","text":"<p>Send a user input to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>@dataclass\nclass RealtimeModelSendUserInput:\n    \"\"\"Send a user input to the model.\"\"\"\n\n    user_input: RealtimeModelUserInput\n    \"\"\"The user input to send.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendUserInput.user_input","title":"user_input  <code>instance-attribute</code>","text":"<pre><code>user_input: RealtimeModelUserInput\n</code></pre> <p>The user input to send.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendAudio","title":"RealtimeModelSendAudio  <code>dataclass</code>","text":"<p>Send audio to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>@dataclass\nclass RealtimeModelSendAudio:\n    \"\"\"Send audio to the model.\"\"\"\n\n    audio: bytes\n    commit: bool = False\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendToolOutput","title":"RealtimeModelSendToolOutput  <code>dataclass</code>","text":"<p>Send tool output to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>@dataclass\nclass RealtimeModelSendToolOutput:\n    \"\"\"Send tool output to the model.\"\"\"\n\n    tool_call: RealtimeModelToolCallEvent\n    \"\"\"The tool call to send.\"\"\"\n\n    output: str\n    \"\"\"The output to send.\"\"\"\n\n    start_response: bool\n    \"\"\"Whether to start a response.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendToolOutput.tool_call","title":"tool_call  <code>instance-attribute</code>","text":"<pre><code>tool_call: RealtimeModelToolCallEvent\n</code></pre> <p>The tool call to send.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendToolOutput.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: str\n</code></pre> <p>The output to send.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendToolOutput.start_response","title":"start_response  <code>instance-attribute</code>","text":"<pre><code>start_response: bool\n</code></pre> <p>Whether to start a response.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendInterrupt","title":"RealtimeModelSendInterrupt  <code>dataclass</code>","text":"<p>Send an interrupt to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>@dataclass\nclass RealtimeModelSendInterrupt:\n    \"\"\"Send an interrupt to the model.\"\"\"\n\n    force_response_cancel: bool = False\n    \"\"\"Force sending a response.cancel event even if automatic cancellation is enabled.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendInterrupt.force_response_cancel","title":"force_response_cancel  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_response_cancel: bool = False\n</code></pre> <p>Force sending a response.cancel event even if automatic cancellation is enabled.</p>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendSessionUpdate","title":"RealtimeModelSendSessionUpdate  <code>dataclass</code>","text":"<p>Send a session update to the model.</p> Source code in <code>src/agents/realtime/model_inputs.py</code> <pre><code>@dataclass\nclass RealtimeModelSendSessionUpdate:\n    \"\"\"Send a session update to the model.\"\"\"\n\n    session_settings: RealtimeSessionModelSettings\n    \"\"\"The updated session settings to send.\"\"\"\n</code></pre>"},{"location":"ref/realtime/model_inputs/#agents.realtime.model_inputs.RealtimeModelSendSessionUpdate.session_settings","title":"session_settings  <code>instance-attribute</code>","text":"<pre><code>session_settings: RealtimeSessionModelSettings\n</code></pre> <p>The updated session settings to send.</p>"},{"location":"ref/realtime/openai_realtime/","title":"<code>Openai Realtime</code>","text":""},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.TransportConfig","title":"TransportConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Low-level network transport configuration.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>class TransportConfig(TypedDict):\n    \"\"\"Low-level network transport configuration.\"\"\"\n\n    ping_interval: NotRequired[float | None]\n    \"\"\"Time in seconds between keepalive pings sent by the client.\n    Default is usually 20.0. Set to None to disable.\"\"\"\n\n    ping_timeout: NotRequired[float | None]\n    \"\"\"Time in seconds to wait for a pong response before disconnecting.\n    Set to None to disable ping timeout and keep an open connection (ignore network lag).\"\"\"\n\n    handshake_timeout: NotRequired[float]\n    \"\"\"Time in seconds to wait for the connection handshake to complete.\"\"\"\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.TransportConfig.ping_interval","title":"ping_interval  <code>instance-attribute</code>","text":"<pre><code>ping_interval: NotRequired[float | None]\n</code></pre> <p>Time in seconds between keepalive pings sent by the client. Default is usually 20.0. Set to None to disable.</p>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.TransportConfig.ping_timeout","title":"ping_timeout  <code>instance-attribute</code>","text":"<pre><code>ping_timeout: NotRequired[float | None]\n</code></pre> <p>Time in seconds to wait for a pong response before disconnecting. Set to None to disable ping timeout and keep an open connection (ignore network lag).</p>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.TransportConfig.handshake_timeout","title":"handshake_timeout  <code>instance-attribute</code>","text":"<pre><code>handshake_timeout: NotRequired[float]\n</code></pre> <p>Time in seconds to wait for the connection handshake to complete.</p>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeWebSocketModel","title":"OpenAIRealtimeWebSocketModel","text":"<p>               Bases: <code>RealtimeModel</code></p> <p>A model that uses OpenAI's WebSocket API.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>class OpenAIRealtimeWebSocketModel(RealtimeModel):\n    \"\"\"A model that uses OpenAI's WebSocket API.\"\"\"\n\n    def __init__(self, *, transport_config: TransportConfig | None = None) -&gt; None:\n        self.model = \"gpt-realtime\"  # Default model\n        self._websocket: ClientConnection | None = None\n        self._websocket_task: asyncio.Task[None] | None = None\n        self._listeners: list[RealtimeModelListener] = []\n        self._current_item_id: str | None = None\n        self._audio_state_tracker: ModelAudioTracker = ModelAudioTracker()\n        self._ongoing_response: bool = False\n        self._tracing_config: RealtimeModelTracingConfig | Literal[\"auto\"] | None = None\n        self._playback_tracker: RealtimePlaybackTracker | None = None\n        self._created_session: OpenAISessionCreateRequest | None = None\n        self._server_event_type_adapter = get_server_event_type_adapter()\n        self._call_id: str | None = None\n        self._transport_config: TransportConfig | None = transport_config\n\n    async def connect(self, options: RealtimeModelConfig) -&gt; None:\n        \"\"\"Establish a connection to the model and keep it alive.\"\"\"\n        assert self._websocket is None, \"Already connected\"\n        assert self._websocket_task is None, \"Already connected\"\n\n        model_settings: RealtimeSessionModelSettings = options.get(\"initial_model_settings\", {})\n\n        self._playback_tracker = options.get(\"playback_tracker\", None)\n\n        call_id = options.get(\"call_id\")\n        model_name = model_settings.get(\"model_name\")\n        if call_id and model_name:\n            error_message = (\n                \"Cannot specify both `call_id` and `model_name` \"\n                \"when attaching to an existing realtime call.\"\n            )\n            raise UserError(error_message)\n\n        if model_name:\n            self.model = model_name\n\n        self._call_id = call_id\n        api_key = await get_api_key(options.get(\"api_key\"))\n\n        if \"tracing\" in model_settings:\n            self._tracing_config = model_settings[\"tracing\"]\n        else:\n            self._tracing_config = \"auto\"\n\n        if call_id:\n            url = options.get(\"url\", f\"wss://api.openai.com/v1/realtime?call_id={call_id}\")\n        else:\n            url = options.get(\"url\", f\"wss://api.openai.com/v1/realtime?model={self.model}\")\n\n        headers: dict[str, str] = {}\n        if options.get(\"headers\") is not None:\n            # For customizing request headers\n            headers.update(options[\"headers\"])\n        else:\n            # OpenAI's Realtime API\n            if not api_key:\n                raise UserError(\"API key is required but was not provided.\")\n\n            headers.update({\"Authorization\": f\"Bearer {api_key}\"})\n\n        self._websocket = await self._create_websocket_connection(\n            url=url,\n            headers=headers,\n            transport_config=self._transport_config,\n        )\n        self._websocket_task = asyncio.create_task(self._listen_for_messages())\n        await self._update_session_config(model_settings)\n\n    async def _create_websocket_connection(\n        self,\n        url: str,\n        headers: dict[str, str],\n        transport_config: TransportConfig | None = None,\n    ) -&gt; ClientConnection:\n        \"\"\"Create a WebSocket connection with the given configuration.\n\n        Args:\n            url: The WebSocket URL to connect to.\n            headers: HTTP headers to include in the connection request.\n            transport_config: Optional low-level transport configuration.\n\n        Returns:\n            A connected WebSocket client connection.\n        \"\"\"\n        connect_kwargs: dict[str, Any] = {\n            \"user_agent_header\": _USER_AGENT,\n            \"additional_headers\": headers,\n            \"max_size\": None,  # Allow any size of message\n        }\n\n        if transport_config:\n            if \"ping_interval\" in transport_config:\n                connect_kwargs[\"ping_interval\"] = transport_config[\"ping_interval\"]\n            if \"ping_timeout\" in transport_config:\n                connect_kwargs[\"ping_timeout\"] = transport_config[\"ping_timeout\"]\n            if \"handshake_timeout\" in transport_config:\n                connect_kwargs[\"open_timeout\"] = transport_config[\"handshake_timeout\"]\n\n        return await websockets.connect(url, **connect_kwargs)\n\n    async def _send_tracing_config(\n        self, tracing_config: RealtimeModelTracingConfig | Literal[\"auto\"] | None\n    ) -&gt; None:\n        \"\"\"Update tracing configuration via session.update event.\"\"\"\n        if tracing_config is not None:\n            converted_tracing_config = _ConversionHelper.convert_tracing_config(tracing_config)\n            await self._send_raw_message(\n                OpenAISessionUpdateEvent(\n                    session=OpenAISessionCreateRequest(\n                        model=self.model,\n                        type=\"realtime\",\n                        tracing=converted_tracing_config,\n                    ),\n                    type=\"session.update\",\n                )\n            )\n\n    def add_listener(self, listener: RealtimeModelListener) -&gt; None:\n        \"\"\"Add a listener to the model.\"\"\"\n        if listener not in self._listeners:\n            self._listeners.append(listener)\n\n    def remove_listener(self, listener: RealtimeModelListener) -&gt; None:\n        \"\"\"Remove a listener from the model.\"\"\"\n        if listener in self._listeners:\n            self._listeners.remove(listener)\n\n    async def _emit_event(self, event: RealtimeModelEvent) -&gt; None:\n        \"\"\"Emit an event to the listeners.\"\"\"\n        # Copy list to avoid modification during iteration\n        for listener in list(self._listeners):\n            await listener.on_event(event)\n\n    async def _listen_for_messages(self):\n        assert self._websocket is not None, \"Not connected\"\n\n        try:\n            async for message in self._websocket:\n                try:\n                    parsed = json.loads(message)\n                    await self._handle_ws_event(parsed)\n                except json.JSONDecodeError as e:\n                    await self._emit_event(\n                        RealtimeModelExceptionEvent(\n                            exception=e, context=\"Failed to parse WebSocket message as JSON\"\n                        )\n                    )\n                except Exception as e:\n                    await self._emit_event(\n                        RealtimeModelExceptionEvent(\n                            exception=e, context=\"Error handling WebSocket event\"\n                        )\n                    )\n\n        except websockets.exceptions.ConnectionClosedOK:\n            # Normal connection closure - no exception event needed\n            logger.debug(\"WebSocket connection closed normally\")\n        except websockets.exceptions.ConnectionClosed as e:\n            await self._emit_event(\n                RealtimeModelExceptionEvent(\n                    exception=e, context=\"WebSocket connection closed unexpectedly\"\n                )\n            )\n        except Exception as e:\n            await self._emit_event(\n                RealtimeModelExceptionEvent(\n                    exception=e, context=\"WebSocket error in message listener\"\n                )\n            )\n\n    async def send_event(self, event: RealtimeModelSendEvent) -&gt; None:\n        \"\"\"Send an event to the model.\"\"\"\n        if isinstance(event, RealtimeModelSendRawMessage):\n            converted = _ConversionHelper.try_convert_raw_message(event)\n            if converted is not None:\n                await self._send_raw_message(converted)\n            else:\n                logger.error(f\"Failed to convert raw message: {event}\")\n        elif isinstance(event, RealtimeModelSendUserInput):\n            await self._send_user_input(event)\n        elif isinstance(event, RealtimeModelSendAudio):\n            await self._send_audio(event)\n        elif isinstance(event, RealtimeModelSendToolOutput):\n            await self._send_tool_output(event)\n        elif isinstance(event, RealtimeModelSendInterrupt):\n            await self._send_interrupt(event)\n        elif isinstance(event, RealtimeModelSendSessionUpdate):\n            await self._send_session_update(event)\n        else:\n            assert_never(event)\n            raise ValueError(f\"Unknown event type: {type(event)}\")\n\n    async def _send_raw_message(self, event: OpenAIRealtimeClientEvent) -&gt; None:\n        \"\"\"Send a raw message to the model.\"\"\"\n        assert self._websocket is not None, \"Not connected\"\n        payload = event.model_dump_json(exclude_unset=True)\n        await self._websocket.send(payload)\n\n    async def _send_user_input(self, event: RealtimeModelSendUserInput) -&gt; None:\n        converted = _ConversionHelper.convert_user_input_to_item_create(event)\n        await self._send_raw_message(converted)\n        await self._send_raw_message(OpenAIResponseCreateEvent(type=\"response.create\"))\n\n    async def _send_audio(self, event: RealtimeModelSendAudio) -&gt; None:\n        converted = _ConversionHelper.convert_audio_to_input_audio_buffer_append(event)\n        await self._send_raw_message(converted)\n        if event.commit:\n            await self._send_raw_message(\n                OpenAIInputAudioBufferCommitEvent(type=\"input_audio_buffer.commit\")\n            )\n\n    async def _send_tool_output(self, event: RealtimeModelSendToolOutput) -&gt; None:\n        converted = _ConversionHelper.convert_tool_output(event)\n        await self._send_raw_message(converted)\n\n        tool_item = RealtimeToolCallItem(\n            item_id=event.tool_call.id or \"\",\n            previous_item_id=event.tool_call.previous_item_id,\n            call_id=event.tool_call.call_id,\n            type=\"function_call\",\n            status=\"completed\",\n            arguments=event.tool_call.arguments,\n            name=event.tool_call.name,\n            output=event.output,\n        )\n        await self._emit_event(RealtimeModelItemUpdatedEvent(item=tool_item))\n\n        if event.start_response:\n            await self._send_raw_message(OpenAIResponseCreateEvent(type=\"response.create\"))\n\n    def _get_playback_state(self) -&gt; RealtimePlaybackState:\n        if self._playback_tracker:\n            return self._playback_tracker.get_state()\n\n        if last_audio_item_id := self._audio_state_tracker.get_last_audio_item():\n            item_id, item_content_index = last_audio_item_id\n            audio_state = self._audio_state_tracker.get_state(item_id, item_content_index)\n            if audio_state:\n                elapsed_ms = (\n                    datetime.now() - audio_state.initial_received_time\n                ).total_seconds() * 1000\n                return {\n                    \"current_item_id\": item_id,\n                    \"current_item_content_index\": item_content_index,\n                    \"elapsed_ms\": elapsed_ms,\n                }\n\n        return {\n            \"current_item_id\": None,\n            \"current_item_content_index\": None,\n            \"elapsed_ms\": None,\n        }\n\n    def _get_audio_limits(self, item_id: str, item_content_index: int) -&gt; tuple[float, int] | None:\n        audio_state = self._audio_state_tracker.get_state(item_id, item_content_index)\n        if audio_state is None:\n            return None\n        max_audio_ms = int(math.ceil(audio_state.audio_length_ms))\n        return audio_state.audio_length_ms, max_audio_ms\n\n    async def _send_interrupt(self, event: RealtimeModelSendInterrupt) -&gt; None:\n        playback_state = self._get_playback_state()\n        current_item_id = playback_state.get(\"current_item_id\")\n        current_item_content_index = playback_state.get(\"current_item_content_index\")\n        elapsed_ms = playback_state.get(\"elapsed_ms\")\n\n        if current_item_id is None or elapsed_ms is None:\n            logger.debug(\n                \"Skipping interrupt. \"\n                f\"Item id: {current_item_id}, \"\n                f\"elapsed ms: {elapsed_ms}, \"\n                f\"content index: {current_item_content_index}\"\n            )\n        else:\n            current_item_content_index = current_item_content_index or 0\n            if elapsed_ms &gt; 0:\n                await self._emit_event(\n                    RealtimeModelAudioInterruptedEvent(\n                        item_id=current_item_id,\n                        content_index=current_item_content_index,\n                    )\n                )\n                max_audio_ms: int | None = None\n                audio_limits = self._get_audio_limits(current_item_id, current_item_content_index)\n                if audio_limits is not None:\n                    _, max_audio_ms = audio_limits\n                truncated_ms = max(int(elapsed_ms), 0)\n                if self._ongoing_response or max_audio_ms is None or truncated_ms &lt; max_audio_ms:\n                    converted = _ConversionHelper.convert_interrupt(\n                        current_item_id,\n                        current_item_content_index,\n                        truncated_ms,\n                    )\n                    await self._send_raw_message(converted)\n            else:\n                logger.debug(\n                    \"Didn't interrupt bc elapsed ms is &lt; 0. \"\n                    f\"Item id: {current_item_id}, \"\n                    f\"elapsed ms: {elapsed_ms}, \"\n                    f\"content index: {current_item_content_index}\"\n                )\n\n        session = self._created_session\n        automatic_response_cancellation_enabled = (\n            session\n            and session.audio is not None\n            and session.audio.input is not None\n            and session.audio.input.turn_detection is not None\n            and session.audio.input.turn_detection.interrupt_response is True\n        )\n        should_cancel_response = event.force_response_cancel or (\n            not automatic_response_cancellation_enabled\n        )\n        if should_cancel_response:\n            await self._cancel_response()\n\n        if current_item_id is not None and elapsed_ms is not None:\n            self._audio_state_tracker.on_interrupted()\n            if self._playback_tracker:\n                self._playback_tracker.on_interrupted()\n\n    async def _send_session_update(self, event: RealtimeModelSendSessionUpdate) -&gt; None:\n        \"\"\"Send a session update to the model.\"\"\"\n        await self._update_session_config(event.session_settings)\n\n    async def _handle_audio_delta(self, parsed: ResponseAudioDeltaEvent) -&gt; None:\n        \"\"\"Handle audio delta events and update audio tracking state.\"\"\"\n        self._current_item_id = parsed.item_id\n\n        audio_bytes = base64.b64decode(parsed.delta)\n\n        self._audio_state_tracker.on_audio_delta(parsed.item_id, parsed.content_index, audio_bytes)\n\n        await self._emit_event(\n            RealtimeModelAudioEvent(\n                data=audio_bytes,\n                response_id=parsed.response_id,\n                item_id=parsed.item_id,\n                content_index=parsed.content_index,\n            )\n        )\n\n    async def _handle_output_item(self, item: ConversationItem) -&gt; None:\n        \"\"\"Handle response output item events (function calls and messages).\"\"\"\n        if item.type == \"function_call\" and item.status == \"completed\":\n            tool_call = RealtimeToolCallItem(\n                item_id=item.id or \"\",\n                previous_item_id=None,\n                call_id=item.call_id,\n                type=\"function_call\",\n                # We use the same item for tool call and output, so it will be completed by the\n                # output being added\n                status=\"in_progress\",\n                arguments=item.arguments or \"\",\n                name=item.name or \"\",\n                output=None,\n            )\n            await self._emit_event(RealtimeModelItemUpdatedEvent(item=tool_call))\n            await self._emit_event(\n                RealtimeModelToolCallEvent(\n                    call_id=item.call_id or \"\",\n                    name=item.name or \"\",\n                    arguments=item.arguments or \"\",\n                    id=item.id or \"\",\n                )\n            )\n        elif item.type == \"message\":\n            # Handle message items from output_item events (no previous_item_id)\n            message_item: RealtimeMessageItem = TypeAdapter(RealtimeMessageItem).validate_python(\n                {\n                    \"item_id\": item.id or \"\",\n                    \"type\": item.type,\n                    \"role\": item.role,\n                    \"content\": (\n                        [content.model_dump() for content in item.content] if item.content else []\n                    ),\n                    \"status\": \"in_progress\",\n                }\n            )\n            await self._emit_event(RealtimeModelItemUpdatedEvent(item=message_item))\n\n    async def _handle_conversation_item(\n        self, item: ConversationItem, previous_item_id: str | None\n    ) -&gt; None:\n        \"\"\"Handle conversation item creation/retrieval events.\"\"\"\n        message_item = _ConversionHelper.conversation_item_to_realtime_message_item(\n            item, previous_item_id\n        )\n        await self._emit_event(RealtimeModelItemUpdatedEvent(item=message_item))\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the session.\"\"\"\n        if self._websocket:\n            await self._websocket.close()\n            self._websocket = None\n        if self._websocket_task:\n            self._websocket_task.cancel()\n            try:\n                await self._websocket_task\n            except asyncio.CancelledError:\n                pass\n            self._websocket_task = None\n\n    async def _cancel_response(self) -&gt; None:\n        if self._ongoing_response:\n            await self._send_raw_message(OpenAIResponseCancelEvent(type=\"response.cancel\"))\n            self._ongoing_response = False\n\n    async def _handle_ws_event(self, event: dict[str, Any]):\n        await self._emit_event(RealtimeModelRawServerEvent(data=event))\n        # The public interface definedo on this Agents SDK side (e.g., RealtimeMessageItem)\n        # must be the same even after the GA migration, so this part does the conversion\n        if isinstance(event, dict) and event.get(\"type\") in (\n            \"response.output_item.added\",\n            \"response.output_item.done\",\n        ):\n            item = event.get(\"item\")\n            if isinstance(item, dict) and item.get(\"type\") == \"message\":\n                raw_content = item.get(\"content\") or []\n                converted_content: list[dict[str, Any]] = []\n                for part in raw_content:\n                    if not isinstance(part, dict):\n                        continue\n                    if part.get(\"type\") == \"audio\":\n                        converted_content.append(\n                            {\n                                \"type\": \"audio\",\n                                \"audio\": part.get(\"audio\"),\n                                \"transcript\": part.get(\"transcript\"),\n                            }\n                        )\n                    elif part.get(\"type\") in (\"text\", \"output_text\"):\n                        converted_content.append({\"type\": \"text\", \"text\": part.get(\"text\")})\n                status = item.get(\"status\")\n                if status not in (\"in_progress\", \"completed\", \"incomplete\"):\n                    is_done = event.get(\"type\") == \"response.output_item.done\"\n                    status = \"completed\" if is_done else \"in_progress\"\n                # Explicitly type the adapter for mypy\n                type_adapter: TypeAdapter[RealtimeMessageItem] = TypeAdapter(RealtimeMessageItem)\n                message_item: RealtimeMessageItem = type_adapter.validate_python(\n                    {\n                        \"item_id\": item.get(\"id\", \"\"),\n                        \"type\": \"message\",\n                        \"role\": item.get(\"role\", \"assistant\"),\n                        \"content\": converted_content,\n                        \"status\": status,\n                    }\n                )\n                await self._emit_event(RealtimeModelItemUpdatedEvent(item=message_item))\n                return\n\n        try:\n            if \"previous_item_id\" in event and event[\"previous_item_id\"] is None:\n                event[\"previous_item_id\"] = \"\"  # TODO (rm) remove\n            parsed: AllRealtimeServerEvents = self._server_event_type_adapter.validate_python(event)\n        except pydantic.ValidationError as e:\n            logger.error(f\"Failed to validate server event: {event}\", exc_info=True)\n            await self._emit_event(RealtimeModelErrorEvent(error=e))\n            return\n        except Exception as e:\n            event_type = event.get(\"type\", \"unknown\") if isinstance(event, dict) else \"unknown\"\n            logger.error(f\"Failed to validate server event: {event}\", exc_info=True)\n            exception_event = RealtimeModelExceptionEvent(\n                exception=e,\n                context=f\"Failed to validate server event: {event_type}\",\n            )\n            await self._emit_event(exception_event)\n            return\n\n        if parsed.type == \"response.output_audio.delta\":\n            await self._handle_audio_delta(parsed)\n        elif parsed.type == \"response.output_audio.done\":\n            audio_done_event = RealtimeModelAudioDoneEvent(\n                item_id=parsed.item_id,\n                content_index=parsed.content_index,\n            )\n            await self._emit_event(audio_done_event)\n        elif parsed.type == \"input_audio_buffer.speech_started\":\n            # On VAD speech start, immediately stop local playback so the user can\n            # barge\u2011in without overlapping assistant audio.\n            last_audio = self._audio_state_tracker.get_last_audio_item()\n            if last_audio is not None:\n                item_id, content_index = last_audio\n                playback_state = self._get_playback_state()\n                playback_item_id = playback_state.get(\"current_item_id\")\n                playback_content_index = playback_state.get(\"current_item_content_index\") or 0\n                playback_elapsed_ms = playback_state.get(\"elapsed_ms\")\n                await self._emit_event(\n                    RealtimeModelAudioInterruptedEvent(item_id=item_id, content_index=content_index)\n                )\n\n                elapsed_override = getattr(parsed, \"audio_end_ms\", None)\n                if elapsed_override is None or elapsed_override &lt;= 0:\n                    effective_elapsed_ms = playback_elapsed_ms\n                else:\n                    effective_elapsed_ms = float(elapsed_override)\n\n                if playback_item_id and effective_elapsed_ms is not None:\n                    max_audio_ms: int | None = None\n                    audio_limits = self._get_audio_limits(playback_item_id, playback_content_index)\n                    if audio_limits is not None:\n                        _, max_audio_ms = audio_limits\n                    truncated_ms = max(int(round(effective_elapsed_ms)), 0)\n                    if (\n                        max_audio_ms is not None\n                        and truncated_ms &gt;= max_audio_ms\n                        and not self._ongoing_response\n                    ):\n                        logger.debug(\n                            \"Skipping truncate because playback appears complete. \"\n                            f\"Item id: {playback_item_id}, \"\n                            f\"elapsed ms: {effective_elapsed_ms}, \"\n                            f\"content index: {playback_content_index}, \"\n                            f\"audio length ms: {max_audio_ms}\"\n                        )\n                    else:\n                        if max_audio_ms is not None:\n                            truncated_ms = min(truncated_ms, max_audio_ms)\n                        await self._send_raw_message(\n                            _ConversionHelper.convert_interrupt(\n                                playback_item_id,\n                                playback_content_index,\n                                truncated_ms,\n                            )\n                        )\n\n                # Reset trackers so subsequent playback state queries don't\n                # reference audio that has been interrupted client\u2011side.\n                self._audio_state_tracker.on_interrupted()\n                if self._playback_tracker:\n                    self._playback_tracker.on_interrupted()\n\n                # If server isn't configured to auto\u2011interrupt/cancel, cancel the\n                # response to prevent further audio.\n                session = self._created_session\n                automatic_response_cancellation_enabled = (\n                    session\n                    and session.audio is not None\n                    and session.audio.input is not None\n                    and session.audio.input.turn_detection is not None\n                    and session.audio.input.turn_detection.interrupt_response is True\n                )\n                if not automatic_response_cancellation_enabled:\n                    await self._cancel_response()\n        elif parsed.type == \"response.created\":\n            self._ongoing_response = True\n            await self._emit_event(RealtimeModelTurnStartedEvent())\n        elif parsed.type == \"response.done\":\n            self._ongoing_response = False\n            await self._emit_event(RealtimeModelTurnEndedEvent())\n        elif parsed.type == \"session.created\":\n            await self._send_tracing_config(self._tracing_config)\n            self._update_created_session(parsed.session)\n        elif parsed.type == \"session.updated\":\n            self._update_created_session(parsed.session)\n        elif parsed.type == \"error\":\n            await self._emit_event(RealtimeModelErrorEvent(error=parsed.error))\n        elif parsed.type == \"conversation.item.deleted\":\n            await self._emit_event(RealtimeModelItemDeletedEvent(item_id=parsed.item_id))\n        elif (\n            parsed.type == \"conversation.item.added\"\n            or parsed.type == \"conversation.item.created\"\n            or parsed.type == \"conversation.item.retrieved\"\n        ):\n            previous_item_id = (\n                parsed.previous_item_id if parsed.type == \"conversation.item.created\" else None\n            )\n            if parsed.item.type == \"message\":\n                await self._handle_conversation_item(parsed.item, previous_item_id)\n        elif (\n            parsed.type == \"conversation.item.input_audio_transcription.completed\"\n            or parsed.type == \"conversation.item.truncated\"\n        ):\n            if self._current_item_id:\n                await self._send_raw_message(\n                    OpenAIConversationItemRetrieveEvent(\n                        type=\"conversation.item.retrieve\",\n                        item_id=self._current_item_id,\n                    )\n                )\n            if parsed.type == \"conversation.item.input_audio_transcription.completed\":\n                await self._emit_event(\n                    RealtimeModelInputAudioTranscriptionCompletedEvent(\n                        item_id=parsed.item_id, transcript=parsed.transcript\n                    )\n                )\n        elif parsed.type == \"response.output_audio_transcript.delta\":\n            await self._emit_event(\n                RealtimeModelTranscriptDeltaEvent(\n                    item_id=parsed.item_id, delta=parsed.delta, response_id=parsed.response_id\n                )\n            )\n        elif (\n            parsed.type == \"conversation.item.input_audio_transcription.delta\"\n            or parsed.type == \"response.output_text.delta\"\n            or parsed.type == \"response.function_call_arguments.delta\"\n        ):\n            # No support for partials yet\n            pass\n        elif (\n            parsed.type == \"response.output_item.added\"\n            or parsed.type == \"response.output_item.done\"\n        ):\n            await self._handle_output_item(parsed.item)\n        elif parsed.type == \"input_audio_buffer.timeout_triggered\":\n            await self._emit_event(\n                RealtimeModelInputAudioTimeoutTriggeredEvent(\n                    item_id=parsed.item_id,\n                    audio_start_ms=parsed.audio_start_ms,\n                    audio_end_ms=parsed.audio_end_ms,\n                )\n            )\n\n    def _update_created_session(\n        self,\n        session: OpenAISessionCreateRequest\n        | OpenAIRealtimeTranscriptionSessionCreateRequest\n        | Mapping[str, object]\n        | pydantic.BaseModel,\n    ) -&gt; None:\n        # Only store/playback-format information for realtime sessions (not transcription-only)\n        normalized_session = self._normalize_session_payload(session)\n        if not normalized_session:\n            return\n\n        self._created_session = normalized_session\n        normalized_format = self._extract_audio_format(normalized_session)\n        if normalized_format is None:\n            return\n\n        self._audio_state_tracker.set_audio_format(normalized_format)\n        if self._playback_tracker:\n            self._playback_tracker.set_audio_format(normalized_format)\n\n    @staticmethod\n    def _normalize_session_payload(\n        session: OpenAISessionCreateRequest\n        | OpenAIRealtimeTranscriptionSessionCreateRequest\n        | Mapping[str, object]\n        | pydantic.BaseModel,\n    ) -&gt; OpenAISessionCreateRequest | None:\n        if isinstance(session, OpenAISessionCreateRequest):\n            return session\n\n        if isinstance(session, OpenAIRealtimeTranscriptionSessionCreateRequest):\n            return None\n\n        session_payload: Mapping[str, object]\n        if isinstance(session, pydantic.BaseModel):\n            session_payload = cast(Mapping[str, object], session.model_dump())\n        elif isinstance(session, Mapping):\n            session_payload = session\n        else:\n            return None\n\n        if OpenAIRealtimeWebSocketModel._is_transcription_session(session_payload):\n            return None\n\n        try:\n            return OpenAISessionCreateRequest.model_validate(session_payload)\n        except pydantic.ValidationError:\n            return None\n\n    @staticmethod\n    def _is_transcription_session(payload: Mapping[str, object]) -&gt; bool:\n        try:\n            OpenAIRealtimeTranscriptionSessionCreateRequest.model_validate(payload)\n        except pydantic.ValidationError:\n            return False\n        else:\n            return True\n\n    @staticmethod\n    def _extract_audio_format(session: OpenAISessionCreateRequest) -&gt; str | None:\n        audio = session.audio\n        if not audio or not audio.output or not audio.output.format:\n            return None\n\n        return OpenAIRealtimeWebSocketModel._normalize_audio_format(audio.output.format)\n\n    @staticmethod\n    def _normalize_audio_format(fmt: object) -&gt; str:\n        if isinstance(fmt, AudioPCM):\n            return \"pcm16\"\n        if isinstance(fmt, AudioPCMU):\n            return \"g711_ulaw\"\n        if isinstance(fmt, AudioPCMA):\n            return \"g711_alaw\"\n\n        fmt_type = OpenAIRealtimeWebSocketModel._read_format_type(fmt)\n        if isinstance(fmt_type, str) and fmt_type:\n            return fmt_type\n\n        return str(fmt)\n\n    @staticmethod\n    def _read_format_type(fmt: object) -&gt; str | None:\n        if isinstance(fmt, str):\n            return fmt\n\n        if isinstance(fmt, Mapping):\n            type_value = fmt.get(\"type\")\n            return type_value if isinstance(type_value, str) else None\n\n        if isinstance(fmt, pydantic.BaseModel):\n            type_value = fmt.model_dump().get(\"type\")\n            return type_value if isinstance(type_value, str) else None\n\n        try:\n            type_value = fmt.type  # type: ignore[attr-defined]\n        except AttributeError:\n            return None\n\n        return type_value if isinstance(type_value, str) else None\n\n    @staticmethod\n    def _normalize_turn_detection_config(config: object) -&gt; object:\n        \"\"\"Normalize camelCase turn detection keys to snake_case for API compatibility.\"\"\"\n        if not isinstance(config, Mapping):\n            return config\n\n        normalized = dict(config)\n        key_map = {\n            \"createResponse\": \"create_response\",\n            \"interruptResponse\": \"interrupt_response\",\n            \"prefixPaddingMs\": \"prefix_padding_ms\",\n            \"silenceDurationMs\": \"silence_duration_ms\",\n            \"idleTimeoutMs\": \"idle_timeout_ms\",\n        }\n        for camel_key, snake_key in key_map.items():\n            if camel_key in normalized and snake_key not in normalized:\n                normalized[snake_key] = normalized[camel_key]\n            normalized.pop(camel_key, None)\n\n        return normalized\n\n    async def _update_session_config(self, model_settings: RealtimeSessionModelSettings) -&gt; None:\n        session_config = self._get_session_config(model_settings)\n        await self._send_raw_message(\n            OpenAISessionUpdateEvent(session=session_config, type=\"session.update\")\n        )\n\n    def _get_session_config(\n        self, model_settings: RealtimeSessionModelSettings\n    ) -&gt; OpenAISessionCreateRequest:\n        \"\"\"Get the session config.\"\"\"\n        audio_input_args: dict[str, Any] = {}\n        audio_output_args: dict[str, Any] = {}\n\n        audio_config = model_settings.get(\"audio\")\n        audio_config_mapping = audio_config if isinstance(audio_config, Mapping) else None\n        input_audio_config: Mapping[str, Any] = (\n            cast(Mapping[str, Any], audio_config_mapping.get(\"input\", {}))\n            if audio_config_mapping\n            else {}\n        )\n        output_audio_config: Mapping[str, Any] = (\n            cast(Mapping[str, Any], audio_config_mapping.get(\"output\", {}))\n            if audio_config_mapping\n            else {}\n        )\n\n        input_format_source: FormatInput = (\n            input_audio_config.get(\"format\") if input_audio_config else None\n        )\n        if input_format_source is None:\n            if self._call_id:\n                input_format_source = model_settings.get(\"input_audio_format\")\n            else:\n                input_format_source = model_settings.get(\n                    \"input_audio_format\", DEFAULT_MODEL_SETTINGS.get(\"input_audio_format\")\n                )\n        audio_input_args[\"format\"] = to_realtime_audio_format(input_format_source)\n\n        if \"noise_reduction\" in input_audio_config:\n            audio_input_args[\"noise_reduction\"] = input_audio_config.get(\"noise_reduction\")\n        elif \"input_audio_noise_reduction\" in model_settings:\n            audio_input_args[\"noise_reduction\"] = model_settings.get(\"input_audio_noise_reduction\")\n\n        if \"transcription\" in input_audio_config:\n            audio_input_args[\"transcription\"] = input_audio_config.get(\"transcription\")\n        elif \"input_audio_transcription\" in model_settings:\n            audio_input_args[\"transcription\"] = model_settings.get(\"input_audio_transcription\")\n        else:\n            audio_input_args[\"transcription\"] = DEFAULT_MODEL_SETTINGS.get(\n                \"input_audio_transcription\"\n            )\n\n        if \"turn_detection\" in input_audio_config:\n            audio_input_args[\"turn_detection\"] = self._normalize_turn_detection_config(\n                input_audio_config.get(\"turn_detection\")\n            )\n        elif \"turn_detection\" in model_settings:\n            audio_input_args[\"turn_detection\"] = self._normalize_turn_detection_config(\n                model_settings.get(\"turn_detection\")\n            )\n        else:\n            audio_input_args[\"turn_detection\"] = DEFAULT_MODEL_SETTINGS.get(\"turn_detection\")\n\n        requested_voice = output_audio_config.get(\"voice\") if output_audio_config else None\n        audio_output_args[\"voice\"] = requested_voice or model_settings.get(\n            \"voice\", DEFAULT_MODEL_SETTINGS.get(\"voice\")\n        )\n\n        output_format_source: FormatInput = (\n            output_audio_config.get(\"format\") if output_audio_config else None\n        )\n        if output_format_source is None:\n            if self._call_id:\n                output_format_source = model_settings.get(\"output_audio_format\")\n            else:\n                output_format_source = model_settings.get(\n                    \"output_audio_format\", DEFAULT_MODEL_SETTINGS.get(\"output_audio_format\")\n                )\n        audio_output_args[\"format\"] = to_realtime_audio_format(output_format_source)\n\n        if \"speed\" in output_audio_config:\n            audio_output_args[\"speed\"] = output_audio_config.get(\"speed\")\n        elif \"speed\" in model_settings:\n            audio_output_args[\"speed\"] = model_settings.get(\"speed\")\n\n        output_modalities = (\n            model_settings.get(\"output_modalities\")\n            or model_settings.get(\"modalities\")\n            or DEFAULT_MODEL_SETTINGS.get(\"modalities\")\n        )\n\n        # Construct full session object. `type` will be excluded at serialization time for updates.\n        session_create_request = OpenAISessionCreateRequest(\n            type=\"realtime\",\n            model=(model_settings.get(\"model_name\") or self.model) or \"gpt-realtime\",\n            output_modalities=output_modalities,\n            audio=OpenAIRealtimeAudioConfig(\n                input=OpenAIRealtimeAudioInput(**audio_input_args),\n                output=OpenAIRealtimeAudioOutput(**audio_output_args),\n            ),\n            tools=cast(\n                Any,\n                self._tools_to_session_tools(\n                    tools=model_settings.get(\"tools\", []),\n                    handoffs=model_settings.get(\"handoffs\", []),\n                ),\n            ),\n        )\n\n        if \"instructions\" in model_settings:\n            session_create_request.instructions = model_settings.get(\"instructions\")\n\n        if \"prompt\" in model_settings:\n            _passed_prompt: Prompt = model_settings[\"prompt\"]\n            variables: dict[str, Any] | None = _passed_prompt.get(\"variables\")\n            session_create_request.prompt = ResponsePrompt(\n                id=_passed_prompt[\"id\"],\n                variables=variables,\n                version=_passed_prompt.get(\"version\"),\n            )\n\n        if \"max_output_tokens\" in model_settings:\n            session_create_request.max_output_tokens = cast(\n                Any, model_settings.get(\"max_output_tokens\")\n            )\n\n        if \"tool_choice\" in model_settings:\n            session_create_request.tool_choice = cast(Any, model_settings.get(\"tool_choice\"))\n\n        return session_create_request\n\n    def _tools_to_session_tools(\n        self, tools: list[Tool], handoffs: list[Handoff]\n    ) -&gt; list[OpenAISessionFunction]:\n        converted_tools: list[OpenAISessionFunction] = []\n        for tool in tools:\n            if not isinstance(tool, FunctionTool):\n                raise UserError(f\"Tool {tool.name} is unsupported. Must be a function tool.\")\n            converted_tools.append(\n                OpenAISessionFunction(\n                    name=tool.name,\n                    description=tool.description,\n                    parameters=tool.params_json_schema,\n                    type=\"function\",\n                )\n            )\n\n        for handoff in handoffs:\n            converted_tools.append(\n                OpenAISessionFunction(\n                    name=handoff.tool_name,\n                    description=handoff.tool_description,\n                    parameters=handoff.input_json_schema,\n                    type=\"function\",\n                )\n            )\n\n        return converted_tools\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeWebSocketModel.connect","title":"connect  <code>async</code>","text":"<pre><code>connect(options: RealtimeModelConfig) -&gt; None\n</code></pre> <p>Establish a connection to the model and keep it alive.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>async def connect(self, options: RealtimeModelConfig) -&gt; None:\n    \"\"\"Establish a connection to the model and keep it alive.\"\"\"\n    assert self._websocket is None, \"Already connected\"\n    assert self._websocket_task is None, \"Already connected\"\n\n    model_settings: RealtimeSessionModelSettings = options.get(\"initial_model_settings\", {})\n\n    self._playback_tracker = options.get(\"playback_tracker\", None)\n\n    call_id = options.get(\"call_id\")\n    model_name = model_settings.get(\"model_name\")\n    if call_id and model_name:\n        error_message = (\n            \"Cannot specify both `call_id` and `model_name` \"\n            \"when attaching to an existing realtime call.\"\n        )\n        raise UserError(error_message)\n\n    if model_name:\n        self.model = model_name\n\n    self._call_id = call_id\n    api_key = await get_api_key(options.get(\"api_key\"))\n\n    if \"tracing\" in model_settings:\n        self._tracing_config = model_settings[\"tracing\"]\n    else:\n        self._tracing_config = \"auto\"\n\n    if call_id:\n        url = options.get(\"url\", f\"wss://api.openai.com/v1/realtime?call_id={call_id}\")\n    else:\n        url = options.get(\"url\", f\"wss://api.openai.com/v1/realtime?model={self.model}\")\n\n    headers: dict[str, str] = {}\n    if options.get(\"headers\") is not None:\n        # For customizing request headers\n        headers.update(options[\"headers\"])\n    else:\n        # OpenAI's Realtime API\n        if not api_key:\n            raise UserError(\"API key is required but was not provided.\")\n\n        headers.update({\"Authorization\": f\"Bearer {api_key}\"})\n\n    self._websocket = await self._create_websocket_connection(\n        url=url,\n        headers=headers,\n        transport_config=self._transport_config,\n    )\n    self._websocket_task = asyncio.create_task(self._listen_for_messages())\n    await self._update_session_config(model_settings)\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeWebSocketModel.add_listener","title":"add_listener","text":"<pre><code>add_listener(listener: RealtimeModelListener) -&gt; None\n</code></pre> <p>Add a listener to the model.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>def add_listener(self, listener: RealtimeModelListener) -&gt; None:\n    \"\"\"Add a listener to the model.\"\"\"\n    if listener not in self._listeners:\n        self._listeners.append(listener)\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeWebSocketModel.remove_listener","title":"remove_listener","text":"<pre><code>remove_listener(listener: RealtimeModelListener) -&gt; None\n</code></pre> <p>Remove a listener from the model.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>def remove_listener(self, listener: RealtimeModelListener) -&gt; None:\n    \"\"\"Remove a listener from the model.\"\"\"\n    if listener in self._listeners:\n        self._listeners.remove(listener)\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeWebSocketModel.send_event","title":"send_event  <code>async</code>","text":"<pre><code>send_event(event: RealtimeModelSendEvent) -&gt; None\n</code></pre> <p>Send an event to the model.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>async def send_event(self, event: RealtimeModelSendEvent) -&gt; None:\n    \"\"\"Send an event to the model.\"\"\"\n    if isinstance(event, RealtimeModelSendRawMessage):\n        converted = _ConversionHelper.try_convert_raw_message(event)\n        if converted is not None:\n            await self._send_raw_message(converted)\n        else:\n            logger.error(f\"Failed to convert raw message: {event}\")\n    elif isinstance(event, RealtimeModelSendUserInput):\n        await self._send_user_input(event)\n    elif isinstance(event, RealtimeModelSendAudio):\n        await self._send_audio(event)\n    elif isinstance(event, RealtimeModelSendToolOutput):\n        await self._send_tool_output(event)\n    elif isinstance(event, RealtimeModelSendInterrupt):\n        await self._send_interrupt(event)\n    elif isinstance(event, RealtimeModelSendSessionUpdate):\n        await self._send_session_update(event)\n    else:\n        assert_never(event)\n        raise ValueError(f\"Unknown event type: {type(event)}\")\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeWebSocketModel.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the session.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the session.\"\"\"\n    if self._websocket:\n        await self._websocket.close()\n        self._websocket = None\n    if self._websocket_task:\n        self._websocket_task.cancel()\n        try:\n            await self._websocket_task\n        except asyncio.CancelledError:\n            pass\n        self._websocket_task = None\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeSIPModel","title":"OpenAIRealtimeSIPModel","text":"<p>               Bases: <code>OpenAIRealtimeWebSocketModel</code></p> <p>Realtime model that attaches to SIP-originated calls using a call ID.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>class OpenAIRealtimeSIPModel(OpenAIRealtimeWebSocketModel):\n    \"\"\"Realtime model that attaches to SIP-originated calls using a call ID.\"\"\"\n\n    @staticmethod\n    async def build_initial_session_payload(\n        agent: RealtimeAgent[Any],\n        *,\n        context: TContext | None = None,\n        model_config: RealtimeModelConfig | None = None,\n        run_config: RealtimeRunConfig | None = None,\n        overrides: RealtimeSessionModelSettings | None = None,\n    ) -&gt; OpenAISessionCreateRequest:\n        \"\"\"Build a session payload that mirrors what a RealtimeSession would send on connect.\n\n        This helper can be used to accept SIP-originated calls by forwarding the returned payload to\n        the Realtime Calls API without duplicating session setup logic.\n        \"\"\"\n        run_config_settings = (run_config or {}).get(\"model_settings\") or {}\n        initial_model_settings = (model_config or {}).get(\"initial_model_settings\") or {}\n        base_settings: RealtimeSessionModelSettings = {\n            **run_config_settings,\n            **initial_model_settings,\n        }\n\n        context_wrapper = RunContextWrapper(context)\n        merged_settings = await _build_model_settings_from_agent(\n            agent=agent,\n            context_wrapper=context_wrapper,\n            base_settings=base_settings,\n            starting_settings=initial_model_settings,\n            run_config=run_config,\n        )\n\n        if overrides:\n            merged_settings.update(overrides)\n\n        model = OpenAIRealtimeWebSocketModel()\n        return model._get_session_config(merged_settings)\n\n    async def connect(self, options: RealtimeModelConfig) -&gt; None:\n        call_id = options.get(\"call_id\")\n        if not call_id:\n            raise UserError(\"OpenAIRealtimeSIPModel requires `call_id` in the model configuration.\")\n\n        sip_options = options.copy()\n        await super().connect(sip_options)\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeSIPModel.build_initial_session_payload","title":"build_initial_session_payload  <code>async</code> <code>staticmethod</code>","text":"<pre><code>build_initial_session_payload(\n    agent: RealtimeAgent[Any],\n    *,\n    context: TContext | None = None,\n    model_config: RealtimeModelConfig | None = None,\n    run_config: RealtimeRunConfig | None = None,\n    overrides: RealtimeSessionModelSettings | None = None,\n) -&gt; RealtimeSessionCreateRequest\n</code></pre> <p>Build a session payload that mirrors what a RealtimeSession would send on connect.</p> <p>This helper can be used to accept SIP-originated calls by forwarding the returned payload to the Realtime Calls API without duplicating session setup logic.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>@staticmethod\nasync def build_initial_session_payload(\n    agent: RealtimeAgent[Any],\n    *,\n    context: TContext | None = None,\n    model_config: RealtimeModelConfig | None = None,\n    run_config: RealtimeRunConfig | None = None,\n    overrides: RealtimeSessionModelSettings | None = None,\n) -&gt; OpenAISessionCreateRequest:\n    \"\"\"Build a session payload that mirrors what a RealtimeSession would send on connect.\n\n    This helper can be used to accept SIP-originated calls by forwarding the returned payload to\n    the Realtime Calls API without duplicating session setup logic.\n    \"\"\"\n    run_config_settings = (run_config or {}).get(\"model_settings\") or {}\n    initial_model_settings = (model_config or {}).get(\"initial_model_settings\") or {}\n    base_settings: RealtimeSessionModelSettings = {\n        **run_config_settings,\n        **initial_model_settings,\n    }\n\n    context_wrapper = RunContextWrapper(context)\n    merged_settings = await _build_model_settings_from_agent(\n        agent=agent,\n        context_wrapper=context_wrapper,\n        base_settings=base_settings,\n        starting_settings=initial_model_settings,\n        run_config=run_config,\n    )\n\n    if overrides:\n        merged_settings.update(overrides)\n\n    model = OpenAIRealtimeWebSocketModel()\n    return model._get_session_config(merged_settings)\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeSIPModel.add_listener","title":"add_listener","text":"<pre><code>add_listener(listener: RealtimeModelListener) -&gt; None\n</code></pre> <p>Add a listener to the model.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>def add_listener(self, listener: RealtimeModelListener) -&gt; None:\n    \"\"\"Add a listener to the model.\"\"\"\n    if listener not in self._listeners:\n        self._listeners.append(listener)\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeSIPModel.remove_listener","title":"remove_listener","text":"<pre><code>remove_listener(listener: RealtimeModelListener) -&gt; None\n</code></pre> <p>Remove a listener from the model.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>def remove_listener(self, listener: RealtimeModelListener) -&gt; None:\n    \"\"\"Remove a listener from the model.\"\"\"\n    if listener in self._listeners:\n        self._listeners.remove(listener)\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeSIPModel.send_event","title":"send_event  <code>async</code>","text":"<pre><code>send_event(event: RealtimeModelSendEvent) -&gt; None\n</code></pre> <p>Send an event to the model.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>async def send_event(self, event: RealtimeModelSendEvent) -&gt; None:\n    \"\"\"Send an event to the model.\"\"\"\n    if isinstance(event, RealtimeModelSendRawMessage):\n        converted = _ConversionHelper.try_convert_raw_message(event)\n        if converted is not None:\n            await self._send_raw_message(converted)\n        else:\n            logger.error(f\"Failed to convert raw message: {event}\")\n    elif isinstance(event, RealtimeModelSendUserInput):\n        await self._send_user_input(event)\n    elif isinstance(event, RealtimeModelSendAudio):\n        await self._send_audio(event)\n    elif isinstance(event, RealtimeModelSendToolOutput):\n        await self._send_tool_output(event)\n    elif isinstance(event, RealtimeModelSendInterrupt):\n        await self._send_interrupt(event)\n    elif isinstance(event, RealtimeModelSendSessionUpdate):\n        await self._send_session_update(event)\n    else:\n        assert_never(event)\n        raise ValueError(f\"Unknown event type: {type(event)}\")\n</code></pre>"},{"location":"ref/realtime/openai_realtime/#agents.realtime.openai_realtime.OpenAIRealtimeSIPModel.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the session.</p> Source code in <code>src/agents/realtime/openai_realtime.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the session.\"\"\"\n    if self._websocket:\n        await self._websocket.close()\n        self._websocket = None\n    if self._websocket_task:\n        self._websocket_task.cancel()\n        try:\n            await self._websocket_task\n        except asyncio.CancelledError:\n            pass\n        self._websocket_task = None\n</code></pre>"},{"location":"ref/realtime/runner/","title":"<code>RealtimeRunner</code>","text":"<p>A <code>RealtimeRunner</code> is the equivalent of <code>Runner</code> for realtime agents. It automatically handles multiple turns by maintaining a persistent connection with the underlying model layer.</p> <p>The session manages the local history copy, executes tools, runs guardrails and facilitates handoffs between agents.</p> <p>Since this code runs on your server, it uses WebSockets by default. You can optionally create your own custom model layer by implementing the <code>RealtimeModel</code> interface.</p> Source code in <code>src/agents/realtime/runner.py</code> <pre><code>class RealtimeRunner:\n    \"\"\"A `RealtimeRunner` is the equivalent of `Runner` for realtime agents. It automatically\n    handles multiple turns by maintaining a persistent connection with the underlying model\n    layer.\n\n    The session manages the local history copy, executes tools, runs guardrails and facilitates\n    handoffs between agents.\n\n    Since this code runs on your server, it uses WebSockets by default. You can optionally create\n    your own custom model layer by implementing the `RealtimeModel` interface.\n    \"\"\"\n\n    def __init__(\n        self,\n        starting_agent: RealtimeAgent,\n        *,\n        model: RealtimeModel | None = None,\n        config: RealtimeRunConfig | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the realtime runner.\n\n        Args:\n            starting_agent: The agent to start the session with.\n            context: The context to use for the session.\n            model: The model to use. If not provided, will use a default OpenAI realtime model.\n            config: Override parameters to use for the entire run.\n        \"\"\"\n        self._starting_agent = starting_agent\n        self._config = config\n        self._model = model or OpenAIRealtimeWebSocketModel()\n\n    async def run(\n        self, *, context: TContext | None = None, model_config: RealtimeModelConfig | None = None\n    ) -&gt; RealtimeSession:\n        \"\"\"Start and returns a realtime session.\n\n        Returns:\n            RealtimeSession: A session object that allows bidirectional communication with the\n            realtime model.\n\n        Example:\n            ```python\n            runner = RealtimeRunner(agent)\n            async with await runner.run() as session:\n                await session.send_message(\"Hello\")\n                async for event in session:\n                    print(event)\n            ```\n        \"\"\"\n        # Create and return the connection\n        session = RealtimeSession(\n            model=self._model,\n            agent=self._starting_agent,\n            context=context,\n            model_config=model_config,\n            run_config=self._config,\n        )\n\n        return session\n</code></pre>"},{"location":"ref/realtime/runner/#agents.realtime.runner.RealtimeRunner.__init__","title":"__init__","text":"<pre><code>__init__(\n    starting_agent: RealtimeAgent,\n    *,\n    model: RealtimeModel | None = None,\n    config: RealtimeRunConfig | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the realtime runner.</p> <p>Parameters:</p> Name Type Description Default <code>starting_agent</code> <code>RealtimeAgent</code> <p>The agent to start the session with.</p> required <code>context</code> <p>The context to use for the session.</p> required <code>model</code> <code>RealtimeModel | None</code> <p>The model to use. If not provided, will use a default OpenAI realtime model.</p> <code>None</code> <code>config</code> <code>RealtimeRunConfig | None</code> <p>Override parameters to use for the entire run.</p> <code>None</code> Source code in <code>src/agents/realtime/runner.py</code> <pre><code>def __init__(\n    self,\n    starting_agent: RealtimeAgent,\n    *,\n    model: RealtimeModel | None = None,\n    config: RealtimeRunConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize the realtime runner.\n\n    Args:\n        starting_agent: The agent to start the session with.\n        context: The context to use for the session.\n        model: The model to use. If not provided, will use a default OpenAI realtime model.\n        config: Override parameters to use for the entire run.\n    \"\"\"\n    self._starting_agent = starting_agent\n    self._config = config\n    self._model = model or OpenAIRealtimeWebSocketModel()\n</code></pre>"},{"location":"ref/realtime/runner/#agents.realtime.runner.RealtimeRunner.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    *,\n    context: TContext | None = None,\n    model_config: RealtimeModelConfig | None = None,\n) -&gt; RealtimeSession\n</code></pre> <p>Start and returns a realtime session.</p> <p>Returns:</p> Name Type Description <code>RealtimeSession</code> <code>RealtimeSession</code> <p>A session object that allows bidirectional communication with the</p> <code>RealtimeSession</code> <p>realtime model.</p> Example <pre><code>runner = RealtimeRunner(agent)\nasync with await runner.run() as session:\n    await session.send_message(\"Hello\")\n    async for event in session:\n        print(event)\n</code></pre> Source code in <code>src/agents/realtime/runner.py</code> <pre><code>async def run(\n    self, *, context: TContext | None = None, model_config: RealtimeModelConfig | None = None\n) -&gt; RealtimeSession:\n    \"\"\"Start and returns a realtime session.\n\n    Returns:\n        RealtimeSession: A session object that allows bidirectional communication with the\n        realtime model.\n\n    Example:\n        ```python\n        runner = RealtimeRunner(agent)\n        async with await runner.run() as session:\n            await session.send_message(\"Hello\")\n            async for event in session:\n                print(event)\n        ```\n    \"\"\"\n    # Create and return the connection\n    session = RealtimeSession(\n        model=self._model,\n        agent=self._starting_agent,\n        context=context,\n        model_config=model_config,\n        run_config=self._config,\n    )\n\n    return session\n</code></pre>"},{"location":"ref/realtime/session/","title":"<code>RealtimeSession</code>","text":"<p>               Bases: <code>RealtimeModelListener</code></p> <p>A connection to a realtime model. It streams events from the model to you, and allows you to send messages and audio to the model.</p> Example <pre><code>runner = RealtimeRunner(agent)\nasync with await runner.run() as session:\n    # Send messages\n    await session.send_message(\"Hello\")\n    await session.send_audio(audio_bytes)\n\n    # Stream events\n    async for event in session:\n        if event.type == \"audio\":\n            # Handle audio event\n            pass\n</code></pre> Source code in <code>src/agents/realtime/session.py</code> <pre><code>class RealtimeSession(RealtimeModelListener):\n    \"\"\"A connection to a realtime model. It streams events from the model to you, and allows you to\n    send messages and audio to the model.\n\n    Example:\n        ```python\n        runner = RealtimeRunner(agent)\n        async with await runner.run() as session:\n            # Send messages\n            await session.send_message(\"Hello\")\n            await session.send_audio(audio_bytes)\n\n            # Stream events\n            async for event in session:\n                if event.type == \"audio\":\n                    # Handle audio event\n                    pass\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: RealtimeModel,\n        agent: RealtimeAgent,\n        context: TContext | None,\n        model_config: RealtimeModelConfig | None = None,\n        run_config: RealtimeRunConfig | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the session.\n\n        Args:\n            model: The model to use.\n            agent: The current agent.\n            context: The context object.\n            model_config: Model configuration.\n            run_config: Runtime configuration including guardrails.\n        \"\"\"\n        self._model = model\n        self._current_agent = agent\n        self._context_wrapper = RunContextWrapper(context)\n        self._event_info = RealtimeEventInfo(context=self._context_wrapper)\n        self._history: list[RealtimeItem] = []\n        self._model_config = model_config or {}\n        self._run_config = run_config or {}\n        initial_model_settings = self._model_config.get(\"initial_model_settings\")\n        run_config_settings = self._run_config.get(\"model_settings\")\n        self._base_model_settings: RealtimeSessionModelSettings = {\n            **(run_config_settings or {}),\n            **(initial_model_settings or {}),\n        }\n        self._event_queue: asyncio.Queue[RealtimeSessionEvent] = asyncio.Queue()\n        self._closed = False\n        self._stored_exception: BaseException | None = None\n        self._pending_tool_calls: dict[\n            str, tuple[RealtimeModelToolCallEvent, RealtimeAgent, FunctionTool, ToolApprovalItem]\n        ] = {}\n\n        # Guardrails state tracking\n        self._interrupted_response_ids: set[str] = set()\n        self._item_transcripts: dict[str, str] = {}  # item_id -&gt; accumulated transcript\n        self._item_guardrail_run_counts: dict[str, int] = {}  # item_id -&gt; run count\n        self._debounce_text_length = self._run_config.get(\"guardrails_settings\", {}).get(\n            \"debounce_text_length\", 100\n        )\n\n        self._guardrail_tasks: set[asyncio.Task[Any]] = set()\n        self._tool_call_tasks: set[asyncio.Task[Any]] = set()\n        self._async_tool_calls: bool = bool(self._run_config.get(\"async_tool_calls\", True))\n\n    @property\n    def model(self) -&gt; RealtimeModel:\n        \"\"\"Access the underlying model for adding listeners or other direct interaction.\"\"\"\n        return self._model\n\n    async def __aenter__(self) -&gt; RealtimeSession:\n        \"\"\"Start the session by connecting to the model. After this, you will be able to stream\n        events from the model and send messages and audio to the model.\n        \"\"\"\n        # Add ourselves as a listener\n        self._model.add_listener(self)\n\n        model_config = self._model_config.copy()\n        model_config[\"initial_model_settings\"] = await self._get_updated_model_settings_from_agent(\n            starting_settings=self._model_config.get(\"initial_model_settings\", None),\n            agent=self._current_agent,\n        )\n\n        # Connect to the model\n        await self._model.connect(model_config)\n\n        # Emit initial history update\n        await self._put_event(\n            RealtimeHistoryUpdated(\n                history=self._history,\n                info=self._event_info,\n            )\n        )\n\n        return self\n\n    async def enter(self) -&gt; RealtimeSession:\n        \"\"\"Enter the async context manager. We strongly recommend using the async context manager\n        pattern instead of this method. If you use this, you need to manually call `close()` when\n        you are done.\n        \"\"\"\n        return await self.__aenter__()\n\n    async def __aexit__(self, _exc_type: Any, _exc_val: Any, _exc_tb: Any) -&gt; None:\n        \"\"\"End the session.\"\"\"\n        await self.close()\n\n    async def __aiter__(self) -&gt; AsyncIterator[RealtimeSessionEvent]:\n        \"\"\"Iterate over events from the session.\"\"\"\n        while not self._closed:\n            try:\n                # Check if there's a stored exception to raise\n                if self._stored_exception is not None:\n                    # Clean up resources before raising\n                    await self._cleanup()\n                    raise self._stored_exception\n\n                event = await self._event_queue.get()\n                yield event\n            except asyncio.CancelledError:\n                break\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the session.\"\"\"\n        await self._cleanup()\n\n    async def send_message(self, message: RealtimeUserInput) -&gt; None:\n        \"\"\"Send a message to the model.\"\"\"\n        await self._model.send_event(RealtimeModelSendUserInput(user_input=message))\n\n    async def send_audio(self, audio: bytes, *, commit: bool = False) -&gt; None:\n        \"\"\"Send a raw audio chunk to the model.\"\"\"\n        await self._model.send_event(RealtimeModelSendAudio(audio=audio, commit=commit))\n\n    async def interrupt(self) -&gt; None:\n        \"\"\"Interrupt the model.\"\"\"\n        await self._model.send_event(RealtimeModelSendInterrupt())\n\n    async def update_agent(self, agent: RealtimeAgent) -&gt; None:\n        \"\"\"Update the active agent for this session and apply its settings to the model.\"\"\"\n        self._current_agent = agent\n\n        updated_settings = await self._get_updated_model_settings_from_agent(\n            starting_settings=None,\n            agent=self._current_agent,\n        )\n\n        await self._model.send_event(\n            RealtimeModelSendSessionUpdate(session_settings=updated_settings)\n        )\n\n    async def on_event(self, event: RealtimeModelEvent) -&gt; None:\n        await self._put_event(RealtimeRawModelEvent(data=event, info=self._event_info))\n\n        if event.type == \"error\":\n            await self._put_event(RealtimeError(info=self._event_info, error=event.error))\n        elif event.type == \"function_call\":\n            agent_snapshot = self._current_agent\n            if self._async_tool_calls:\n                self._enqueue_tool_call_task(event, agent_snapshot)\n            else:\n                await self._handle_tool_call(event, agent_snapshot=agent_snapshot)\n        elif event.type == \"audio\":\n            await self._put_event(\n                RealtimeAudio(\n                    info=self._event_info,\n                    audio=event,\n                    item_id=event.item_id,\n                    content_index=event.content_index,\n                )\n            )\n        elif event.type == \"audio_interrupted\":\n            await self._put_event(\n                RealtimeAudioInterrupted(\n                    info=self._event_info, item_id=event.item_id, content_index=event.content_index\n                )\n            )\n        elif event.type == \"audio_done\":\n            await self._put_event(\n                RealtimeAudioEnd(\n                    info=self._event_info, item_id=event.item_id, content_index=event.content_index\n                )\n            )\n        elif event.type == \"input_audio_transcription_completed\":\n            prev_len = len(self._history)\n            self._history = RealtimeSession._get_new_history(self._history, event)\n            # If a new user item was appended (no existing item),\n            # emit history_added for incremental UIs.\n            if len(self._history) &gt; prev_len and len(self._history) &gt; 0:\n                new_item = self._history[-1]\n                await self._put_event(RealtimeHistoryAdded(info=self._event_info, item=new_item))\n            else:\n                await self._put_event(\n                    RealtimeHistoryUpdated(info=self._event_info, history=self._history)\n                )\n        elif event.type == \"input_audio_timeout_triggered\":\n            await self._put_event(\n                RealtimeInputAudioTimeoutTriggered(\n                    info=self._event_info,\n                )\n            )\n        elif event.type == \"transcript_delta\":\n            # Accumulate transcript text for guardrail debouncing per item_id\n            item_id = event.item_id\n            if item_id not in self._item_transcripts:\n                self._item_transcripts[item_id] = \"\"\n                self._item_guardrail_run_counts[item_id] = 0\n\n            self._item_transcripts[item_id] += event.delta\n            self._history = self._get_new_history(\n                self._history,\n                AssistantMessageItem(\n                    item_id=item_id,\n                    content=[AssistantAudio(transcript=self._item_transcripts[item_id])],\n                ),\n            )\n\n            # Check if we should run guardrails based on debounce threshold\n            current_length = len(self._item_transcripts[item_id])\n            threshold = self._debounce_text_length\n            next_run_threshold = (self._item_guardrail_run_counts[item_id] + 1) * threshold\n\n            if current_length &gt;= next_run_threshold:\n                self._item_guardrail_run_counts[item_id] += 1\n                # Pass response_id so we can ensure only a single interrupt per response\n                self._enqueue_guardrail_task(self._item_transcripts[item_id], event.response_id)\n        elif event.type == \"item_updated\":\n            is_new = not any(item.item_id == event.item.item_id for item in self._history)\n\n            # Preserve previously known transcripts when updating existing items.\n            # This prevents transcripts from disappearing when an item is later\n            # retrieved without transcript fields populated.\n            incoming_item = event.item\n            existing_item = next(\n                (i for i in self._history if i.item_id == incoming_item.item_id), None\n            )\n\n            if (\n                existing_item is not None\n                and existing_item.type == \"message\"\n                and incoming_item.type == \"message\"\n            ):\n                try:\n                    # Merge transcripts for matching content indices\n                    existing_content = existing_item.content\n                    new_content = []\n                    for idx, entry in enumerate(incoming_item.content):\n                        # Only attempt to preserve for audio-like content\n                        if entry.type in (\"audio\", \"input_audio\"):\n                            # Use tuple form for Python 3.9 compatibility\n                            assert isinstance(entry, (InputAudio, AssistantAudio))\n                            # Determine if transcript is missing/empty on the incoming entry\n                            entry_transcript = entry.transcript\n                            if not entry_transcript:\n                                preserved: str | None = None\n                                # First prefer any transcript from the existing history item\n                                if idx &lt; len(existing_content):\n                                    this_content = existing_content[idx]\n                                    if isinstance(this_content, AssistantAudio) or isinstance(\n                                        this_content, InputAudio\n                                    ):\n                                        preserved = this_content.transcript\n\n                                # If still missing and this is an assistant item, fall back to\n                                # accumulated transcript deltas tracked during the turn.\n                                if incoming_item.role == \"assistant\":\n                                    preserved = self._item_transcripts.get(incoming_item.item_id)\n\n                                if preserved:\n                                    entry = entry.model_copy(update={\"transcript\": preserved})\n\n                        new_content.append(entry)\n\n                    if new_content:\n                        incoming_item = incoming_item.model_copy(update={\"content\": new_content})\n                except Exception:\n                    logger.error(\"Error merging transcripts\", exc_info=True)\n                    pass\n\n            self._history = self._get_new_history(self._history, incoming_item)\n            if is_new:\n                new_item = next(\n                    item for item in self._history if item.item_id == event.item.item_id\n                )\n                await self._put_event(RealtimeHistoryAdded(info=self._event_info, item=new_item))\n            else:\n                await self._put_event(\n                    RealtimeHistoryUpdated(info=self._event_info, history=self._history)\n                )\n        elif event.type == \"item_deleted\":\n            deleted_id = event.item_id\n            self._history = [item for item in self._history if item.item_id != deleted_id]\n            await self._put_event(\n                RealtimeHistoryUpdated(info=self._event_info, history=self._history)\n            )\n        elif event.type == \"connection_status\":\n            pass\n        elif event.type == \"turn_started\":\n            await self._put_event(\n                RealtimeAgentStartEvent(\n                    agent=self._current_agent,\n                    info=self._event_info,\n                )\n            )\n        elif event.type == \"turn_ended\":\n            # Clear guardrail state for next turn\n            self._item_transcripts.clear()\n            self._item_guardrail_run_counts.clear()\n\n            await self._put_event(\n                RealtimeAgentEndEvent(\n                    agent=self._current_agent,\n                    info=self._event_info,\n                )\n            )\n        elif event.type == \"exception\":\n            # Store the exception to be raised in __aiter__\n            self._stored_exception = event.exception\n        elif event.type == \"other\":\n            pass\n        elif event.type == \"raw_server_event\":\n            pass\n        else:\n            assert_never(event)\n\n    async def _put_event(self, event: RealtimeSessionEvent) -&gt; None:\n        \"\"\"Put an event into the queue.\"\"\"\n        await self._event_queue.put(event)\n\n    async def _function_needs_approval(\n        self, function_tool: FunctionTool, tool_call: RealtimeModelToolCallEvent\n    ) -&gt; bool:\n        \"\"\"Evaluate a function tool's needs_approval setting with parsed args.\"\"\"\n        needs_setting = getattr(function_tool, \"needs_approval\", False)\n        parsed_args: dict[str, Any] = {}\n        if callable(needs_setting):\n            try:\n                parsed_args = json.loads(tool_call.arguments or \"{}\")\n            except json.JSONDecodeError:\n                parsed_args = {}\n        return await evaluate_needs_approval_setting(\n            needs_setting,\n            self._context_wrapper,\n            parsed_args,\n            tool_call.call_id,\n            strict=False,\n        )\n\n    def _build_tool_approval_item(\n        self, tool: FunctionTool, tool_call: RealtimeModelToolCallEvent, agent: RealtimeAgent\n    ) -&gt; ToolApprovalItem:\n        \"\"\"Create a ToolApprovalItem for approval tracking.\"\"\"\n        raw_item = {\n            \"type\": \"function_call\",\n            \"name\": tool.name,\n            \"call_id\": tool_call.call_id,\n            \"arguments\": tool_call.arguments,\n        }\n        return ToolApprovalItem(agent=cast(Any, agent), raw_item=raw_item, tool_name=tool.name)\n\n    async def _maybe_request_tool_approval(\n        self,\n        tool_call: RealtimeModelToolCallEvent,\n        *,\n        function_tool: FunctionTool,\n        agent: RealtimeAgent,\n    ) -&gt; bool | None:\n        \"\"\"Return True/False when approved/rejected, or None when awaiting approval.\"\"\"\n        approval_item = self._build_tool_approval_item(function_tool, tool_call, agent)\n\n        needs_approval = await self._function_needs_approval(function_tool, tool_call)\n        if not needs_approval:\n            return True\n\n        approval_status = self._context_wrapper.is_tool_approved(\n            function_tool.name, tool_call.call_id\n        )\n        if approval_status is True:\n            return True\n        if approval_status is False:\n            return False\n\n        self._pending_tool_calls[tool_call.call_id] = (\n            tool_call,\n            agent,\n            function_tool,\n            approval_item,\n        )\n        await self._put_event(\n            RealtimeToolApprovalRequired(\n                agent=agent,\n                tool=function_tool,\n                call_id=tool_call.call_id,\n                arguments=tool_call.arguments,\n                info=self._event_info,\n            )\n        )\n        return None\n\n    async def _send_tool_rejection(\n        self,\n        event: RealtimeModelToolCallEvent,\n        *,\n        tool: FunctionTool,\n        agent: RealtimeAgent,\n    ) -&gt; None:\n        \"\"\"Send a rejection response back to the model and emit an end event.\"\"\"\n        rejection_message = await self._resolve_approval_rejection_message(\n            tool=tool,\n            call_id=event.call_id,\n        )\n        await self._model.send_event(\n            RealtimeModelSendToolOutput(\n                tool_call=event,\n                output=rejection_message,\n                start_response=True,\n            )\n        )\n\n        await self._put_event(\n            RealtimeToolEnd(\n                info=self._event_info,\n                tool=tool,\n                output=rejection_message,\n                agent=agent,\n                arguments=event.arguments,\n            )\n        )\n\n    async def _resolve_approval_rejection_message(self, *, tool: FunctionTool, call_id: str) -&gt; str:\n        \"\"\"Resolve model-visible output text for approval rejections.\"\"\"\n        formatter = self._run_config.get(\"tool_error_formatter\")\n        if formatter is None:\n            return REJECTION_MESSAGE\n\n        try:\n            maybe_message = formatter(\n                ToolErrorFormatterArgs(\n                    kind=\"approval_rejected\",\n                    tool_type=\"function\",\n                    tool_name=tool.name,\n                    call_id=call_id,\n                    default_message=REJECTION_MESSAGE,\n                    run_context=self._context_wrapper,\n                )\n            )\n            message = await maybe_message if inspect.isawaitable(maybe_message) else maybe_message\n        except Exception as exc:\n            logger.error(\"Tool error formatter failed for %s: %s\", tool.name, exc)\n            return REJECTION_MESSAGE\n\n        if message is None:\n            return REJECTION_MESSAGE\n\n        if not isinstance(message, str):\n            logger.error(\n                \"Tool error formatter returned non-string for %s: %s\",\n                tool.name,\n                type(message).__name__,\n            )\n            return REJECTION_MESSAGE\n\n        return message\n\n    async def approve_tool_call(self, call_id: str, *, always: bool = False) -&gt; None:\n        \"\"\"Approve a pending tool call and resume execution.\"\"\"\n        pending = self._pending_tool_calls.pop(call_id, None)\n        if pending is None:\n            return\n\n        tool_call, agent_snapshot, function_tool, approval_item = pending\n        self._context_wrapper.approve_tool(approval_item, always_approve=always)\n\n        if self._async_tool_calls:\n            self._enqueue_tool_call_task(tool_call, agent_snapshot)\n        else:\n            await self._handle_tool_call(tool_call, agent_snapshot=agent_snapshot)\n\n    async def reject_tool_call(self, call_id: str, *, always: bool = False) -&gt; None:\n        \"\"\"Reject a pending tool call and notify the model.\"\"\"\n        pending = self._pending_tool_calls.pop(call_id, None)\n        if pending is None:\n            return\n\n        tool_call, agent_snapshot, function_tool, approval_item = pending\n        self._context_wrapper.reject_tool(approval_item, always_reject=always)\n        await self._send_tool_rejection(tool_call, tool=function_tool, agent=agent_snapshot)\n\n    async def _handle_tool_call(\n        self,\n        event: RealtimeModelToolCallEvent,\n        *,\n        agent_snapshot: RealtimeAgent | None = None,\n    ) -&gt; None:\n        \"\"\"Handle a tool call event.\"\"\"\n        agent = agent_snapshot or self._current_agent\n        tools, handoffs = await asyncio.gather(\n            agent.get_all_tools(self._context_wrapper),\n            self._get_handoffs(agent, self._context_wrapper),\n        )\n        function_map = {tool.name: tool for tool in tools if isinstance(tool, FunctionTool)}\n        handoff_map = {handoff.tool_name: handoff for handoff in handoffs}\n\n        if event.name in function_map:\n            func_tool = function_map[event.name]\n            approval_status = await self._maybe_request_tool_approval(\n                event, function_tool=func_tool, agent=agent\n            )\n            if approval_status is False:\n                await self._send_tool_rejection(event, tool=func_tool, agent=agent)\n                return\n            if approval_status is None:\n                return\n\n            await self._put_event(\n                RealtimeToolStart(\n                    info=self._event_info,\n                    tool=func_tool,\n                    agent=agent,\n                    arguments=event.arguments,\n                )\n            )\n\n            tool_context = ToolContext(\n                context=self._context_wrapper.context,\n                usage=self._context_wrapper.usage,\n                tool_name=event.name,\n                tool_call_id=event.call_id,\n                tool_arguments=event.arguments,\n            )\n            result = await func_tool.on_invoke_tool(tool_context, event.arguments)\n\n            await self._model.send_event(\n                RealtimeModelSendToolOutput(\n                    tool_call=event, output=str(result), start_response=True\n                )\n            )\n\n            await self._put_event(\n                RealtimeToolEnd(\n                    info=self._event_info,\n                    tool=func_tool,\n                    output=result,\n                    agent=agent,\n                    arguments=event.arguments,\n                )\n            )\n        elif event.name in handoff_map:\n            handoff = handoff_map[event.name]\n            tool_context = ToolContext(\n                context=self._context_wrapper.context,\n                usage=self._context_wrapper.usage,\n                tool_name=event.name,\n                tool_call_id=event.call_id,\n                tool_arguments=event.arguments,\n            )\n\n            # Execute the handoff to get the new agent\n            result = await handoff.on_invoke_handoff(self._context_wrapper, event.arguments)\n            if not isinstance(result, RealtimeAgent):\n                raise UserError(\n                    f\"Handoff {handoff.tool_name} returned invalid result: {type(result)}\"\n                )\n\n            # Store previous agent for event\n            previous_agent = agent\n\n            # Update current agent\n            self._current_agent = result\n\n            # Get updated model settings from new agent\n            updated_settings = await self._get_updated_model_settings_from_agent(\n                starting_settings=None,\n                agent=self._current_agent,\n            )\n\n            # Send handoff event\n            await self._put_event(\n                RealtimeHandoffEvent(\n                    from_agent=previous_agent,\n                    to_agent=self._current_agent,\n                    info=self._event_info,\n                )\n            )\n\n            # First, send the session update so the model receives the new instructions\n            await self._model.send_event(\n                RealtimeModelSendSessionUpdate(session_settings=updated_settings)\n            )\n\n            # Then send tool output to complete the handoff (this triggers a new response)\n            transfer_message = handoff.get_transfer_message(result)\n            await self._model.send_event(\n                RealtimeModelSendToolOutput(\n                    tool_call=event,\n                    output=transfer_message,\n                    start_response=True,\n                )\n            )\n        else:\n            await self._put_event(\n                RealtimeError(\n                    info=self._event_info,\n                    error={\"message\": f\"Tool {event.name} not found\"},\n                )\n            )\n\n    @classmethod\n    def _get_new_history(\n        cls,\n        old_history: list[RealtimeItem],\n        event: RealtimeModelInputAudioTranscriptionCompletedEvent | RealtimeItem,\n    ) -&gt; list[RealtimeItem]:\n        if isinstance(event, RealtimeModelInputAudioTranscriptionCompletedEvent):\n            new_history: list[RealtimeItem] = []\n            existing_item_found = False\n            for item in old_history:\n                if item.item_id == event.item_id and item.type == \"message\" and item.role == \"user\":\n                    content: list[InputText | InputAudio] = []\n                    for entry in item.content:\n                        if entry.type == \"input_audio\":\n                            copied_entry = entry.model_copy(update={\"transcript\": event.transcript})\n                            content.append(copied_entry)\n                        else:\n                            content.append(entry)  # type: ignore\n                    new_history.append(\n                        item.model_copy(update={\"content\": content, \"status\": \"completed\"})\n                    )\n                    existing_item_found = True\n                else:\n                    new_history.append(item)\n\n            if existing_item_found is False:\n                new_history.append(\n                    UserMessageItem(\n                        item_id=event.item_id, content=[InputText(text=event.transcript)]\n                    )\n                )\n            return new_history\n\n        # TODO (rm) Add support for audio storage config\n\n        # If the item already exists, update it\n        existing_index = next(\n            (i for i, item in enumerate(old_history) if item.item_id == event.item_id), None\n        )\n        if existing_index is not None:\n            new_history = old_history.copy()\n            if event.type == \"message\" and event.content is not None and len(event.content) &gt; 0:\n                existing_item = old_history[existing_index]\n                if existing_item.type == \"message\":\n                    # Merge content preserving existing transcript/text when incoming entry is empty\n                    if event.role == \"assistant\" and existing_item.role == \"assistant\":\n                        assistant_existing_content = existing_item.content\n                        assistant_incoming = event.content\n                        assistant_new_content: list[AssistantText | AssistantAudio] = []\n                        for idx, ac in enumerate(assistant_incoming):\n                            if idx &gt;= len(assistant_existing_content):\n                                assistant_new_content.append(ac)\n                                continue\n                            assistant_current = assistant_existing_content[idx]\n                            if ac.type == \"audio\":\n                                if ac.transcript is None:\n                                    assistant_new_content.append(assistant_current)\n                                else:\n                                    assistant_new_content.append(ac)\n                            else:  # text\n                                cur_text = (\n                                    assistant_current.text\n                                    if isinstance(assistant_current, AssistantText)\n                                    else None\n                                )\n                                if cur_text is not None and ac.text is None:\n                                    assistant_new_content.append(assistant_current)\n                                else:\n                                    assistant_new_content.append(ac)\n                        updated_assistant = event.model_copy(\n                            update={\"content\": assistant_new_content}\n                        )\n                        new_history[existing_index] = updated_assistant\n                    elif event.role == \"user\" and existing_item.role == \"user\":\n                        user_existing_content = existing_item.content\n                        user_incoming = event.content\n\n                        # Start from incoming content (prefer latest fields)\n                        user_new_content: list[InputText | InputAudio | InputImage] = list(\n                            user_incoming\n                        )\n\n                        # Merge by type with special handling for images and transcripts\n                        def _image_url_str(val: object) -&gt; str | None:\n                            if isinstance(val, InputImage):\n                                return val.image_url or None\n                            return None\n\n                        # 1) Preserve any existing images that are missing from the incoming payload\n                        incoming_image_urls: set[str] = set()\n                        for part in user_incoming:\n                            if isinstance(part, InputImage):\n                                u = _image_url_str(part)\n                                if u:\n                                    incoming_image_urls.add(u)\n\n                        missing_images: list[InputImage] = []\n                        for part in user_existing_content:\n                            if isinstance(part, InputImage):\n                                u = _image_url_str(part)\n                                if u and u not in incoming_image_urls:\n                                    missing_images.append(part)\n\n                        # Insert missing images at the beginning to keep them visible and stable\n                        if missing_images:\n                            user_new_content = missing_images + user_new_content\n\n                        # 2) For text/audio entries, preserve existing when incoming entry is empty\n                        merged: list[InputText | InputAudio | InputImage] = []\n                        for idx, uc in enumerate(user_new_content):\n                            if uc.type == \"input_audio\":\n                                # Attempt to preserve transcript if empty\n                                transcript = getattr(uc, \"transcript\", None)\n                                if transcript is None and idx &lt; len(user_existing_content):\n                                    prev = user_existing_content[idx]\n                                    if isinstance(prev, InputAudio) and prev.transcript is not None:\n                                        uc = uc.model_copy(update={\"transcript\": prev.transcript})\n                                merged.append(uc)\n                            elif uc.type == \"input_text\":\n                                text = getattr(uc, \"text\", None)\n                                if (text is None or text == \"\") and idx &lt; len(\n                                    user_existing_content\n                                ):\n                                    prev = user_existing_content[idx]\n                                    if isinstance(prev, InputText) and prev.text:\n                                        uc = uc.model_copy(update={\"text\": prev.text})\n                                merged.append(uc)\n                            else:\n                                merged.append(uc)\n\n                        updated_user = event.model_copy(update={\"content\": merged})\n                        new_history[existing_index] = updated_user\n                    elif event.role == \"system\" and existing_item.role == \"system\":\n                        system_existing_content = existing_item.content\n                        system_incoming = event.content\n                        # Prefer existing non-empty text when incoming is empty\n                        system_new_content: list[InputText] = []\n                        for idx, sc in enumerate(system_incoming):\n                            if idx &gt;= len(system_existing_content):\n                                system_new_content.append(sc)\n                                continue\n                            system_current = system_existing_content[idx]\n                            cur_text = system_current.text\n                            if cur_text is not None and sc.text is None:\n                                system_new_content.append(system_current)\n                            else:\n                                system_new_content.append(sc)\n                        updated_system = event.model_copy(update={\"content\": system_new_content})\n                        new_history[existing_index] = updated_system\n                    else:\n                        # Role changed or mismatched; just replace\n                        new_history[existing_index] = event\n                else:\n                    # If the existing item is not a message, just replace it.\n                    new_history[existing_index] = event\n            return new_history\n\n        # Otherwise, insert it after the previous_item_id if that is set\n        elif event.previous_item_id:\n            # Insert the new item after the previous item\n            previous_index = next(\n                (i for i, item in enumerate(old_history) if item.item_id == event.previous_item_id),\n                None,\n            )\n            if previous_index is not None:\n                new_history = old_history.copy()\n                new_history.insert(previous_index + 1, event)\n                return new_history\n\n        # Otherwise, add it to the end\n        return old_history + [event]\n\n    async def _run_output_guardrails(self, text: str, response_id: str) -&gt; bool:\n        \"\"\"Run output guardrails on the given text. Returns True if any guardrail was triggered.\"\"\"\n        combined_guardrails = self._current_agent.output_guardrails + self._run_config.get(\n            \"output_guardrails\", []\n        )\n        seen_ids: set[int] = set()\n        output_guardrails = []\n        for guardrail in combined_guardrails:\n            guardrail_id = id(guardrail)\n            if guardrail_id not in seen_ids:\n                output_guardrails.append(guardrail)\n                seen_ids.add(guardrail_id)\n\n        # If we've already interrupted this response, skip\n        if not output_guardrails or response_id in self._interrupted_response_ids:\n            return False\n\n        triggered_results = []\n\n        for guardrail in output_guardrails:\n            try:\n                result = await guardrail.run(\n                    # TODO (rm) Remove this cast, it's wrong\n                    self._context_wrapper,\n                    cast(Agent[Any], self._current_agent),\n                    text,\n                )\n                if result.output.tripwire_triggered:\n                    triggered_results.append(result)\n            except Exception:\n                # Continue with other guardrails if one fails\n                continue\n\n        if triggered_results:\n            # Double-check: bail if already interrupted for this response\n            if response_id in self._interrupted_response_ids:\n                return False\n\n            # Mark as interrupted immediately (before any awaits) to minimize race window\n            self._interrupted_response_ids.add(response_id)\n\n            # Emit guardrail tripped event\n            await self._put_event(\n                RealtimeGuardrailTripped(\n                    guardrail_results=triggered_results,\n                    message=text,\n                    info=self._event_info,\n                )\n            )\n\n            # Interrupt the model\n            await self._model.send_event(RealtimeModelSendInterrupt(force_response_cancel=True))\n\n            # Send guardrail triggered message\n            guardrail_names = [result.guardrail.get_name() for result in triggered_results]\n            await self._model.send_event(\n                RealtimeModelSendUserInput(\n                    user_input=f\"guardrail triggered: {', '.join(guardrail_names)}\"\n                )\n            )\n\n            return True\n\n        return False\n\n    def _enqueue_guardrail_task(self, text: str, response_id: str) -&gt; None:\n        # Runs the guardrails in a separate task to avoid blocking the main loop\n\n        task = asyncio.create_task(self._run_output_guardrails(text, response_id))\n        self._guardrail_tasks.add(task)\n\n        # Add callback to remove completed tasks and handle exceptions\n        task.add_done_callback(self._on_guardrail_task_done)\n\n    def _on_guardrail_task_done(self, task: asyncio.Task[Any]) -&gt; None:\n        \"\"\"Handle completion of a guardrail task.\"\"\"\n        # Remove from tracking set\n        self._guardrail_tasks.discard(task)\n\n        # Check for exceptions and propagate as events\n        if not task.cancelled():\n            exception = task.exception()\n            if exception:\n                # Create an exception event instead of raising\n                asyncio.create_task(\n                    self._put_event(\n                        RealtimeError(\n                            info=self._event_info,\n                            error={\"message\": f\"Guardrail task failed: {str(exception)}\"},\n                        )\n                    )\n                )\n\n    def _cleanup_guardrail_tasks(self) -&gt; None:\n        for task in self._guardrail_tasks:\n            if not task.done():\n                task.cancel()\n        self._guardrail_tasks.clear()\n\n    def _enqueue_tool_call_task(\n        self, event: RealtimeModelToolCallEvent, agent_snapshot: RealtimeAgent\n    ) -&gt; None:\n        \"\"\"Run tool calls in the background to avoid blocking realtime transport.\"\"\"\n        task = asyncio.create_task(self._handle_tool_call(event, agent_snapshot=agent_snapshot))\n        self._tool_call_tasks.add(task)\n        task.add_done_callback(self._on_tool_call_task_done)\n\n    def _on_tool_call_task_done(self, task: asyncio.Task[Any]) -&gt; None:\n        self._tool_call_tasks.discard(task)\n\n        if task.cancelled():\n            return\n\n        exception = task.exception()\n        if exception is None:\n            return\n\n        logger.exception(\"Realtime tool call task failed\", exc_info=exception)\n\n        if self._stored_exception is None:\n            self._stored_exception = exception\n\n        asyncio.create_task(\n            self._put_event(\n                RealtimeError(\n                    info=self._event_info,\n                    error={\"message\": f\"Tool call task failed: {exception}\"},\n                )\n            )\n        )\n\n    def _cleanup_tool_call_tasks(self) -&gt; None:\n        for task in self._tool_call_tasks:\n            if not task.done():\n                task.cancel()\n        self._tool_call_tasks.clear()\n\n    async def _cleanup(self) -&gt; None:\n        \"\"\"Clean up all resources and mark session as closed.\"\"\"\n        # Cancel and cleanup guardrail tasks\n        self._cleanup_guardrail_tasks()\n        self._cleanup_tool_call_tasks()\n\n        # Remove ourselves as a listener\n        self._model.remove_listener(self)\n\n        # Close the model connection\n        await self._model.close()\n\n        # Clear pending approval tracking\n        self._pending_tool_calls.clear()\n\n        # Mark as closed\n        self._closed = True\n\n    async def _get_updated_model_settings_from_agent(\n        self,\n        starting_settings: RealtimeSessionModelSettings | None,\n        agent: RealtimeAgent,\n    ) -&gt; RealtimeSessionModelSettings:\n        # Start with the merged base settings from run and model configuration.\n        updated_settings = self._base_model_settings.copy()\n\n        if agent.prompt is not None:\n            updated_settings[\"prompt\"] = agent.prompt\n\n        instructions, tools, handoffs = await asyncio.gather(\n            agent.get_system_prompt(self._context_wrapper),\n            agent.get_all_tools(self._context_wrapper),\n            self._get_handoffs(agent, self._context_wrapper),\n        )\n        updated_settings[\"instructions\"] = instructions or \"\"\n        updated_settings[\"tools\"] = tools or []\n        updated_settings[\"handoffs\"] = handoffs or []\n\n        # Apply starting settings (from model config) next\n        if starting_settings:\n            updated_settings.update(starting_settings)\n\n        disable_tracing = self._run_config.get(\"tracing_disabled\", False)\n        if disable_tracing:\n            updated_settings[\"tracing\"] = None\n\n        return updated_settings\n\n    @classmethod\n    async def _get_handoffs(\n        cls, agent: RealtimeAgent[Any], context_wrapper: RunContextWrapper[Any]\n    ) -&gt; list[Handoff[Any, RealtimeAgent[Any]]]:\n        handoffs: list[Handoff[Any, RealtimeAgent[Any]]] = []\n        for handoff_item in agent.handoffs:\n            if isinstance(handoff_item, Handoff):\n                handoffs.append(handoff_item)\n            elif isinstance(handoff_item, RealtimeAgent):\n                handoffs.append(realtime_handoff(handoff_item))\n\n        async def _check_handoff_enabled(handoff_obj: Handoff[Any, RealtimeAgent[Any]]) -&gt; bool:\n            attr = handoff_obj.is_enabled\n            if isinstance(attr, bool):\n                return attr\n            res = attr(context_wrapper, agent)\n            if inspect.isawaitable(res):\n                return await res\n            return res\n\n        results = await asyncio.gather(*(_check_handoff_enabled(h) for h in handoffs))\n        enabled = [h for h, ok in zip(handoffs, results) if ok]\n        return enabled\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.model","title":"model  <code>property</code>","text":"<pre><code>model: RealtimeModel\n</code></pre> <p>Access the underlying model for adding listeners or other direct interaction.</p>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: RealtimeModel,\n    agent: RealtimeAgent,\n    context: TContext | None,\n    model_config: RealtimeModelConfig | None = None,\n    run_config: RealtimeRunConfig | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the session.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>RealtimeModel</code> <p>The model to use.</p> required <code>agent</code> <code>RealtimeAgent</code> <p>The current agent.</p> required <code>context</code> <code>TContext | None</code> <p>The context object.</p> required <code>model_config</code> <code>RealtimeModelConfig | None</code> <p>Model configuration.</p> <code>None</code> <code>run_config</code> <code>RealtimeRunConfig | None</code> <p>Runtime configuration including guardrails.</p> <code>None</code> Source code in <code>src/agents/realtime/session.py</code> <pre><code>def __init__(\n    self,\n    model: RealtimeModel,\n    agent: RealtimeAgent,\n    context: TContext | None,\n    model_config: RealtimeModelConfig | None = None,\n    run_config: RealtimeRunConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize the session.\n\n    Args:\n        model: The model to use.\n        agent: The current agent.\n        context: The context object.\n        model_config: Model configuration.\n        run_config: Runtime configuration including guardrails.\n    \"\"\"\n    self._model = model\n    self._current_agent = agent\n    self._context_wrapper = RunContextWrapper(context)\n    self._event_info = RealtimeEventInfo(context=self._context_wrapper)\n    self._history: list[RealtimeItem] = []\n    self._model_config = model_config or {}\n    self._run_config = run_config or {}\n    initial_model_settings = self._model_config.get(\"initial_model_settings\")\n    run_config_settings = self._run_config.get(\"model_settings\")\n    self._base_model_settings: RealtimeSessionModelSettings = {\n        **(run_config_settings or {}),\n        **(initial_model_settings or {}),\n    }\n    self._event_queue: asyncio.Queue[RealtimeSessionEvent] = asyncio.Queue()\n    self._closed = False\n    self._stored_exception: BaseException | None = None\n    self._pending_tool_calls: dict[\n        str, tuple[RealtimeModelToolCallEvent, RealtimeAgent, FunctionTool, ToolApprovalItem]\n    ] = {}\n\n    # Guardrails state tracking\n    self._interrupted_response_ids: set[str] = set()\n    self._item_transcripts: dict[str, str] = {}  # item_id -&gt; accumulated transcript\n    self._item_guardrail_run_counts: dict[str, int] = {}  # item_id -&gt; run count\n    self._debounce_text_length = self._run_config.get(\"guardrails_settings\", {}).get(\n        \"debounce_text_length\", 100\n    )\n\n    self._guardrail_tasks: set[asyncio.Task[Any]] = set()\n    self._tool_call_tasks: set[asyncio.Task[Any]] = set()\n    self._async_tool_calls: bool = bool(self._run_config.get(\"async_tool_calls\", True))\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; RealtimeSession\n</code></pre> <p>Start the session by connecting to the model. After this, you will be able to stream events from the model and send messages and audio to the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def __aenter__(self) -&gt; RealtimeSession:\n    \"\"\"Start the session by connecting to the model. After this, you will be able to stream\n    events from the model and send messages and audio to the model.\n    \"\"\"\n    # Add ourselves as a listener\n    self._model.add_listener(self)\n\n    model_config = self._model_config.copy()\n    model_config[\"initial_model_settings\"] = await self._get_updated_model_settings_from_agent(\n        starting_settings=self._model_config.get(\"initial_model_settings\", None),\n        agent=self._current_agent,\n    )\n\n    # Connect to the model\n    await self._model.connect(model_config)\n\n    # Emit initial history update\n    await self._put_event(\n        RealtimeHistoryUpdated(\n            history=self._history,\n            info=self._event_info,\n        )\n    )\n\n    return self\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.enter","title":"enter  <code>async</code>","text":"<pre><code>enter() -&gt; RealtimeSession\n</code></pre> <p>Enter the async context manager. We strongly recommend using the async context manager pattern instead of this method. If you use this, you need to manually call <code>close()</code> when you are done.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def enter(self) -&gt; RealtimeSession:\n    \"\"\"Enter the async context manager. We strongly recommend using the async context manager\n    pattern instead of this method. If you use this, you need to manually call `close()` when\n    you are done.\n    \"\"\"\n    return await self.__aenter__()\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    _exc_type: Any, _exc_val: Any, _exc_tb: Any\n) -&gt; None\n</code></pre> <p>End the session.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def __aexit__(self, _exc_type: Any, _exc_val: Any, _exc_tb: Any) -&gt; None:\n    \"\"\"End the session.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__aiter__","title":"__aiter__  <code>async</code>","text":"<pre><code>__aiter__() -&gt; AsyncIterator[RealtimeSessionEvent]\n</code></pre> <p>Iterate over events from the session.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def __aiter__(self) -&gt; AsyncIterator[RealtimeSessionEvent]:\n    \"\"\"Iterate over events from the session.\"\"\"\n    while not self._closed:\n        try:\n            # Check if there's a stored exception to raise\n            if self._stored_exception is not None:\n                # Clean up resources before raising\n                await self._cleanup()\n                raise self._stored_exception\n\n            event = await self._event_queue.get()\n            yield event\n        except asyncio.CancelledError:\n            break\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the session.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the session.\"\"\"\n    await self._cleanup()\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.send_message","title":"send_message  <code>async</code>","text":"<pre><code>send_message(message: RealtimeUserInput) -&gt; None\n</code></pre> <p>Send a message to the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def send_message(self, message: RealtimeUserInput) -&gt; None:\n    \"\"\"Send a message to the model.\"\"\"\n    await self._model.send_event(RealtimeModelSendUserInput(user_input=message))\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.send_audio","title":"send_audio  <code>async</code>","text":"<pre><code>send_audio(audio: bytes, *, commit: bool = False) -&gt; None\n</code></pre> <p>Send a raw audio chunk to the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def send_audio(self, audio: bytes, *, commit: bool = False) -&gt; None:\n    \"\"\"Send a raw audio chunk to the model.\"\"\"\n    await self._model.send_event(RealtimeModelSendAudio(audio=audio, commit=commit))\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.interrupt","title":"interrupt  <code>async</code>","text":"<pre><code>interrupt() -&gt; None\n</code></pre> <p>Interrupt the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def interrupt(self) -&gt; None:\n    \"\"\"Interrupt the model.\"\"\"\n    await self._model.send_event(RealtimeModelSendInterrupt())\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.update_agent","title":"update_agent  <code>async</code>","text":"<pre><code>update_agent(agent: RealtimeAgent) -&gt; None\n</code></pre> <p>Update the active agent for this session and apply its settings to the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def update_agent(self, agent: RealtimeAgent) -&gt; None:\n    \"\"\"Update the active agent for this session and apply its settings to the model.\"\"\"\n    self._current_agent = agent\n\n    updated_settings = await self._get_updated_model_settings_from_agent(\n        starting_settings=None,\n        agent=self._current_agent,\n    )\n\n    await self._model.send_event(\n        RealtimeModelSendSessionUpdate(session_settings=updated_settings)\n    )\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.approve_tool_call","title":"approve_tool_call  <code>async</code>","text":"<pre><code>approve_tool_call(\n    call_id: str, *, always: bool = False\n) -&gt; None\n</code></pre> <p>Approve a pending tool call and resume execution.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def approve_tool_call(self, call_id: str, *, always: bool = False) -&gt; None:\n    \"\"\"Approve a pending tool call and resume execution.\"\"\"\n    pending = self._pending_tool_calls.pop(call_id, None)\n    if pending is None:\n        return\n\n    tool_call, agent_snapshot, function_tool, approval_item = pending\n    self._context_wrapper.approve_tool(approval_item, always_approve=always)\n\n    if self._async_tool_calls:\n        self._enqueue_tool_call_task(tool_call, agent_snapshot)\n    else:\n        await self._handle_tool_call(tool_call, agent_snapshot=agent_snapshot)\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.reject_tool_call","title":"reject_tool_call  <code>async</code>","text":"<pre><code>reject_tool_call(\n    call_id: str, *, always: bool = False\n) -&gt; None\n</code></pre> <p>Reject a pending tool call and notify the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def reject_tool_call(self, call_id: str, *, always: bool = False) -&gt; None:\n    \"\"\"Reject a pending tool call and notify the model.\"\"\"\n    pending = self._pending_tool_calls.pop(call_id, None)\n    if pending is None:\n        return\n\n    tool_call, agent_snapshot, function_tool, approval_item = pending\n    self._context_wrapper.reject_tool(approval_item, always_reject=always)\n    await self._send_tool_rejection(tool_call, tool=function_tool, agent=agent_snapshot)\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/","title":"<code>Agent Runner Helpers</code>","text":"<p>Internal helpers for AgentRunner.run.</p>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.should_cancel_parallel_model_task_on_input_guardrail_trip","title":"should_cancel_parallel_model_task_on_input_guardrail_trip","text":"<pre><code>should_cancel_parallel_model_task_on_input_guardrail_trip() -&gt; (\n    bool\n)\n</code></pre> <p>Return whether an in-flight model task should be cancelled on guardrail trip.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def should_cancel_parallel_model_task_on_input_guardrail_trip() -&gt; bool:\n    \"\"\"Return whether an in-flight model task should be cancelled on guardrail trip.\"\"\"\n    try:\n        from temporalio import workflow as temporal_workflow  # type: ignore[import-not-found]\n    except Exception:\n        return True\n\n    try:\n        if not temporal_workflow.in_workflow():\n            return True\n        # Preserve replay compatibility for histories created before cancellation.\n        return bool(temporal_workflow.patched(_PARALLEL_INPUT_GUARDRAIL_CANCEL_PATCH_ID))\n    except Exception:\n        return True\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.apply_resumed_conversation_settings","title":"apply_resumed_conversation_settings","text":"<pre><code>apply_resumed_conversation_settings(\n    *,\n    run_state: RunState[TContext],\n    conversation_id: str | None,\n    previous_response_id: str | None,\n    auto_previous_response_id: bool,\n) -&gt; tuple[str | None, str | None, bool]\n</code></pre> <p>Apply RunState conversation identifiers and return the resolved values.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def apply_resumed_conversation_settings(\n    *,\n    run_state: RunState[TContext],\n    conversation_id: str | None,\n    previous_response_id: str | None,\n    auto_previous_response_id: bool,\n) -&gt; tuple[str | None, str | None, bool]:\n    \"\"\"Apply RunState conversation identifiers and return the resolved values.\"\"\"\n    conversation_id = conversation_id or run_state._conversation_id\n    previous_response_id = previous_response_id or run_state._previous_response_id\n    if auto_previous_response_id is False and run_state._auto_previous_response_id:\n        auto_previous_response_id = True\n    run_state._conversation_id = conversation_id\n    run_state._previous_response_id = previous_response_id\n    run_state._auto_previous_response_id = auto_previous_response_id\n    return conversation_id, previous_response_id, auto_previous_response_id\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.resolve_trace_settings","title":"resolve_trace_settings","text":"<pre><code>resolve_trace_settings(\n    *,\n    run_state: RunState[TContext] | None,\n    run_config: RunConfig,\n) -&gt; tuple[\n    str,\n    str | None,\n    str | None,\n    dict[str, Any] | None,\n    TracingConfig | None,\n]\n</code></pre> <p>Resolve tracing settings, preferring explicit run_config overrides.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def resolve_trace_settings(\n    *,\n    run_state: RunState[TContext] | None,\n    run_config: RunConfig,\n) -&gt; tuple[str, str | None, str | None, dict[str, Any] | None, TracingConfig | None]:\n    \"\"\"Resolve tracing settings, preferring explicit run_config overrides.\"\"\"\n    trace_state: TraceState | None = run_state._trace_state if run_state is not None else None\n    default_workflow_name = RunConfig().workflow_name\n    workflow_name = run_config.workflow_name\n\n    trace_id: str | None = run_config.trace_id\n    group_id: str | None = run_config.group_id\n    metadata: dict[str, Any] | None = run_config.trace_metadata\n    tracing: TracingConfig | None = run_config.tracing\n\n    if trace_state:\n        if workflow_name == default_workflow_name and trace_state.workflow_name:\n            workflow_name = trace_state.workflow_name\n        if trace_id is None:\n            trace_id = trace_state.trace_id\n        if group_id is None:\n            group_id = trace_state.group_id\n        if metadata is None and trace_state.metadata is not None:\n            metadata = dict(trace_state.metadata)\n        if tracing is None and trace_state.tracing_api_key:\n            tracing = {\"api_key\": trace_state.tracing_api_key}\n\n    return workflow_name, trace_id, group_id, metadata, tracing\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.resolve_resumed_context","title":"resolve_resumed_context","text":"<pre><code>resolve_resumed_context(\n    *,\n    run_state: RunState[TContext],\n    context: RunContextWrapper[TContext] | TContext | None,\n) -&gt; RunContextWrapper[TContext]\n</code></pre> <p>Return the context wrapper for a resumed run, overriding when provided.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def resolve_resumed_context(\n    *,\n    run_state: RunState[TContext],\n    context: RunContextWrapper[TContext] | TContext | None,\n) -&gt; RunContextWrapper[TContext]:\n    \"\"\"Return the context wrapper for a resumed run, overriding when provided.\"\"\"\n    if context is not None:\n        context_wrapper = ensure_context_wrapper(context)\n        run_state._context = context_wrapper\n        return context_wrapper\n    if run_state._context is None:\n        run_state._context = ensure_context_wrapper(context)\n    return run_state._context\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.ensure_context_wrapper","title":"ensure_context_wrapper","text":"<pre><code>ensure_context_wrapper(\n    context: RunContextWrapper[TContext] | TContext | None,\n) -&gt; RunContextWrapper[TContext]\n</code></pre> <p>Normalize a context value into a RunContextWrapper.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def ensure_context_wrapper(\n    context: RunContextWrapper[TContext] | TContext | None,\n) -&gt; RunContextWrapper[TContext]:\n    \"\"\"Normalize a context value into a RunContextWrapper.\"\"\"\n    if isinstance(context, RunContextWrapper):\n        return context\n    return RunContextWrapper(context=cast(TContext, context))\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.describe_run_state_step","title":"describe_run_state_step","text":"<pre><code>describe_run_state_step(\n    step: object | None,\n) -&gt; str | int | None\n</code></pre> <p>Return a debug-friendly label for the current run state step.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def describe_run_state_step(step: object | None) -&gt; str | int | None:\n    \"\"\"Return a debug-friendly label for the current run state step.\"\"\"\n    if step is None:\n        return None\n    if isinstance(step, NextStepInterruption):\n        return \"next_step_interruption\"\n    if isinstance(step, NextStepHandoff):\n        return \"next_step_handoff\"\n    if isinstance(step, NextStepFinalOutput):\n        return \"next_step_final_output\"\n    if isinstance(step, NextStepRunAgain):\n        return \"next_step_run_again\"\n    return type(step).__name__\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.build_generated_items_details","title":"build_generated_items_details","text":"<pre><code>build_generated_items_details(\n    items: list[RunItem], *, include_tool_output: bool\n) -&gt; list[dict[str, object]]\n</code></pre> <p>Return debug-friendly metadata for generated items.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def build_generated_items_details(\n    items: list[RunItem],\n    *,\n    include_tool_output: bool,\n) -&gt; list[dict[str, object]]:\n    \"\"\"Return debug-friendly metadata for generated items.\"\"\"\n    details: list[dict[str, object]] = []\n    for idx, item in enumerate(items):\n        item_info: dict[str, object] = {\"index\": idx, \"type\": item.type}\n        if hasattr(item, \"raw_item\") and isinstance(item.raw_item, dict):\n            item_info[\"raw_type\"] = item.raw_item.get(\"type\")\n            item_info[\"name\"] = item.raw_item.get(\"name\")\n            item_info[\"call_id\"] = item.raw_item.get(\"call_id\")\n            if item.type == \"tool_call_output_item\" and include_tool_output:\n                output_str = str(item.raw_item.get(\"output\", \"\"))[:100]\n                item_info[\"output\"] = output_str\n        details.append(item_info)\n    return details\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.build_resumed_stream_debug_extra","title":"build_resumed_stream_debug_extra","text":"<pre><code>build_resumed_stream_debug_extra(\n    run_state: RunState[TContext],\n    *,\n    include_tool_output: bool,\n) -&gt; dict[str, object]\n</code></pre> <p>Build the logger extra payload when resuming a streamed run.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def build_resumed_stream_debug_extra(\n    run_state: RunState[TContext],\n    *,\n    include_tool_output: bool,\n) -&gt; dict[str, object]:\n    \"\"\"Build the logger extra payload when resuming a streamed run.\"\"\"\n    return {\n        \"current_turn\": run_state._current_turn,\n        \"current_agent\": run_state._current_agent.name if run_state._current_agent else None,\n        \"generated_items_count\": len(run_state._generated_items),\n        \"generated_items_types\": [item.type for item in run_state._generated_items],\n        \"generated_items_details\": build_generated_items_details(\n            run_state._generated_items,\n            include_tool_output=include_tool_output,\n        ),\n        \"current_step_type\": describe_run_state_step(run_state._current_step),\n    }\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.finalize_conversation_tracking","title":"finalize_conversation_tracking","text":"<pre><code>finalize_conversation_tracking(\n    result: RunResult,\n    *,\n    server_conversation_tracker: OpenAIServerConversationTracker\n    | None,\n    run_state: RunState | None,\n) -&gt; RunResult\n</code></pre> <p>Propagate conversation metadata to the result and run state.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def finalize_conversation_tracking(\n    result: RunResult,\n    *,\n    server_conversation_tracker: OpenAIServerConversationTracker | None,\n    run_state: RunState | None,\n) -&gt; RunResult:\n    \"\"\"Propagate conversation metadata to the result and run state.\"\"\"\n    if server_conversation_tracker is None:\n        return result\n    result._conversation_id = server_conversation_tracker.conversation_id\n    result._previous_response_id = server_conversation_tracker.previous_response_id\n    result._auto_previous_response_id = server_conversation_tracker.auto_previous_response_id\n    if run_state is not None:\n        run_state._conversation_id = server_conversation_tracker.conversation_id\n        run_state._previous_response_id = server_conversation_tracker.previous_response_id\n        run_state._auto_previous_response_id = server_conversation_tracker.auto_previous_response_id\n    return result\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.build_interruption_result","title":"build_interruption_result","text":"<pre><code>build_interruption_result(\n    *,\n    result_input: str | list[TResponseInputItem],\n    session_items: list[RunItem],\n    model_responses: list[ModelResponse],\n    current_agent: Agent[Any],\n    input_guardrail_results: list[InputGuardrailResult],\n    tool_input_guardrail_results: list[\n        ToolInputGuardrailResult\n    ],\n    tool_output_guardrail_results: list[\n        ToolOutputGuardrailResult\n    ],\n    context_wrapper: RunContextWrapper[TContext],\n    interruptions: list[ToolApprovalItem],\n    processed_response: ProcessedResponse | None,\n    tool_use_tracker: AgentToolUseTracker,\n    max_turns: int,\n    current_turn: int,\n    generated_items: list[RunItem],\n    run_state: RunState | None,\n    original_input: str | list[TResponseInputItem],\n) -&gt; RunResult\n</code></pre> <p>Create a RunResult for an interruption path.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def build_interruption_result(\n    *,\n    result_input: str | list[TResponseInputItem],\n    session_items: list[RunItem],\n    model_responses: list[ModelResponse],\n    current_agent: Agent[Any],\n    input_guardrail_results: list[InputGuardrailResult],\n    tool_input_guardrail_results: list[ToolInputGuardrailResult],\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult],\n    context_wrapper: RunContextWrapper[TContext],\n    interruptions: list[ToolApprovalItem],\n    processed_response: ProcessedResponse | None,\n    tool_use_tracker: AgentToolUseTracker,\n    max_turns: int,\n    current_turn: int,\n    generated_items: list[RunItem],\n    run_state: RunState | None,\n    original_input: str | list[TResponseInputItem],\n) -&gt; RunResult:\n    \"\"\"Create a RunResult for an interruption path.\"\"\"\n    result = RunResult(\n        input=result_input,\n        new_items=session_items,\n        raw_responses=model_responses,\n        final_output=None,\n        _last_agent=current_agent,\n        input_guardrail_results=input_guardrail_results,\n        output_guardrail_results=[],\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n        context_wrapper=context_wrapper,\n        interruptions=interruptions,\n        _last_processed_response=processed_response,\n        _tool_use_tracker_snapshot=serialize_tool_use_tracker(tool_use_tracker),\n        max_turns=max_turns,\n    )\n    result._current_turn = current_turn\n    result._model_input_items = list(generated_items)\n    if run_state is not None:\n        result._current_turn_persisted_item_count = run_state._current_turn_persisted_item_count\n        result._trace_state = run_state._trace_state\n    result._original_input = copy_input_items(original_input)\n    return result\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.append_model_response_if_new","title":"append_model_response_if_new","text":"<pre><code>append_model_response_if_new(\n    model_responses: list[ModelResponse],\n    response: ModelResponse,\n) -&gt; None\n</code></pre> <p>Append a model response only when it is not already in the list tail.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def append_model_response_if_new(\n    model_responses: list[ModelResponse],\n    response: ModelResponse,\n) -&gt; None:\n    \"\"\"Append a model response only when it is not already in the list tail.\"\"\"\n    if not model_responses or model_responses[-1] is not response:\n        model_responses.append(response)\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.input_guardrails_triggered","title":"input_guardrails_triggered","text":"<pre><code>input_guardrails_triggered(\n    results: list[InputGuardrailResult],\n) -&gt; bool\n</code></pre> <p>Return True when any guardrail tripwire has fired.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def input_guardrails_triggered(results: list[InputGuardrailResult]) -&gt; bool:\n    \"\"\"Return True when any guardrail tripwire has fired.\"\"\"\n    return any(result.output.tripwire_triggered for result in results)\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.update_run_state_for_interruption","title":"update_run_state_for_interruption","text":"<pre><code>update_run_state_for_interruption(\n    *,\n    run_state: RunState[TContext],\n    model_responses: list[ModelResponse],\n    processed_response: ProcessedResponse | None,\n    generated_items: list[RunItem],\n    session_items: list[RunItem] | None,\n    current_turn: int,\n    next_step: NextStepInterruption,\n) -&gt; None\n</code></pre> <p>Sync run-state fields needed to resume after an interruption.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def update_run_state_for_interruption(\n    *,\n    run_state: RunState[TContext],\n    model_responses: list[ModelResponse],\n    processed_response: ProcessedResponse | None,\n    generated_items: list[RunItem],\n    session_items: list[RunItem] | None,\n    current_turn: int,\n    next_step: NextStepInterruption,\n) -&gt; None:\n    \"\"\"Sync run-state fields needed to resume after an interruption.\"\"\"\n    run_state._model_responses = model_responses\n    run_state._last_processed_response = processed_response\n    run_state._generated_items = generated_items\n    if session_items is not None:\n        run_state._session_items = list(session_items)\n    run_state._current_step = next_step\n    run_state._current_turn = current_turn\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.save_turn_items_if_needed","title":"save_turn_items_if_needed  <code>async</code>","text":"<pre><code>save_turn_items_if_needed(\n    *,\n    session: Session | None,\n    run_state: RunState | None,\n    session_persistence_enabled: bool,\n    input_guardrail_results: list[InputGuardrailResult],\n    items: list[RunItem],\n    response_id: str | None,\n    store: bool | None = None,\n) -&gt; None\n</code></pre> <p>Persist turn items when persistence is enabled and guardrails allow it.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>async def save_turn_items_if_needed(\n    *,\n    session: Session | None,\n    run_state: RunState | None,\n    session_persistence_enabled: bool,\n    input_guardrail_results: list[InputGuardrailResult],\n    items: list[RunItem],\n    response_id: str | None,\n    store: bool | None = None,\n) -&gt; None:\n    \"\"\"Persist turn items when persistence is enabled and guardrails allow it.\"\"\"\n    if not session_persistence_enabled:\n        return\n    if input_guardrails_triggered(input_guardrail_results):\n        return\n    if run_state is not None and run_state._current_turn_persisted_item_count &gt; 0:\n        return\n    await save_result_to_session(\n        session,\n        [],\n        list(items),\n        run_state,\n        response_id=response_id,\n        store=store,\n    )\n</code></pre>"},{"location":"ref/run_internal/agent_runner_helpers/#agents.run_internal.agent_runner_helpers.resolve_processed_response","title":"resolve_processed_response","text":"<pre><code>resolve_processed_response(\n    *,\n    run_state: RunState | None,\n    processed_response: ProcessedResponse | None,\n) -&gt; ProcessedResponse | None\n</code></pre> <p>Return a processed response, falling back to the run state when missing.</p> Source code in <code>src/agents/run_internal/agent_runner_helpers.py</code> <pre><code>def resolve_processed_response(\n    *,\n    run_state: RunState | None,\n    processed_response: ProcessedResponse | None,\n) -&gt; ProcessedResponse | None:\n    \"\"\"Return a processed response, falling back to the run state when missing.\"\"\"\n    if processed_response is None and run_state is not None:\n        return run_state._last_processed_response\n    return processed_response\n</code></pre>"},{"location":"ref/run_internal/approvals/","title":"<code>Approvals</code>","text":"<p>Helpers for approval handling within the run loop. Keep only execution-time utilities that coordinate approval placeholders and normalization; public APIs should stay in run.py or peer modules.</p>"},{"location":"ref/run_internal/approvals/#agents.run_internal.approvals.append_approval_error_output","title":"append_approval_error_output","text":"<pre><code>append_approval_error_output(\n    *,\n    generated_items: list[RunItem],\n    agent: Agent[Any],\n    tool_call: Any,\n    tool_name: str,\n    call_id: str | None,\n    message: str,\n) -&gt; None\n</code></pre> <p>Emit a synthetic tool output so users see why an approval failed.</p> Source code in <code>src/agents/run_internal/approvals.py</code> <pre><code>def append_approval_error_output(\n    *,\n    generated_items: list[RunItem],\n    agent: Agent[Any],\n    tool_call: Any,\n    tool_name: str,\n    call_id: str | None,\n    message: str,\n) -&gt; None:\n    \"\"\"Emit a synthetic tool output so users see why an approval failed.\"\"\"\n    error_tool_call = _build_function_tool_call_for_approval_error(tool_call, tool_name, call_id)\n    generated_items.append(\n        ToolCallOutputItem(\n            output=message,\n            raw_item=ItemHelpers.tool_call_output_item(error_tool_call, message),\n            agent=agent,\n        )\n    )\n</code></pre>"},{"location":"ref/run_internal/approvals/#agents.run_internal.approvals.filter_tool_approvals","title":"filter_tool_approvals","text":"<pre><code>filter_tool_approvals(\n    interruptions: Sequence[Any],\n) -&gt; list[ToolApprovalItem]\n</code></pre> <p>Keep only approval items from a mixed interruption payload.</p> Source code in <code>src/agents/run_internal/approvals.py</code> <pre><code>def filter_tool_approvals(interruptions: Sequence[Any]) -&gt; list[ToolApprovalItem]:\n    \"\"\"Keep only approval items from a mixed interruption payload.\"\"\"\n    return [item for item in interruptions if isinstance(item, ToolApprovalItem)]\n</code></pre>"},{"location":"ref/run_internal/approvals/#agents.run_internal.approvals.approvals_from_step","title":"approvals_from_step","text":"<pre><code>approvals_from_step(step: Any) -&gt; list[ToolApprovalItem]\n</code></pre> <p>Return approvals from a step that may or may not contain interruptions.</p> Source code in <code>src/agents/run_internal/approvals.py</code> <pre><code>def approvals_from_step(step: Any) -&gt; list[ToolApprovalItem]:\n    \"\"\"Return approvals from a step that may or may not contain interruptions.\"\"\"\n    interruptions = getattr(step, \"interruptions\", None)\n    if interruptions is None:\n        return []\n    return filter_tool_approvals(interruptions)\n</code></pre>"},{"location":"ref/run_internal/approvals/#agents.run_internal.approvals.append_input_items_excluding_approvals","title":"append_input_items_excluding_approvals","text":"<pre><code>append_input_items_excluding_approvals(\n    base_input: list[TResponseInputItem],\n    items: Sequence[RunItem],\n) -&gt; None\n</code></pre> <p>Append tool outputs to model input while skipping approval placeholders.</p> Source code in <code>src/agents/run_internal/approvals.py</code> <pre><code>def append_input_items_excluding_approvals(\n    base_input: list[TResponseInputItem],\n    items: Sequence[RunItem],\n) -&gt; None:\n    \"\"\"Append tool outputs to model input while skipping approval placeholders.\"\"\"\n    for item in items:\n        if item.type == \"tool_approval_item\":\n            continue\n        base_input.append(item.to_input_item())\n</code></pre>"},{"location":"ref/run_internal/error_handlers/","title":"<code>Error Handlers</code>","text":""},{"location":"ref/run_internal/guardrails/","title":"<code>Guardrails</code>","text":""},{"location":"ref/run_internal/guardrails/#agents.run_internal.guardrails.run_input_guardrails_with_queue","title":"run_input_guardrails_with_queue  <code>async</code>","text":"<pre><code>run_input_guardrails_with_queue(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n    streamed_result: RunResultStreaming,\n    parent_span: Span[Any],\n) -&gt; None\n</code></pre> <p>Run guardrails concurrently and stream results into the queue.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def run_input_guardrails_with_queue(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n    streamed_result: RunResultStreaming,\n    parent_span: Span[Any],\n) -&gt; None:\n    \"\"\"Run guardrails concurrently and stream results into the queue.\"\"\"\n    queue = streamed_result._input_guardrail_queue\n\n    guardrail_tasks = [\n        asyncio.create_task(run_single_input_guardrail(agent, guardrail, input, context))\n        for guardrail in guardrails\n    ]\n    guardrail_results = []\n    try:\n        for done in asyncio.as_completed(guardrail_tasks):\n            result = await done\n            if result.output.tripwire_triggered:\n                for t in guardrail_tasks:\n                    t.cancel()\n                await asyncio.gather(*guardrail_tasks, return_exceptions=True)\n                _error_tracing.attach_error_to_span(\n                    parent_span,\n                    SpanError(\n                        message=\"Guardrail tripwire triggered\",\n                        data={\n                            \"guardrail\": result.guardrail.get_name(),\n                            \"type\": \"input_guardrail\",\n                        },\n                    ),\n                )\n                queue.put_nowait(result)\n                guardrail_results.append(result)\n                break\n            queue.put_nowait(result)\n            guardrail_results.append(result)\n    except Exception:\n        for t in guardrail_tasks:\n            t.cancel()\n        raise\n\n    streamed_result.input_guardrail_results = (\n        streamed_result.input_guardrail_results + guardrail_results\n    )\n</code></pre>"},{"location":"ref/run_internal/guardrails/#agents.run_internal.guardrails.run_input_guardrails","title":"run_input_guardrails  <code>async</code>","text":"<pre><code>run_input_guardrails(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n) -&gt; list[InputGuardrailResult]\n</code></pre> <p>Run input guardrails concurrently and raise on tripwires.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def run_input_guardrails(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n) -&gt; list[InputGuardrailResult]:\n    \"\"\"Run input guardrails concurrently and raise on tripwires.\"\"\"\n    if not guardrails:\n        return []\n\n    guardrail_tasks = [\n        asyncio.create_task(run_single_input_guardrail(agent, guardrail, input, context))\n        for guardrail in guardrails\n    ]\n\n    guardrail_results: list[InputGuardrailResult] = []\n\n    for done in asyncio.as_completed(guardrail_tasks):\n        result = await done\n        if result.output.tripwire_triggered:\n            for t in guardrail_tasks:\n                t.cancel()\n            await asyncio.gather(*guardrail_tasks, return_exceptions=True)\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Guardrail tripwire triggered\",\n                    data={\"guardrail\": result.guardrail.get_name()},\n                )\n            )\n            raise InputGuardrailTripwireTriggered(result)\n        guardrail_results.append(result)\n\n    return guardrail_results\n</code></pre>"},{"location":"ref/run_internal/guardrails/#agents.run_internal.guardrails.run_output_guardrails","title":"run_output_guardrails  <code>async</code>","text":"<pre><code>run_output_guardrails(\n    guardrails: list[OutputGuardrail[TContext]],\n    agent: Agent[TContext],\n    agent_output: Any,\n    context: RunContextWrapper[TContext],\n) -&gt; list[OutputGuardrailResult]\n</code></pre> <p>Run output guardrails in parallel and raise on tripwires.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def run_output_guardrails(\n    guardrails: list[OutputGuardrail[TContext]],\n    agent: Agent[TContext],\n    agent_output: Any,\n    context: RunContextWrapper[TContext],\n) -&gt; list[OutputGuardrailResult]:\n    \"\"\"Run output guardrails in parallel and raise on tripwires.\"\"\"\n    if not guardrails:\n        return []\n\n    guardrail_tasks = [\n        asyncio.create_task(run_single_output_guardrail(guardrail, agent, agent_output, context))\n        for guardrail in guardrails\n    ]\n\n    guardrail_results: list[OutputGuardrailResult] = []\n\n    for done in asyncio.as_completed(guardrail_tasks):\n        result = await done\n        if result.output.tripwire_triggered:\n            for t in guardrail_tasks:\n                t.cancel()\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Guardrail tripwire triggered\",\n                    data={\"guardrail\": result.guardrail.get_name()},\n                )\n            )\n            raise OutputGuardrailTripwireTriggered(result)\n        guardrail_results.append(result)\n\n    return guardrail_results\n</code></pre>"},{"location":"ref/run_internal/guardrails/#agents.run_internal.guardrails.input_guardrail_tripwire_triggered_for_stream","title":"input_guardrail_tripwire_triggered_for_stream  <code>async</code>","text":"<pre><code>input_guardrail_tripwire_triggered_for_stream(\n    streamed_result: RunResultStreaming,\n) -&gt; bool\n</code></pre> <p>Return True if any input guardrail triggered during a streamed run.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def input_guardrail_tripwire_triggered_for_stream(\n    streamed_result: RunResultStreaming,\n) -&gt; bool:\n    \"\"\"Return True if any input guardrail triggered during a streamed run.\"\"\"\n    task = streamed_result._input_guardrails_task\n    if task is None:\n        return False\n\n    if not task.done():\n        await task\n\n    return any(\n        guardrail_result.output.tripwire_triggered\n        for guardrail_result in streamed_result.input_guardrail_results\n    )\n</code></pre>"},{"location":"ref/run_internal/items/","title":"<code>Items</code>","text":"<p>Item utilities for the run pipeline. Hosts input normalization helpers and lightweight builders for synthetic run items or IDs used during tool execution. Internal use only.</p>"},{"location":"ref/run_internal/items/#agents.run_internal.items.copy_input_items","title":"copy_input_items","text":"<pre><code>copy_input_items(\n    value: str | list[TResponseInputItem],\n) -&gt; str | list[TResponseInputItem]\n</code></pre> <p>Return a shallow copy of input items so mutations do not leak between turns.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def copy_input_items(value: str | list[TResponseInputItem]) -&gt; str | list[TResponseInputItem]:\n    \"\"\"Return a shallow copy of input items so mutations do not leak between turns.\"\"\"\n    return value if isinstance(value, str) else value.copy()\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.drop_orphan_function_calls","title":"drop_orphan_function_calls","text":"<pre><code>drop_orphan_function_calls(\n    items: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Remove tool call items that do not have corresponding outputs so resumptions or retries do not replay stale tool calls.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def drop_orphan_function_calls(items: list[TResponseInputItem]) -&gt; list[TResponseInputItem]:\n    \"\"\"\n    Remove tool call items that do not have corresponding outputs so resumptions or retries do not\n    replay stale tool calls.\n    \"\"\"\n\n    completed_call_ids = _completed_call_ids_by_type(items)\n\n    filtered: list[TResponseInputItem] = []\n    for entry in items:\n        if not isinstance(entry, dict):\n            filtered.append(entry)\n            continue\n        entry_type = entry.get(\"type\")\n        if not isinstance(entry_type, str):\n            filtered.append(entry)\n            continue\n        output_type = _TOOL_CALL_TO_OUTPUT_TYPE.get(entry_type)\n        if output_type is None:\n            filtered.append(entry)\n            continue\n        call_id = entry.get(\"call_id\")\n        if isinstance(call_id, str) and call_id in completed_call_ids.get(output_type, set()):\n            filtered.append(entry)\n    return filtered\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.ensure_input_item_format","title":"ensure_input_item_format","text":"<pre><code>ensure_input_item_format(\n    item: TResponseInputItem,\n) -&gt; TResponseInputItem\n</code></pre> <p>Ensure a single item is normalized for model input.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def ensure_input_item_format(item: TResponseInputItem) -&gt; TResponseInputItem:\n    \"\"\"Ensure a single item is normalized for model input.\"\"\"\n    coerced = _coerce_to_dict(item)\n    if coerced is None:\n        return item\n\n    return cast(TResponseInputItem, coerced)\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.normalize_input_items_for_api","title":"normalize_input_items_for_api","text":"<pre><code>normalize_input_items_for_api(\n    items: list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Normalize input items for API submission.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def normalize_input_items_for_api(items: list[TResponseInputItem]) -&gt; list[TResponseInputItem]:\n    \"\"\"Normalize input items for API submission.\"\"\"\n\n    normalized: list[TResponseInputItem] = []\n    for item in items:\n        coerced = _coerce_to_dict(item)\n        if coerced is None:\n            normalized.append(item)\n            continue\n\n        normalized_item = dict(coerced)\n        normalized.append(cast(TResponseInputItem, normalized_item))\n    return normalized\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.normalize_resumed_input","title":"normalize_resumed_input","text":"<pre><code>normalize_resumed_input(\n    raw_input: str | list[TResponseInputItem],\n) -&gt; str | list[TResponseInputItem]\n</code></pre> <p>Normalize resumed list inputs and drop orphan tool calls.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def normalize_resumed_input(\n    raw_input: str | list[TResponseInputItem],\n) -&gt; str | list[TResponseInputItem]:\n    \"\"\"Normalize resumed list inputs and drop orphan tool calls.\"\"\"\n    if isinstance(raw_input, list):\n        normalized = normalize_input_items_for_api(raw_input)\n        return drop_orphan_function_calls(normalized)\n    return raw_input\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.fingerprint_input_item","title":"fingerprint_input_item","text":"<pre><code>fingerprint_input_item(\n    item: Any, *, ignore_ids_for_matching: bool = False\n) -&gt; str | None\n</code></pre> <p>Hashable fingerprint used to dedupe or rewind input items across resumes.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def fingerprint_input_item(item: Any, *, ignore_ids_for_matching: bool = False) -&gt; str | None:\n    \"\"\"Hashable fingerprint used to dedupe or rewind input items across resumes.\"\"\"\n    if item is None:\n        return None\n\n    try:\n        if hasattr(item, \"model_dump\"):\n            payload = item.model_dump(exclude_unset=True)\n        elif isinstance(item, dict):\n            payload = dict(item)\n            if ignore_ids_for_matching:\n                payload.pop(\"id\", None)\n        else:\n            payload = ensure_input_item_format(item)\n            if ignore_ids_for_matching and isinstance(payload, dict):\n                payload.pop(\"id\", None)\n\n        return json.dumps(payload, sort_keys=True, default=str)\n    except Exception:\n        return None\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.deduplicate_input_items","title":"deduplicate_input_items","text":"<pre><code>deduplicate_input_items(\n    items: Sequence[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Remove duplicate items that share stable identifiers to avoid re-sending tool outputs.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def deduplicate_input_items(items: Sequence[TResponseInputItem]) -&gt; list[TResponseInputItem]:\n    \"\"\"Remove duplicate items that share stable identifiers to avoid re-sending tool outputs.\"\"\"\n    seen_keys: set[str] = set()\n    deduplicated: list[TResponseInputItem] = []\n    for item in items:\n        dedupe_key = _dedupe_key(item)\n        if dedupe_key is None:\n            deduplicated.append(item)\n            continue\n        if dedupe_key in seen_keys:\n            continue\n        seen_keys.add(dedupe_key)\n        deduplicated.append(item)\n    return deduplicated\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.deduplicate_input_items_preferring_latest","title":"deduplicate_input_items_preferring_latest","text":"<pre><code>deduplicate_input_items_preferring_latest(\n    items: Sequence[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Deduplicate by stable identifiers while keeping the latest occurrence.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def deduplicate_input_items_preferring_latest(\n    items: Sequence[TResponseInputItem],\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Deduplicate by stable identifiers while keeping the latest occurrence.\"\"\"\n    # deduplicate_input_items keeps the first item per dedupe key. Reverse twice so that\n    # the latest item in the original order wins for duplicate IDs/call_ids.\n    return list(reversed(deduplicate_input_items(list(reversed(items)))))\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.function_rejection_item","title":"function_rejection_item","text":"<pre><code>function_rejection_item(\n    agent: Any,\n    tool_call: Any,\n    *,\n    rejection_message: str = REJECTION_MESSAGE,\n) -&gt; ToolCallOutputItem\n</code></pre> <p>Build a ToolCallOutputItem representing a rejected function tool call.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def function_rejection_item(\n    agent: Any,\n    tool_call: Any,\n    *,\n    rejection_message: str = REJECTION_MESSAGE,\n) -&gt; ToolCallOutputItem:\n    \"\"\"Build a ToolCallOutputItem representing a rejected function tool call.\"\"\"\n    if isinstance(tool_call, ResponseFunctionToolCall):\n        drop_agent_tool_run_result(tool_call)\n    return ToolCallOutputItem(\n        output=rejection_message,\n        raw_item=ItemHelpers.tool_call_output_item(tool_call, rejection_message),\n        agent=agent,\n    )\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.shell_rejection_item","title":"shell_rejection_item","text":"<pre><code>shell_rejection_item(\n    agent: Any,\n    call_id: str,\n    *,\n    rejection_message: str = REJECTION_MESSAGE,\n) -&gt; ToolCallOutputItem\n</code></pre> <p>Build a ToolCallOutputItem representing a rejected shell call.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def shell_rejection_item(\n    agent: Any,\n    call_id: str,\n    *,\n    rejection_message: str = REJECTION_MESSAGE,\n) -&gt; ToolCallOutputItem:\n    \"\"\"Build a ToolCallOutputItem representing a rejected shell call.\"\"\"\n    rejection_output: dict[str, Any] = {\n        \"stdout\": \"\",\n        \"stderr\": rejection_message,\n        \"outcome\": {\"type\": \"exit\", \"exit_code\": 1},\n    }\n    rejection_raw_item: dict[str, Any] = {\n        \"type\": \"shell_call_output\",\n        \"call_id\": call_id,\n        \"output\": [rejection_output],\n    }\n    return ToolCallOutputItem(agent=agent, output=rejection_message, raw_item=rejection_raw_item)\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.apply_patch_rejection_item","title":"apply_patch_rejection_item","text":"<pre><code>apply_patch_rejection_item(\n    agent: Any,\n    call_id: str,\n    *,\n    rejection_message: str = REJECTION_MESSAGE,\n) -&gt; ToolCallOutputItem\n</code></pre> <p>Build a ToolCallOutputItem representing a rejected apply_patch call.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def apply_patch_rejection_item(\n    agent: Any,\n    call_id: str,\n    *,\n    rejection_message: str = REJECTION_MESSAGE,\n) -&gt; ToolCallOutputItem:\n    \"\"\"Build a ToolCallOutputItem representing a rejected apply_patch call.\"\"\"\n    rejection_raw_item: dict[str, Any] = {\n        \"type\": \"apply_patch_call_output\",\n        \"call_id\": call_id,\n        \"status\": \"failed\",\n        \"output\": rejection_message,\n    }\n    return ToolCallOutputItem(\n        agent=agent,\n        output=rejection_message,\n        raw_item=rejection_raw_item,\n    )\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.extract_mcp_request_id","title":"extract_mcp_request_id","text":"<pre><code>extract_mcp_request_id(raw_item: Any) -&gt; str | None\n</code></pre> <p>Pull the request id from hosted MCP approval payloads.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def extract_mcp_request_id(raw_item: Any) -&gt; str | None:\n    \"\"\"Pull the request id from hosted MCP approval payloads.\"\"\"\n    if isinstance(raw_item, dict):\n        provider_data = raw_item.get(\"provider_data\")\n        if isinstance(provider_data, dict):\n            candidate = provider_data.get(\"id\")\n            if isinstance(candidate, str):\n                return candidate\n        candidate = raw_item.get(\"id\") or raw_item.get(\"call_id\")\n        return candidate if isinstance(candidate, str) else None\n    try:\n        provider_data = getattr(raw_item, \"provider_data\", None)\n    except Exception:\n        provider_data = None\n    if isinstance(provider_data, dict):\n        candidate = provider_data.get(\"id\")\n        if isinstance(candidate, str):\n            return candidate\n    try:\n        candidate = getattr(raw_item, \"id\", None) or getattr(raw_item, \"call_id\", None)\n    except Exception:\n        candidate = None\n    return candidate if isinstance(candidate, str) else None\n</code></pre>"},{"location":"ref/run_internal/items/#agents.run_internal.items.extract_mcp_request_id_from_run","title":"extract_mcp_request_id_from_run","text":"<pre><code>extract_mcp_request_id_from_run(mcp_run: Any) -&gt; str | None\n</code></pre> <p>Extract the hosted MCP request id from a streaming run item.</p> Source code in <code>src/agents/run_internal/items.py</code> <pre><code>def extract_mcp_request_id_from_run(mcp_run: Any) -&gt; str | None:\n    \"\"\"Extract the hosted MCP request id from a streaming run item.\"\"\"\n    request_item = getattr(mcp_run, \"request_item\", None) or getattr(mcp_run, \"requestItem\", None)\n    if isinstance(request_item, dict):\n        provider_data = request_item.get(\"provider_data\")\n        if isinstance(provider_data, dict):\n            candidate = provider_data.get(\"id\")\n            if isinstance(candidate, str):\n                return candidate\n        candidate = request_item.get(\"id\") or request_item.get(\"call_id\")\n    else:\n        provider_data = getattr(request_item, \"provider_data\", None)\n        if isinstance(provider_data, dict):\n            candidate = provider_data.get(\"id\")\n            if isinstance(candidate, str):\n                return candidate\n        candidate = getattr(request_item, \"id\", None) or getattr(request_item, \"call_id\", None)\n    return candidate if isinstance(candidate, str) else None\n</code></pre>"},{"location":"ref/run_internal/oai_conversation/","title":"<code>Oai Conversation</code>","text":"<p>Conversation-state helpers used during agent runs. This module should only host internal tracking and normalization logic for conversation-aware execution, not public-facing APIs.</p>"},{"location":"ref/run_internal/oai_conversation/#agents.run_internal.oai_conversation.OpenAIServerConversationTracker","title":"OpenAIServerConversationTracker  <code>dataclass</code>","text":"<p>Track server-side conversation state for conversation-aware runs.</p> Source code in <code>src/agents/run_internal/oai_conversation.py</code> <pre><code>@dataclass\nclass OpenAIServerConversationTracker:\n    \"\"\"Track server-side conversation state for conversation-aware runs.\"\"\"\n\n    conversation_id: str | None = None\n    previous_response_id: str | None = None\n    auto_previous_response_id: bool = False\n    sent_items: set[int] = field(default_factory=set)\n    server_items: set[int] = field(default_factory=set)\n    server_item_ids: set[str] = field(default_factory=set)\n    server_tool_call_ids: set[str] = field(default_factory=set)\n    sent_item_fingerprints: set[str] = field(default_factory=set)\n    sent_initial_input: bool = False\n    remaining_initial_input: list[TResponseInputItem] | None = None\n    primed_from_state: bool = False\n\n    def __post_init__(self):\n        \"\"\"Log initial tracker state to make conversation resume behavior debuggable.\"\"\"\n        logger.debug(\n            \"Created OpenAIServerConversationTracker for conv_id=%s, prev_resp_id=%s\",\n            self.conversation_id,\n            self.previous_response_id,\n        )\n\n    def hydrate_from_state(\n        self,\n        *,\n        original_input: str | list[TResponseInputItem],\n        generated_items: list[RunItem],\n        model_responses: list[ModelResponse],\n        session_items: list[TResponseInputItem] | None = None,\n    ) -&gt; None:\n        \"\"\"Seed tracking from prior state so resumed runs do not replay already-sent content.\"\"\"\n        if self.sent_initial_input:\n            return\n\n        normalized_input = original_input\n        if isinstance(original_input, list):\n            normalized = normalize_input_items_for_api(original_input)\n            normalized_input = drop_orphan_function_calls(normalized)\n\n        for item in ItemHelpers.input_to_new_input_list(normalized_input):\n            if item is None:\n                continue\n            self.sent_items.add(id(item))\n            item_id = _normalize_server_item_id(\n                item.get(\"id\") if isinstance(item, dict) else getattr(item, \"id\", None)\n            )\n            if item_id is not None:\n                self.server_item_ids.add(item_id)\n            fp = _fingerprint_for_tracker(item)\n            if fp:\n                self.sent_item_fingerprints.add(fp)\n\n        self.sent_initial_input = True\n        self.remaining_initial_input = None\n\n        latest_response = model_responses[-1] if model_responses else None\n        for response in model_responses:\n            for output_item in response.output:\n                if output_item is None:\n                    continue\n                self.server_items.add(id(output_item))\n                item_id = _normalize_server_item_id(\n                    output_item.get(\"id\")\n                    if isinstance(output_item, dict)\n                    else getattr(output_item, \"id\", None)\n                )\n                if item_id is not None:\n                    self.server_item_ids.add(item_id)\n                call_id = (\n                    output_item.get(\"call_id\")\n                    if isinstance(output_item, dict)\n                    else getattr(output_item, \"call_id\", None)\n                )\n                has_output_payload = isinstance(output_item, dict) and \"output\" in output_item\n                has_output_payload = has_output_payload or hasattr(output_item, \"output\")\n                if isinstance(call_id, str) and has_output_payload:\n                    self.server_tool_call_ids.add(call_id)\n\n        if self.conversation_id is None and latest_response and latest_response.response_id:\n            self.previous_response_id = latest_response.response_id\n\n        if session_items:\n            for item in session_items:\n                item_id = _normalize_server_item_id(\n                    item.get(\"id\") if isinstance(item, dict) else getattr(item, \"id\", None)\n                )\n                if item_id is not None:\n                    self.server_item_ids.add(item_id)\n                call_id = (\n                    item.get(\"call_id\")\n                    if isinstance(item, dict)\n                    else getattr(item, \"call_id\", None)\n                )\n                has_output = isinstance(item, dict) and \"output\" in item\n                has_output = has_output or hasattr(item, \"output\")\n                if isinstance(call_id, str) and has_output:\n                    self.server_tool_call_ids.add(call_id)\n                fp = _fingerprint_for_tracker(item)\n                if fp:\n                    self.sent_item_fingerprints.add(fp)\n        for item in generated_items:  # type: ignore[assignment]\n            run_item: RunItem = cast(RunItem, item)\n            raw_item = run_item.raw_item\n            if raw_item is None:\n                continue\n            is_tool_call_item = run_item.type in {\"tool_call_item\", \"handoff_call_item\"}\n\n            if isinstance(raw_item, dict):\n                item_id = _normalize_server_item_id(raw_item.get(\"id\"))\n                call_id = raw_item.get(\"call_id\")\n                has_output_payload = \"output\" in raw_item\n                has_output_payload = has_output_payload or hasattr(raw_item, \"output\")\n                has_call_id = isinstance(call_id, str)\n                should_mark = item_id is not None or (\n                    has_call_id and (has_output_payload or is_tool_call_item)\n                )\n                if not should_mark:\n                    continue\n\n                raw_item_id = id(raw_item)\n                self.sent_items.add(raw_item_id)\n                fp = _fingerprint_for_tracker(raw_item)\n                if fp:\n                    self.sent_item_fingerprints.add(fp)\n\n                if item_id is not None:\n                    self.server_item_ids.add(item_id)\n                if isinstance(call_id, str) and has_output_payload:\n                    self.server_tool_call_ids.add(call_id)\n            else:\n                item_id = _normalize_server_item_id(getattr(raw_item, \"id\", None))\n                call_id = getattr(raw_item, \"call_id\", None)\n                has_output_payload = hasattr(raw_item, \"output\")\n                has_call_id = isinstance(call_id, str)\n                should_mark = item_id is not None or (\n                    has_call_id and (has_output_payload or is_tool_call_item)\n                )\n                if not should_mark:\n                    continue\n\n                self.sent_items.add(id(raw_item))\n                fp = _fingerprint_for_tracker(raw_item)\n                if fp:\n                    self.sent_item_fingerprints.add(fp)\n                if item_id is not None:\n                    self.server_item_ids.add(item_id)\n                if isinstance(call_id, str) and has_output_payload:\n                    self.server_tool_call_ids.add(call_id)\n        self.primed_from_state = True\n\n    def track_server_items(self, model_response: ModelResponse | None) -&gt; None:\n        \"\"\"Track server-acknowledged outputs to avoid re-sending them on retries.\"\"\"\n        if model_response is None:\n            return\n\n        server_item_fingerprints: set[str] = set()\n        for output_item in model_response.output:\n            if output_item is None:\n                continue\n            self.server_items.add(id(output_item))\n            item_id = _normalize_server_item_id(\n                output_item.get(\"id\")\n                if isinstance(output_item, dict)\n                else getattr(output_item, \"id\", None)\n            )\n            if item_id is not None:\n                self.server_item_ids.add(item_id)\n            call_id = (\n                output_item.get(\"call_id\")\n                if isinstance(output_item, dict)\n                else getattr(output_item, \"call_id\", None)\n            )\n            has_output_payload = isinstance(output_item, dict) and \"output\" in output_item\n            has_output_payload = has_output_payload or hasattr(output_item, \"output\")\n            if isinstance(call_id, str) and has_output_payload:\n                self.server_tool_call_ids.add(call_id)\n            fp = _fingerprint_for_tracker(output_item)\n            if fp:\n                self.sent_item_fingerprints.add(fp)\n                server_item_fingerprints.add(fp)\n\n        if self.remaining_initial_input and server_item_fingerprints:\n            remaining: list[TResponseInputItem] = []\n            for pending in self.remaining_initial_input:\n                pending_fp = _fingerprint_for_tracker(pending)\n                if pending_fp and pending_fp in server_item_fingerprints:\n                    continue\n                remaining.append(pending)\n            self.remaining_initial_input = remaining or None\n\n        if (\n            self.conversation_id is None\n            and (self.previous_response_id is not None or self.auto_previous_response_id)\n            and model_response.response_id is not None\n        ):\n            self.previous_response_id = model_response.response_id\n\n    def mark_input_as_sent(self, items: Sequence[TResponseInputItem]) -&gt; None:\n        \"\"\"Mark delivered inputs so we do not send them again after pauses or retries.\"\"\"\n        if not items:\n            return\n\n        delivered_ids: set[int] = set()\n        for item in items:\n            if item is None:\n                continue\n            delivered_ids.add(id(item))\n            self.sent_items.add(id(item))\n\n        if not self.remaining_initial_input:\n            return\n\n        delivered_by_content: set[str] = set()\n        for item in items:\n            fp = _fingerprint_for_tracker(item)\n            if fp:\n                delivered_by_content.add(fp)\n\n        remaining: list[TResponseInputItem] = []\n        for pending in self.remaining_initial_input:\n            if id(pending) in delivered_ids:\n                continue\n            pending_fp = _fingerprint_for_tracker(pending)\n            if pending_fp and pending_fp in delivered_by_content:\n                continue\n            remaining.append(pending)\n\n        self.remaining_initial_input = remaining or None\n\n    def rewind_input(self, items: Sequence[TResponseInputItem]) -&gt; None:\n        \"\"\"Rewind previously marked inputs so they can be resent.\"\"\"\n        if not items:\n            return\n\n        rewind_items: list[TResponseInputItem] = []\n        for item in items:\n            if item is None:\n                continue\n            rewind_items.append(item)\n            self.sent_items.discard(id(item))\n            fp = _fingerprint_for_tracker(item)\n            if fp:\n                self.sent_item_fingerprints.discard(fp)\n\n        if not rewind_items:\n            return\n\n        logger.debug(\"Queued %d items to resend after conversation retry\", len(rewind_items))\n        existing = self.remaining_initial_input or []\n        self.remaining_initial_input = rewind_items + existing\n\n    def prepare_input(\n        self,\n        original_input: str | list[TResponseInputItem],\n        generated_items: list[RunItem],\n    ) -&gt; list[TResponseInputItem]:\n        \"\"\"Assemble the next model input while skipping duplicates and approvals.\"\"\"\n        input_items: list[TResponseInputItem] = []\n\n        if not self.sent_initial_input:\n            initial_items = ItemHelpers.input_to_new_input_list(original_input)\n            input_items.extend(initial_items)\n            filtered_initials = []\n            for item in initial_items:\n                if item is None or isinstance(item, (str, bytes)):\n                    continue\n                filtered_initials.append(item)\n            self.remaining_initial_input = filtered_initials or None\n            self.sent_initial_input = True\n        elif self.remaining_initial_input:\n            input_items.extend(self.remaining_initial_input)\n\n        for item in generated_items:  # type: ignore[assignment]\n            run_item: RunItem = cast(RunItem, item)\n            if run_item.type == \"tool_approval_item\":\n                continue\n\n            raw_item = run_item.raw_item\n            if raw_item is None:\n                continue\n\n            item_id = _normalize_server_item_id(\n                raw_item.get(\"id\") if isinstance(raw_item, dict) else getattr(raw_item, \"id\", None)\n            )\n            if item_id is not None and item_id in self.server_item_ids:\n                continue\n\n            call_id = (\n                raw_item.get(\"call_id\")\n                if isinstance(raw_item, dict)\n                else getattr(raw_item, \"call_id\", None)\n            )\n            has_output_payload = isinstance(raw_item, dict) and \"output\" in raw_item\n            has_output_payload = has_output_payload or hasattr(raw_item, \"output\")\n            if (\n                isinstance(call_id, str)\n                and has_output_payload\n                and call_id in self.server_tool_call_ids\n            ):\n                continue\n\n            raw_item_id = id(raw_item)\n            if raw_item_id in self.sent_items or raw_item_id in self.server_items:\n                continue\n\n            to_input = getattr(run_item, \"to_input_item\", None)\n            input_item = to_input() if callable(to_input) else cast(TResponseInputItem, raw_item)\n\n            fp = _fingerprint_for_tracker(input_item)\n            if fp and self.primed_from_state and fp in self.sent_item_fingerprints:\n                continue\n\n            input_items.append(input_item)\n\n            self.sent_items.add(raw_item_id)\n\n        return input_items\n</code></pre>"},{"location":"ref/run_internal/oai_conversation/#agents.run_internal.oai_conversation.OpenAIServerConversationTracker.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Log initial tracker state to make conversation resume behavior debuggable.</p> Source code in <code>src/agents/run_internal/oai_conversation.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Log initial tracker state to make conversation resume behavior debuggable.\"\"\"\n    logger.debug(\n        \"Created OpenAIServerConversationTracker for conv_id=%s, prev_resp_id=%s\",\n        self.conversation_id,\n        self.previous_response_id,\n    )\n</code></pre>"},{"location":"ref/run_internal/oai_conversation/#agents.run_internal.oai_conversation.OpenAIServerConversationTracker.hydrate_from_state","title":"hydrate_from_state","text":"<pre><code>hydrate_from_state(\n    *,\n    original_input: str | list[TResponseInputItem],\n    generated_items: list[RunItem],\n    model_responses: list[ModelResponse],\n    session_items: list[TResponseInputItem] | None = None,\n) -&gt; None\n</code></pre> <p>Seed tracking from prior state so resumed runs do not replay already-sent content.</p> Source code in <code>src/agents/run_internal/oai_conversation.py</code> <pre><code>def hydrate_from_state(\n    self,\n    *,\n    original_input: str | list[TResponseInputItem],\n    generated_items: list[RunItem],\n    model_responses: list[ModelResponse],\n    session_items: list[TResponseInputItem] | None = None,\n) -&gt; None:\n    \"\"\"Seed tracking from prior state so resumed runs do not replay already-sent content.\"\"\"\n    if self.sent_initial_input:\n        return\n\n    normalized_input = original_input\n    if isinstance(original_input, list):\n        normalized = normalize_input_items_for_api(original_input)\n        normalized_input = drop_orphan_function_calls(normalized)\n\n    for item in ItemHelpers.input_to_new_input_list(normalized_input):\n        if item is None:\n            continue\n        self.sent_items.add(id(item))\n        item_id = _normalize_server_item_id(\n            item.get(\"id\") if isinstance(item, dict) else getattr(item, \"id\", None)\n        )\n        if item_id is not None:\n            self.server_item_ids.add(item_id)\n        fp = _fingerprint_for_tracker(item)\n        if fp:\n            self.sent_item_fingerprints.add(fp)\n\n    self.sent_initial_input = True\n    self.remaining_initial_input = None\n\n    latest_response = model_responses[-1] if model_responses else None\n    for response in model_responses:\n        for output_item in response.output:\n            if output_item is None:\n                continue\n            self.server_items.add(id(output_item))\n            item_id = _normalize_server_item_id(\n                output_item.get(\"id\")\n                if isinstance(output_item, dict)\n                else getattr(output_item, \"id\", None)\n            )\n            if item_id is not None:\n                self.server_item_ids.add(item_id)\n            call_id = (\n                output_item.get(\"call_id\")\n                if isinstance(output_item, dict)\n                else getattr(output_item, \"call_id\", None)\n            )\n            has_output_payload = isinstance(output_item, dict) and \"output\" in output_item\n            has_output_payload = has_output_payload or hasattr(output_item, \"output\")\n            if isinstance(call_id, str) and has_output_payload:\n                self.server_tool_call_ids.add(call_id)\n\n    if self.conversation_id is None and latest_response and latest_response.response_id:\n        self.previous_response_id = latest_response.response_id\n\n    if session_items:\n        for item in session_items:\n            item_id = _normalize_server_item_id(\n                item.get(\"id\") if isinstance(item, dict) else getattr(item, \"id\", None)\n            )\n            if item_id is not None:\n                self.server_item_ids.add(item_id)\n            call_id = (\n                item.get(\"call_id\")\n                if isinstance(item, dict)\n                else getattr(item, \"call_id\", None)\n            )\n            has_output = isinstance(item, dict) and \"output\" in item\n            has_output = has_output or hasattr(item, \"output\")\n            if isinstance(call_id, str) and has_output:\n                self.server_tool_call_ids.add(call_id)\n            fp = _fingerprint_for_tracker(item)\n            if fp:\n                self.sent_item_fingerprints.add(fp)\n    for item in generated_items:  # type: ignore[assignment]\n        run_item: RunItem = cast(RunItem, item)\n        raw_item = run_item.raw_item\n        if raw_item is None:\n            continue\n        is_tool_call_item = run_item.type in {\"tool_call_item\", \"handoff_call_item\"}\n\n        if isinstance(raw_item, dict):\n            item_id = _normalize_server_item_id(raw_item.get(\"id\"))\n            call_id = raw_item.get(\"call_id\")\n            has_output_payload = \"output\" in raw_item\n            has_output_payload = has_output_payload or hasattr(raw_item, \"output\")\n            has_call_id = isinstance(call_id, str)\n            should_mark = item_id is not None or (\n                has_call_id and (has_output_payload or is_tool_call_item)\n            )\n            if not should_mark:\n                continue\n\n            raw_item_id = id(raw_item)\n            self.sent_items.add(raw_item_id)\n            fp = _fingerprint_for_tracker(raw_item)\n            if fp:\n                self.sent_item_fingerprints.add(fp)\n\n            if item_id is not None:\n                self.server_item_ids.add(item_id)\n            if isinstance(call_id, str) and has_output_payload:\n                self.server_tool_call_ids.add(call_id)\n        else:\n            item_id = _normalize_server_item_id(getattr(raw_item, \"id\", None))\n            call_id = getattr(raw_item, \"call_id\", None)\n            has_output_payload = hasattr(raw_item, \"output\")\n            has_call_id = isinstance(call_id, str)\n            should_mark = item_id is not None or (\n                has_call_id and (has_output_payload or is_tool_call_item)\n            )\n            if not should_mark:\n                continue\n\n            self.sent_items.add(id(raw_item))\n            fp = _fingerprint_for_tracker(raw_item)\n            if fp:\n                self.sent_item_fingerprints.add(fp)\n            if item_id is not None:\n                self.server_item_ids.add(item_id)\n            if isinstance(call_id, str) and has_output_payload:\n                self.server_tool_call_ids.add(call_id)\n    self.primed_from_state = True\n</code></pre>"},{"location":"ref/run_internal/oai_conversation/#agents.run_internal.oai_conversation.OpenAIServerConversationTracker.track_server_items","title":"track_server_items","text":"<pre><code>track_server_items(\n    model_response: ModelResponse | None,\n) -&gt; None\n</code></pre> <p>Track server-acknowledged outputs to avoid re-sending them on retries.</p> Source code in <code>src/agents/run_internal/oai_conversation.py</code> <pre><code>def track_server_items(self, model_response: ModelResponse | None) -&gt; None:\n    \"\"\"Track server-acknowledged outputs to avoid re-sending them on retries.\"\"\"\n    if model_response is None:\n        return\n\n    server_item_fingerprints: set[str] = set()\n    for output_item in model_response.output:\n        if output_item is None:\n            continue\n        self.server_items.add(id(output_item))\n        item_id = _normalize_server_item_id(\n            output_item.get(\"id\")\n            if isinstance(output_item, dict)\n            else getattr(output_item, \"id\", None)\n        )\n        if item_id is not None:\n            self.server_item_ids.add(item_id)\n        call_id = (\n            output_item.get(\"call_id\")\n            if isinstance(output_item, dict)\n            else getattr(output_item, \"call_id\", None)\n        )\n        has_output_payload = isinstance(output_item, dict) and \"output\" in output_item\n        has_output_payload = has_output_payload or hasattr(output_item, \"output\")\n        if isinstance(call_id, str) and has_output_payload:\n            self.server_tool_call_ids.add(call_id)\n        fp = _fingerprint_for_tracker(output_item)\n        if fp:\n            self.sent_item_fingerprints.add(fp)\n            server_item_fingerprints.add(fp)\n\n    if self.remaining_initial_input and server_item_fingerprints:\n        remaining: list[TResponseInputItem] = []\n        for pending in self.remaining_initial_input:\n            pending_fp = _fingerprint_for_tracker(pending)\n            if pending_fp and pending_fp in server_item_fingerprints:\n                continue\n            remaining.append(pending)\n        self.remaining_initial_input = remaining or None\n\n    if (\n        self.conversation_id is None\n        and (self.previous_response_id is not None or self.auto_previous_response_id)\n        and model_response.response_id is not None\n    ):\n        self.previous_response_id = model_response.response_id\n</code></pre>"},{"location":"ref/run_internal/oai_conversation/#agents.run_internal.oai_conversation.OpenAIServerConversationTracker.mark_input_as_sent","title":"mark_input_as_sent","text":"<pre><code>mark_input_as_sent(\n    items: Sequence[TResponseInputItem],\n) -&gt; None\n</code></pre> <p>Mark delivered inputs so we do not send them again after pauses or retries.</p> Source code in <code>src/agents/run_internal/oai_conversation.py</code> <pre><code>def mark_input_as_sent(self, items: Sequence[TResponseInputItem]) -&gt; None:\n    \"\"\"Mark delivered inputs so we do not send them again after pauses or retries.\"\"\"\n    if not items:\n        return\n\n    delivered_ids: set[int] = set()\n    for item in items:\n        if item is None:\n            continue\n        delivered_ids.add(id(item))\n        self.sent_items.add(id(item))\n\n    if not self.remaining_initial_input:\n        return\n\n    delivered_by_content: set[str] = set()\n    for item in items:\n        fp = _fingerprint_for_tracker(item)\n        if fp:\n            delivered_by_content.add(fp)\n\n    remaining: list[TResponseInputItem] = []\n    for pending in self.remaining_initial_input:\n        if id(pending) in delivered_ids:\n            continue\n        pending_fp = _fingerprint_for_tracker(pending)\n        if pending_fp and pending_fp in delivered_by_content:\n            continue\n        remaining.append(pending)\n\n    self.remaining_initial_input = remaining or None\n</code></pre>"},{"location":"ref/run_internal/oai_conversation/#agents.run_internal.oai_conversation.OpenAIServerConversationTracker.rewind_input","title":"rewind_input","text":"<pre><code>rewind_input(items: Sequence[TResponseInputItem]) -&gt; None\n</code></pre> <p>Rewind previously marked inputs so they can be resent.</p> Source code in <code>src/agents/run_internal/oai_conversation.py</code> <pre><code>def rewind_input(self, items: Sequence[TResponseInputItem]) -&gt; None:\n    \"\"\"Rewind previously marked inputs so they can be resent.\"\"\"\n    if not items:\n        return\n\n    rewind_items: list[TResponseInputItem] = []\n    for item in items:\n        if item is None:\n            continue\n        rewind_items.append(item)\n        self.sent_items.discard(id(item))\n        fp = _fingerprint_for_tracker(item)\n        if fp:\n            self.sent_item_fingerprints.discard(fp)\n\n    if not rewind_items:\n        return\n\n    logger.debug(\"Queued %d items to resend after conversation retry\", len(rewind_items))\n    existing = self.remaining_initial_input or []\n    self.remaining_initial_input = rewind_items + existing\n</code></pre>"},{"location":"ref/run_internal/oai_conversation/#agents.run_internal.oai_conversation.OpenAIServerConversationTracker.prepare_input","title":"prepare_input","text":"<pre><code>prepare_input(\n    original_input: str | list[TResponseInputItem],\n    generated_items: list[RunItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Assemble the next model input while skipping duplicates and approvals.</p> Source code in <code>src/agents/run_internal/oai_conversation.py</code> <pre><code>def prepare_input(\n    self,\n    original_input: str | list[TResponseInputItem],\n    generated_items: list[RunItem],\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Assemble the next model input while skipping duplicates and approvals.\"\"\"\n    input_items: list[TResponseInputItem] = []\n\n    if not self.sent_initial_input:\n        initial_items = ItemHelpers.input_to_new_input_list(original_input)\n        input_items.extend(initial_items)\n        filtered_initials = []\n        for item in initial_items:\n            if item is None or isinstance(item, (str, bytes)):\n                continue\n            filtered_initials.append(item)\n        self.remaining_initial_input = filtered_initials or None\n        self.sent_initial_input = True\n    elif self.remaining_initial_input:\n        input_items.extend(self.remaining_initial_input)\n\n    for item in generated_items:  # type: ignore[assignment]\n        run_item: RunItem = cast(RunItem, item)\n        if run_item.type == \"tool_approval_item\":\n            continue\n\n        raw_item = run_item.raw_item\n        if raw_item is None:\n            continue\n\n        item_id = _normalize_server_item_id(\n            raw_item.get(\"id\") if isinstance(raw_item, dict) else getattr(raw_item, \"id\", None)\n        )\n        if item_id is not None and item_id in self.server_item_ids:\n            continue\n\n        call_id = (\n            raw_item.get(\"call_id\")\n            if isinstance(raw_item, dict)\n            else getattr(raw_item, \"call_id\", None)\n        )\n        has_output_payload = isinstance(raw_item, dict) and \"output\" in raw_item\n        has_output_payload = has_output_payload or hasattr(raw_item, \"output\")\n        if (\n            isinstance(call_id, str)\n            and has_output_payload\n            and call_id in self.server_tool_call_ids\n        ):\n            continue\n\n        raw_item_id = id(raw_item)\n        if raw_item_id in self.sent_items or raw_item_id in self.server_items:\n            continue\n\n        to_input = getattr(run_item, \"to_input_item\", None)\n        input_item = to_input() if callable(to_input) else cast(TResponseInputItem, raw_item)\n\n        fp = _fingerprint_for_tracker(input_item)\n        if fp and self.primed_from_state and fp in self.sent_item_fingerprints:\n            continue\n\n        input_items.append(input_item)\n\n        self.sent_items.add(raw_item_id)\n\n    return input_items\n</code></pre>"},{"location":"ref/run_internal/run_loop/","title":"<code>Run Loop</code>","text":"<p>Run-loop orchestration helpers used by the Agent runner. This module coordinates tool execution, approvals, and turn processing; all symbols here are internal and not part of the public SDK.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.NextStepInterruption","title":"NextStepInterruption  <code>dataclass</code>","text":"<p>Represents an interruption in the agent run due to tool approval requests.</p> Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>@dataclass\nclass NextStepInterruption:\n    \"\"\"Represents an interruption in the agent run due to tool approval requests.\"\"\"\n\n    interruptions: list[ToolApprovalItem]\n    \"\"\"The list of tool calls awaiting approval.\"\"\"\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.NextStepInterruption.interruptions","title":"interruptions  <code>instance-attribute</code>","text":"<pre><code>interruptions: list[ToolApprovalItem]\n</code></pre> <p>The list of tool calls awaiting approval.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ProcessedResponse","title":"ProcessedResponse  <code>dataclass</code>","text":"Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>@dataclass\nclass ProcessedResponse:\n    new_items: list[RunItem]\n    handoffs: list[ToolRunHandoff]\n    functions: list[ToolRunFunction]\n    computer_actions: list[ToolRunComputerAction]\n    local_shell_calls: list[ToolRunLocalShellCall]\n    shell_calls: list[ToolRunShellCall]\n    apply_patch_calls: list[ToolRunApplyPatchCall]\n    tools_used: list[str]  # Names of all tools used, including hosted tools\n    mcp_approval_requests: list[ToolRunMCPApprovalRequest]  # Only requests with callbacks\n    interruptions: list[ToolApprovalItem]  # Tool approval items awaiting user decision\n\n    def has_tools_or_approvals_to_run(self) -&gt; bool:\n        # Handoffs, functions and computer actions need local processing\n        # Hosted tools have already run, so there's nothing to do.\n        return any(\n            [\n                self.handoffs,\n                self.functions,\n                self.computer_actions,\n                self.local_shell_calls,\n                self.shell_calls,\n                self.apply_patch_calls,\n                self.mcp_approval_requests,\n            ]\n        )\n\n    def has_interruptions(self) -&gt; bool:\n        \"\"\"Check if there are tool calls awaiting approval.\"\"\"\n        return len(self.interruptions) &gt; 0\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ProcessedResponse.has_interruptions","title":"has_interruptions","text":"<pre><code>has_interruptions() -&gt; bool\n</code></pre> <p>Check if there are tool calls awaiting approval.</p> Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>def has_interruptions(self) -&gt; bool:\n    \"\"\"Check if there are tool calls awaiting approval.\"\"\"\n    return len(self.interruptions) &gt; 0\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.QueueCompleteSentinel","title":"QueueCompleteSentinel","text":"<p>Sentinel used to signal completion when streaming run loop results.</p> Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>class QueueCompleteSentinel:\n    \"\"\"Sentinel used to signal completion when streaming run loop results.\"\"\"\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult","title":"SingleStepResult  <code>dataclass</code>","text":"Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>@dataclass\nclass SingleStepResult:\n    original_input: str | list[TResponseInputItem]\n    \"\"\"The input items i.e. the items before run() was called. May be mutated by handoff input\n    filters.\"\"\"\n\n    model_response: ModelResponse\n    \"\"\"The model response for the current step.\"\"\"\n\n    pre_step_items: list[RunItem]\n    \"\"\"Items generated before the current step.\"\"\"\n\n    new_step_items: list[RunItem]\n    \"\"\"Items generated during this current step.\"\"\"\n\n    next_step: NextStepHandoff | NextStepFinalOutput | NextStepRunAgain | NextStepInterruption\n    \"\"\"The next step to take.\"\"\"\n\n    tool_input_guardrail_results: list[ToolInputGuardrailResult]\n    \"\"\"Tool input guardrail results from this step.\"\"\"\n\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult]\n    \"\"\"Tool output guardrail results from this step.\"\"\"\n\n    session_step_items: list[RunItem] | None = None\n    \"\"\"Full unfiltered items for session history. When set, these are used instead of\n    new_step_items for session saving and generated_items property.\"\"\"\n\n    output_guardrail_results: list[OutputGuardrailResult] = dataclasses.field(default_factory=list)\n    \"\"\"Output guardrail results (populated when a final output is produced).\"\"\"\n\n    processed_response: ProcessedResponse | None = None\n    \"\"\"The processed model response. This is needed for resuming from interruptions.\"\"\"\n\n    @property\n    def generated_items(self) -&gt; list[RunItem]:\n        \"\"\"Items generated during the agent run (i.e. everything generated after\n        `original_input`). Uses session_step_items when available for full observability.\"\"\"\n        items = (\n            self.session_step_items if self.session_step_items is not None else self.new_step_items\n        )\n        return self.pre_step_items + items\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.original_input","title":"original_input  <code>instance-attribute</code>","text":"<pre><code>original_input: str | list[TResponseInputItem]\n</code></pre> <p>The input items i.e. the items before run() was called. May be mutated by handoff input filters.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.model_response","title":"model_response  <code>instance-attribute</code>","text":"<pre><code>model_response: ModelResponse\n</code></pre> <p>The model response for the current step.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.pre_step_items","title":"pre_step_items  <code>instance-attribute</code>","text":"<pre><code>pre_step_items: list[RunItem]\n</code></pre> <p>Items generated before the current step.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.new_step_items","title":"new_step_items  <code>instance-attribute</code>","text":"<pre><code>new_step_items: list[RunItem]\n</code></pre> <p>Items generated during this current step.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.next_step","title":"next_step  <code>instance-attribute</code>","text":"<pre><code>next_step: (\n    NextStepHandoff\n    | NextStepFinalOutput\n    | NextStepRunAgain\n    | NextStepInterruption\n)\n</code></pre> <p>The next step to take.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.tool_input_guardrail_results","title":"tool_input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_input_guardrail_results: list[ToolInputGuardrailResult]\n</code></pre> <p>Tool input guardrail results from this step.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.tool_output_guardrail_results","title":"tool_output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_output_guardrail_results: list[\n    ToolOutputGuardrailResult\n]\n</code></pre> <p>Tool output guardrail results from this step.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.session_step_items","title":"session_step_items  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_step_items: list[RunItem] | None = None\n</code></pre> <p>Full unfiltered items for session history. When set, these are used instead of new_step_items for session saving and generated_items property.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.output_guardrail_results","title":"output_guardrail_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult] = (\n    field(default_factory=list)\n)\n</code></pre> <p>Output guardrail results (populated when a final output is produced).</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.processed_response","title":"processed_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>processed_response: ProcessedResponse | None = None\n</code></pre> <p>The processed model response. This is needed for resuming from interruptions.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.SingleStepResult.generated_items","title":"generated_items  <code>property</code>","text":"<pre><code>generated_items: list[RunItem]\n</code></pre> <p>Items generated during the agent run (i.e. everything generated after <code>original_input</code>). Uses session_step_items when available for full observability.</p>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ApplyPatchAction","title":"ApplyPatchAction","text":"<p>Execute apply_patch operations with approvals and editor integration.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class ApplyPatchAction:\n    \"\"\"Execute apply_patch operations with approvals and editor integration.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        call: ToolRunApplyPatchCall,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n    ) -&gt; RunItem:\n        \"\"\"Run an apply_patch call and serialize the editor result for the model.\"\"\"\n        apply_patch_tool: ApplyPatchTool = call.apply_patch_tool\n        agent_hooks = agent.hooks\n        operation = coerce_apply_patch_operation(\n            call.tool_call,\n            context_wrapper=context_wrapper,\n        )\n\n        call_id = extract_apply_patch_call_id(call.tool_call)\n\n        needs_approval_result = await evaluate_needs_approval_setting(\n            apply_patch_tool.needs_approval, context_wrapper, operation, call_id\n        )\n\n        if needs_approval_result:\n            approval_status, approval_item = await resolve_approval_status(\n                tool_name=apply_patch_tool.name,\n                call_id=call_id,\n                raw_item=call.tool_call,\n                agent=agent,\n                context_wrapper=context_wrapper,\n                on_approval=apply_patch_tool.on_approval,\n            )\n\n            if approval_status is False:\n                rejection_message = await resolve_approval_rejection_message(\n                    context_wrapper=context_wrapper,\n                    run_config=config,\n                    tool_type=\"apply_patch\",\n                    tool_name=apply_patch_tool.name,\n                    call_id=call_id,\n                )\n                return apply_patch_rejection_item(\n                    agent,\n                    call_id,\n                    rejection_message=rejection_message,\n                )\n\n            if approval_status is not True:\n                return approval_item\n\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, apply_patch_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, apply_patch_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        status: Literal[\"completed\", \"failed\"] = \"completed\"\n        output_text = \"\"\n\n        try:\n            editor = apply_patch_tool.editor\n            if operation.type == \"create_file\":\n                result = editor.create_file(operation)\n            elif operation.type == \"update_file\":\n                result = editor.update_file(operation)\n            elif operation.type == \"delete_file\":\n                result = editor.delete_file(operation)\n            else:  # pragma: no cover - validated in coerce_apply_patch_operation\n                raise ModelBehaviorError(f\"Unsupported apply_patch operation: {operation.type}\")\n\n            awaited = await result if inspect.isawaitable(result) else result\n            normalized = normalize_apply_patch_result(awaited)\n            if normalized:\n                if normalized.status in {\"completed\", \"failed\"}:\n                    status = normalized.status\n                if normalized.output:\n                    output_text = normalized.output\n        except Exception as exc:\n            status = \"failed\"\n            output_text = format_shell_error(exc)\n            logger.error(\"Apply patch editor failed: %s\", exc, exc_info=True)\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        raw_item: dict[str, Any] = {\n            \"type\": \"apply_patch_call_output\",\n            \"call_id\": call_id,\n            \"status\": status,\n        }\n        if output_text:\n            raw_item[\"output\"] = output_text\n\n        return ToolCallOutputItem(\n            agent=agent,\n            output=output_text,\n            raw_item=raw_item,\n        )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ApplyPatchAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    call: ToolRunApplyPatchCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem\n</code></pre> <p>Run an apply_patch call and serialize the editor result for the model.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    call: ToolRunApplyPatchCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem:\n    \"\"\"Run an apply_patch call and serialize the editor result for the model.\"\"\"\n    apply_patch_tool: ApplyPatchTool = call.apply_patch_tool\n    agent_hooks = agent.hooks\n    operation = coerce_apply_patch_operation(\n        call.tool_call,\n        context_wrapper=context_wrapper,\n    )\n\n    call_id = extract_apply_patch_call_id(call.tool_call)\n\n    needs_approval_result = await evaluate_needs_approval_setting(\n        apply_patch_tool.needs_approval, context_wrapper, operation, call_id\n    )\n\n    if needs_approval_result:\n        approval_status, approval_item = await resolve_approval_status(\n            tool_name=apply_patch_tool.name,\n            call_id=call_id,\n            raw_item=call.tool_call,\n            agent=agent,\n            context_wrapper=context_wrapper,\n            on_approval=apply_patch_tool.on_approval,\n        )\n\n        if approval_status is False:\n            rejection_message = await resolve_approval_rejection_message(\n                context_wrapper=context_wrapper,\n                run_config=config,\n                tool_type=\"apply_patch\",\n                tool_name=apply_patch_tool.name,\n                call_id=call_id,\n            )\n            return apply_patch_rejection_item(\n                agent,\n                call_id,\n                rejection_message=rejection_message,\n            )\n\n        if approval_status is not True:\n            return approval_item\n\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, apply_patch_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, apply_patch_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    status: Literal[\"completed\", \"failed\"] = \"completed\"\n    output_text = \"\"\n\n    try:\n        editor = apply_patch_tool.editor\n        if operation.type == \"create_file\":\n            result = editor.create_file(operation)\n        elif operation.type == \"update_file\":\n            result = editor.update_file(operation)\n        elif operation.type == \"delete_file\":\n            result = editor.delete_file(operation)\n        else:  # pragma: no cover - validated in coerce_apply_patch_operation\n            raise ModelBehaviorError(f\"Unsupported apply_patch operation: {operation.type}\")\n\n        awaited = await result if inspect.isawaitable(result) else result\n        normalized = normalize_apply_patch_result(awaited)\n        if normalized:\n            if normalized.status in {\"completed\", \"failed\"}:\n                status = normalized.status\n            if normalized.output:\n                output_text = normalized.output\n    except Exception as exc:\n        status = \"failed\"\n        output_text = format_shell_error(exc)\n        logger.error(\"Apply patch editor failed: %s\", exc, exc_info=True)\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    raw_item: dict[str, Any] = {\n        \"type\": \"apply_patch_call_output\",\n        \"call_id\": call_id,\n        \"status\": status,\n    }\n    if output_text:\n        raw_item[\"output\"] = output_text\n\n    return ToolCallOutputItem(\n        agent=agent,\n        output=output_text,\n        raw_item=raw_item,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ComputerAction","title":"ComputerAction","text":"<p>Execute computer tool actions and emit screenshot outputs with hooks fired.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class ComputerAction:\n    \"\"\"Execute computer tool actions and emit screenshot outputs with hooks fired.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        action: ToolRunComputerAction,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n        acknowledged_safety_checks: list[ComputerCallOutputAcknowledgedSafetyCheck] | None = None,\n    ) -&gt; RunItem:\n        \"\"\"Run a computer action, capturing a screenshot and notifying hooks.\"\"\"\n        computer = await resolve_computer(tool=action.computer_tool, run_context=context_wrapper)\n        agent_hooks = agent.hooks\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, action.computer_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, action.computer_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        output = await cls._execute_action_and_capture(computer, action.tool_call)\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        image_url = f\"data:image/png;base64,{output}\"\n        return ToolCallOutputItem(\n            agent=agent,\n            output=image_url,\n            raw_item=ComputerCallOutput(\n                call_id=action.tool_call.call_id,\n                output={\n                    \"type\": \"computer_screenshot\",\n                    \"image_url\": image_url,\n                },\n                type=\"computer_call_output\",\n                acknowledged_safety_checks=acknowledged_safety_checks,\n            ),\n        )\n\n    @classmethod\n    async def _execute_action_and_capture(\n        cls, computer: Any, tool_call: ResponseComputerToolCall\n    ) -&gt; str:\n        \"\"\"Execute the computer action (sync or async drivers) and return the screenshot.\"\"\"\n\n        async def maybe_call(method_name: str, *args: Any) -&gt; Any:\n            method = getattr(computer, method_name, None)\n            if method is None or not callable(method):\n                raise ModelBehaviorError(f\"Computer driver missing method {method_name}\")\n            result = method(*args)\n            return await result if inspect.isawaitable(result) else result\n\n        action = tool_call.action\n        if isinstance(action, ActionClick):\n            await maybe_call(\"click\", action.x, action.y, action.button)\n        elif isinstance(action, ActionDoubleClick):\n            await maybe_call(\"double_click\", action.x, action.y)\n        elif isinstance(action, ActionDrag):\n            await maybe_call(\"drag\", [(p.x, p.y) for p in action.path])\n        elif isinstance(action, ActionKeypress):\n            await maybe_call(\"keypress\", action.keys)\n        elif isinstance(action, ActionMove):\n            await maybe_call(\"move\", action.x, action.y)\n        elif isinstance(action, ActionScreenshot):\n            await maybe_call(\"screenshot\")\n        elif isinstance(action, ActionScroll):\n            await maybe_call(\"scroll\", action.x, action.y, action.scroll_x, action.scroll_y)\n        elif isinstance(action, ActionType):\n            await maybe_call(\"type\", action.text)\n        elif isinstance(action, ActionWait):\n            await maybe_call(\"wait\")\n\n        screenshot_result = await maybe_call(\"screenshot\")\n        return cast(str, screenshot_result)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ComputerAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    action: ToolRunComputerAction,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n    acknowledged_safety_checks: list[\n        ComputerCallOutputAcknowledgedSafetyCheck\n    ]\n    | None = None,\n) -&gt; RunItem\n</code></pre> <p>Run a computer action, capturing a screenshot and notifying hooks.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    action: ToolRunComputerAction,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n    acknowledged_safety_checks: list[ComputerCallOutputAcknowledgedSafetyCheck] | None = None,\n) -&gt; RunItem:\n    \"\"\"Run a computer action, capturing a screenshot and notifying hooks.\"\"\"\n    computer = await resolve_computer(tool=action.computer_tool, run_context=context_wrapper)\n    agent_hooks = agent.hooks\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, action.computer_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, action.computer_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    output = await cls._execute_action_and_capture(computer, action.tool_call)\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    image_url = f\"data:image/png;base64,{output}\"\n    return ToolCallOutputItem(\n        agent=agent,\n        output=image_url,\n        raw_item=ComputerCallOutput(\n            call_id=action.tool_call.call_id,\n            output={\n                \"type\": \"computer_screenshot\",\n                \"image_url\": image_url,\n            },\n            type=\"computer_call_output\",\n            acknowledged_safety_checks=acknowledged_safety_checks,\n        ),\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.LocalShellAction","title":"LocalShellAction","text":"<p>Execute local shell commands via the LocalShellTool with lifecycle hooks.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class LocalShellAction:\n    \"\"\"Execute local shell commands via the LocalShellTool with lifecycle hooks.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        call: ToolRunLocalShellCall,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n    ) -&gt; RunItem:\n        \"\"\"Run a local shell tool call and wrap the result as a ToolCallOutputItem.\"\"\"\n        agent_hooks = agent.hooks\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        request = LocalShellCommandRequest(\n            ctx_wrapper=context_wrapper,\n            data=call.tool_call,\n        )\n        output = call.local_shell_tool.executor(request)\n        result = await output if inspect.isawaitable(output) else output\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        raw_payload: dict[str, Any] = {\n            \"type\": \"local_shell_call_output\",\n            \"call_id\": call.tool_call.call_id,\n            \"output\": result,\n        }\n        return ToolCallOutputItem(\n            agent=agent,\n            output=result,\n            raw_item=raw_payload,\n        )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.LocalShellAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    call: ToolRunLocalShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem\n</code></pre> <p>Run a local shell tool call and wrap the result as a ToolCallOutputItem.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    call: ToolRunLocalShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem:\n    \"\"\"Run a local shell tool call and wrap the result as a ToolCallOutputItem.\"\"\"\n    agent_hooks = agent.hooks\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    request = LocalShellCommandRequest(\n        ctx_wrapper=context_wrapper,\n        data=call.tool_call,\n    )\n    output = call.local_shell_tool.executor(request)\n    result = await output if inspect.isawaitable(output) else output\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    raw_payload: dict[str, Any] = {\n        \"type\": \"local_shell_call_output\",\n        \"call_id\": call.tool_call.call_id,\n        \"output\": result,\n    }\n    return ToolCallOutputItem(\n        agent=agent,\n        output=result,\n        raw_item=raw_payload,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ShellAction","title":"ShellAction","text":"<p>Execute shell calls, handling approvals and normalizing outputs.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class ShellAction:\n    \"\"\"Execute shell calls, handling approvals and normalizing outputs.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        call: ToolRunShellCall,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n    ) -&gt; RunItem:\n        \"\"\"Run a shell tool call and return a normalized ToolCallOutputItem.\"\"\"\n        shell_call = coerce_shell_call(call.tool_call)\n        shell_tool = call.shell_tool\n        agent_hooks = agent.hooks\n\n        needs_approval_result = await evaluate_needs_approval_setting(\n            shell_tool.needs_approval, context_wrapper, shell_call.action, shell_call.call_id\n        )\n\n        if needs_approval_result:\n            approval_status, approval_item = await resolve_approval_status(\n                tool_name=shell_tool.name,\n                call_id=shell_call.call_id,\n                raw_item=call.tool_call,\n                agent=agent,\n                context_wrapper=context_wrapper,\n                on_approval=shell_tool.on_approval,\n            )\n\n            if approval_status is False:\n                rejection_message = await resolve_approval_rejection_message(\n                    context_wrapper=context_wrapper,\n                    run_config=config,\n                    tool_type=\"shell\",\n                    tool_name=shell_tool.name,\n                    call_id=shell_call.call_id,\n                )\n                return shell_rejection_item(\n                    agent,\n                    shell_call.call_id,\n                    rejection_message=rejection_message,\n                )\n\n            if approval_status is not True:\n                return approval_item\n\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, shell_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, shell_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n        request = ShellCommandRequest(ctx_wrapper=context_wrapper, data=shell_call)\n        status: Literal[\"completed\", \"failed\"] = \"completed\"\n        output_text = \"\"\n        shell_output_payload: list[dict[str, Any]] | None = None\n        provider_meta: dict[str, Any] | None = None\n        max_output_length: int | None = None\n        requested_max_output_length = normalize_max_output_length(\n            shell_call.action.max_output_length\n        )\n\n        try:\n            executor_result = call.shell_tool.executor(request)\n            result = (\n                await executor_result if inspect.isawaitable(executor_result) else executor_result\n            )\n\n            if isinstance(result, ShellResult):\n                normalized = [normalize_shell_output(entry) for entry in result.output]\n                result_max_output_length = normalize_max_output_length(result.max_output_length)\n                if result_max_output_length is None:\n                    max_output_length = requested_max_output_length\n                elif requested_max_output_length is None:\n                    max_output_length = result_max_output_length\n                else:\n                    max_output_length = min(result_max_output_length, requested_max_output_length)\n                if max_output_length is not None:\n                    normalized = truncate_shell_outputs(normalized, max_output_length)\n                output_text = render_shell_outputs(normalized)\n                if max_output_length is not None:\n                    output_text = output_text[:max_output_length]\n                shell_output_payload = [serialize_shell_output(entry) for entry in normalized]\n                provider_meta = dict(result.provider_data or {})\n            else:\n                output_text = str(result)\n                if requested_max_output_length is not None:\n                    max_output_length = requested_max_output_length\n                    output_text = output_text[:max_output_length]\n        except Exception as exc:\n            status = \"failed\"\n            output_text = format_shell_error(exc)\n            if requested_max_output_length is not None:\n                max_output_length = requested_max_output_length\n                output_text = output_text[:max_output_length]\n            logger.error(\"Shell executor failed: %s\", exc, exc_info=True)\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        raw_entries: list[dict[str, Any]] | None = None\n        if shell_output_payload:\n            raw_entries = shell_output_payload\n        elif output_text:\n            raw_entries = [\n                {\n                    \"stdout\": output_text,\n                    \"stderr\": \"\",\n                    \"status\": status,\n                    \"outcome\": \"success\" if status == \"completed\" else \"failure\",\n                }\n            ]\n\n        structured_output = normalize_shell_output_entries(raw_entries) if raw_entries else []\n\n        raw_item: dict[str, Any] = {\n            \"type\": \"shell_call_output\",\n            \"call_id\": shell_call.call_id,\n            \"output\": structured_output,\n            \"status\": status,\n        }\n        if max_output_length is not None:\n            raw_item[\"max_output_length\"] = max_output_length\n        if raw_entries:\n            raw_item[\"shell_output\"] = raw_entries\n        if provider_meta:\n            raw_item[\"provider_data\"] = provider_meta\n\n        return ToolCallOutputItem(\n            agent=agent,\n            output=output_text,\n            raw_item=raw_item,\n        )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.ShellAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    call: ToolRunShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem\n</code></pre> <p>Run a shell tool call and return a normalized ToolCallOutputItem.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    call: ToolRunShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem:\n    \"\"\"Run a shell tool call and return a normalized ToolCallOutputItem.\"\"\"\n    shell_call = coerce_shell_call(call.tool_call)\n    shell_tool = call.shell_tool\n    agent_hooks = agent.hooks\n\n    needs_approval_result = await evaluate_needs_approval_setting(\n        shell_tool.needs_approval, context_wrapper, shell_call.action, shell_call.call_id\n    )\n\n    if needs_approval_result:\n        approval_status, approval_item = await resolve_approval_status(\n            tool_name=shell_tool.name,\n            call_id=shell_call.call_id,\n            raw_item=call.tool_call,\n            agent=agent,\n            context_wrapper=context_wrapper,\n            on_approval=shell_tool.on_approval,\n        )\n\n        if approval_status is False:\n            rejection_message = await resolve_approval_rejection_message(\n                context_wrapper=context_wrapper,\n                run_config=config,\n                tool_type=\"shell\",\n                tool_name=shell_tool.name,\n                call_id=shell_call.call_id,\n            )\n            return shell_rejection_item(\n                agent,\n                shell_call.call_id,\n                rejection_message=rejection_message,\n            )\n\n        if approval_status is not True:\n            return approval_item\n\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, shell_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, shell_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n    request = ShellCommandRequest(ctx_wrapper=context_wrapper, data=shell_call)\n    status: Literal[\"completed\", \"failed\"] = \"completed\"\n    output_text = \"\"\n    shell_output_payload: list[dict[str, Any]] | None = None\n    provider_meta: dict[str, Any] | None = None\n    max_output_length: int | None = None\n    requested_max_output_length = normalize_max_output_length(\n        shell_call.action.max_output_length\n    )\n\n    try:\n        executor_result = call.shell_tool.executor(request)\n        result = (\n            await executor_result if inspect.isawaitable(executor_result) else executor_result\n        )\n\n        if isinstance(result, ShellResult):\n            normalized = [normalize_shell_output(entry) for entry in result.output]\n            result_max_output_length = normalize_max_output_length(result.max_output_length)\n            if result_max_output_length is None:\n                max_output_length = requested_max_output_length\n            elif requested_max_output_length is None:\n                max_output_length = result_max_output_length\n            else:\n                max_output_length = min(result_max_output_length, requested_max_output_length)\n            if max_output_length is not None:\n                normalized = truncate_shell_outputs(normalized, max_output_length)\n            output_text = render_shell_outputs(normalized)\n            if max_output_length is not None:\n                output_text = output_text[:max_output_length]\n            shell_output_payload = [serialize_shell_output(entry) for entry in normalized]\n            provider_meta = dict(result.provider_data or {})\n        else:\n            output_text = str(result)\n            if requested_max_output_length is not None:\n                max_output_length = requested_max_output_length\n                output_text = output_text[:max_output_length]\n    except Exception as exc:\n        status = \"failed\"\n        output_text = format_shell_error(exc)\n        if requested_max_output_length is not None:\n            max_output_length = requested_max_output_length\n            output_text = output_text[:max_output_length]\n        logger.error(\"Shell executor failed: %s\", exc, exc_info=True)\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    raw_entries: list[dict[str, Any]] | None = None\n    if shell_output_payload:\n        raw_entries = shell_output_payload\n    elif output_text:\n        raw_entries = [\n            {\n                \"stdout\": output_text,\n                \"stderr\": \"\",\n                \"status\": status,\n                \"outcome\": \"success\" if status == \"completed\" else \"failure\",\n            }\n        ]\n\n    structured_output = normalize_shell_output_entries(raw_entries) if raw_entries else []\n\n    raw_item: dict[str, Any] = {\n        \"type\": \"shell_call_output\",\n        \"call_id\": shell_call.call_id,\n        \"output\": structured_output,\n        \"status\": status,\n    }\n    if max_output_length is not None:\n        raw_item[\"max_output_length\"] = max_output_length\n    if raw_entries:\n        raw_item[\"shell_output\"] = raw_entries\n    if provider_meta:\n        raw_item[\"provider_data\"] = provider_meta\n\n    return ToolCallOutputItem(\n        agent=agent,\n        output=output_text,\n        raw_item=raw_item,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.AgentToolUseTracker","title":"AgentToolUseTracker","text":"<p>Track which tools an agent has used to support model_settings resets.</p> Source code in <code>src/agents/run_internal/tool_use_tracker.py</code> <pre><code>class AgentToolUseTracker:\n    \"\"\"Track which tools an agent has used to support model_settings resets.\"\"\"\n\n    def __init__(self) -&gt; None:\n        # Name-keyed map is used for serialization/hydration only.\n        self.agent_map: dict[str, set[str]] = {}\n        # Instance-keyed list is used for runtime checks.\n        self.agent_to_tools: list[tuple[Agent[Any], list[str]]] = []\n\n    def record_used_tools(self, agent: Agent[Any], tools: list[ToolRunFunction]) -&gt; None:\n        tool_names = [tool.function_tool.name for tool in tools]\n        self.add_tool_use(agent, tool_names)\n\n    def add_tool_use(self, agent: Agent[Any], tool_names: list[str]) -&gt; None:\n        \"\"\"Maintain compatibility for callers that append tool usage directly.\"\"\"\n        agent_name = getattr(agent, \"name\", agent.__class__.__name__)\n        names_set = self.agent_map.setdefault(agent_name, set())\n        names_set.update(tool_names)\n\n        existing = next((item for item in self.agent_to_tools if item[0] is agent), None)\n        if existing:\n            existing[1].extend(tool_names)\n        else:\n            self.agent_to_tools.append((agent, list(tool_names)))\n\n    def has_used_tools(self, agent: Agent[Any]) -&gt; bool:\n        existing = next((item for item in self.agent_to_tools if item[0] is agent), None)\n        return bool(existing and existing[1])\n\n    def as_serializable(self) -&gt; dict[str, list[str]]:\n        if self.agent_map:\n            return {name: sorted(tool_names) for name, tool_names in self.agent_map.items()}\n\n        snapshot: dict[str, set[str]] = {}\n        for agent, names in self.agent_to_tools:\n            agent_name = getattr(agent, \"name\", agent.__class__.__name__)\n            snapshot.setdefault(agent_name, set()).update(names)\n        return {name: sorted(tool_names) for name, tool_names in snapshot.items()}\n\n    @classmethod\n    def from_serializable(cls, data: dict[str, list[str]]) -&gt; AgentToolUseTracker:\n        tracker = cls()\n        tracker.agent_map = {name: set(tools) for name, tools in data.items()}\n        return tracker\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.AgentToolUseTracker.add_tool_use","title":"add_tool_use","text":"<pre><code>add_tool_use(\n    agent: Agent[Any], tool_names: list[str]\n) -&gt; None\n</code></pre> <p>Maintain compatibility for callers that append tool usage directly.</p> Source code in <code>src/agents/run_internal/tool_use_tracker.py</code> <pre><code>def add_tool_use(self, agent: Agent[Any], tool_names: list[str]) -&gt; None:\n    \"\"\"Maintain compatibility for callers that append tool usage directly.\"\"\"\n    agent_name = getattr(agent, \"name\", agent.__class__.__name__)\n    names_set = self.agent_map.setdefault(agent_name, set())\n    names_set.update(tool_names)\n\n    existing = next((item for item in self.agent_to_tools if item[0] is agent), None)\n    if existing:\n        existing[1].extend(tool_names)\n    else:\n        self.agent_to_tools.append((agent, list(tool_names)))\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.get_model_tracing_impl","title":"get_model_tracing_impl","text":"<pre><code>get_model_tracing_impl(\n    tracing_disabled: bool,\n    trace_include_sensitive_data: bool,\n) -&gt; ModelTracing\n</code></pre> <p>Return the ModelTracing setting based on run-level tracing configuration.</p> Source code in <code>src/agents/tracing/model_tracing.py</code> <pre><code>def get_model_tracing_impl(\n    tracing_disabled: bool, trace_include_sensitive_data: bool\n) -&gt; ModelTracing:\n    \"\"\"Return the ModelTracing setting based on run-level tracing configuration.\"\"\"\n    if tracing_disabled:\n        return ModelTracing.DISABLED\n    if trace_include_sensitive_data:\n        return ModelTracing.ENABLED\n    return ModelTracing.ENABLED_WITHOUT_DATA\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.input_guardrail_tripwire_triggered_for_stream","title":"input_guardrail_tripwire_triggered_for_stream  <code>async</code>","text":"<pre><code>input_guardrail_tripwire_triggered_for_stream(\n    streamed_result: RunResultStreaming,\n) -&gt; bool\n</code></pre> <p>Return True if any input guardrail triggered during a streamed run.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def input_guardrail_tripwire_triggered_for_stream(\n    streamed_result: RunResultStreaming,\n) -&gt; bool:\n    \"\"\"Return True if any input guardrail triggered during a streamed run.\"\"\"\n    task = streamed_result._input_guardrails_task\n    if task is None:\n        return False\n\n    if not task.done():\n        await task\n\n    return any(\n        guardrail_result.output.tripwire_triggered\n        for guardrail_result in streamed_result.input_guardrail_results\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.run_input_guardrails","title":"run_input_guardrails  <code>async</code>","text":"<pre><code>run_input_guardrails(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n) -&gt; list[InputGuardrailResult]\n</code></pre> <p>Run input guardrails concurrently and raise on tripwires.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def run_input_guardrails(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n) -&gt; list[InputGuardrailResult]:\n    \"\"\"Run input guardrails concurrently and raise on tripwires.\"\"\"\n    if not guardrails:\n        return []\n\n    guardrail_tasks = [\n        asyncio.create_task(run_single_input_guardrail(agent, guardrail, input, context))\n        for guardrail in guardrails\n    ]\n\n    guardrail_results: list[InputGuardrailResult] = []\n\n    for done in asyncio.as_completed(guardrail_tasks):\n        result = await done\n        if result.output.tripwire_triggered:\n            for t in guardrail_tasks:\n                t.cancel()\n            await asyncio.gather(*guardrail_tasks, return_exceptions=True)\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Guardrail tripwire triggered\",\n                    data={\"guardrail\": result.guardrail.get_name()},\n                )\n            )\n            raise InputGuardrailTripwireTriggered(result)\n        guardrail_results.append(result)\n\n    return guardrail_results\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.run_input_guardrails_with_queue","title":"run_input_guardrails_with_queue  <code>async</code>","text":"<pre><code>run_input_guardrails_with_queue(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n    streamed_result: RunResultStreaming,\n    parent_span: Span[Any],\n) -&gt; None\n</code></pre> <p>Run guardrails concurrently and stream results into the queue.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def run_input_guardrails_with_queue(\n    agent: Agent[Any],\n    guardrails: list[InputGuardrail[TContext]],\n    input: str | list[TResponseInputItem],\n    context: RunContextWrapper[TContext],\n    streamed_result: RunResultStreaming,\n    parent_span: Span[Any],\n) -&gt; None:\n    \"\"\"Run guardrails concurrently and stream results into the queue.\"\"\"\n    queue = streamed_result._input_guardrail_queue\n\n    guardrail_tasks = [\n        asyncio.create_task(run_single_input_guardrail(agent, guardrail, input, context))\n        for guardrail in guardrails\n    ]\n    guardrail_results = []\n    try:\n        for done in asyncio.as_completed(guardrail_tasks):\n            result = await done\n            if result.output.tripwire_triggered:\n                for t in guardrail_tasks:\n                    t.cancel()\n                await asyncio.gather(*guardrail_tasks, return_exceptions=True)\n                _error_tracing.attach_error_to_span(\n                    parent_span,\n                    SpanError(\n                        message=\"Guardrail tripwire triggered\",\n                        data={\n                            \"guardrail\": result.guardrail.get_name(),\n                            \"type\": \"input_guardrail\",\n                        },\n                    ),\n                )\n                queue.put_nowait(result)\n                guardrail_results.append(result)\n                break\n            queue.put_nowait(result)\n            guardrail_results.append(result)\n    except Exception:\n        for t in guardrail_tasks:\n            t.cancel()\n        raise\n\n    streamed_result.input_guardrail_results = (\n        streamed_result.input_guardrail_results + guardrail_results\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.run_output_guardrails","title":"run_output_guardrails  <code>async</code>","text":"<pre><code>run_output_guardrails(\n    guardrails: list[OutputGuardrail[TContext]],\n    agent: Agent[TContext],\n    agent_output: Any,\n    context: RunContextWrapper[TContext],\n) -&gt; list[OutputGuardrailResult]\n</code></pre> <p>Run output guardrails in parallel and raise on tripwires.</p> Source code in <code>src/agents/run_internal/guardrails.py</code> <pre><code>async def run_output_guardrails(\n    guardrails: list[OutputGuardrail[TContext]],\n    agent: Agent[TContext],\n    agent_output: Any,\n    context: RunContextWrapper[TContext],\n) -&gt; list[OutputGuardrailResult]:\n    \"\"\"Run output guardrails in parallel and raise on tripwires.\"\"\"\n    if not guardrails:\n        return []\n\n    guardrail_tasks = [\n        asyncio.create_task(run_single_output_guardrail(guardrail, agent, agent_output, context))\n        for guardrail in guardrails\n    ]\n\n    guardrail_results: list[OutputGuardrailResult] = []\n\n    for done in asyncio.as_completed(guardrail_tasks):\n        result = await done\n        if result.output.tripwire_triggered:\n            for t in guardrail_tasks:\n                t.cancel()\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Guardrail tripwire triggered\",\n                    data={\"guardrail\": result.guardrail.get_name()},\n                )\n            )\n            raise OutputGuardrailTripwireTriggered(result)\n        guardrail_results.append(result)\n\n    return guardrail_results\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.stream_step_items_to_queue","title":"stream_step_items_to_queue","text":"<pre><code>stream_step_items_to_queue(\n    new_step_items: list[RunItem],\n    queue: Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None\n</code></pre> <p>Emit run items as streaming events, skipping approval placeholders.</p> Source code in <code>src/agents/run_internal/streaming.py</code> <pre><code>def stream_step_items_to_queue(\n    new_step_items: list[RunItem],\n    queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None:\n    \"\"\"Emit run items as streaming events, skipping approval placeholders.\"\"\"\n    for item in new_step_items:\n        if isinstance(item, MessageOutputItem):\n            event = RunItemStreamEvent(item=item, name=\"message_output_created\")\n        elif isinstance(item, HandoffCallItem):\n            event = RunItemStreamEvent(item=item, name=\"handoff_requested\")\n        elif isinstance(item, HandoffOutputItem):\n            event = RunItemStreamEvent(item=item, name=\"handoff_occured\")\n        elif isinstance(item, ToolCallItem):\n            event = RunItemStreamEvent(item=item, name=\"tool_called\")\n        elif isinstance(item, ToolCallOutputItem):\n            event = RunItemStreamEvent(item=item, name=\"tool_output\")\n        elif isinstance(item, ReasoningItem):\n            event = RunItemStreamEvent(item=item, name=\"reasoning_item_created\")\n        elif isinstance(item, MCPApprovalRequestItem):\n            event = RunItemStreamEvent(item=item, name=\"mcp_approval_requested\")\n        elif isinstance(item, MCPApprovalResponseItem):\n            event = RunItemStreamEvent(item=item, name=\"mcp_approval_response\")\n        elif isinstance(item, MCPListToolsItem):\n            event = RunItemStreamEvent(item=item, name=\"mcp_list_tools\")\n        elif isinstance(item, ToolApprovalItem):\n            event = None  # approvals represent interruptions, not streamed items\n        else:\n            logger.warning(\"Unexpected item type: %s\", type(item))\n            event = None\n\n        if event:\n            queue.put_nowait(event)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.stream_step_result_to_queue","title":"stream_step_result_to_queue","text":"<pre><code>stream_step_result_to_queue(\n    step_result,\n    queue: Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None\n</code></pre> <p>Emit all new items in a step result to the event queue.</p> Source code in <code>src/agents/run_internal/streaming.py</code> <pre><code>def stream_step_result_to_queue(\n    step_result,  # SingleStepResult (kept untyped to avoid circular imports)\n    queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None:\n    \"\"\"Emit all new items in a step result to the event queue.\"\"\"\n    stream_step_items_to_queue(step_result.new_step_items, queue)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.coerce_shell_call","title":"coerce_shell_call","text":"<pre><code>coerce_shell_call(tool_call: Any) -&gt; ShellCallData\n</code></pre> <p>Normalize a shell call payload into ShellCallData for consistent execution.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def coerce_shell_call(tool_call: Any) -&gt; ShellCallData:\n    \"\"\"Normalize a shell call payload into ShellCallData for consistent execution.\"\"\"\n    call_id = extract_shell_call_id(tool_call)\n    action_payload = get_mapping_or_attr(tool_call, \"action\")\n    if action_payload is None:\n        raise ModelBehaviorError(\"Shell call is missing an action payload.\")\n\n    commands_value = get_mapping_or_attr(action_payload, \"commands\")\n    if not isinstance(commands_value, Sequence):\n        raise ModelBehaviorError(\"Shell call action is missing commands.\")\n    commands: list[str] = []\n    for entry in commands_value:\n        if entry is None:\n            continue\n        commands.append(str(entry))\n    if not commands:\n        raise ModelBehaviorError(\"Shell call action must include at least one command.\")\n\n    timeout_value = (\n        get_mapping_or_attr(action_payload, \"timeout_ms\")\n        or get_mapping_or_attr(action_payload, \"timeoutMs\")\n        or get_mapping_or_attr(action_payload, \"timeout\")\n    )\n    timeout_ms = int(timeout_value) if isinstance(timeout_value, (int, float)) else None\n\n    max_length_value = get_mapping_or_attr(action_payload, \"max_output_length\")\n    if max_length_value is None:\n        max_length_value = get_mapping_or_attr(action_payload, \"maxOutputLength\")\n    max_output_length = (\n        int(max_length_value) if isinstance(max_length_value, (int, float)) else None\n    )\n\n    action = ShellActionRequest(\n        commands=commands,\n        timeout_ms=timeout_ms,\n        max_output_length=max_output_length,\n    )\n\n    status_value = get_mapping_or_attr(tool_call, \"status\")\n    status_literal: Literal[\"in_progress\", \"completed\"] | None = None\n    if isinstance(status_value, str):\n        lowered = status_value.lower()\n        if lowered in {\"in_progress\", \"completed\"}:\n            status_literal = cast(Literal[\"in_progress\", \"completed\"], lowered)\n\n    return ShellCallData(call_id=call_id, action=action, status=status_literal, raw=tool_call)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_apply_patch_calls","title":"execute_apply_patch_calls  <code>async</code>","text":"<pre><code>execute_apply_patch_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunApplyPatchCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run apply_patch tool calls serially and normalize outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_apply_patch_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunApplyPatchCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run apply_patch tool calls serially and normalize outputs.\"\"\"\n    from .tool_actions import ApplyPatchAction\n\n    results: list[RunItem] = []\n    for call in calls:\n        results.append(\n            await ApplyPatchAction.execute(\n                agent=agent,\n                call=call,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n            )\n        )\n    return results\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_computer_actions","title":"execute_computer_actions  <code>async</code>","text":"<pre><code>execute_computer_actions(\n    *,\n    agent: Agent[Any],\n    actions: list[ToolRunComputerAction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run computer actions serially and emit screenshot outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_computer_actions(\n    *,\n    agent: Agent[Any],\n    actions: list[ToolRunComputerAction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run computer actions serially and emit screenshot outputs.\"\"\"\n    from .tool_actions import ComputerAction\n\n    results: list[RunItem] = []\n    for action in actions:\n        acknowledged: list[ComputerCallOutputAcknowledgedSafetyCheck] | None = None\n        if action.tool_call.pending_safety_checks and action.computer_tool.on_safety_check:\n            acknowledged = []\n            for check in action.tool_call.pending_safety_checks:\n                data = ComputerToolSafetyCheckData(\n                    ctx_wrapper=context_wrapper,\n                    agent=agent,\n                    tool_call=action.tool_call,\n                    safety_check=check,\n                )\n                maybe = action.computer_tool.on_safety_check(data)\n                ack = await maybe if inspect.isawaitable(maybe) else maybe\n                if ack:\n                    acknowledged.append(\n                        ComputerCallOutputAcknowledgedSafetyCheck(\n                            id=check.id,\n                            code=check.code,\n                            message=check.message,\n                        )\n                    )\n                else:\n                    raise UserError(\"Computer tool safety check was not acknowledged\")\n\n        results.append(\n            await ComputerAction.execute(\n                agent=agent,\n                action=action,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n                acknowledged_safety_checks=acknowledged,\n            )\n        )\n\n    return results\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_function_tool_calls","title":"execute_function_tool_calls  <code>async</code>","text":"<pre><code>execute_function_tool_calls(\n    *,\n    agent: Agent[Any],\n    tool_runs: list[ToolRunFunction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; tuple[\n    list[FunctionToolResult],\n    list[ToolInputGuardrailResult],\n    list[ToolOutputGuardrailResult],\n]\n</code></pre> <p>Execute function tool calls with approvals, guardrails, and hooks.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_function_tool_calls(\n    *,\n    agent: Agent[Any],\n    tool_runs: list[ToolRunFunction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; tuple[\n    list[FunctionToolResult], list[ToolInputGuardrailResult], list[ToolOutputGuardrailResult]\n]:\n    \"\"\"Execute function tool calls with approvals, guardrails, and hooks.\"\"\"\n    tool_input_guardrail_results: list[ToolInputGuardrailResult] = []\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult] = []\n\n    async def run_single_tool(func_tool: FunctionTool, tool_call: ResponseFunctionToolCall) -&gt; Any:\n        with function_span(func_tool.name) as span_fn:\n            tool_context = ToolContext.from_agent_context(\n                context_wrapper,\n                tool_call.call_id,\n                tool_call=tool_call,\n            )\n            agent_hooks = agent.hooks\n            if config.trace_include_sensitive_data:\n                span_fn.span_data.input = tool_call.arguments\n            try:\n                needs_approval_result = await function_needs_approval(\n                    func_tool,\n                    context_wrapper,\n                    tool_call,\n                )\n\n                if needs_approval_result:\n                    approval_status = context_wrapper.get_approval_status(\n                        func_tool.name,\n                        tool_call.call_id,\n                    )\n\n                    if approval_status is None:\n                        approval_item = ToolApprovalItem(\n                            agent=agent, raw_item=tool_call, tool_name=func_tool.name\n                        )\n                        return FunctionToolResult(\n                            tool=func_tool, output=None, run_item=approval_item\n                        )\n\n                    if approval_status is False:\n                        rejection_message = await resolve_approval_rejection_message(\n                            context_wrapper=context_wrapper,\n                            run_config=config,\n                            tool_type=\"function\",\n                            tool_name=func_tool.name,\n                            call_id=tool_call.call_id,\n                        )\n                        span_fn.set_error(\n                            SpanError(\n                                message=rejection_message,\n                                data={\n                                    \"tool_name\": func_tool.name,\n                                    \"error\": (\n                                        f\"Tool execution for {tool_call.call_id} \"\n                                        \"was manually rejected by user.\"\n                                    ),\n                                },\n                            )\n                        )\n                        result = rejection_message\n                        span_fn.span_data.output = result\n                        return FunctionToolResult(\n                            tool=func_tool,\n                            output=result,\n                            run_item=function_rejection_item(\n                                agent,\n                                tool_call,\n                                rejection_message=rejection_message,\n                            ),\n                        )\n\n                rejected_message = await _execute_tool_input_guardrails(\n                    func_tool=func_tool,\n                    tool_context=tool_context,\n                    agent=agent,\n                    tool_input_guardrail_results=tool_input_guardrail_results,\n                )\n\n                if rejected_message is not None:\n                    final_result = rejected_message\n                else:\n                    await asyncio.gather(\n                        hooks.on_tool_start(tool_context, agent, func_tool),\n                        (\n                            agent_hooks.on_tool_start(tool_context, agent, func_tool)\n                            if agent_hooks\n                            else _coro.noop_coroutine()\n                        ),\n                    )\n                    real_result = await func_tool.on_invoke_tool(tool_context, tool_call.arguments)\n\n                    final_result = await _execute_tool_output_guardrails(\n                        func_tool=func_tool,\n                        tool_context=tool_context,\n                        agent=agent,\n                        real_result=real_result,\n                        tool_output_guardrail_results=tool_output_guardrail_results,\n                    )\n\n                    await asyncio.gather(\n                        hooks.on_tool_end(tool_context, agent, func_tool, final_result),\n                        (\n                            agent_hooks.on_tool_end(tool_context, agent, func_tool, final_result)\n                            if agent_hooks\n                            else _coro.noop_coroutine()\n                        ),\n                    )\n                result = final_result\n            except Exception as e:\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Error running tool\",\n                        data={\"tool_name\": func_tool.name, \"error\": str(e)},\n                    )\n                )\n                if isinstance(e, AgentsException):\n                    raise e\n                raise UserError(f\"Error running tool {func_tool.name}: {e}\") from e\n\n            if config.trace_include_sensitive_data:\n                span_fn.span_data.output = result\n        return result\n\n    tasks = []\n    for tool_run in tool_runs:\n        function_tool = tool_run.function_tool\n        tasks.append(run_single_tool(function_tool, tool_run.tool_call))\n\n    results = await asyncio.gather(*tasks)\n\n    function_tool_results = []\n    for tool_run, result in zip(tool_runs, results):\n        if isinstance(result, FunctionToolResult):\n            nested_run_result = consume_agent_tool_run_result(tool_run.tool_call)\n            if nested_run_result:\n                result.agent_run_result = nested_run_result\n                nested_interruptions_from_result: list[ToolApprovalItem] = (\n                    nested_run_result.interruptions\n                    if hasattr(nested_run_result, \"interruptions\")\n                    else []\n                )\n                if nested_interruptions_from_result:\n                    result.interruptions = nested_interruptions_from_result\n\n            function_tool_results.append(result)\n        else:\n            nested_run_result = peek_agent_tool_run_result(tool_run.tool_call)\n            nested_interruptions: list[ToolApprovalItem] = []\n            if nested_run_result:\n                nested_interruptions = (\n                    nested_run_result.interruptions\n                    if hasattr(nested_run_result, \"interruptions\")\n                    else []\n                )\n            if nested_run_result and not nested_interruptions:\n                nested_run_result = consume_agent_tool_run_result(tool_run.tool_call)\n            elif nested_run_result is None:\n                nested_run_result = consume_agent_tool_run_result(tool_run.tool_call)\n                if nested_run_result:\n                    nested_interruptions = (\n                        nested_run_result.interruptions\n                        if hasattr(nested_run_result, \"interruptions\")\n                        else []\n                    )\n\n            run_item: RunItem | None = None\n            if not nested_interruptions:\n                run_item = ToolCallOutputItem(\n                    output=result,\n                    raw_item=ItemHelpers.tool_call_output_item(tool_run.tool_call, result),\n                    agent=agent,\n                )\n            else:\n                # Skip tool output until nested interruptions are resolved.\n                run_item = None\n\n            function_tool_results.append(\n                FunctionToolResult(\n                    tool=tool_run.function_tool,\n                    output=result,\n                    run_item=run_item,\n                    interruptions=nested_interruptions,\n                    agent_run_result=nested_run_result,\n                )\n            )\n\n    return function_tool_results, tool_input_guardrail_results, tool_output_guardrail_results\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_local_shell_calls","title":"execute_local_shell_calls  <code>async</code>","text":"<pre><code>execute_local_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunLocalShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run local shell tool calls serially and wrap outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_local_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunLocalShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run local shell tool calls serially and wrap outputs.\"\"\"\n    from .tool_actions import LocalShellAction\n\n    results: list[RunItem] = []\n    for call in calls:\n        results.append(\n            await LocalShellAction.execute(\n                agent=agent,\n                call=call,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n            )\n        )\n    return results\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_shell_calls","title":"execute_shell_calls  <code>async</code>","text":"<pre><code>execute_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run shell tool calls serially and wrap outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run shell tool calls serially and wrap outputs.\"\"\"\n    from .tool_actions import ShellAction\n\n    results: list[RunItem] = []\n    for call in calls:\n        results.append(\n            await ShellAction.execute(\n                agent=agent,\n                call=call,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n            )\n        )\n    return results\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.extract_tool_call_id","title":"extract_tool_call_id","text":"<pre><code>extract_tool_call_id(raw: Any) -&gt; str | None\n</code></pre> <p>Return a call ID from tool call payloads or approval items.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def extract_tool_call_id(raw: Any) -&gt; str | None:\n    \"\"\"Return a call ID from tool call payloads or approval items.\"\"\"\n    # OpenAI tool call payloads are documented to include a call_id/id so outputs can be matched.\n    # See https://platform.openai.com/docs/guides/function-calling\n    # We still guard against missing IDs to avoid hard failures on malformed or non-OpenAI inputs.\n    if isinstance(raw, Mapping):\n        candidate = raw.get(\"call_id\") or raw.get(\"id\")\n        return candidate if isinstance(candidate, str) else None\n    candidate = get_mapping_or_attr(raw, \"call_id\") or get_mapping_or_attr(raw, \"id\")\n    return candidate if isinstance(candidate, str) else None\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.initialize_computer_tools","title":"initialize_computer_tools  <code>async</code>","text":"<pre><code>initialize_computer_tools(\n    *,\n    tools: list[Tool],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; None\n</code></pre> <p>Resolve computer tools ahead of model invocation so each run gets its own instance.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def initialize_computer_tools(\n    *,\n    tools: list[Tool],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; None:\n    \"\"\"Resolve computer tools ahead of model invocation so each run gets its own instance.\"\"\"\n    computer_tools = [tool for tool in tools if isinstance(tool, ComputerTool)]\n    if not computer_tools:\n        return\n\n    await asyncio.gather(\n        *(resolve_computer(tool=tool, run_context=context_wrapper) for tool in computer_tools)\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.maybe_reset_tool_choice","title":"maybe_reset_tool_choice","text":"<pre><code>maybe_reset_tool_choice(\n    agent: Agent[Any],\n    tool_use_tracker: AgentToolUseTracker,\n    model_settings: ModelSettings,\n) -&gt; ModelSettings\n</code></pre> <p>Reset tool_choice if the agent was forced to pick a tool previously and should be reset.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def maybe_reset_tool_choice(\n    agent: Agent[Any],\n    tool_use_tracker: AgentToolUseTracker,\n    model_settings: ModelSettings,\n) -&gt; ModelSettings:\n    \"\"\"Reset tool_choice if the agent was forced to pick a tool previously and should be reset.\"\"\"\n    if agent.reset_tool_choice is True and tool_use_tracker.has_used_tools(agent):\n        return dataclasses.replace(model_settings, tool_choice=None)\n    return model_settings\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.normalize_shell_output","title":"normalize_shell_output","text":"<pre><code>normalize_shell_output(\n    entry: ShellCommandOutput | Mapping[str, Any],\n) -&gt; ShellCommandOutput\n</code></pre> <p>Normalize shell output into ShellCommandOutput so downstream code sees a stable shape.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def normalize_shell_output(entry: ShellCommandOutput | Mapping[str, Any]) -&gt; ShellCommandOutput:\n    \"\"\"Normalize shell output into ShellCommandOutput so downstream code sees a stable shape.\"\"\"\n    if isinstance(entry, ShellCommandOutput):\n        return entry\n\n    stdout = str(entry.get(\"stdout\", \"\") or \"\")\n    stderr = str(entry.get(\"stderr\", \"\") or \"\")\n    command_value = entry.get(\"command\")\n    provider_data_value = entry.get(\"provider_data\")\n    outcome_value = entry.get(\"outcome\")\n\n    outcome_type: Literal[\"exit\", \"timeout\"] = \"exit\"\n    exit_code_value: Any | None = None\n\n    if isinstance(outcome_value, Mapping):\n        type_value = outcome_value.get(\"type\")\n        if type_value == \"timeout\":\n            outcome_type = \"timeout\"\n        elif isinstance(type_value, str):\n            outcome_type = \"exit\"\n        exit_code_value = outcome_value.get(\"exit_code\")\n    else:\n        status_str = str(entry.get(\"status\", \"completed\") or \"completed\").lower()\n        if status_str == \"timeout\":\n            outcome_type = \"timeout\"\n        if isinstance(outcome_value, str):\n            if outcome_value == \"failure\":\n                exit_code_value = 1\n            elif outcome_value == \"success\":\n                exit_code_value = 0\n        if exit_code_value is None and \"exit_code\" in entry:\n            exit_code_value = entry.get(\"exit_code\")\n\n    outcome = ShellCallOutcome(\n        type=outcome_type,\n        exit_code=_normalize_exit_code(exit_code_value),\n    )\n\n    return ShellCommandOutput(\n        stdout=stdout,\n        stderr=stderr,\n        outcome=outcome,\n        command=str(command_value) if command_value is not None else None,\n        provider_data=cast(dict[str, Any], provider_data_value)\n        if isinstance(provider_data_value, Mapping)\n        else provider_data_value,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.serialize_shell_output","title":"serialize_shell_output","text":"<pre><code>serialize_shell_output(\n    output: ShellCommandOutput,\n) -&gt; dict[str, Any]\n</code></pre> <p>Serialize ShellCommandOutput for persistence or cross-run transmission.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def serialize_shell_output(output: ShellCommandOutput) -&gt; dict[str, Any]:\n    \"\"\"Serialize ShellCommandOutput for persistence or cross-run transmission.\"\"\"\n    payload: dict[str, Any] = {\n        \"stdout\": output.stdout,\n        \"stderr\": output.stderr,\n        \"status\": output.status,\n        \"outcome\": {\"type\": output.outcome.type},\n    }\n    if output.outcome.type == \"exit\":\n        payload[\"outcome\"][\"exit_code\"] = output.outcome.exit_code\n        if output.outcome.exit_code is not None:\n            payload[\"exit_code\"] = output.outcome.exit_code\n    if output.command is not None:\n        payload[\"command\"] = output.command\n    if output.provider_data:\n        payload[\"provider_data\"] = output.provider_data\n    return payload\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_mcp_approval_requests","title":"execute_mcp_approval_requests  <code>async</code>","text":"<pre><code>execute_mcp_approval_requests(\n    *,\n    agent: Agent[Any],\n    approval_requests: list[ToolRunMCPApprovalRequest],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[RunItem]\n</code></pre> <p>Run hosted MCP approval callbacks and return approval response items.</p> Source code in <code>src/agents/run_internal/tool_planning.py</code> <pre><code>async def execute_mcp_approval_requests(\n    *,\n    agent: Agent[Any],\n    approval_requests: list[ToolRunMCPApprovalRequest],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[RunItem]:\n    \"\"\"Run hosted MCP approval callbacks and return approval response items.\"\"\"\n\n    async def run_single_approval(approval_request: ToolRunMCPApprovalRequest) -&gt; RunItem:\n        callback = approval_request.mcp_tool.on_approval_request\n        assert callback is not None, \"Callback is required for MCP approval requests\"\n        maybe_awaitable_result = callback(\n            MCPToolApprovalRequest(context_wrapper, approval_request.request_item)\n        )\n        if inspect.isawaitable(maybe_awaitable_result):\n            result = await maybe_awaitable_result\n        else:\n            result = maybe_awaitable_result\n        reason = result.get(\"reason\", None)\n        request_item = approval_request.request_item\n        request_id = (\n            request_item.id\n            if hasattr(request_item, \"id\")\n            else cast(dict[str, Any], request_item).get(\"id\", \"\")\n        )\n        raw_item: McpApprovalResponse = {\n            \"approval_request_id\": request_id,\n            \"approve\": result[\"approve\"],\n            \"type\": \"mcp_approval_response\",\n        }\n        if not result[\"approve\"] and reason:\n            raw_item[\"reason\"] = reason\n        return MCPApprovalResponseItem(\n            raw_item=raw_item,\n            agent=agent,\n        )\n\n    tasks = [run_single_approval(approval_request) for approval_request in approval_requests]\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    agent: Agent[Any],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[Tool]\n</code></pre> <p>Fetch all tools available to the agent.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>async def get_all_tools(agent: Agent[Any], context_wrapper: RunContextWrapper[Any]) -&gt; list[Tool]:\n    \"\"\"Fetch all tools available to the agent.\"\"\"\n    return await agent.get_all_tools(context_wrapper)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.get_handoffs","title":"get_handoffs  <code>async</code>","text":"<pre><code>get_handoffs(\n    agent: Agent[Any],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[Handoff]\n</code></pre> <p>Return enabled handoffs for the agent.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>async def get_handoffs(agent: Agent[Any], context_wrapper: RunContextWrapper[Any]) -&gt; list[Handoff]:\n    \"\"\"Return enabled handoffs for the agent.\"\"\"\n    handoffs = []\n    for handoff_item in agent.handoffs:\n        if isinstance(handoff_item, Handoff):\n            handoffs.append(handoff_item)\n        elif isinstance(handoff_item, Agent):\n            handoffs.append(handoff(handoff_item))\n\n    async def check_handoff_enabled(handoff_obj: Handoff) -&gt; bool:\n        attr = handoff_obj.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(context_wrapper, agent)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(check_handoff_enabled(h) for h in handoffs))\n    enabled: list[Handoff] = [h for h, ok in zip(handoffs, results) if ok]\n    return enabled\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.get_model","title":"get_model","text":"<pre><code>get_model(\n    agent: Agent[Any], run_config: RunConfig\n) -&gt; Model\n</code></pre> <p>Resolve the model instance for this run.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>def get_model(agent: Agent[Any], run_config: RunConfig) -&gt; Model:\n    \"\"\"Resolve the model instance for this run.\"\"\"\n    if isinstance(run_config.model, Model):\n        return run_config.model\n    elif isinstance(run_config.model, str):\n        return run_config.model_provider.get_model(run_config.model)\n    elif isinstance(agent.model, Model):\n        return agent.model\n\n    return run_config.model_provider.get_model(agent.model)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.get_output_schema","title":"get_output_schema","text":"<pre><code>get_output_schema(\n    agent: Agent[Any],\n) -&gt; AgentOutputSchemaBase | None\n</code></pre> <p>Return the resolved output schema for the agent, if any.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>def get_output_schema(agent: Agent[Any]) -&gt; AgentOutputSchemaBase | None:\n    \"\"\"Return the resolved output schema for the agent, if any.\"\"\"\n    if agent.output_type is None or agent.output_type is str:\n        return None\n    elif isinstance(agent.output_type, AgentOutputSchemaBase):\n        return agent.output_type\n\n    return AgentOutputSchema(agent.output_type)\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.maybe_filter_model_input","title":"maybe_filter_model_input  <code>async</code>","text":"<pre><code>maybe_filter_model_input(\n    *,\n    agent: Agent[TContext],\n    run_config: RunConfig,\n    context_wrapper: RunContextWrapper[TContext],\n    input_items: list[TResponseInputItem],\n    system_instructions: str | None,\n) -&gt; ModelInputData\n</code></pre> <p>Apply optional call_model_input_filter to modify model input.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>async def maybe_filter_model_input(\n    *,\n    agent: Agent[TContext],\n    run_config: RunConfig,\n    context_wrapper: RunContextWrapper[TContext],\n    input_items: list[TResponseInputItem],\n    system_instructions: str | None,\n) -&gt; ModelInputData:\n    \"\"\"Apply optional call_model_input_filter to modify model input.\"\"\"\n    effective_instructions = system_instructions\n    effective_input: list[TResponseInputItem] = input_items\n\n    if run_config.call_model_input_filter is None:\n        return ModelInputData(input=effective_input, instructions=effective_instructions)\n\n    try:\n        model_input = ModelInputData(\n            input=effective_input.copy(),\n            instructions=effective_instructions,\n        )\n        filter_payload: CallModelData[TContext] = CallModelData(\n            model_data=model_input,\n            agent=agent,\n            context=context_wrapper.context,\n        )\n        maybe_updated = run_config.call_model_input_filter(filter_payload)\n        updated = await maybe_updated if inspect.isawaitable(maybe_updated) else maybe_updated\n        if not isinstance(updated, ModelInputData):\n            raise UserError(\"call_model_input_filter must return a ModelInputData instance\")\n        return updated\n    except Exception as e:\n        _error_tracing.attach_error_to_current_span(\n            SpanError(message=\"Error in call_model_input_filter\", data={\"error\": str(e)})\n        )\n        raise\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.validate_run_hooks","title":"validate_run_hooks","text":"<pre><code>validate_run_hooks(\n    hooks: RunHooksBase[Any, Agent[Any]]\n    | AgentHooksBase[Any, Agent[Any]]\n    | Any\n    | None,\n) -&gt; RunHooks[Any]\n</code></pre> <p>Normalize hooks input and enforce RunHooks type.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>def validate_run_hooks(\n    hooks: RunHooksBase[Any, Agent[Any]] | AgentHooksBase[Any, Agent[Any]] | Any | None,\n) -&gt; RunHooks[Any]:\n    \"\"\"Normalize hooks input and enforce RunHooks type.\"\"\"\n    if hooks is None:\n        return RunHooks[Any]()\n    input_hook_type = type(hooks).__name__\n    if isinstance(hooks, AgentHooksBase):\n        raise TypeError(\n            \"Run hooks must be instances of RunHooks. \"\n            f\"Received agent-scoped hooks ({input_hook_type}). \"\n            \"Attach AgentHooks to an Agent via Agent(..., hooks=...).\"\n        )\n    if not isinstance(hooks, RunHooksBase):\n        raise TypeError(f\"Run hooks must be instances of RunHooks. Received {input_hook_type}.\")\n    return hooks\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.check_for_final_output_from_tools","title":"check_for_final_output_from_tools  <code>async</code>","text":"<pre><code>check_for_final_output_from_tools(\n    agent: Agent[TContext],\n    tool_results: list[FunctionToolResult],\n    context_wrapper: RunContextWrapper[TContext],\n) -&gt; ToolsToFinalOutputResult\n</code></pre> <p>Determine if tool results should produce a final output.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def check_for_final_output_from_tools(\n    agent: Agent[TContext],\n    tool_results: list[FunctionToolResult],\n    context_wrapper: RunContextWrapper[TContext],\n) -&gt; ToolsToFinalOutputResult:\n    \"\"\"Determine if tool results should produce a final output.\"\"\"\n    if not tool_results:\n        return NOT_FINAL_OUTPUT\n\n    if agent.tool_use_behavior == \"run_llm_again\":\n        return NOT_FINAL_OUTPUT\n    elif agent.tool_use_behavior == \"stop_on_first_tool\":\n        return ToolsToFinalOutputResult(is_final_output=True, final_output=tool_results[0].output)\n    elif isinstance(agent.tool_use_behavior, dict):\n        names = agent.tool_use_behavior.get(\"stop_at_tool_names\", [])\n        for tool_result in tool_results:\n            if tool_result.tool.name in names:\n                return ToolsToFinalOutputResult(\n                    is_final_output=True, final_output=tool_result.output\n                )\n        return ToolsToFinalOutputResult(is_final_output=False, final_output=None)\n    elif callable(agent.tool_use_behavior):\n        if inspect.iscoroutinefunction(agent.tool_use_behavior):\n            return await cast(\n                Awaitable[ToolsToFinalOutputResult],\n                agent.tool_use_behavior(context_wrapper, tool_results),\n            )\n        return cast(\n            ToolsToFinalOutputResult, agent.tool_use_behavior(context_wrapper, tool_results)\n        )\n\n    logger.error(\"Invalid tool_use_behavior: %s\", agent.tool_use_behavior)\n    raise UserError(f\"Invalid tool_use_behavior: {agent.tool_use_behavior}\")\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_final_output","title":"execute_final_output  <code>async</code>","text":"<pre><code>execute_final_output(\n    *,\n    agent: Agent[Any],\n    original_input: str | list[TResponseInputItem],\n    new_response: ModelResponse,\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    final_output: Any,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    tool_input_guardrail_results: list[\n        ToolInputGuardrailResult\n    ],\n    tool_output_guardrail_results: list[\n        ToolOutputGuardrailResult\n    ],\n    run_final_output_hooks_fn: Callable[\n        [\n            Agent[Any],\n            RunHooks[Any],\n            RunContextWrapper[Any],\n            Any,\n        ],\n        Awaitable[None],\n    ]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Convenience wrapper to finalize a turn and run end hooks.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def execute_final_output(\n    *,\n    agent: Agent[Any],\n    original_input: str | list[TResponseInputItem],\n    new_response: ModelResponse,\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    final_output: Any,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    tool_input_guardrail_results: list[ToolInputGuardrailResult],\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult],\n    run_final_output_hooks_fn: Callable[\n        [Agent[Any], RunHooks[Any], RunContextWrapper[Any], Any], Awaitable[None]\n    ]\n    | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Convenience wrapper to finalize a turn and run end hooks.\"\"\"\n    return await execute_final_output_step(\n        agent=agent,\n        original_input=original_input,\n        new_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        final_output=final_output,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n        run_final_output_hooks_fn=run_final_output_hooks_fn,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_handoffs","title":"execute_handoffs  <code>async</code>","text":"<pre><code>execute_handoffs(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    new_response: ModelResponse,\n    run_handoffs: list[ToolRunHandoff],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    nest_handoff_history_fn: Callable[..., HandoffInputData]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Execute a handoff and prepare the next turn for the new agent.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def execute_handoffs(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    new_response: ModelResponse,\n    run_handoffs: list[ToolRunHandoff],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    nest_handoff_history_fn: Callable[..., HandoffInputData] | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Execute a handoff and prepare the next turn for the new agent.\"\"\"\n\n    def nest_history(data: HandoffInputData, mapper: Any | None = None) -&gt; HandoffInputData:\n        if nest_handoff_history_fn is None:\n            return nest_handoff_history(data, history_mapper=mapper)\n        return nest_handoff_history_fn(data, mapper)\n\n    multiple_handoffs = len(run_handoffs) &gt; 1\n    if multiple_handoffs:\n        output_message = \"Multiple handoffs detected, ignoring this one.\"\n        new_step_items.extend(\n            [\n                ToolCallOutputItem(\n                    output=output_message,\n                    raw_item=ItemHelpers.tool_call_output_item(handoff.tool_call, output_message),\n                    agent=agent,\n                )\n                for handoff in run_handoffs[1:]\n            ]\n        )\n\n    actual_handoff = run_handoffs[0]\n    with handoff_span(from_agent=agent.name) as span_handoff:\n        handoff = actual_handoff.handoff\n        new_agent: Agent[Any] = await handoff.on_invoke_handoff(\n            context_wrapper, actual_handoff.tool_call.arguments\n        )\n        span_handoff.span_data.to_agent = new_agent.name\n        if multiple_handoffs:\n            requested_agents = [handoff.handoff.agent_name for handoff in run_handoffs]\n            span_handoff.set_error(\n                SpanError(\n                    message=\"Multiple handoffs requested\",\n                    data={\n                        \"requested_agents\": requested_agents,\n                    },\n                )\n            )\n\n        new_step_items.append(\n            HandoffOutputItem(\n                agent=agent,\n                raw_item=ItemHelpers.tool_call_output_item(\n                    actual_handoff.tool_call,\n                    handoff.get_transfer_message(new_agent),\n                ),\n                source_agent=agent,\n                target_agent=new_agent,\n            )\n        )\n\n        await asyncio.gather(\n            hooks.on_handoff(\n                context=context_wrapper,\n                from_agent=agent,\n                to_agent=new_agent,\n            ),\n            (\n                agent.hooks.on_handoff(\n                    context_wrapper,\n                    agent=new_agent,\n                    source=agent,\n                )\n                if agent.hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        input_filter = handoff.input_filter or (\n            run_config.handoff_input_filter if run_config else None\n        )\n        handoff_nest_setting = handoff.nest_handoff_history\n        should_nest_history = (\n            handoff_nest_setting\n            if handoff_nest_setting is not None\n            else run_config.nest_handoff_history\n        )\n        handoff_input_data: HandoffInputData | None = None\n        session_step_items: list[RunItem] | None = None\n        if input_filter or should_nest_history:\n            handoff_input_data = HandoffInputData(\n                input_history=tuple(original_input)\n                if isinstance(original_input, list)\n                else original_input,\n                pre_handoff_items=tuple(pre_step_items),\n                new_items=tuple(new_step_items),\n                run_context=context_wrapper,\n            )\n\n        if input_filter and handoff_input_data is not None:\n            filter_name = getattr(input_filter, \"__qualname__\", repr(input_filter))\n            from_agent = getattr(agent, \"name\", agent.__class__.__name__)\n            to_agent = getattr(new_agent, \"name\", new_agent.__class__.__name__)\n            logger.debug(\n                \"Filtering handoff inputs with %s for %s -&gt; %s\",\n                filter_name,\n                from_agent,\n                to_agent,\n            )\n            if not callable(input_filter):\n                _error_tracing.attach_error_to_span(\n                    span_handoff,\n                    SpanError(\n                        message=\"Invalid input filter\",\n                        data={\"details\": \"not callable()\"},\n                    ),\n                )\n                raise UserError(f\"Invalid input filter: {input_filter}\")\n            filtered = input_filter(handoff_input_data)\n            if inspect.isawaitable(filtered):\n                filtered = await filtered\n            if not isinstance(filtered, HandoffInputData):\n                _error_tracing.attach_error_to_span(\n                    span_handoff,\n                    SpanError(\n                        message=\"Invalid input filter result\",\n                        data={\"details\": \"not a HandoffInputData\"},\n                    ),\n                )\n                raise UserError(f\"Invalid input filter result: {filtered}\")\n\n            original_input = (\n                filtered.input_history\n                if isinstance(filtered.input_history, str)\n                else list(filtered.input_history)\n            )\n            pre_step_items = list(filtered.pre_handoff_items)\n            new_step_items = list(filtered.new_items)\n            # For custom input filters, keep full new_items for session history and\n            # use input_items for model input when provided.\n            if filtered.input_items is not None:\n                session_step_items = list(filtered.new_items)\n                new_step_items = list(filtered.input_items)\n            else:\n                session_step_items = None\n        elif should_nest_history and handoff_input_data is not None:\n            nested = nest_history(handoff_input_data, run_config.handoff_history_mapper)\n            original_input = (\n                nested.input_history\n                if isinstance(nested.input_history, str)\n                else list(nested.input_history)\n            )\n            pre_step_items = list(nested.pre_handoff_items)\n            # Keep full new_items for session history.\n            session_step_items = list(nested.new_items)\n            # Use input_items (filtered) for model input if available.\n            if nested.input_items is not None:\n                new_step_items = list(nested.input_items)\n            else:\n                new_step_items = session_step_items\n        else:\n            # No filtering or nesting - session_step_items not needed.\n            session_step_items = None\n\n    return SingleStepResult(\n        original_input=original_input,\n        model_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        next_step=NextStepHandoff(new_agent),\n        tool_input_guardrail_results=[],\n        tool_output_guardrail_results=[],\n        session_step_items=session_step_items,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.execute_tools_and_side_effects","title":"execute_tools_and_side_effects  <code>async</code>","text":"<pre><code>execute_tools_and_side_effects(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    output_schema: AgentOutputSchemaBase | None,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n) -&gt; SingleStepResult\n</code></pre> <p>Run one turn of the loop, coordinating tools, approvals, guardrails, and handoffs.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def execute_tools_and_side_effects(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    output_schema: AgentOutputSchemaBase | None,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n) -&gt; SingleStepResult:\n    \"\"\"Run one turn of the loop, coordinating tools, approvals, guardrails, and handoffs.\"\"\"\n\n    execute_final_output_call = execute_final_output\n    execute_handoffs_call = execute_handoffs\n\n    pre_step_items = list(pre_step_items)\n    approval_items_by_call_id = index_approval_items_by_call_id(pre_step_items)\n\n    plan = _build_plan_for_fresh_turn(\n        processed_response=processed_response,\n        agent=agent,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n    )\n\n    new_step_items = _dedupe_tool_call_items(\n        existing_items=pre_step_items,\n        new_items=processed_response.new_items,\n    )\n\n    (\n        function_results,\n        tool_input_guardrail_results,\n        tool_output_guardrail_results,\n        computer_results,\n        shell_results,\n        apply_patch_results,\n        local_shell_results,\n    ) = await _execute_tool_plan(\n        plan=plan,\n        agent=agent,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        run_config=run_config,\n    )\n    new_step_items.extend(\n        _build_tool_result_items(\n            function_results=function_results,\n            computer_results=computer_results,\n            shell_results=shell_results,\n            apply_patch_results=apply_patch_results,\n            local_shell_results=local_shell_results,\n        )\n    )\n\n    interruptions = _collect_tool_interruptions(\n        function_results=function_results,\n        shell_results=shell_results,\n        apply_patch_results=apply_patch_results,\n    )\n    if plan.approved_mcp_responses:\n        new_step_items.extend(plan.approved_mcp_responses)\n    if plan.pending_interruptions:\n        interruptions.extend(plan.pending_interruptions)\n        new_step_items.extend(plan.pending_interruptions)\n\n    processed_response.interruptions = interruptions\n\n    if interruptions:\n        return SingleStepResult(\n            original_input=original_input,\n            model_response=new_response,\n            pre_step_items=pre_step_items,\n            new_step_items=new_step_items,\n            next_step=NextStepInterruption(interruptions=interruptions),\n            tool_input_guardrail_results=tool_input_guardrail_results,\n            tool_output_guardrail_results=tool_output_guardrail_results,\n            processed_response=processed_response,\n        )\n\n    await _append_mcp_callback_results(\n        agent=agent,\n        requests=plan.mcp_requests_with_callback,\n        context_wrapper=context_wrapper,\n        append_item=new_step_items.append,\n    )\n\n    if run_handoffs := processed_response.handoffs:\n        return await execute_handoffs_call(\n            agent=agent,\n            original_input=original_input,\n            pre_step_items=pre_step_items,\n            new_step_items=new_step_items,\n            new_response=new_response,\n            run_handoffs=run_handoffs,\n            hooks=hooks,\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n        )\n\n    tool_final_output = await _maybe_finalize_from_tool_results(\n        agent=agent,\n        original_input=original_input,\n        new_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        function_results=function_results,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n    if tool_final_output is not None:\n        return tool_final_output\n\n    message_items = [item for item in new_step_items if isinstance(item, MessageOutputItem)]\n    potential_final_output_text = (\n        ItemHelpers.extract_last_text(message_items[-1].raw_item) if message_items else None\n    )\n\n    if not processed_response.has_tools_or_approvals_to_run():\n        if output_schema and not output_schema.is_plain_text() and potential_final_output_text:\n            final_output = output_schema.validate_json(potential_final_output_text)\n            return await execute_final_output_call(\n                agent=agent,\n                original_input=original_input,\n                new_response=new_response,\n                pre_step_items=pre_step_items,\n                new_step_items=new_step_items,\n                final_output=final_output,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                tool_input_guardrail_results=tool_input_guardrail_results,\n                tool_output_guardrail_results=tool_output_guardrail_results,\n            )\n        if not output_schema or output_schema.is_plain_text():\n            return await execute_final_output_call(\n                agent=agent,\n                original_input=original_input,\n                new_response=new_response,\n                pre_step_items=pre_step_items,\n                new_step_items=new_step_items,\n                final_output=potential_final_output_text or \"\",\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                tool_input_guardrail_results=tool_input_guardrail_results,\n                tool_output_guardrail_results=tool_output_guardrail_results,\n            )\n\n    return SingleStepResult(\n        original_input=original_input,\n        model_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        next_step=NextStepRunAgain(),\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.resolve_interrupted_turn","title":"resolve_interrupted_turn  <code>async</code>","text":"<pre><code>resolve_interrupted_turn(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    original_pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    run_state: RunState | None = None,\n    nest_handoff_history_fn: Callable[..., HandoffInputData]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Continue a turn that was previously interrupted waiting for tool approval.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def resolve_interrupted_turn(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    original_pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    run_state: RunState | None = None,\n    nest_handoff_history_fn: Callable[..., HandoffInputData] | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Continue a turn that was previously interrupted waiting for tool approval.\"\"\"\n\n    execute_handoffs_call = execute_handoffs\n\n    def nest_history(data: HandoffInputData, mapper: Any | None = None) -&gt; HandoffInputData:\n        if nest_handoff_history_fn is None:\n            return nest_handoff_history(data, history_mapper=mapper)\n        return nest_handoff_history_fn(data, mapper)\n\n    def _pending_approvals_from_state() -&gt; list[ToolApprovalItem]:\n        if (\n            run_state is not None\n            and hasattr(run_state, \"_current_step\")\n            and isinstance(run_state._current_step, NextStepInterruption)\n        ):\n            return [\n                item\n                for item in run_state._current_step.interruptions\n                if isinstance(item, ToolApprovalItem)\n            ]\n        return [item for item in original_pre_step_items if isinstance(item, ToolApprovalItem)]\n\n    async def _record_function_rejection(\n        call_id: str | None,\n        tool_call: ResponseFunctionToolCall,\n        function_tool: FunctionTool,\n    ) -&gt; None:\n        if isinstance(call_id, str) and call_id in rejected_function_call_ids:\n            return\n        rejection_message = REJECTION_MESSAGE\n        if call_id:\n            rejection_message = await resolve_approval_rejection_message(\n                context_wrapper=context_wrapper,\n                run_config=run_config,\n                tool_type=\"function\",\n                tool_name=function_tool.name,\n                call_id=call_id,\n            )\n        rejected_function_outputs.append(\n            function_rejection_item(agent, tool_call, rejection_message=rejection_message)\n        )\n        if isinstance(call_id, str):\n            rejected_function_call_ids.add(call_id)\n\n    async def _function_requires_approval(run: ToolRunFunction) -&gt; bool:\n        call_id = run.tool_call.call_id\n        if call_id and call_id in approval_items_by_call_id:\n            return True\n\n        try:\n            return await function_needs_approval(\n                run.function_tool,\n                context_wrapper,\n                run.tool_call,\n            )\n        except UserError:\n            raise\n        except Exception:\n            return True\n\n    try:\n        context_wrapper.turn_input = ItemHelpers.input_to_new_input_list(original_input)\n    except Exception:\n        context_wrapper.turn_input = []\n\n    pending_approval_items = _pending_approvals_from_state()\n    approval_items_by_call_id = index_approval_items_by_call_id(pending_approval_items)\n\n    rejected_function_outputs: list[RunItem] = []\n    rejected_function_call_ids: set[str] = set()\n    rerun_function_call_ids: set[str] = set()\n    pending_interruptions: list[ToolApprovalItem] = []\n    pending_interruption_keys: set[str] = set()\n\n    output_index = _build_tool_output_index(original_pre_step_items)\n\n    def _has_output_item(call_id: str, expected_type: str) -&gt; bool:\n        return (expected_type, call_id) in output_index\n\n    def _shell_call_id_from_run(run: ToolRunShellCall) -&gt; str:\n        return extract_shell_call_id(run.tool_call)\n\n    def _apply_patch_call_id_from_run(run: ToolRunApplyPatchCall) -&gt; str:\n        return extract_apply_patch_call_id(run.tool_call)\n\n    def _computer_call_id_from_run(run: ToolRunComputerAction) -&gt; str:\n        call_id = extract_tool_call_id(run.tool_call)\n        if not call_id:\n            raise ModelBehaviorError(\"Computer action is missing call_id.\")\n        return call_id\n\n    def _shell_tool_name(run: ToolRunShellCall) -&gt; str:\n        return run.shell_tool.name\n\n    def _apply_patch_tool_name(run: ToolRunApplyPatchCall) -&gt; str:\n        return run.apply_patch_tool.name\n\n    async def _build_shell_rejection(run: ToolRunShellCall, call_id: str) -&gt; RunItem:\n        rejection_message = await resolve_approval_rejection_message(\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n            tool_type=\"shell\",\n            tool_name=run.shell_tool.name,\n            call_id=call_id,\n        )\n        return cast(\n            RunItem,\n            shell_rejection_item(\n                agent,\n                call_id,\n                rejection_message=rejection_message,\n            ),\n        )\n\n    async def _build_apply_patch_rejection(run: ToolRunApplyPatchCall, call_id: str) -&gt; RunItem:\n        rejection_message = await resolve_approval_rejection_message(\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n            tool_type=\"apply_patch\",\n            tool_name=run.apply_patch_tool.name,\n            call_id=call_id,\n        )\n        return cast(\n            RunItem,\n            apply_patch_rejection_item(\n                agent,\n                call_id,\n                rejection_message=rejection_message,\n            ),\n        )\n\n    async def _shell_needs_approval(run: ToolRunShellCall) -&gt; bool:\n        shell_call = coerce_shell_call(run.tool_call)\n        return await evaluate_needs_approval_setting(\n            run.shell_tool.needs_approval,\n            context_wrapper,\n            shell_call.action,\n            shell_call.call_id,\n        )\n\n    async def _apply_patch_needs_approval(run: ToolRunApplyPatchCall) -&gt; bool:\n        operation = coerce_apply_patch_operation(\n            run.tool_call,\n            context_wrapper=context_wrapper,\n        )\n        call_id = extract_apply_patch_call_id(run.tool_call)\n        return await evaluate_needs_approval_setting(\n            run.apply_patch_tool.needs_approval, context_wrapper, operation, call_id\n        )\n\n    def _shell_output_exists(call_id: str) -&gt; bool:\n        return _has_output_item(call_id, \"shell_call_output\")\n\n    def _apply_patch_output_exists(call_id: str) -&gt; bool:\n        return _has_output_item(call_id, \"apply_patch_call_output\")\n\n    def _computer_output_exists(call_id: str) -&gt; bool:\n        return _has_output_item(call_id, \"computer_call_output\")\n\n    def _nested_interruptions_status(\n        interruptions: Sequence[ToolApprovalItem],\n    ) -&gt; Literal[\"approved\", \"pending\", \"rejected\"]:\n        has_pending = False\n        for interruption in interruptions:\n            call_id = extract_tool_call_id(interruption.raw_item)\n            if not call_id:\n                has_pending = True\n                continue\n            status = context_wrapper.get_approval_status(\n                interruption.tool_name or \"\", call_id, existing_pending=interruption\n            )\n            if status is False:\n                return \"rejected\"\n            if status is None:\n                has_pending = True\n        return \"pending\" if has_pending else \"approved\"\n\n    def _function_output_exists(run: ToolRunFunction) -&gt; bool:\n        call_id = extract_tool_call_id(run.tool_call)\n        if not call_id:\n            return False\n\n        pending_run_result = peek_agent_tool_run_result(run.tool_call)\n        if pending_run_result and getattr(pending_run_result, \"interruptions\", None):\n            status = _nested_interruptions_status(pending_run_result.interruptions)\n            if status in (\"approved\", \"rejected\"):\n                rerun_function_call_ids.add(call_id)\n                return False\n            return True\n\n        return _has_output_item(call_id, \"function_call_output\")\n\n    def _add_pending_interruption(item: ToolApprovalItem | None) -&gt; None:\n        if item is None:\n            return\n        call_id = extract_tool_call_id(item.raw_item)\n        key = call_id or f\"raw:{id(item.raw_item)}\"\n        if key in pending_interruption_keys:\n            return\n        pending_interruption_keys.add(key)\n        pending_interruptions.append(item)\n\n    def _approval_matches_agent(approval: ToolApprovalItem) -&gt; bool:\n        approval_agent = approval.agent\n        if approval_agent is None:\n            return False\n        if approval_agent is agent:\n            return True\n        return getattr(approval_agent, \"name\", None) == agent.name\n\n    async def _rebuild_function_runs_from_approvals() -&gt; list[ToolRunFunction]:\n        if not pending_approval_items:\n            return []\n        all_tools = await agent.get_all_tools(context_wrapper)\n        tool_map: dict[str, FunctionTool] = {\n            tool.name: tool for tool in all_tools if isinstance(tool, FunctionTool)\n        }\n        existing_pending_call_ids: set[str] = set()\n        for existing_pending in pending_interruptions:\n            if isinstance(existing_pending, ToolApprovalItem):\n                existing_call_id = extract_tool_call_id(existing_pending.raw_item)\n                if existing_call_id:\n                    existing_pending_call_ids.add(existing_call_id)\n        rebuilt_runs: list[ToolRunFunction] = []\n\n        def _add_unmatched_pending(approval: ToolApprovalItem) -&gt; None:\n            call_id = extract_tool_call_id(approval.raw_item)\n            if not call_id:\n                _add_pending_interruption(approval)\n                return\n            tool_name = approval.tool_name or \"\"\n            approval_status = context_wrapper.get_approval_status(\n                tool_name, call_id, existing_pending=approval\n            )\n            if approval_status is None:\n                _add_pending_interruption(approval)\n\n        for approval in pending_approval_items:\n            if not isinstance(approval, ToolApprovalItem):\n                continue\n            if not _approval_matches_agent(approval):\n                _add_unmatched_pending(approval)\n                continue\n            raw = approval.raw_item\n            raw_type = get_mapping_or_attr(raw, \"type\")\n            if raw_type != \"function_call\":\n                _add_unmatched_pending(approval)\n                continue\n            name = get_mapping_or_attr(raw, \"name\")\n            if not (isinstance(name, str) and name in tool_map):\n                _add_unmatched_pending(approval)\n                continue\n\n            rebuilt_call_id: str | None\n            arguments: str | None\n            tool_call: ResponseFunctionToolCall\n            if isinstance(raw, ResponseFunctionToolCall):\n                rebuilt_call_id = raw.call_id\n                arguments = raw.arguments\n                tool_call = raw\n            else:\n                rebuilt_call_id = extract_tool_call_id(raw)\n                arguments = get_mapping_or_attr(raw, \"arguments\") or \"{}\"\n                status = get_mapping_or_attr(raw, \"status\")\n                if not (isinstance(rebuilt_call_id, str) and isinstance(arguments, str)):\n                    _add_unmatched_pending(approval)\n                    continue\n                valid_status: Literal[\"in_progress\", \"completed\", \"incomplete\"] | None = None\n                if isinstance(status, str) and status in (\n                    \"in_progress\",\n                    \"completed\",\n                    \"incomplete\",\n                ):\n                    valid_status = status  # type: ignore[assignment]\n                tool_call = ResponseFunctionToolCall(\n                    type=\"function_call\",\n                    name=name,\n                    call_id=rebuilt_call_id,\n                    arguments=arguments,\n                    status=valid_status,\n                )\n\n            if not (isinstance(rebuilt_call_id, str) and isinstance(arguments, str)):\n                _add_unmatched_pending(approval)\n                continue\n\n            approval_status = context_wrapper.get_approval_status(\n                name, rebuilt_call_id, existing_pending=approval\n            )\n            if approval_status is False:\n                await _record_function_rejection(\n                    rebuilt_call_id,\n                    tool_call,\n                    tool_map[name],\n                )\n                continue\n            if approval_status is None:\n                if rebuilt_call_id not in existing_pending_call_ids:\n                    _add_pending_interruption(approval)\n                    existing_pending_call_ids.add(rebuilt_call_id)\n                continue\n            rebuilt_runs.append(ToolRunFunction(function_tool=tool_map[name], tool_call=tool_call))\n        return rebuilt_runs\n\n    function_tool_runs = await _select_function_tool_runs_for_resume(\n        processed_response.functions,\n        approval_items_by_call_id=approval_items_by_call_id,\n        context_wrapper=context_wrapper,\n        needs_approval_checker=_function_requires_approval,\n        output_exists_checker=_function_output_exists,\n        record_rejection=_record_function_rejection,\n        pending_interruption_adder=_add_pending_interruption,\n        pending_item_builder=lambda run: ToolApprovalItem(agent=agent, raw_item=run.tool_call),\n    )\n\n    rebuilt_function_tool_runs = await _rebuild_function_runs_from_approvals()\n    if rebuilt_function_tool_runs:\n        existing_call_ids: set[str] = set()\n        for run in function_tool_runs:\n            call_id = extract_tool_call_id(run.tool_call)\n            if call_id:\n                existing_call_ids.add(call_id)\n        for run in rebuilt_function_tool_runs:\n            call_id = extract_tool_call_id(run.tool_call)\n            if call_id and call_id in existing_call_ids:\n                continue\n            function_tool_runs.append(run)\n            if call_id:\n                existing_call_ids.add(call_id)\n\n    pending_computer_actions: list[ToolRunComputerAction] = []\n    for action in processed_response.computer_actions:\n        call_id = _computer_call_id_from_run(action)\n        if _computer_output_exists(call_id):\n            continue\n        pending_computer_actions.append(action)\n\n    approved_shell_calls, rejected_shell_results = await _collect_runs_by_approval(\n        processed_response.shell_calls,\n        call_id_extractor=_shell_call_id_from_run,\n        tool_name_resolver=_shell_tool_name,\n        rejection_builder=_build_shell_rejection,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n        agent=agent,\n        pending_interruption_adder=_add_pending_interruption,\n        needs_approval_checker=_shell_needs_approval,\n        output_exists_checker=_shell_output_exists,\n    )\n\n    approved_apply_patch_calls, rejected_apply_patch_results = await _collect_runs_by_approval(\n        processed_response.apply_patch_calls,\n        call_id_extractor=_apply_patch_call_id_from_run,\n        tool_name_resolver=_apply_patch_tool_name,\n        rejection_builder=_build_apply_patch_rejection,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n        agent=agent,\n        pending_interruption_adder=_add_pending_interruption,\n        needs_approval_checker=_apply_patch_needs_approval,\n        output_exists_checker=_apply_patch_output_exists,\n    )\n\n    plan = _build_plan_for_resume_turn(\n        processed_response=processed_response,\n        agent=agent,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n        pending_interruptions=pending_interruptions,\n        pending_interruption_adder=_add_pending_interruption,\n        function_runs=function_tool_runs,\n        computer_actions=pending_computer_actions,\n        shell_calls=approved_shell_calls,\n        apply_patch_calls=approved_apply_patch_calls,\n    )\n\n    (\n        function_results,\n        tool_input_guardrail_results,\n        tool_output_guardrail_results,\n        computer_results,\n        shell_results,\n        apply_patch_results,\n        _local_shell_results,\n    ) = await _execute_tool_plan(\n        plan=plan,\n        agent=agent,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        run_config=run_config,\n    )\n\n    for interruption in _collect_tool_interruptions(\n        function_results=function_results,\n        shell_results=[],\n        apply_patch_results=[],\n    ):\n        _add_pending_interruption(interruption)\n\n    new_items, append_if_new = _make_unique_item_appender(original_pre_step_items)\n\n    for item in _build_tool_result_items(\n        function_results=function_results,\n        computer_results=computer_results,\n        shell_results=shell_results,\n        apply_patch_results=apply_patch_results,\n        local_shell_results=[],\n    ):\n        append_if_new(item)\n    for rejection_item in rejected_function_outputs:\n        append_if_new(rejection_item)\n    for pending_item in pending_interruptions:\n        if pending_item:\n            append_if_new(pending_item)\n    for shell_rejection in rejected_shell_results:\n        append_if_new(shell_rejection)\n    for apply_patch_rejection in rejected_apply_patch_results:\n        append_if_new(apply_patch_rejection)\n    for approved_response in plan.approved_mcp_responses:\n        append_if_new(approved_response)\n\n    processed_response.interruptions = pending_interruptions\n    if pending_interruptions:\n        return SingleStepResult(\n            original_input=original_input,\n            model_response=new_response,\n            pre_step_items=original_pre_step_items,\n            new_step_items=new_items,\n            next_step=NextStepInterruption(\n                interruptions=[item for item in pending_interruptions if item]\n            ),\n            tool_input_guardrail_results=tool_input_guardrail_results,\n            tool_output_guardrail_results=tool_output_guardrail_results,\n            processed_response=processed_response,\n        )\n\n    await _append_mcp_callback_results(\n        agent=agent,\n        requests=plan.mcp_requests_with_callback,\n        context_wrapper=context_wrapper,\n        append_item=append_if_new,\n    )\n\n    (\n        pending_hosted_mcp_approvals,\n        pending_hosted_mcp_approval_ids,\n    ) = process_hosted_mcp_approvals(\n        original_pre_step_items=original_pre_step_items,\n        mcp_approval_requests=processed_response.mcp_approval_requests,\n        context_wrapper=context_wrapper,\n        agent=agent,\n        append_item=append_if_new,\n    )\n\n    pre_step_items = [\n        item\n        for item in original_pre_step_items\n        if should_keep_hosted_mcp_item(\n            item,\n            pending_hosted_mcp_approvals=pending_hosted_mcp_approvals,\n            pending_hosted_mcp_approval_ids=pending_hosted_mcp_approval_ids,\n        )\n    ]\n\n    if rejected_function_call_ids:\n        pre_step_items = [\n            item\n            for item in pre_step_items\n            if not (\n                item.type == \"tool_call_output_item\"\n                and (\n                    extract_tool_call_id(getattr(item, \"raw_item\", None))\n                    in rejected_function_call_ids\n                )\n            )\n        ]\n\n    if rerun_function_call_ids:\n        pre_step_items = [\n            item\n            for item in pre_step_items\n            if not (\n                item.type == \"tool_call_output_item\"\n                and (\n                    extract_tool_call_id(getattr(item, \"raw_item\", None)) in rerun_function_call_ids\n                )\n            )\n        ]\n\n    executed_handoff_call_ids: set[str] = set()\n    for item in original_pre_step_items:\n        if isinstance(item, HandoffCallItem):\n            handoff_call_id = extract_tool_call_id(item.raw_item)\n            if handoff_call_id:\n                executed_handoff_call_ids.add(handoff_call_id)\n\n    pending_handoffs = [\n        handoff\n        for handoff in processed_response.handoffs\n        if not handoff.tool_call.call_id\n        or handoff.tool_call.call_id not in executed_handoff_call_ids\n    ]\n\n    if pending_handoffs:\n        return await execute_handoffs_call(\n            agent=agent,\n            original_input=original_input,\n            pre_step_items=pre_step_items,\n            new_step_items=new_items,\n            new_response=new_response,\n            run_handoffs=pending_handoffs,\n            hooks=hooks,\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n            nest_handoff_history_fn=nest_history,\n        )\n\n    tool_final_output = await _maybe_finalize_from_tool_results(\n        agent=agent,\n        original_input=original_input,\n        new_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_items,\n        function_results=function_results,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n    if tool_final_output is not None:\n        return tool_final_output\n\n    return SingleStepResult(\n        original_input=original_input,\n        model_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_items,\n        next_step=NextStepRunAgain(),\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.start_streaming","title":"start_streaming  <code>async</code>","text":"<pre><code>start_streaming(\n    starting_input: str | list[TResponseInputItem],\n    streamed_result: RunResultStreaming,\n    starting_agent: Agent[TContext],\n    max_turns: int,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    error_handlers: RunErrorHandlers[TContext] | None,\n    previous_response_id: str | None,\n    auto_previous_response_id: bool,\n    conversation_id: str | None,\n    session: Session | None,\n    run_state: RunState[TContext] | None = None,\n    *,\n    is_resumed_state: bool = False,\n)\n</code></pre> <p>Run the streaming loop for a run result.</p> Source code in <code>src/agents/run_internal/run_loop.py</code> <pre><code>async def start_streaming(\n    starting_input: str | list[TResponseInputItem],\n    streamed_result: RunResultStreaming,\n    starting_agent: Agent[TContext],\n    max_turns: int,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    error_handlers: RunErrorHandlers[TContext] | None,\n    previous_response_id: str | None,\n    auto_previous_response_id: bool,\n    conversation_id: str | None,\n    session: Session | None,\n    run_state: RunState[TContext] | None = None,\n    *,\n    is_resumed_state: bool = False,\n):\n    \"\"\"Run the streaming loop for a run result.\"\"\"\n    if streamed_result.trace:\n        streamed_result.trace.start(mark_as_current=True)\n    if run_state is not None:\n        run_state.set_trace(get_current_trace() or streamed_result.trace)\n        streamed_result._trace_state = run_state._trace_state\n\n    if is_resumed_state and run_state is not None:\n        (\n            conversation_id,\n            previous_response_id,\n            auto_previous_response_id,\n        ) = apply_resumed_conversation_settings(\n            run_state=run_state,\n            conversation_id=conversation_id,\n            previous_response_id=previous_response_id,\n            auto_previous_response_id=auto_previous_response_id,\n        )\n\n    if conversation_id is not None or previous_response_id is not None or auto_previous_response_id:\n        server_conversation_tracker = OpenAIServerConversationTracker(\n            conversation_id=conversation_id,\n            previous_response_id=previous_response_id,\n            auto_previous_response_id=auto_previous_response_id,\n        )\n    else:\n        server_conversation_tracker = None\n\n    def _sync_conversation_tracking_from_tracker() -&gt; None:\n        if server_conversation_tracker is None:\n            return\n        if run_state is not None:\n            run_state._conversation_id = server_conversation_tracker.conversation_id\n            run_state._previous_response_id = server_conversation_tracker.previous_response_id\n            run_state._auto_previous_response_id = (\n                server_conversation_tracker.auto_previous_response_id\n            )\n        streamed_result._conversation_id = server_conversation_tracker.conversation_id\n        streamed_result._previous_response_id = server_conversation_tracker.previous_response_id\n        streamed_result._auto_previous_response_id = (\n            server_conversation_tracker.auto_previous_response_id\n        )\n\n    if run_state is None:\n        run_state = RunState(\n            context=context_wrapper,\n            original_input=copy_input_items(starting_input),\n            starting_agent=starting_agent,\n            max_turns=max_turns,\n            conversation_id=conversation_id,\n            previous_response_id=previous_response_id,\n            auto_previous_response_id=auto_previous_response_id,\n        )\n        streamed_result._state = run_state\n    elif streamed_result._state is None:\n        streamed_result._state = run_state\n    if run_state is not None:\n        streamed_result._model_input_items = list(run_state._generated_items)\n\n    if run_state is not None:\n        run_state._conversation_id = conversation_id\n        run_state._previous_response_id = previous_response_id\n        run_state._auto_previous_response_id = auto_previous_response_id\n    streamed_result._conversation_id = conversation_id\n    streamed_result._previous_response_id = previous_response_id\n    streamed_result._auto_previous_response_id = auto_previous_response_id\n\n    current_span: Span[AgentSpanData] | None = None\n    if run_state is not None and run_state._current_agent is not None:\n        current_agent = run_state._current_agent\n    else:\n        current_agent = starting_agent\n    if run_state is not None:\n        current_turn = run_state._current_turn\n    else:\n        current_turn = 0\n    should_run_agent_start_hooks = True\n    tool_use_tracker = AgentToolUseTracker()\n    if run_state is not None:\n        hydrate_tool_use_tracker(tool_use_tracker, run_state, starting_agent)\n\n    pending_server_items: list[RunItem] | None = None\n    session_input_items_for_persistence: list[TResponseInputItem] | None = None\n\n    if is_resumed_state and server_conversation_tracker is not None and run_state is not None:\n        session_items: list[TResponseInputItem] | None = None\n        if session is not None:\n            try:\n                session_items = await session.get_items()\n            except Exception:\n                session_items = None\n        server_conversation_tracker.hydrate_from_state(\n            original_input=run_state._original_input,\n            generated_items=run_state._generated_items,\n            model_responses=run_state._model_responses,\n            session_items=session_items,\n        )\n\n    streamed_result._event_queue.put_nowait(AgentUpdatedStreamEvent(new_agent=current_agent))\n\n    prepared_input: str | list[TResponseInputItem]\n    if is_resumed_state and run_state is not None:\n        prepared_input = normalize_resumed_input(starting_input)\n        streamed_result.input = prepared_input\n        streamed_result._original_input_for_persistence = []\n        streamed_result._stream_input_persisted = True\n    else:\n        server_manages_conversation = server_conversation_tracker is not None\n        prepared_input, session_items_snapshot = await prepare_input_with_session(\n            starting_input,\n            session,\n            run_config.session_input_callback,\n            run_config.session_settings,\n            include_history_in_prepared_input=not server_manages_conversation,\n            preserve_dropped_new_items=True,\n        )\n        streamed_result.input = prepared_input\n        streamed_result._original_input = copy_input_items(prepared_input)\n        if server_manages_conversation:\n            streamed_result._original_input_for_persistence = []\n            streamed_result._stream_input_persisted = True\n        else:\n            session_input_items_for_persistence = session_items_snapshot\n            streamed_result._original_input_for_persistence = session_items_snapshot\n\n    async def _save_resumed_items(\n        items: list[RunItem], response_id: str | None, store_setting: bool | None\n    ) -&gt; None:\n        await _save_resumed_stream_items(\n            session=session,\n            server_conversation_tracker=server_conversation_tracker,\n            streamed_result=streamed_result,\n            run_state=run_state,\n            items=items,\n            response_id=response_id,\n            store=store_setting,\n        )\n\n    async def _save_stream_items_with_count(\n        items: list[RunItem], response_id: str | None, store_setting: bool | None\n    ) -&gt; None:\n        await _save_stream_items(\n            session=session,\n            server_conversation_tracker=server_conversation_tracker,\n            streamed_result=streamed_result,\n            run_state=run_state,\n            items=items,\n            response_id=response_id,\n            update_persisted_count=True,\n            store=store_setting,\n        )\n\n    async def _save_stream_items_without_count(\n        items: list[RunItem], response_id: str | None, store_setting: bool | None\n    ) -&gt; None:\n        await _save_stream_items(\n            session=session,\n            server_conversation_tracker=server_conversation_tracker,\n            streamed_result=streamed_result,\n            run_state=run_state,\n            items=items,\n            response_id=response_id,\n            update_persisted_count=False,\n            store=store_setting,\n        )\n\n    try:\n        while True:\n            if is_resumed_state and run_state is not None and run_state._current_step is not None:\n                if isinstance(run_state._current_step, NextStepInterruption):\n                    if not run_state._model_responses or not run_state._last_processed_response:\n                        raise UserError(\"No model response found in previous state\")\n\n                    last_model_response = run_state._model_responses[-1]\n\n                    turn_result = await resolve_interrupted_turn(\n                        agent=current_agent,\n                        original_input=run_state._original_input,\n                        original_pre_step_items=run_state._generated_items,\n                        new_response=last_model_response,\n                        processed_response=run_state._last_processed_response,\n                        hooks=hooks,\n                        context_wrapper=context_wrapper,\n                        run_config=run_config,\n                        run_state=run_state,\n                    )\n\n                    tool_use_tracker.add_tool_use(\n                        current_agent, run_state._last_processed_response.tools_used\n                    )\n                    streamed_result._tool_use_tracker_snapshot = serialize_tool_use_tracker(\n                        tool_use_tracker\n                    )\n\n                    streamed_result.input = turn_result.original_input\n                    streamed_result._original_input = copy_input_items(turn_result.original_input)\n                    generated_items, turn_session_items = resumed_turn_items(turn_result)\n                    base_session_items = (\n                        list(run_state._session_items) if run_state is not None else []\n                    )\n                    streamed_result._model_input_items = generated_items\n                    streamed_result.new_items = base_session_items + list(turn_session_items)\n                    if run_state is not None:\n                        update_run_state_after_resume(\n                            run_state,\n                            turn_result=turn_result,\n                            generated_items=generated_items,\n                            session_items=streamed_result.new_items,\n                        )\n                        run_state._current_turn_persisted_item_count = (\n                            streamed_result._current_turn_persisted_item_count\n                        )\n\n                    stream_step_items_to_queue(\n                        list(turn_session_items), streamed_result._event_queue\n                    )\n                    store_setting = current_agent.model_settings.resolve(\n                        run_config.model_settings\n                    ).store\n\n                    if isinstance(turn_result.next_step, NextStepInterruption):\n                        await _finalize_streamed_interruption(\n                            streamed_result=streamed_result,\n                            save_items=_save_resumed_items,\n                            items=list(turn_session_items),\n                            response_id=turn_result.model_response.response_id,\n                            store_setting=store_setting,\n                            interruptions=approvals_from_step(turn_result.next_step),\n                            processed_response=run_state._last_processed_response,\n                        )\n                        break\n\n                    if isinstance(turn_result.next_step, NextStepHandoff):\n                        current_agent = turn_result.next_step.new_agent\n                        if run_state is not None:\n                            run_state._current_agent = current_agent\n                        if current_span:\n                            current_span.finish(reset_current=True)\n                        current_span = None\n                        should_run_agent_start_hooks = True\n                        streamed_result._event_queue.put_nowait(\n                            AgentUpdatedStreamEvent(new_agent=current_agent)\n                        )\n                        run_state._current_step = NextStepRunAgain()  # type: ignore[assignment]\n                        continue\n\n                    if isinstance(turn_result.next_step, NextStepFinalOutput):\n                        await _finalize_streamed_final_output(\n                            streamed_result=streamed_result,\n                            agent=current_agent,\n                            run_config=run_config,\n                            output=turn_result.next_step.output,\n                            context_wrapper=context_wrapper,\n                            save_items=_save_resumed_items,\n                            items=list(turn_session_items),\n                            response_id=turn_result.model_response.response_id,\n                            store_setting=store_setting,\n                        )\n                        break\n\n                    if isinstance(turn_result.next_step, NextStepRunAgain):\n                        await _save_resumed_items(\n                            list(turn_session_items),\n                            turn_result.model_response.response_id,\n                            store_setting,\n                        )\n                        run_state._current_step = NextStepRunAgain()  # type: ignore[assignment]\n                        continue\n\n                    run_state._current_step = None\n\n            if streamed_result._cancel_mode == \"after_turn\":\n                streamed_result.is_complete = True\n                streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n                break\n\n            if streamed_result.is_complete:\n                break\n\n            all_tools = await get_all_tools(current_agent, context_wrapper)\n            await initialize_computer_tools(tools=all_tools, context_wrapper=context_wrapper)\n\n            if current_span is None:\n                handoff_names = [\n                    h.agent_name for h in await get_handoffs(current_agent, context_wrapper)\n                ]\n                if output_schema := get_output_schema(current_agent):\n                    output_type_name = output_schema.name()\n                else:\n                    output_type_name = \"str\"\n\n                current_span = agent_span(\n                    name=current_agent.name,\n                    handoffs=handoff_names,\n                    output_type=output_type_name,\n                )\n                current_span.start(mark_as_current=True)\n                tool_names = [t.name for t in all_tools]\n                current_span.span_data.tools = tool_names\n\n            current_turn += 1\n            streamed_result.current_turn = current_turn\n            streamed_result._current_turn_persisted_item_count = 0\n            if run_state:\n                run_state._current_turn_persisted_item_count = 0\n\n            if current_turn &gt; max_turns:\n                _error_tracing.attach_error_to_span(\n                    current_span,\n                    SpanError(\n                        message=\"Max turns exceeded\",\n                        data={\"max_turns\": max_turns},\n                    ),\n                )\n                max_turns_error = MaxTurnsExceeded(f\"Max turns ({max_turns}) exceeded\")\n                handler_configured = bool(\n                    error_handlers and error_handlers.get(\"max_turns\") is not None\n                )\n                if handler_configured:\n                    streamed_result._max_turns_handled = True\n                run_error_data = build_run_error_data(\n                    input=streamed_result.input,\n                    new_items=streamed_result.new_items,\n                    raw_responses=streamed_result.raw_responses,\n                    last_agent=current_agent,\n                )\n                handler_result = await resolve_run_error_handler_result(\n                    error_handlers=error_handlers,\n                    error=max_turns_error,\n                    context_wrapper=context_wrapper,\n                    run_data=run_error_data,\n                )\n                if handler_result is None:\n                    if handler_configured:\n                        streamed_result._max_turns_handled = False\n                    streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n                    break\n\n                validated_output = validate_handler_final_output(\n                    current_agent, handler_result.final_output\n                )\n                output_text = format_final_output_text(current_agent, validated_output)\n                synthesized_item = create_message_output_item(current_agent, output_text)\n                include_in_history = handler_result.include_in_history\n                if include_in_history:\n                    streamed_result._model_input_items.append(synthesized_item)\n                    streamed_result.new_items.append(synthesized_item)\n                    if run_state is not None:\n                        run_state._generated_items = list(streamed_result._model_input_items)\n                        run_state._session_items = list(streamed_result.new_items)\n                    stream_step_items_to_queue([synthesized_item], streamed_result._event_queue)\n                    store_setting = current_agent.model_settings.resolve(\n                        run_config.model_settings\n                    ).store\n                    if is_resumed_state:\n                        await _save_resumed_items([synthesized_item], None, store_setting)\n                    else:\n                        await _save_stream_items_with_count([synthesized_item], None, store_setting)\n\n                await run_final_output_hooks(\n                    current_agent, hooks, context_wrapper, validated_output\n                )\n                output_guardrail_results = await _run_output_guardrails_for_stream(\n                    agent=current_agent,\n                    run_config=run_config,\n                    output=validated_output,\n                    context_wrapper=context_wrapper,\n                    streamed_result=streamed_result,\n                )\n                streamed_result.output_guardrail_results = output_guardrail_results\n                streamed_result.final_output = validated_output\n                streamed_result.is_complete = True\n                streamed_result._stored_exception = None\n                streamed_result._max_turns_handled = True\n                streamed_result.current_turn = max_turns\n                if run_state is not None:\n                    run_state._current_turn = max_turns\n                    run_state._current_step = None\n                streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n                break\n\n            if current_turn == 1:\n                all_input_guardrails = starting_agent.input_guardrails + (\n                    run_config.input_guardrails or []\n                )\n                sequential_guardrails = [g for g in all_input_guardrails if not g.run_in_parallel]\n                parallel_guardrails = [g for g in all_input_guardrails if g.run_in_parallel]\n\n                if sequential_guardrails:\n                    await run_input_guardrails_with_queue(\n                        starting_agent,\n                        sequential_guardrails,\n                        ItemHelpers.input_to_new_input_list(prepared_input),\n                        context_wrapper,\n                        streamed_result,\n                        current_span,\n                    )\n                    for result in streamed_result.input_guardrail_results:\n                        if result.output.tripwire_triggered:\n                            streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n                            session_input_items_for_persistence = (\n                                await persist_session_items_for_guardrail_trip(\n                                    session,\n                                    server_conversation_tracker,\n                                    session_input_items_for_persistence,\n                                    starting_input,\n                                    run_state,\n                                    store=current_agent.model_settings.resolve(\n                                        run_config.model_settings\n                                    ).store,\n                                )\n                            )\n                            raise InputGuardrailTripwireTriggered(result)\n\n                if parallel_guardrails:\n                    streamed_result._input_guardrails_task = asyncio.create_task(\n                        run_input_guardrails_with_queue(\n                            starting_agent,\n                            parallel_guardrails,\n                            ItemHelpers.input_to_new_input_list(prepared_input),\n                            context_wrapper,\n                            streamed_result,\n                            current_span,\n                        )\n                    )\n            try:\n                logger.debug(\n                    \"Starting turn %s, current_agent=%s\",\n                    current_turn,\n                    current_agent.name,\n                )\n                if (\n                    session is not None\n                    and server_conversation_tracker is None\n                    and not streamed_result._stream_input_persisted\n                ):\n                    streamed_result._original_input_for_persistence = (\n                        session_input_items_for_persistence\n                        if session_input_items_for_persistence is not None\n                        else []\n                    )\n                turn_result = await run_single_turn_streamed(\n                    streamed_result,\n                    current_agent,\n                    hooks,\n                    context_wrapper,\n                    run_config,\n                    should_run_agent_start_hooks,\n                    tool_use_tracker,\n                    all_tools,\n                    server_conversation_tracker,\n                    pending_server_items=pending_server_items,\n                    session=session,\n                    session_items_to_rewind=(\n                        streamed_result._original_input_for_persistence\n                        if session is not None and server_conversation_tracker is None\n                        else None\n                    ),\n                )\n                logger.debug(\n                    \"Turn %s complete, next_step type=%s\",\n                    current_turn,\n                    type(turn_result.next_step).__name__,\n                )\n                should_run_agent_start_hooks = False\n                streamed_result._tool_use_tracker_snapshot = serialize_tool_use_tracker(\n                    tool_use_tracker\n                )\n\n                streamed_result.raw_responses = streamed_result.raw_responses + [\n                    turn_result.model_response\n                ]\n                streamed_result.input = turn_result.original_input\n                if isinstance(turn_result.next_step, NextStepHandoff):\n                    streamed_result._original_input = copy_input_items(turn_result.original_input)\n                    if run_state is not None:\n                        run_state._original_input = copy_input_items(turn_result.original_input)\n                streamed_result._model_input_items = (\n                    turn_result.pre_step_items + turn_result.new_step_items\n                )\n                turn_session_items = session_items_for_turn(turn_result)\n                streamed_result.new_items.extend(turn_session_items)\n                store_setting = current_agent.model_settings.resolve(\n                    run_config.model_settings\n                ).store\n                if server_conversation_tracker is not None:\n                    pending_server_items = list(turn_result.new_step_items)\n\n                if isinstance(turn_result.next_step, NextStepRunAgain):\n                    streamed_result._current_turn_persisted_item_count = 0\n                    if run_state:\n                        run_state._current_turn_persisted_item_count = 0\n\n                if server_conversation_tracker is not None:\n                    server_conversation_tracker.track_server_items(turn_result.model_response)\n\n                if isinstance(turn_result.next_step, NextStepHandoff):\n                    await _save_stream_items_without_count(\n                        turn_session_items,\n                        turn_result.model_response.response_id,\n                        store_setting,\n                    )\n                    current_agent = turn_result.next_step.new_agent\n                    if run_state is not None:\n                        run_state._current_agent = current_agent\n                    current_span.finish(reset_current=True)\n                    current_span = None\n                    should_run_agent_start_hooks = True\n                    streamed_result._event_queue.put_nowait(\n                        AgentUpdatedStreamEvent(new_agent=current_agent)\n                    )\n                    if streamed_result._state is not None:\n                        streamed_result._state._current_step = NextStepRunAgain()\n\n                    if streamed_result._cancel_mode == \"after_turn\":  # type: ignore[comparison-overlap]\n                        streamed_result.is_complete = True\n                        streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n                        break\n                elif isinstance(turn_result.next_step, NextStepFinalOutput):\n                    await _finalize_streamed_final_output(\n                        streamed_result=streamed_result,\n                        agent=current_agent,\n                        run_config=run_config,\n                        output=turn_result.next_step.output,\n                        context_wrapper=context_wrapper,\n                        save_items=_save_stream_items_with_count,\n                        items=turn_session_items,\n                        response_id=turn_result.model_response.response_id,\n                        store_setting=store_setting,\n                    )\n                    break\n                elif isinstance(turn_result.next_step, NextStepInterruption):\n                    processed_response_for_state = turn_result.processed_response\n                    if processed_response_for_state is None and run_state is not None:\n                        processed_response_for_state = run_state._last_processed_response\n                    if run_state is not None:\n                        run_state._model_responses = streamed_result.raw_responses\n                        run_state._last_processed_response = processed_response_for_state\n                        run_state._generated_items = streamed_result._model_input_items\n                        run_state._session_items = list(streamed_result.new_items)\n                        run_state._current_step = turn_result.next_step\n                        run_state._current_turn = current_turn\n                        run_state._current_turn_persisted_item_count = (\n                            streamed_result._current_turn_persisted_item_count\n                        )\n                    await _finalize_streamed_interruption(\n                        streamed_result=streamed_result,\n                        save_items=_save_stream_items_with_count,\n                        items=turn_session_items,\n                        response_id=turn_result.model_response.response_id,\n                        store_setting=store_setting,\n                        interruptions=approvals_from_step(turn_result.next_step),\n                        processed_response=processed_response_for_state,\n                    )\n                    break\n                elif isinstance(turn_result.next_step, NextStepRunAgain):\n                    if streamed_result._state is not None:\n                        streamed_result._state._current_step = NextStepRunAgain()\n\n                    if streamed_result._cancel_mode == \"after_turn\":  # type: ignore[comparison-overlap]\n                        streamed_result.is_complete = True\n                        streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n                        break\n            except Exception as e:\n                if current_span and not isinstance(e, ModelBehaviorError):\n                    _error_tracing.attach_error_to_span(\n                        current_span,\n                        SpanError(\n                            message=\"Error in agent run\",\n                            data={\"error\": str(e)},\n                        ),\n                    )\n                raise\n    except AgentsException as exc:\n        streamed_result.is_complete = True\n        streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n        exc.run_data = RunErrorDetails(\n            input=streamed_result.input,\n            new_items=streamed_result.new_items,\n            raw_responses=streamed_result.raw_responses,\n            last_agent=current_agent,\n            context_wrapper=context_wrapper,\n            input_guardrail_results=streamed_result.input_guardrail_results,\n            output_guardrail_results=streamed_result.output_guardrail_results,\n        )\n        raise\n    except Exception as e:\n        if current_span and not isinstance(e, ModelBehaviorError):\n            _error_tracing.attach_error_to_span(\n                current_span,\n                SpanError(\n                    message=\"Error in agent run\",\n                    data={\"error\": str(e)},\n                ),\n            )\n        streamed_result.is_complete = True\n        streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n        raise\n    else:\n        streamed_result.is_complete = True\n    finally:\n        _sync_conversation_tracking_from_tracker()\n        if streamed_result._input_guardrails_task:\n            try:\n                triggered = await input_guardrail_tripwire_triggered_for_stream(streamed_result)\n                if triggered:\n                    first_trigger = next(\n                        (\n                            result\n                            for result in streamed_result.input_guardrail_results\n                            if result.output.tripwire_triggered\n                        ),\n                        None,\n                    )\n                    if first_trigger is not None:\n                        raise InputGuardrailTripwireTriggered(first_trigger)\n            except Exception as e:\n                logger.debug(\n                    f\"Error in streamed_result finalize for agent {current_agent.name} - {e}\"\n                )\n        try:\n            await dispose_resolved_computers(run_context=context_wrapper)\n        except Exception as error:\n            logger.warning(\"Failed to dispose computers after streamed run: %s\", error)\n        if current_span:\n            current_span.finish(reset_current=True)\n        if streamed_result.trace:\n            streamed_result.trace.finish(reset_current=True)\n\n        if not streamed_result.is_complete:\n            streamed_result.is_complete = True\n            streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.run_single_turn_streamed","title":"run_single_turn_streamed  <code>async</code>","text":"<pre><code>run_single_turn_streamed(\n    streamed_result: RunResultStreaming,\n    agent: Agent[TContext],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    should_run_agent_start_hooks: bool,\n    tool_use_tracker: AgentToolUseTracker,\n    all_tools: list[Tool],\n    server_conversation_tracker: OpenAIServerConversationTracker\n    | None = None,\n    session: Session | None = None,\n    session_items_to_rewind: list[TResponseInputItem]\n    | None = None,\n    pending_server_items: list[RunItem] | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Run a single streamed turn and emit events as results arrive.</p> Source code in <code>src/agents/run_internal/run_loop.py</code> <pre><code>async def run_single_turn_streamed(\n    streamed_result: RunResultStreaming,\n    agent: Agent[TContext],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    should_run_agent_start_hooks: bool,\n    tool_use_tracker: AgentToolUseTracker,\n    all_tools: list[Tool],\n    server_conversation_tracker: OpenAIServerConversationTracker | None = None,\n    session: Session | None = None,\n    session_items_to_rewind: list[TResponseInputItem] | None = None,\n    pending_server_items: list[RunItem] | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Run a single streamed turn and emit events as results arrive.\"\"\"\n    emitted_tool_call_ids: set[str] = set()\n    emitted_reasoning_item_ids: set[str] = set()\n    # Precompute tool name -&gt; tool map once per turn. Dict \"last wins\" semantics match\n    # execution in process_model_response, so duplicate names (e.g., MCP + local tool)\n    # stream the same description that execution uses.\n    tool_map = {t.name: t for t in all_tools if hasattr(t, \"name\") and t.name}\n\n    try:\n        turn_input = ItemHelpers.input_to_new_input_list(streamed_result.input)\n    except Exception:\n        turn_input = []\n    context_wrapper.turn_input = list(turn_input)\n\n    if should_run_agent_start_hooks:\n        agent_hook_context = AgentHookContext(\n            context=context_wrapper.context,\n            usage=context_wrapper.usage,\n            _approvals=context_wrapper._approvals,\n            turn_input=turn_input,\n        )\n        await asyncio.gather(\n            hooks.on_agent_start(agent_hook_context, agent),\n            (\n                agent.hooks.on_start(agent_hook_context, agent)\n                if agent.hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n    output_schema = get_output_schema(agent)\n\n    streamed_result.current_agent = agent\n    streamed_result._current_agent_output_schema = output_schema\n\n    system_prompt, prompt_config = await asyncio.gather(\n        agent.get_system_prompt(context_wrapper),\n        agent.get_prompt(context_wrapper),\n    )\n\n    handoffs = await get_handoffs(agent, context_wrapper)\n    model = get_model(agent, run_config)\n    model_settings = agent.model_settings.resolve(run_config.model_settings)\n    model_settings = maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\n    final_response: ModelResponse | None = None\n\n    if server_conversation_tracker is not None:\n        items_for_input = (\n            pending_server_items if pending_server_items else streamed_result._model_input_items\n        )\n        input = server_conversation_tracker.prepare_input(streamed_result.input, items_for_input)\n        logger.debug(\n            \"prepare_input returned %s items; remaining_initial_input=%s\",\n            len(input),\n            len(server_conversation_tracker.remaining_initial_input)\n            if server_conversation_tracker.remaining_initial_input\n            else 0,\n        )\n    else:\n        input = ItemHelpers.input_to_new_input_list(streamed_result.input)\n        append_input_items_excluding_approvals(input, streamed_result._model_input_items)\n\n    if isinstance(input, list):\n        input = normalize_input_items_for_api(input)\n\n    filtered = await maybe_filter_model_input(\n        agent=agent,\n        run_config=run_config,\n        context_wrapper=context_wrapper,\n        input_items=input,\n        system_instructions=system_prompt,\n    )\n    if isinstance(filtered.input, list):\n        filtered.input = deduplicate_input_items_preferring_latest(filtered.input)\n    if server_conversation_tracker is not None:\n        logger.debug(\n            \"filtered.input has %s items; ids=%s\",\n            len(filtered.input),\n            [id(i) for i in filtered.input],\n        )\n        # Track only the items actually sent after call_model_input_filter runs.\n        server_conversation_tracker.mark_input_as_sent(filtered.input)\n    if not filtered.input and server_conversation_tracker is None:\n        raise RuntimeError(\"Prepared model input is empty\")\n\n    await asyncio.gather(\n        hooks.on_llm_start(context_wrapper, agent, filtered.instructions, filtered.input),\n        (\n            agent.hooks.on_llm_start(context_wrapper, agent, filtered.instructions, filtered.input)\n            if agent.hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    if (\n        not streamed_result._stream_input_persisted\n        and session is not None\n        and server_conversation_tracker is None\n        and streamed_result._original_input_for_persistence\n        and len(streamed_result._original_input_for_persistence) &gt; 0\n    ):\n        streamed_result._stream_input_persisted = True\n        input_items_to_save = [\n            ensure_input_item_format(item)\n            for item in ItemHelpers.input_to_new_input_list(\n                streamed_result._original_input_for_persistence\n            )\n        ]\n        if input_items_to_save:\n            await save_result_to_session(session, input_items_to_save, [], streamed_result._state)\n\n    previous_response_id = (\n        server_conversation_tracker.previous_response_id\n        if server_conversation_tracker\n        and server_conversation_tracker.previous_response_id is not None\n        else None\n    )\n    conversation_id = (\n        server_conversation_tracker.conversation_id if server_conversation_tracker else None\n    )\n    if conversation_id:\n        logger.debug(\"Using conversation_id=%s\", conversation_id)\n    else:\n        logger.debug(\"No conversation_id available for request\")\n\n    async for event in model.stream_response(\n        filtered.instructions,\n        filtered.input,\n        model_settings,\n        all_tools,\n        output_schema,\n        handoffs,\n        get_model_tracing_impl(\n            run_config.tracing_disabled, run_config.trace_include_sensitive_data\n        ),\n        previous_response_id=previous_response_id,\n        conversation_id=conversation_id,\n        prompt=prompt_config,\n    ):\n        streamed_result._event_queue.put_nowait(RawResponsesStreamEvent(data=event))\n\n        if isinstance(event, ResponseCompletedEvent):\n            usage = (\n                Usage(\n                    requests=1,\n                    input_tokens=event.response.usage.input_tokens,\n                    output_tokens=event.response.usage.output_tokens,\n                    total_tokens=event.response.usage.total_tokens,\n                    input_tokens_details=event.response.usage.input_tokens_details,\n                    output_tokens_details=event.response.usage.output_tokens_details,\n                )\n                if event.response.usage\n                else Usage()\n            )\n            final_response = ModelResponse(\n                output=event.response.output,\n                usage=usage,\n                response_id=event.response.id,\n            )\n            context_wrapper.usage.add(usage)\n\n        if isinstance(event, ResponseOutputItemDoneEvent):\n            output_item = event.item\n\n            if isinstance(output_item, TOOL_CALL_TYPES):\n                output_call_id: str | None = getattr(\n                    output_item, \"call_id\", getattr(output_item, \"id\", None)\n                )\n\n                if (\n                    output_call_id\n                    and isinstance(output_call_id, str)\n                    and output_call_id not in emitted_tool_call_ids\n                ):\n                    emitted_tool_call_ids.add(output_call_id)\n\n                    # Look up tool description from precomputed map (\"last wins\" matches\n                    # execution behavior in process_model_response).\n                    tool_name = getattr(output_item, \"name\", None)\n                    tool_description: str | None = None\n                    if isinstance(tool_name, str) and tool_name in tool_map:\n                        tool_description = getattr(tool_map[tool_name], \"description\", None)\n\n                    tool_item = ToolCallItem(\n                        raw_item=cast(ToolCallItemTypes, output_item),\n                        agent=agent,\n                        description=tool_description,\n                    )\n                    streamed_result._event_queue.put_nowait(\n                        RunItemStreamEvent(item=tool_item, name=\"tool_called\")\n                    )\n\n            elif isinstance(output_item, ResponseReasoningItem):\n                reasoning_id: str | None = getattr(output_item, \"id\", None)\n\n                if reasoning_id and reasoning_id not in emitted_reasoning_item_ids:\n                    emitted_reasoning_item_ids.add(reasoning_id)\n\n                    reasoning_item = ReasoningItem(raw_item=output_item, agent=agent)\n                    streamed_result._event_queue.put_nowait(\n                        RunItemStreamEvent(item=reasoning_item, name=\"reasoning_item_created\")\n                    )\n\n    if final_response is not None:\n        await asyncio.gather(\n            (\n                agent.hooks.on_llm_end(context_wrapper, agent, final_response)\n                if agent.hooks\n                else _coro.noop_coroutine()\n            ),\n            hooks.on_llm_end(context_wrapper, agent, final_response),\n        )\n\n    if not final_response:\n        raise ModelBehaviorError(\"Model did not produce a final response!\")\n\n    if server_conversation_tracker is not None:\n        server_conversation_tracker.track_server_items(final_response)\n\n    single_step_result = await get_single_step_result_from_response(\n        agent=agent,\n        original_input=streamed_result.input,\n        pre_step_items=streamed_result._model_input_items,\n        new_response=final_response,\n        output_schema=output_schema,\n        all_tools=all_tools,\n        handoffs=handoffs,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        run_config=run_config,\n        tool_use_tracker=tool_use_tracker,\n        event_queue=streamed_result._event_queue,\n    )\n\n    items_to_filter = session_items_for_turn(single_step_result)\n\n    if emitted_tool_call_ids:\n        items_to_filter = [\n            item\n            for item in items_to_filter\n            if not (\n                isinstance(item, ToolCallItem)\n                and (\n                    call_id := getattr(item.raw_item, \"call_id\", getattr(item.raw_item, \"id\", None))\n                )\n                and call_id in emitted_tool_call_ids\n            )\n        ]\n\n    if emitted_reasoning_item_ids:\n        items_to_filter = [\n            item\n            for item in items_to_filter\n            if not (\n                isinstance(item, ReasoningItem)\n                and (reasoning_id := getattr(item.raw_item, \"id\", None))\n                and reasoning_id in emitted_reasoning_item_ids\n            )\n        ]\n\n    items_to_filter = [item for item in items_to_filter if not isinstance(item, HandoffCallItem)]\n\n    filtered_result = _dc.replace(single_step_result, new_step_items=items_to_filter)\n    stream_step_result_to_queue(filtered_result, streamed_result._event_queue)\n    return single_step_result\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.run_single_turn","title":"run_single_turn  <code>async</code>","text":"<pre><code>run_single_turn(\n    *,\n    agent: Agent[TContext],\n    all_tools: list[Tool],\n    original_input: str | list[TResponseInputItem],\n    generated_items: list[RunItem],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    should_run_agent_start_hooks: bool,\n    tool_use_tracker: AgentToolUseTracker,\n    server_conversation_tracker: OpenAIServerConversationTracker\n    | None = None,\n    session: Session | None = None,\n    session_items_to_rewind: list[TResponseInputItem]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Run a single non-streaming turn of the agent loop.</p> Source code in <code>src/agents/run_internal/run_loop.py</code> <pre><code>async def run_single_turn(\n    *,\n    agent: Agent[TContext],\n    all_tools: list[Tool],\n    original_input: str | list[TResponseInputItem],\n    generated_items: list[RunItem],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    should_run_agent_start_hooks: bool,\n    tool_use_tracker: AgentToolUseTracker,\n    server_conversation_tracker: OpenAIServerConversationTracker | None = None,\n    session: Session | None = None,\n    session_items_to_rewind: list[TResponseInputItem] | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Run a single non-streaming turn of the agent loop.\"\"\"\n    try:\n        turn_input = ItemHelpers.input_to_new_input_list(original_input)\n    except Exception:\n        turn_input = []\n    context_wrapper.turn_input = list(turn_input)\n\n    if should_run_agent_start_hooks:\n        agent_hook_context = AgentHookContext(\n            context=context_wrapper.context,\n            usage=context_wrapper.usage,\n            _approvals=context_wrapper._approvals,\n            turn_input=turn_input,\n        )\n        await asyncio.gather(\n            hooks.on_agent_start(agent_hook_context, agent),\n            (\n                agent.hooks.on_start(agent_hook_context, agent)\n                if agent.hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n    system_prompt, prompt_config = await asyncio.gather(\n        agent.get_system_prompt(context_wrapper),\n        agent.get_prompt(context_wrapper),\n    )\n\n    output_schema = get_output_schema(agent)\n    handoffs = await get_handoffs(agent, context_wrapper)\n    if server_conversation_tracker is not None:\n        input = server_conversation_tracker.prepare_input(original_input, generated_items)\n    else:\n        input = ItemHelpers.input_to_new_input_list(original_input)\n        if isinstance(input, list):\n            append_input_items_excluding_approvals(input, generated_items)\n        else:\n            input = ItemHelpers.input_to_new_input_list(input)\n            append_input_items_excluding_approvals(input, generated_items)\n\n    if isinstance(input, list):\n        input = normalize_input_items_for_api(input)\n\n    new_response = await get_new_response(\n        agent,\n        system_prompt,\n        input,\n        output_schema,\n        all_tools,\n        handoffs,\n        hooks,\n        context_wrapper,\n        run_config,\n        tool_use_tracker,\n        server_conversation_tracker,\n        prompt_config,\n        session=session,\n        session_items_to_rewind=session_items_to_rewind,\n    )\n\n    return await get_single_step_result_from_response(\n        agent=agent,\n        original_input=original_input,\n        pre_step_items=generated_items,\n        new_response=new_response,\n        output_schema=output_schema,\n        all_tools=all_tools,\n        handoffs=handoffs,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        run_config=run_config,\n        tool_use_tracker=tool_use_tracker,\n    )\n</code></pre>"},{"location":"ref/run_internal/run_loop/#agents.run_internal.run_loop.get_new_response","title":"get_new_response  <code>async</code>","text":"<pre><code>get_new_response(\n    agent: Agent[TContext],\n    system_prompt: str | None,\n    input: list[TResponseInputItem],\n    output_schema: AgentOutputSchemaBase | None,\n    all_tools: list[Tool],\n    handoffs: list[Handoff],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    tool_use_tracker: AgentToolUseTracker,\n    server_conversation_tracker: OpenAIServerConversationTracker\n    | None,\n    prompt_config: ResponsePromptParam | None,\n    session: Session | None = None,\n    session_items_to_rewind: list[TResponseInputItem]\n    | None = None,\n) -&gt; ModelResponse\n</code></pre> <p>Call the model and return the raw response, handling retries and hooks.</p> Source code in <code>src/agents/run_internal/run_loop.py</code> <pre><code>async def get_new_response(\n    agent: Agent[TContext],\n    system_prompt: str | None,\n    input: list[TResponseInputItem],\n    output_schema: AgentOutputSchemaBase | None,\n    all_tools: list[Tool],\n    handoffs: list[Handoff],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    tool_use_tracker: AgentToolUseTracker,\n    server_conversation_tracker: OpenAIServerConversationTracker | None,\n    prompt_config: ResponsePromptParam | None,\n    session: Session | None = None,\n    session_items_to_rewind: list[TResponseInputItem] | None = None,\n) -&gt; ModelResponse:\n    \"\"\"Call the model and return the raw response, handling retries and hooks.\"\"\"\n    filtered = await maybe_filter_model_input(\n        agent=agent,\n        run_config=run_config,\n        context_wrapper=context_wrapper,\n        input_items=input,\n        system_instructions=system_prompt,\n    )\n    if isinstance(filtered.input, list):\n        filtered.input = deduplicate_input_items_preferring_latest(filtered.input)\n\n    if server_conversation_tracker is not None:\n        server_conversation_tracker.mark_input_as_sent(filtered.input)\n\n    model = get_model(agent, run_config)\n    model_settings = agent.model_settings.resolve(run_config.model_settings)\n    model_settings = maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\n    await asyncio.gather(\n        hooks.on_llm_start(context_wrapper, agent, filtered.instructions, filtered.input),\n        (\n            agent.hooks.on_llm_start(\n                context_wrapper,\n                agent,\n                filtered.instructions,\n                filtered.input,\n            )\n            if agent.hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    previous_response_id = (\n        server_conversation_tracker.previous_response_id\n        if server_conversation_tracker\n        and server_conversation_tracker.previous_response_id is not None\n        else None\n    )\n    conversation_id = (\n        server_conversation_tracker.conversation_id if server_conversation_tracker else None\n    )\n    if conversation_id:\n        logger.debug(\"Using conversation_id=%s\", conversation_id)\n    else:\n        logger.debug(\"No conversation_id available for request\")\n\n    try:\n        new_response = await model.get_response(\n            system_instructions=filtered.instructions,\n            input=filtered.input,\n            model_settings=model_settings,\n            tools=all_tools,\n            output_schema=output_schema,\n            handoffs=handoffs,\n            tracing=get_model_tracing_impl(\n                run_config.tracing_disabled, run_config.trace_include_sensitive_data\n            ),\n            previous_response_id=previous_response_id,\n            conversation_id=conversation_id,\n            prompt=prompt_config,\n        )\n    except Exception as exc:\n        from openai import BadRequestError\n\n        if isinstance(exc, BadRequestError) and getattr(exc, \"code\", \"\") == \"conversation_locked\":\n            max_retries = 3\n            last_exception = exc\n            for attempt in range(max_retries):\n                wait_time = 1.0 * (2**attempt)\n                logger.debug(\n                    \"Conversation locked, retrying in %ss (attempt %s/%s)\",\n                    wait_time,\n                    attempt + 1,\n                    max_retries,\n                )\n                await asyncio.sleep(wait_time)\n                items_to_rewind = (\n                    session_items_to_rewind if session_items_to_rewind is not None else []\n                )\n                await rewind_session_items(session, items_to_rewind, server_conversation_tracker)\n                if server_conversation_tracker is not None:\n                    server_conversation_tracker.rewind_input(filtered.input)\n                try:\n                    new_response = await model.get_response(\n                        system_instructions=filtered.instructions,\n                        input=filtered.input,\n                        model_settings=model_settings,\n                        tools=all_tools,\n                        output_schema=output_schema,\n                        handoffs=handoffs,\n                        tracing=get_model_tracing_impl(\n                            run_config.tracing_disabled, run_config.trace_include_sensitive_data\n                        ),\n                        previous_response_id=previous_response_id,\n                        conversation_id=conversation_id,\n                        prompt=prompt_config,\n                    )\n                    break\n                except BadRequestError as retry_exc:\n                    last_exception = retry_exc\n                    if (\n                        getattr(retry_exc, \"code\", \"\") == \"conversation_locked\"\n                        and attempt &lt; max_retries - 1\n                    ):\n                        continue\n                    else:\n                        raise\n            else:\n                logger.error(\n                    \"Conversation locked after all retries; filtered.input=%s\", filtered.input\n                )\n                raise last_exception\n        else:\n            logger.error(\"Error getting response; filtered.input=%s\", filtered.input)\n            raise\n\n    context_wrapper.usage.add(new_response.usage)\n\n    await asyncio.gather(\n        (\n            agent.hooks.on_llm_end(context_wrapper, agent, new_response)\n            if agent.hooks\n            else _coro.noop_coroutine()\n        ),\n        hooks.on_llm_end(context_wrapper, agent, new_response),\n    )\n\n    return new_response\n</code></pre>"},{"location":"ref/run_internal/run_steps/","title":"<code>Run Steps</code>","text":"<p>Internal step/result data structures used by the run loop orchestration. These types are not part of the public SDK surface.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.QueueCompleteSentinel","title":"QueueCompleteSentinel","text":"<p>Sentinel used to signal completion when streaming run loop results.</p> Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>class QueueCompleteSentinel:\n    \"\"\"Sentinel used to signal completion when streaming run loop results.\"\"\"\n</code></pre>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.ProcessedResponse","title":"ProcessedResponse  <code>dataclass</code>","text":"Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>@dataclass\nclass ProcessedResponse:\n    new_items: list[RunItem]\n    handoffs: list[ToolRunHandoff]\n    functions: list[ToolRunFunction]\n    computer_actions: list[ToolRunComputerAction]\n    local_shell_calls: list[ToolRunLocalShellCall]\n    shell_calls: list[ToolRunShellCall]\n    apply_patch_calls: list[ToolRunApplyPatchCall]\n    tools_used: list[str]  # Names of all tools used, including hosted tools\n    mcp_approval_requests: list[ToolRunMCPApprovalRequest]  # Only requests with callbacks\n    interruptions: list[ToolApprovalItem]  # Tool approval items awaiting user decision\n\n    def has_tools_or_approvals_to_run(self) -&gt; bool:\n        # Handoffs, functions and computer actions need local processing\n        # Hosted tools have already run, so there's nothing to do.\n        return any(\n            [\n                self.handoffs,\n                self.functions,\n                self.computer_actions,\n                self.local_shell_calls,\n                self.shell_calls,\n                self.apply_patch_calls,\n                self.mcp_approval_requests,\n            ]\n        )\n\n    def has_interruptions(self) -&gt; bool:\n        \"\"\"Check if there are tool calls awaiting approval.\"\"\"\n        return len(self.interruptions) &gt; 0\n</code></pre>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.ProcessedResponse.has_interruptions","title":"has_interruptions","text":"<pre><code>has_interruptions() -&gt; bool\n</code></pre> <p>Check if there are tool calls awaiting approval.</p> Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>def has_interruptions(self) -&gt; bool:\n    \"\"\"Check if there are tool calls awaiting approval.\"\"\"\n    return len(self.interruptions) &gt; 0\n</code></pre>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.NextStepInterruption","title":"NextStepInterruption  <code>dataclass</code>","text":"<p>Represents an interruption in the agent run due to tool approval requests.</p> Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>@dataclass\nclass NextStepInterruption:\n    \"\"\"Represents an interruption in the agent run due to tool approval requests.\"\"\"\n\n    interruptions: list[ToolApprovalItem]\n    \"\"\"The list of tool calls awaiting approval.\"\"\"\n</code></pre>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.NextStepInterruption.interruptions","title":"interruptions  <code>instance-attribute</code>","text":"<pre><code>interruptions: list[ToolApprovalItem]\n</code></pre> <p>The list of tool calls awaiting approval.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult","title":"SingleStepResult  <code>dataclass</code>","text":"Source code in <code>src/agents/run_internal/run_steps.py</code> <pre><code>@dataclass\nclass SingleStepResult:\n    original_input: str | list[TResponseInputItem]\n    \"\"\"The input items i.e. the items before run() was called. May be mutated by handoff input\n    filters.\"\"\"\n\n    model_response: ModelResponse\n    \"\"\"The model response for the current step.\"\"\"\n\n    pre_step_items: list[RunItem]\n    \"\"\"Items generated before the current step.\"\"\"\n\n    new_step_items: list[RunItem]\n    \"\"\"Items generated during this current step.\"\"\"\n\n    next_step: NextStepHandoff | NextStepFinalOutput | NextStepRunAgain | NextStepInterruption\n    \"\"\"The next step to take.\"\"\"\n\n    tool_input_guardrail_results: list[ToolInputGuardrailResult]\n    \"\"\"Tool input guardrail results from this step.\"\"\"\n\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult]\n    \"\"\"Tool output guardrail results from this step.\"\"\"\n\n    session_step_items: list[RunItem] | None = None\n    \"\"\"Full unfiltered items for session history. When set, these are used instead of\n    new_step_items for session saving and generated_items property.\"\"\"\n\n    output_guardrail_results: list[OutputGuardrailResult] = dataclasses.field(default_factory=list)\n    \"\"\"Output guardrail results (populated when a final output is produced).\"\"\"\n\n    processed_response: ProcessedResponse | None = None\n    \"\"\"The processed model response. This is needed for resuming from interruptions.\"\"\"\n\n    @property\n    def generated_items(self) -&gt; list[RunItem]:\n        \"\"\"Items generated during the agent run (i.e. everything generated after\n        `original_input`). Uses session_step_items when available for full observability.\"\"\"\n        items = (\n            self.session_step_items if self.session_step_items is not None else self.new_step_items\n        )\n        return self.pre_step_items + items\n</code></pre>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.original_input","title":"original_input  <code>instance-attribute</code>","text":"<pre><code>original_input: str | list[TResponseInputItem]\n</code></pre> <p>The input items i.e. the items before run() was called. May be mutated by handoff input filters.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.model_response","title":"model_response  <code>instance-attribute</code>","text":"<pre><code>model_response: ModelResponse\n</code></pre> <p>The model response for the current step.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.pre_step_items","title":"pre_step_items  <code>instance-attribute</code>","text":"<pre><code>pre_step_items: list[RunItem]\n</code></pre> <p>Items generated before the current step.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.new_step_items","title":"new_step_items  <code>instance-attribute</code>","text":"<pre><code>new_step_items: list[RunItem]\n</code></pre> <p>Items generated during this current step.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.next_step","title":"next_step  <code>instance-attribute</code>","text":"<pre><code>next_step: (\n    NextStepHandoff\n    | NextStepFinalOutput\n    | NextStepRunAgain\n    | NextStepInterruption\n)\n</code></pre> <p>The next step to take.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.tool_input_guardrail_results","title":"tool_input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_input_guardrail_results: list[ToolInputGuardrailResult]\n</code></pre> <p>Tool input guardrail results from this step.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.tool_output_guardrail_results","title":"tool_output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>tool_output_guardrail_results: list[\n    ToolOutputGuardrailResult\n]\n</code></pre> <p>Tool output guardrail results from this step.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.session_step_items","title":"session_step_items  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>session_step_items: list[RunItem] | None = None\n</code></pre> <p>Full unfiltered items for session history. When set, these are used instead of new_step_items for session saving and generated_items property.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.output_guardrail_results","title":"output_guardrail_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult] = (\n    field(default_factory=list)\n)\n</code></pre> <p>Output guardrail results (populated when a final output is produced).</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.processed_response","title":"processed_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>processed_response: ProcessedResponse | None = None\n</code></pre> <p>The processed model response. This is needed for resuming from interruptions.</p>"},{"location":"ref/run_internal/run_steps/#agents.run_internal.run_steps.SingleStepResult.generated_items","title":"generated_items  <code>property</code>","text":"<pre><code>generated_items: list[RunItem]\n</code></pre> <p>Items generated during the agent run (i.e. everything generated after <code>original_input</code>). Uses session_step_items when available for full observability.</p>"},{"location":"ref/run_internal/session_persistence/","title":"<code>Session Persistence</code>","text":"<p>Session persistence helpers for the run pipeline. Only internal persistence/retry helpers live here; public session interfaces stay in higher-level modules.</p>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.prepare_input_with_session","title":"prepare_input_with_session  <code>async</code>","text":"<pre><code>prepare_input_with_session(\n    input: str | list[TResponseInputItem],\n    session: Session | None,\n    session_input_callback: SessionInputCallback | None,\n    session_settings: SessionSettings | None = None,\n    *,\n    include_history_in_prepared_input: bool = True,\n    preserve_dropped_new_items: bool = False,\n) -&gt; tuple[\n    str | list[TResponseInputItem], list[TResponseInputItem]\n]\n</code></pre> <p>Prepare input by combining it with session history and applying the optional input callback. Returns the prepared input plus the appended items that should be persisted separately.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>async def prepare_input_with_session(\n    input: str | list[TResponseInputItem],\n    session: Session | None,\n    session_input_callback: SessionInputCallback | None,\n    session_settings: SessionSettings | None = None,\n    *,\n    include_history_in_prepared_input: bool = True,\n    preserve_dropped_new_items: bool = False,\n) -&gt; tuple[str | list[TResponseInputItem], list[TResponseInputItem]]:\n    \"\"\"\n    Prepare input by combining it with session history and applying the optional input callback.\n    Returns the prepared input plus the appended items that should be persisted separately.\n    \"\"\"\n\n    if session is None:\n        return input, []\n\n    resolved_settings = getattr(session, \"session_settings\", None) or SessionSettings()\n    if session_settings is not None:\n        resolved_settings = resolved_settings.resolve(session_settings)\n\n    if resolved_settings.limit is not None:\n        history = await session.get_items(limit=resolved_settings.limit)\n    else:\n        history = await session.get_items()\n    converted_history = [ensure_input_item_format(item) for item in history]\n\n    new_input_list = [\n        ensure_input_item_format(item) for item in ItemHelpers.input_to_new_input_list(input)\n    ]\n\n    if session_input_callback is None or not include_history_in_prepared_input:\n        prepared_items_raw: list[TResponseInputItem] = (\n            converted_history + new_input_list\n            if include_history_in_prepared_input\n            else list(new_input_list)\n        )\n        appended_items = list(new_input_list)\n    else:\n        if not callable(session_input_callback):\n            raise UserError(\n                f\"Invalid `session_input_callback` value: {session_input_callback}. \"\n                \"Choose between `None` or a custom callable function.\"\n            )\n        history_for_callback = copy.deepcopy(converted_history)\n        new_items_for_callback = copy.deepcopy(new_input_list)\n        combined = session_input_callback(history_for_callback, new_items_for_callback)\n        if inspect.isawaitable(combined):\n            combined = await combined\n        if not isinstance(combined, list):\n            raise UserError(\"Session input callback must return a list of input items.\")\n\n        history_refs = _build_reference_map(history_for_callback)\n        new_refs = _build_reference_map(new_items_for_callback)\n        history_counts = _build_frequency_map(history_for_callback)\n        new_counts = _build_frequency_map(new_items_for_callback)\n\n        appended: list[Any] = []\n        for item in combined:\n            key = _session_item_key(item)\n            if _consume_reference(new_refs, key, item):\n                new_counts[key] = max(new_counts.get(key, 0) - 1, 0)\n                appended.append(item)\n                continue\n            if _consume_reference(history_refs, key, item):\n                history_counts[key] = max(history_counts.get(key, 0) - 1, 0)\n                continue\n            if history_counts.get(key, 0) &gt; 0:\n                history_counts[key] = history_counts.get(key, 0) - 1\n                continue\n            if new_counts.get(key, 0) &gt; 0:\n                new_counts[key] = max(new_counts.get(key, 0) - 1, 0)\n                appended.append(item)\n                continue\n            appended.append(item)\n\n        appended_items = [ensure_input_item_format(item) for item in appended]\n\n        if include_history_in_prepared_input:\n            prepared_items_raw = combined\n        elif appended_items:\n            prepared_items_raw = appended_items\n        else:\n            prepared_items_raw = new_items_for_callback if preserve_dropped_new_items else []\n\n    prepared_as_inputs = [ensure_input_item_format(item) for item in prepared_items_raw]\n    filtered = drop_orphan_function_calls(prepared_as_inputs)\n    normalized = normalize_input_items_for_api(filtered)\n    deduplicated = deduplicate_input_items_preferring_latest(normalized)\n\n    return deduplicated, [ensure_input_item_format(item) for item in appended_items]\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.persist_session_items_for_guardrail_trip","title":"persist_session_items_for_guardrail_trip  <code>async</code>","text":"<pre><code>persist_session_items_for_guardrail_trip(\n    session: Session | None,\n    server_conversation_tracker: OpenAIServerConversationTracker\n    | None,\n    session_input_items_for_persistence: list[\n        TResponseInputItem\n    ]\n    | None,\n    original_user_input: str\n    | list[TResponseInputItem]\n    | None,\n    run_state: RunState | None,\n    store: bool | None = None,\n) -&gt; list[TResponseInputItem] | None\n</code></pre> <p>Persist input items when a guardrail tripwire is triggered.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>async def persist_session_items_for_guardrail_trip(\n    session: Session | None,\n    server_conversation_tracker: OpenAIServerConversationTracker | None,\n    session_input_items_for_persistence: list[TResponseInputItem] | None,\n    original_user_input: str | list[TResponseInputItem] | None,\n    run_state: RunState | None,\n    store: bool | None = None,\n) -&gt; list[TResponseInputItem] | None:\n    \"\"\"\n    Persist input items when a guardrail tripwire is triggered.\n    \"\"\"\n    if session is None or server_conversation_tracker is not None:\n        return session_input_items_for_persistence\n\n    updated_session_input_items = session_input_items_for_persistence\n    if updated_session_input_items is None and original_user_input is not None:\n        updated_session_input_items = ItemHelpers.input_to_new_input_list(original_user_input)\n\n    input_items_for_save: list[TResponseInputItem] = (\n        updated_session_input_items if updated_session_input_items is not None else []\n    )\n    await save_result_to_session(session, input_items_for_save, [], run_state, store=store)\n    return updated_session_input_items\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.session_items_for_turn","title":"session_items_for_turn","text":"<pre><code>session_items_for_turn(\n    turn_result: SingleStepResult,\n) -&gt; list[RunItem]\n</code></pre> <p>Return the items to persist for a turn, preferring session_step_items when set.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>def session_items_for_turn(turn_result: SingleStepResult) -&gt; list[RunItem]:\n    \"\"\"Return the items to persist for a turn, preferring session_step_items when set.\"\"\"\n    items = (\n        turn_result.session_step_items\n        if turn_result.session_step_items is not None\n        else turn_result.new_step_items\n    )\n    return list(items)\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.resumed_turn_items","title":"resumed_turn_items","text":"<pre><code>resumed_turn_items(\n    turn_result: SingleStepResult,\n) -&gt; tuple[list[RunItem], list[RunItem]]\n</code></pre> <p>Return generated and session items for a resumed turn.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>def resumed_turn_items(turn_result: SingleStepResult) -&gt; tuple[list[RunItem], list[RunItem]]:\n    \"\"\"Return generated and session items for a resumed turn.\"\"\"\n    generated_items = list(turn_result.pre_step_items) + list(turn_result.new_step_items)\n    turn_session_items = session_items_for_turn(turn_result)\n    return generated_items, turn_session_items\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.update_run_state_after_resume","title":"update_run_state_after_resume","text":"<pre><code>update_run_state_after_resume(\n    run_state: RunState,\n    *,\n    turn_result: SingleStepResult,\n    generated_items: list[RunItem],\n    session_items: list[RunItem] | None = None,\n) -&gt; None\n</code></pre> <p>Update run state fields after resolving an interruption.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>def update_run_state_after_resume(\n    run_state: RunState,\n    *,\n    turn_result: SingleStepResult,\n    generated_items: list[RunItem],\n    session_items: list[RunItem] | None = None,\n) -&gt; None:\n    \"\"\"Update run state fields after resolving an interruption.\"\"\"\n    run_state._original_input = copy_input_items(turn_result.original_input)\n    run_state._generated_items = generated_items\n    if session_items is not None:\n        run_state._session_items = list(session_items)\n    run_state._current_step = turn_result.next_step  # type: ignore[assignment]\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.save_result_to_session","title":"save_result_to_session  <code>async</code>","text":"<pre><code>save_result_to_session(\n    session: Session | None,\n    original_input: str | list[TResponseInputItem],\n    new_items: list[RunItem],\n    run_state: RunState | None = None,\n    *,\n    response_id: str | None = None,\n    store: bool | None = None,\n) -&gt; int\n</code></pre> <p>Persist a turn to the session store, keeping track of what was already saved so retries during streaming do not duplicate tool outputs or inputs.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of new run items persisted for this call.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>async def save_result_to_session(\n    session: Session | None,\n    original_input: str | list[TResponseInputItem],\n    new_items: list[RunItem],\n    run_state: RunState | None = None,\n    *,\n    response_id: str | None = None,\n    store: bool | None = None,\n) -&gt; int:\n    \"\"\"\n    Persist a turn to the session store, keeping track of what was already saved so retries\n    during streaming do not duplicate tool outputs or inputs.\n\n    Returns:\n        The number of new run items persisted for this call.\n    \"\"\"\n    already_persisted = run_state._current_turn_persisted_item_count if run_state else 0\n\n    if session is None:\n        return 0\n\n    new_run_items: list[RunItem]\n    if already_persisted &gt;= len(new_items):\n        new_run_items = []\n    else:\n        new_run_items = new_items[already_persisted:]\n    if run_state and new_items and new_run_items:\n        missing_outputs = [\n            item\n            for item in new_items\n            if item.type == \"tool_call_output_item\" and item not in new_run_items\n        ]\n        if missing_outputs:\n            new_run_items = missing_outputs + new_run_items\n\n    input_list: list[TResponseInputItem] = []\n    if original_input:\n        input_list = [\n            ensure_input_item_format(item)\n            for item in ItemHelpers.input_to_new_input_list(original_input)\n        ]\n\n    items_to_convert = [item for item in new_run_items if item.type != \"tool_approval_item\"]\n\n    new_items_as_input: list[TResponseInputItem] = [\n        ensure_input_item_format(item.to_input_item()) for item in items_to_convert\n    ]\n\n    is_openai_conversation_session = isinstance(session, OpenAIConversationsSession)\n    ignore_ids_for_matching = _ignore_ids_for_matching(session)\n\n    new_items_for_fingerprint = (\n        [_sanitize_openai_conversation_item(item) for item in new_items_as_input]\n        if is_openai_conversation_session\n        else new_items_as_input\n    )\n    serialized_new_items = [\n        _fingerprint_or_repr(item, ignore_ids_for_matching=ignore_ids_for_matching)\n        for item in new_items_for_fingerprint\n    ]\n\n    items_to_save = deduplicate_input_items_preferring_latest(input_list + new_items_as_input)\n\n    if is_openai_conversation_session and items_to_save:\n        items_to_save = [_sanitize_openai_conversation_item(item) for item in items_to_save]\n\n    serialized_to_save: list[str] = [\n        _fingerprint_or_repr(item, ignore_ids_for_matching=ignore_ids_for_matching)\n        for item in items_to_save\n    ]\n    serialized_to_save_counts: dict[str, int] = {}\n    for serialized in serialized_to_save:\n        serialized_to_save_counts[serialized] = serialized_to_save_counts.get(serialized, 0) + 1\n\n    saved_run_items_count = 0\n    for serialized in serialized_new_items:\n        if serialized_to_save_counts.get(serialized, 0) &gt; 0:\n            serialized_to_save_counts[serialized] -= 1\n            saved_run_items_count += 1\n\n    if len(items_to_save) == 0:\n        if run_state:\n            run_state._current_turn_persisted_item_count = already_persisted + saved_run_items_count\n        return saved_run_items_count\n\n    await session.add_items(items_to_save)\n\n    if run_state:\n        run_state._current_turn_persisted_item_count = already_persisted + saved_run_items_count\n\n    if response_id and is_openai_responses_compaction_aware_session(session):\n        has_local_tool_outputs = any(\n            isinstance(item, (ToolCallOutputItem, HandoffOutputItem)) for item in new_items\n        )\n        if has_local_tool_outputs:\n            defer_compaction = getattr(session, \"_defer_compaction\", None)\n            if callable(defer_compaction):\n                result = defer_compaction(response_id, store=store)\n                if inspect.isawaitable(result):\n                    await result\n            logger.debug(\n                \"skip: deferring compaction for response %s due to local tool outputs\",\n                response_id,\n            )\n            return saved_run_items_count\n\n        deferred_response_id = None\n        get_deferred = getattr(session, \"_get_deferred_compaction_response_id\", None)\n        if callable(get_deferred):\n            deferred_response_id = get_deferred()\n        force_compaction = deferred_response_id is not None\n        if force_compaction:\n            logger.debug(\n                \"compact: forcing for response %s after deferred %s\",\n                response_id,\n                deferred_response_id,\n            )\n        compaction_args: OpenAIResponsesCompactionArgs = {\n            \"response_id\": response_id,\n            \"force\": force_compaction,\n        }\n        if store is not None:\n            compaction_args[\"store\"] = store\n        await session.run_compaction(compaction_args)\n\n    return saved_run_items_count\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.save_resumed_turn_items","title":"save_resumed_turn_items  <code>async</code>","text":"<pre><code>save_resumed_turn_items(\n    *,\n    session: Session | None,\n    items: list[RunItem],\n    persisted_count: int,\n    response_id: str | None,\n    store: bool | None = None,\n) -&gt; int\n</code></pre> <p>Persist resumed turn items and return the updated persisted count.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>async def save_resumed_turn_items(\n    *,\n    session: Session | None,\n    items: list[RunItem],\n    persisted_count: int,\n    response_id: str | None,\n    store: bool | None = None,\n) -&gt; int:\n    \"\"\"Persist resumed turn items and return the updated persisted count.\"\"\"\n    if session is None or not items:\n        return persisted_count\n    saved_count = await save_result_to_session(\n        session,\n        [],\n        list(items),\n        None,\n        response_id=response_id,\n        store=store,\n    )\n    return persisted_count + saved_count\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.rewind_session_items","title":"rewind_session_items  <code>async</code>","text":"<pre><code>rewind_session_items(\n    session: Session | None,\n    items: Sequence[TResponseInputItem],\n    server_tracker: OpenAIServerConversationTracker\n    | None = None,\n) -&gt; None\n</code></pre> <p>Best-effort helper to roll back items recently persisted to a session when a conversation retry is needed, so we do not accumulate duplicate inputs on lock errors.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>async def rewind_session_items(\n    session: Session | None,\n    items: Sequence[TResponseInputItem],\n    server_tracker: OpenAIServerConversationTracker | None = None,\n) -&gt; None:\n    \"\"\"\n    Best-effort helper to roll back items recently persisted to a session when a conversation\n    retry is needed, so we do not accumulate duplicate inputs on lock errors.\n    \"\"\"\n    if session is None or not items:\n        return\n\n    pop_item = getattr(session, \"pop_item\", None)\n    if not callable(pop_item):\n        return\n\n    ignore_ids_for_matching = _ignore_ids_for_matching(session)\n    target_serializations: list[str] = []\n    for item in items:\n        serialized = fingerprint_input_item(item, ignore_ids_for_matching=ignore_ids_for_matching)\n        if serialized:\n            target_serializations.append(serialized)\n\n    if not target_serializations:\n        return\n\n    logger.debug(\n        \"Rewinding session items due to conversation retry (targets=%d)\",\n        len(target_serializations),\n    )\n\n    for i, target in enumerate(target_serializations):\n        logger.debug(\"Rewind target %d (first 300 chars): %s\", i, target[:300])\n\n    snapshot_serializations = target_serializations.copy()\n\n    remaining = target_serializations.copy()\n\n    while remaining:\n        try:\n            result = pop_item()\n            if inspect.isawaitable(result):\n                result = await result\n        except Exception as exc:\n            logger.warning(\"Failed to rewind session item: %s\", exc)\n            break\n        else:\n            if result is None:\n                break\n\n            popped_serialized = fingerprint_input_item(\n                result, ignore_ids_for_matching=ignore_ids_for_matching\n            )\n\n            logger.debug(\"Popped item type during rewind: %s\", type(result).__name__)\n            if popped_serialized:\n                logger.debug(\"Popped serialized (first 300 chars): %s\", popped_serialized[:300])\n            else:\n                logger.debug(\"Popped serialized: None\")\n\n            logger.debug(\"Number of remaining targets: %d\", len(remaining))\n            if remaining and popped_serialized:\n                logger.debug(\"First target (first 300 chars): %s\", remaining[0][:300])\n                logger.debug(\"Match found: %s\", popped_serialized in remaining)\n                if len(remaining) &gt; 0:\n                    first_target = remaining[0]\n                    if abs(len(first_target) - len(popped_serialized)) &lt; 50:\n                        logger.debug(\n                            \"Length comparison - popped: %d, target: %d\",\n                            len(popped_serialized),\n                            len(first_target),\n                        )\n\n            if popped_serialized and popped_serialized in remaining:\n                remaining.remove(popped_serialized)\n\n    if remaining:\n        logger.warning(\n            \"Unable to fully rewind session; %d items still unmatched after retry\",\n            len(remaining),\n        )\n    else:\n        await wait_for_session_cleanup(\n            session,\n            snapshot_serializations,\n            ignore_ids_for_matching=ignore_ids_for_matching,\n        )\n\n    if session is None or server_tracker is None:\n        return\n\n    try:\n        latest_items = await session.get_items(limit=1)\n    except Exception as exc:\n        logger.debug(\"Failed to peek session items while rewinding: %s\", exc)\n        return\n\n    if not latest_items:\n        return\n\n    latest_id = latest_items[0].get(\"id\")\n    if isinstance(latest_id, str) and latest_id in server_tracker.server_item_ids:\n        return\n\n    logger.debug(\"Stripping stray conversation items until we reach a known server item\")\n    while True:\n        try:\n            result = pop_item()\n            if inspect.isawaitable(result):\n                result = await result\n        except Exception as exc:\n            logger.warning(\"Failed to strip stray session item: %s\", exc)\n            break\n\n        if result is None:\n            break\n\n        stripped_id = result.get(\"id\") if isinstance(result, dict) else getattr(result, \"id\", None)\n        if isinstance(stripped_id, str) and stripped_id in server_tracker.server_item_ids:\n            break\n</code></pre>"},{"location":"ref/run_internal/session_persistence/#agents.run_internal.session_persistence.wait_for_session_cleanup","title":"wait_for_session_cleanup  <code>async</code>","text":"<pre><code>wait_for_session_cleanup(\n    session: Session | None,\n    serialized_targets: Sequence[str],\n    *,\n    max_attempts: int = 5,\n    ignore_ids_for_matching: bool = False,\n) -&gt; None\n</code></pre> <p>Confirm that rewound items are no longer present in the session tail so the store stays consistent before the next retry attempt begins.</p> Source code in <code>src/agents/run_internal/session_persistence.py</code> <pre><code>async def wait_for_session_cleanup(\n    session: Session | None,\n    serialized_targets: Sequence[str],\n    *,\n    max_attempts: int = 5,\n    ignore_ids_for_matching: bool = False,\n) -&gt; None:\n    \"\"\"\n    Confirm that rewound items are no longer present in the session tail so the store stays\n    consistent before the next retry attempt begins.\n    \"\"\"\n    if session is None or not serialized_targets:\n        return\n\n    window = len(serialized_targets) + 2\n\n    for attempt in range(max_attempts):\n        try:\n            tail_items = await session.get_items(limit=window)\n        except Exception as exc:\n            logger.debug(\"Failed to verify session cleanup (attempt %d): %s\", attempt + 1, exc)\n            await asyncio.sleep(0.1 * (attempt + 1))\n            continue\n\n        serialized_tail: set[str] = set()\n        for item in tail_items:\n            serialized = fingerprint_input_item(\n                item, ignore_ids_for_matching=ignore_ids_for_matching\n            )\n            if serialized:\n                serialized_tail.add(serialized)\n\n        if not any(serial in serialized_tail for serial in serialized_targets):\n            return\n\n        await asyncio.sleep(0.1 * (attempt + 1))\n\n    logger.debug(\n        \"Session cleanup verification exhausted attempts; targets may still linger temporarily\"\n    )\n</code></pre>"},{"location":"ref/run_internal/streaming/","title":"<code>Streaming</code>","text":""},{"location":"ref/run_internal/streaming/#agents.run_internal.streaming.stream_step_items_to_queue","title":"stream_step_items_to_queue","text":"<pre><code>stream_step_items_to_queue(\n    new_step_items: list[RunItem],\n    queue: Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None\n</code></pre> <p>Emit run items as streaming events, skipping approval placeholders.</p> Source code in <code>src/agents/run_internal/streaming.py</code> <pre><code>def stream_step_items_to_queue(\n    new_step_items: list[RunItem],\n    queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None:\n    \"\"\"Emit run items as streaming events, skipping approval placeholders.\"\"\"\n    for item in new_step_items:\n        if isinstance(item, MessageOutputItem):\n            event = RunItemStreamEvent(item=item, name=\"message_output_created\")\n        elif isinstance(item, HandoffCallItem):\n            event = RunItemStreamEvent(item=item, name=\"handoff_requested\")\n        elif isinstance(item, HandoffOutputItem):\n            event = RunItemStreamEvent(item=item, name=\"handoff_occured\")\n        elif isinstance(item, ToolCallItem):\n            event = RunItemStreamEvent(item=item, name=\"tool_called\")\n        elif isinstance(item, ToolCallOutputItem):\n            event = RunItemStreamEvent(item=item, name=\"tool_output\")\n        elif isinstance(item, ReasoningItem):\n            event = RunItemStreamEvent(item=item, name=\"reasoning_item_created\")\n        elif isinstance(item, MCPApprovalRequestItem):\n            event = RunItemStreamEvent(item=item, name=\"mcp_approval_requested\")\n        elif isinstance(item, MCPApprovalResponseItem):\n            event = RunItemStreamEvent(item=item, name=\"mcp_approval_response\")\n        elif isinstance(item, MCPListToolsItem):\n            event = RunItemStreamEvent(item=item, name=\"mcp_list_tools\")\n        elif isinstance(item, ToolApprovalItem):\n            event = None  # approvals represent interruptions, not streamed items\n        else:\n            logger.warning(\"Unexpected item type: %s\", type(item))\n            event = None\n\n        if event:\n            queue.put_nowait(event)\n</code></pre>"},{"location":"ref/run_internal/streaming/#agents.run_internal.streaming.stream_step_result_to_queue","title":"stream_step_result_to_queue","text":"<pre><code>stream_step_result_to_queue(\n    step_result,\n    queue: Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None\n</code></pre> <p>Emit all new items in a step result to the event queue.</p> Source code in <code>src/agents/run_internal/streaming.py</code> <pre><code>def stream_step_result_to_queue(\n    step_result,  # SingleStepResult (kept untyped to avoid circular imports)\n    queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel],\n) -&gt; None:\n    \"\"\"Emit all new items in a step result to the event queue.\"\"\"\n    stream_step_items_to_queue(step_result.new_step_items, queue)\n</code></pre>"},{"location":"ref/run_internal/tool_actions/","title":"<code>Tool Actions</code>","text":"<p>Action executors used by the run loop. This module only houses XXXAction classes; helper functions and approval plumbing live in tool_execution.py.</p>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.ComputerAction","title":"ComputerAction","text":"<p>Execute computer tool actions and emit screenshot outputs with hooks fired.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class ComputerAction:\n    \"\"\"Execute computer tool actions and emit screenshot outputs with hooks fired.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        action: ToolRunComputerAction,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n        acknowledged_safety_checks: list[ComputerCallOutputAcknowledgedSafetyCheck] | None = None,\n    ) -&gt; RunItem:\n        \"\"\"Run a computer action, capturing a screenshot and notifying hooks.\"\"\"\n        computer = await resolve_computer(tool=action.computer_tool, run_context=context_wrapper)\n        agent_hooks = agent.hooks\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, action.computer_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, action.computer_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        output = await cls._execute_action_and_capture(computer, action.tool_call)\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        image_url = f\"data:image/png;base64,{output}\"\n        return ToolCallOutputItem(\n            agent=agent,\n            output=image_url,\n            raw_item=ComputerCallOutput(\n                call_id=action.tool_call.call_id,\n                output={\n                    \"type\": \"computer_screenshot\",\n                    \"image_url\": image_url,\n                },\n                type=\"computer_call_output\",\n                acknowledged_safety_checks=acknowledged_safety_checks,\n            ),\n        )\n\n    @classmethod\n    async def _execute_action_and_capture(\n        cls, computer: Any, tool_call: ResponseComputerToolCall\n    ) -&gt; str:\n        \"\"\"Execute the computer action (sync or async drivers) and return the screenshot.\"\"\"\n\n        async def maybe_call(method_name: str, *args: Any) -&gt; Any:\n            method = getattr(computer, method_name, None)\n            if method is None or not callable(method):\n                raise ModelBehaviorError(f\"Computer driver missing method {method_name}\")\n            result = method(*args)\n            return await result if inspect.isawaitable(result) else result\n\n        action = tool_call.action\n        if isinstance(action, ActionClick):\n            await maybe_call(\"click\", action.x, action.y, action.button)\n        elif isinstance(action, ActionDoubleClick):\n            await maybe_call(\"double_click\", action.x, action.y)\n        elif isinstance(action, ActionDrag):\n            await maybe_call(\"drag\", [(p.x, p.y) for p in action.path])\n        elif isinstance(action, ActionKeypress):\n            await maybe_call(\"keypress\", action.keys)\n        elif isinstance(action, ActionMove):\n            await maybe_call(\"move\", action.x, action.y)\n        elif isinstance(action, ActionScreenshot):\n            await maybe_call(\"screenshot\")\n        elif isinstance(action, ActionScroll):\n            await maybe_call(\"scroll\", action.x, action.y, action.scroll_x, action.scroll_y)\n        elif isinstance(action, ActionType):\n            await maybe_call(\"type\", action.text)\n        elif isinstance(action, ActionWait):\n            await maybe_call(\"wait\")\n\n        screenshot_result = await maybe_call(\"screenshot\")\n        return cast(str, screenshot_result)\n</code></pre>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.ComputerAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    action: ToolRunComputerAction,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n    acknowledged_safety_checks: list[\n        ComputerCallOutputAcknowledgedSafetyCheck\n    ]\n    | None = None,\n) -&gt; RunItem\n</code></pre> <p>Run a computer action, capturing a screenshot and notifying hooks.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    action: ToolRunComputerAction,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n    acknowledged_safety_checks: list[ComputerCallOutputAcknowledgedSafetyCheck] | None = None,\n) -&gt; RunItem:\n    \"\"\"Run a computer action, capturing a screenshot and notifying hooks.\"\"\"\n    computer = await resolve_computer(tool=action.computer_tool, run_context=context_wrapper)\n    agent_hooks = agent.hooks\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, action.computer_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, action.computer_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    output = await cls._execute_action_and_capture(computer, action.tool_call)\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, action.computer_tool, output)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    image_url = f\"data:image/png;base64,{output}\"\n    return ToolCallOutputItem(\n        agent=agent,\n        output=image_url,\n        raw_item=ComputerCallOutput(\n            call_id=action.tool_call.call_id,\n            output={\n                \"type\": \"computer_screenshot\",\n                \"image_url\": image_url,\n            },\n            type=\"computer_call_output\",\n            acknowledged_safety_checks=acknowledged_safety_checks,\n        ),\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.LocalShellAction","title":"LocalShellAction","text":"<p>Execute local shell commands via the LocalShellTool with lifecycle hooks.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class LocalShellAction:\n    \"\"\"Execute local shell commands via the LocalShellTool with lifecycle hooks.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        call: ToolRunLocalShellCall,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n    ) -&gt; RunItem:\n        \"\"\"Run a local shell tool call and wrap the result as a ToolCallOutputItem.\"\"\"\n        agent_hooks = agent.hooks\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        request = LocalShellCommandRequest(\n            ctx_wrapper=context_wrapper,\n            data=call.tool_call,\n        )\n        output = call.local_shell_tool.executor(request)\n        result = await output if inspect.isawaitable(output) else output\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        raw_payload: dict[str, Any] = {\n            \"type\": \"local_shell_call_output\",\n            \"call_id\": call.tool_call.call_id,\n            \"output\": result,\n        }\n        return ToolCallOutputItem(\n            agent=agent,\n            output=result,\n            raw_item=raw_payload,\n        )\n</code></pre>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.LocalShellAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    call: ToolRunLocalShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem\n</code></pre> <p>Run a local shell tool call and wrap the result as a ToolCallOutputItem.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    call: ToolRunLocalShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem:\n    \"\"\"Run a local shell tool call and wrap the result as a ToolCallOutputItem.\"\"\"\n    agent_hooks = agent.hooks\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, call.local_shell_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    request = LocalShellCommandRequest(\n        ctx_wrapper=context_wrapper,\n        data=call.tool_call,\n    )\n    output = call.local_shell_tool.executor(request)\n    result = await output if inspect.isawaitable(output) else output\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, call.local_shell_tool, result)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    raw_payload: dict[str, Any] = {\n        \"type\": \"local_shell_call_output\",\n        \"call_id\": call.tool_call.call_id,\n        \"output\": result,\n    }\n    return ToolCallOutputItem(\n        agent=agent,\n        output=result,\n        raw_item=raw_payload,\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.ShellAction","title":"ShellAction","text":"<p>Execute shell calls, handling approvals and normalizing outputs.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class ShellAction:\n    \"\"\"Execute shell calls, handling approvals and normalizing outputs.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        call: ToolRunShellCall,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n    ) -&gt; RunItem:\n        \"\"\"Run a shell tool call and return a normalized ToolCallOutputItem.\"\"\"\n        shell_call = coerce_shell_call(call.tool_call)\n        shell_tool = call.shell_tool\n        agent_hooks = agent.hooks\n\n        needs_approval_result = await evaluate_needs_approval_setting(\n            shell_tool.needs_approval, context_wrapper, shell_call.action, shell_call.call_id\n        )\n\n        if needs_approval_result:\n            approval_status, approval_item = await resolve_approval_status(\n                tool_name=shell_tool.name,\n                call_id=shell_call.call_id,\n                raw_item=call.tool_call,\n                agent=agent,\n                context_wrapper=context_wrapper,\n                on_approval=shell_tool.on_approval,\n            )\n\n            if approval_status is False:\n                rejection_message = await resolve_approval_rejection_message(\n                    context_wrapper=context_wrapper,\n                    run_config=config,\n                    tool_type=\"shell\",\n                    tool_name=shell_tool.name,\n                    call_id=shell_call.call_id,\n                )\n                return shell_rejection_item(\n                    agent,\n                    shell_call.call_id,\n                    rejection_message=rejection_message,\n                )\n\n            if approval_status is not True:\n                return approval_item\n\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, shell_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, shell_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n        request = ShellCommandRequest(ctx_wrapper=context_wrapper, data=shell_call)\n        status: Literal[\"completed\", \"failed\"] = \"completed\"\n        output_text = \"\"\n        shell_output_payload: list[dict[str, Any]] | None = None\n        provider_meta: dict[str, Any] | None = None\n        max_output_length: int | None = None\n        requested_max_output_length = normalize_max_output_length(\n            shell_call.action.max_output_length\n        )\n\n        try:\n            executor_result = call.shell_tool.executor(request)\n            result = (\n                await executor_result if inspect.isawaitable(executor_result) else executor_result\n            )\n\n            if isinstance(result, ShellResult):\n                normalized = [normalize_shell_output(entry) for entry in result.output]\n                result_max_output_length = normalize_max_output_length(result.max_output_length)\n                if result_max_output_length is None:\n                    max_output_length = requested_max_output_length\n                elif requested_max_output_length is None:\n                    max_output_length = result_max_output_length\n                else:\n                    max_output_length = min(result_max_output_length, requested_max_output_length)\n                if max_output_length is not None:\n                    normalized = truncate_shell_outputs(normalized, max_output_length)\n                output_text = render_shell_outputs(normalized)\n                if max_output_length is not None:\n                    output_text = output_text[:max_output_length]\n                shell_output_payload = [serialize_shell_output(entry) for entry in normalized]\n                provider_meta = dict(result.provider_data or {})\n            else:\n                output_text = str(result)\n                if requested_max_output_length is not None:\n                    max_output_length = requested_max_output_length\n                    output_text = output_text[:max_output_length]\n        except Exception as exc:\n            status = \"failed\"\n            output_text = format_shell_error(exc)\n            if requested_max_output_length is not None:\n                max_output_length = requested_max_output_length\n                output_text = output_text[:max_output_length]\n            logger.error(\"Shell executor failed: %s\", exc, exc_info=True)\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        raw_entries: list[dict[str, Any]] | None = None\n        if shell_output_payload:\n            raw_entries = shell_output_payload\n        elif output_text:\n            raw_entries = [\n                {\n                    \"stdout\": output_text,\n                    \"stderr\": \"\",\n                    \"status\": status,\n                    \"outcome\": \"success\" if status == \"completed\" else \"failure\",\n                }\n            ]\n\n        structured_output = normalize_shell_output_entries(raw_entries) if raw_entries else []\n\n        raw_item: dict[str, Any] = {\n            \"type\": \"shell_call_output\",\n            \"call_id\": shell_call.call_id,\n            \"output\": structured_output,\n            \"status\": status,\n        }\n        if max_output_length is not None:\n            raw_item[\"max_output_length\"] = max_output_length\n        if raw_entries:\n            raw_item[\"shell_output\"] = raw_entries\n        if provider_meta:\n            raw_item[\"provider_data\"] = provider_meta\n\n        return ToolCallOutputItem(\n            agent=agent,\n            output=output_text,\n            raw_item=raw_item,\n        )\n</code></pre>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.ShellAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    call: ToolRunShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem\n</code></pre> <p>Run a shell tool call and return a normalized ToolCallOutputItem.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    call: ToolRunShellCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem:\n    \"\"\"Run a shell tool call and return a normalized ToolCallOutputItem.\"\"\"\n    shell_call = coerce_shell_call(call.tool_call)\n    shell_tool = call.shell_tool\n    agent_hooks = agent.hooks\n\n    needs_approval_result = await evaluate_needs_approval_setting(\n        shell_tool.needs_approval, context_wrapper, shell_call.action, shell_call.call_id\n    )\n\n    if needs_approval_result:\n        approval_status, approval_item = await resolve_approval_status(\n            tool_name=shell_tool.name,\n            call_id=shell_call.call_id,\n            raw_item=call.tool_call,\n            agent=agent,\n            context_wrapper=context_wrapper,\n            on_approval=shell_tool.on_approval,\n        )\n\n        if approval_status is False:\n            rejection_message = await resolve_approval_rejection_message(\n                context_wrapper=context_wrapper,\n                run_config=config,\n                tool_type=\"shell\",\n                tool_name=shell_tool.name,\n                call_id=shell_call.call_id,\n            )\n            return shell_rejection_item(\n                agent,\n                shell_call.call_id,\n                rejection_message=rejection_message,\n            )\n\n        if approval_status is not True:\n            return approval_item\n\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, shell_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, shell_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n    request = ShellCommandRequest(ctx_wrapper=context_wrapper, data=shell_call)\n    status: Literal[\"completed\", \"failed\"] = \"completed\"\n    output_text = \"\"\n    shell_output_payload: list[dict[str, Any]] | None = None\n    provider_meta: dict[str, Any] | None = None\n    max_output_length: int | None = None\n    requested_max_output_length = normalize_max_output_length(\n        shell_call.action.max_output_length\n    )\n\n    try:\n        executor_result = call.shell_tool.executor(request)\n        result = (\n            await executor_result if inspect.isawaitable(executor_result) else executor_result\n        )\n\n        if isinstance(result, ShellResult):\n            normalized = [normalize_shell_output(entry) for entry in result.output]\n            result_max_output_length = normalize_max_output_length(result.max_output_length)\n            if result_max_output_length is None:\n                max_output_length = requested_max_output_length\n            elif requested_max_output_length is None:\n                max_output_length = result_max_output_length\n            else:\n                max_output_length = min(result_max_output_length, requested_max_output_length)\n            if max_output_length is not None:\n                normalized = truncate_shell_outputs(normalized, max_output_length)\n            output_text = render_shell_outputs(normalized)\n            if max_output_length is not None:\n                output_text = output_text[:max_output_length]\n            shell_output_payload = [serialize_shell_output(entry) for entry in normalized]\n            provider_meta = dict(result.provider_data or {})\n        else:\n            output_text = str(result)\n            if requested_max_output_length is not None:\n                max_output_length = requested_max_output_length\n                output_text = output_text[:max_output_length]\n    except Exception as exc:\n        status = \"failed\"\n        output_text = format_shell_error(exc)\n        if requested_max_output_length is not None:\n            max_output_length = requested_max_output_length\n            output_text = output_text[:max_output_length]\n        logger.error(\"Shell executor failed: %s\", exc, exc_info=True)\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, call.shell_tool, output_text)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    raw_entries: list[dict[str, Any]] | None = None\n    if shell_output_payload:\n        raw_entries = shell_output_payload\n    elif output_text:\n        raw_entries = [\n            {\n                \"stdout\": output_text,\n                \"stderr\": \"\",\n                \"status\": status,\n                \"outcome\": \"success\" if status == \"completed\" else \"failure\",\n            }\n        ]\n\n    structured_output = normalize_shell_output_entries(raw_entries) if raw_entries else []\n\n    raw_item: dict[str, Any] = {\n        \"type\": \"shell_call_output\",\n        \"call_id\": shell_call.call_id,\n        \"output\": structured_output,\n        \"status\": status,\n    }\n    if max_output_length is not None:\n        raw_item[\"max_output_length\"] = max_output_length\n    if raw_entries:\n        raw_item[\"shell_output\"] = raw_entries\n    if provider_meta:\n        raw_item[\"provider_data\"] = provider_meta\n\n    return ToolCallOutputItem(\n        agent=agent,\n        output=output_text,\n        raw_item=raw_item,\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.ApplyPatchAction","title":"ApplyPatchAction","text":"<p>Execute apply_patch operations with approvals and editor integration.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>class ApplyPatchAction:\n    \"\"\"Execute apply_patch operations with approvals and editor integration.\"\"\"\n\n    @classmethod\n    async def execute(\n        cls,\n        *,\n        agent: Agent[Any],\n        call: ToolRunApplyPatchCall,\n        hooks: RunHooks[Any],\n        context_wrapper: RunContextWrapper[Any],\n        config: RunConfig,\n    ) -&gt; RunItem:\n        \"\"\"Run an apply_patch call and serialize the editor result for the model.\"\"\"\n        apply_patch_tool: ApplyPatchTool = call.apply_patch_tool\n        agent_hooks = agent.hooks\n        operation = coerce_apply_patch_operation(\n            call.tool_call,\n            context_wrapper=context_wrapper,\n        )\n\n        call_id = extract_apply_patch_call_id(call.tool_call)\n\n        needs_approval_result = await evaluate_needs_approval_setting(\n            apply_patch_tool.needs_approval, context_wrapper, operation, call_id\n        )\n\n        if needs_approval_result:\n            approval_status, approval_item = await resolve_approval_status(\n                tool_name=apply_patch_tool.name,\n                call_id=call_id,\n                raw_item=call.tool_call,\n                agent=agent,\n                context_wrapper=context_wrapper,\n                on_approval=apply_patch_tool.on_approval,\n            )\n\n            if approval_status is False:\n                rejection_message = await resolve_approval_rejection_message(\n                    context_wrapper=context_wrapper,\n                    run_config=config,\n                    tool_type=\"apply_patch\",\n                    tool_name=apply_patch_tool.name,\n                    call_id=call_id,\n                )\n                return apply_patch_rejection_item(\n                    agent,\n                    call_id,\n                    rejection_message=rejection_message,\n                )\n\n            if approval_status is not True:\n                return approval_item\n\n        await asyncio.gather(\n            hooks.on_tool_start(context_wrapper, agent, apply_patch_tool),\n            (\n                agent_hooks.on_tool_start(context_wrapper, agent, apply_patch_tool)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        status: Literal[\"completed\", \"failed\"] = \"completed\"\n        output_text = \"\"\n\n        try:\n            editor = apply_patch_tool.editor\n            if operation.type == \"create_file\":\n                result = editor.create_file(operation)\n            elif operation.type == \"update_file\":\n                result = editor.update_file(operation)\n            elif operation.type == \"delete_file\":\n                result = editor.delete_file(operation)\n            else:  # pragma: no cover - validated in coerce_apply_patch_operation\n                raise ModelBehaviorError(f\"Unsupported apply_patch operation: {operation.type}\")\n\n            awaited = await result if inspect.isawaitable(result) else result\n            normalized = normalize_apply_patch_result(awaited)\n            if normalized:\n                if normalized.status in {\"completed\", \"failed\"}:\n                    status = normalized.status\n                if normalized.output:\n                    output_text = normalized.output\n        except Exception as exc:\n            status = \"failed\"\n            output_text = format_shell_error(exc)\n            logger.error(\"Apply patch editor failed: %s\", exc, exc_info=True)\n\n        await asyncio.gather(\n            hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text),\n            (\n                agent_hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text)\n                if agent_hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        raw_item: dict[str, Any] = {\n            \"type\": \"apply_patch_call_output\",\n            \"call_id\": call_id,\n            \"status\": status,\n        }\n        if output_text:\n            raw_item[\"output\"] = output_text\n\n        return ToolCallOutputItem(\n            agent=agent,\n            output=output_text,\n            raw_item=raw_item,\n        )\n</code></pre>"},{"location":"ref/run_internal/tool_actions/#agents.run_internal.tool_actions.ApplyPatchAction.execute","title":"execute  <code>async</code> <code>classmethod</code>","text":"<pre><code>execute(\n    *,\n    agent: Agent[Any],\n    call: ToolRunApplyPatchCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem\n</code></pre> <p>Run an apply_patch call and serialize the editor result for the model.</p> Source code in <code>src/agents/run_internal/tool_actions.py</code> <pre><code>@classmethod\nasync def execute(\n    cls,\n    *,\n    agent: Agent[Any],\n    call: ToolRunApplyPatchCall,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; RunItem:\n    \"\"\"Run an apply_patch call and serialize the editor result for the model.\"\"\"\n    apply_patch_tool: ApplyPatchTool = call.apply_patch_tool\n    agent_hooks = agent.hooks\n    operation = coerce_apply_patch_operation(\n        call.tool_call,\n        context_wrapper=context_wrapper,\n    )\n\n    call_id = extract_apply_patch_call_id(call.tool_call)\n\n    needs_approval_result = await evaluate_needs_approval_setting(\n        apply_patch_tool.needs_approval, context_wrapper, operation, call_id\n    )\n\n    if needs_approval_result:\n        approval_status, approval_item = await resolve_approval_status(\n            tool_name=apply_patch_tool.name,\n            call_id=call_id,\n            raw_item=call.tool_call,\n            agent=agent,\n            context_wrapper=context_wrapper,\n            on_approval=apply_patch_tool.on_approval,\n        )\n\n        if approval_status is False:\n            rejection_message = await resolve_approval_rejection_message(\n                context_wrapper=context_wrapper,\n                run_config=config,\n                tool_type=\"apply_patch\",\n                tool_name=apply_patch_tool.name,\n                call_id=call_id,\n            )\n            return apply_patch_rejection_item(\n                agent,\n                call_id,\n                rejection_message=rejection_message,\n            )\n\n        if approval_status is not True:\n            return approval_item\n\n    await asyncio.gather(\n        hooks.on_tool_start(context_wrapper, agent, apply_patch_tool),\n        (\n            agent_hooks.on_tool_start(context_wrapper, agent, apply_patch_tool)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    status: Literal[\"completed\", \"failed\"] = \"completed\"\n    output_text = \"\"\n\n    try:\n        editor = apply_patch_tool.editor\n        if operation.type == \"create_file\":\n            result = editor.create_file(operation)\n        elif operation.type == \"update_file\":\n            result = editor.update_file(operation)\n        elif operation.type == \"delete_file\":\n            result = editor.delete_file(operation)\n        else:  # pragma: no cover - validated in coerce_apply_patch_operation\n            raise ModelBehaviorError(f\"Unsupported apply_patch operation: {operation.type}\")\n\n        awaited = await result if inspect.isawaitable(result) else result\n        normalized = normalize_apply_patch_result(awaited)\n        if normalized:\n            if normalized.status in {\"completed\", \"failed\"}:\n                status = normalized.status\n            if normalized.output:\n                output_text = normalized.output\n    except Exception as exc:\n        status = \"failed\"\n        output_text = format_shell_error(exc)\n        logger.error(\"Apply patch editor failed: %s\", exc, exc_info=True)\n\n    await asyncio.gather(\n        hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text),\n        (\n            agent_hooks.on_tool_end(context_wrapper, agent, apply_patch_tool, output_text)\n            if agent_hooks\n            else _coro.noop_coroutine()\n        ),\n    )\n\n    raw_item: dict[str, Any] = {\n        \"type\": \"apply_patch_call_output\",\n        \"call_id\": call_id,\n        \"status\": status,\n    }\n    if output_text:\n        raw_item[\"output\"] = output_text\n\n    return ToolCallOutputItem(\n        agent=agent,\n        output=output_text,\n        raw_item=raw_item,\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_execution/","title":"<code>Tool Execution</code>","text":"<p>Tool execution helpers for the run pipeline. This module hosts execution-time helpers, approval plumbing, and payload coercion. Action classes live in tool_actions.py.</p>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.maybe_reset_tool_choice","title":"maybe_reset_tool_choice","text":"<pre><code>maybe_reset_tool_choice(\n    agent: Agent[Any],\n    tool_use_tracker: AgentToolUseTracker,\n    model_settings: ModelSettings,\n) -&gt; ModelSettings\n</code></pre> <p>Reset tool_choice if the agent was forced to pick a tool previously and should be reset.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def maybe_reset_tool_choice(\n    agent: Agent[Any],\n    tool_use_tracker: AgentToolUseTracker,\n    model_settings: ModelSettings,\n) -&gt; ModelSettings:\n    \"\"\"Reset tool_choice if the agent was forced to pick a tool previously and should be reset.\"\"\"\n    if agent.reset_tool_choice is True and tool_use_tracker.has_used_tools(agent):\n        return dataclasses.replace(model_settings, tool_choice=None)\n    return model_settings\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.initialize_computer_tools","title":"initialize_computer_tools  <code>async</code>","text":"<pre><code>initialize_computer_tools(\n    *,\n    tools: list[Tool],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; None\n</code></pre> <p>Resolve computer tools ahead of model invocation so each run gets its own instance.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def initialize_computer_tools(\n    *,\n    tools: list[Tool],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; None:\n    \"\"\"Resolve computer tools ahead of model invocation so each run gets its own instance.\"\"\"\n    computer_tools = [tool for tool in tools if isinstance(tool, ComputerTool)]\n    if not computer_tools:\n        return\n\n    await asyncio.gather(\n        *(resolve_computer(tool=tool, run_context=context_wrapper) for tool in computer_tools)\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.get_mapping_or_attr","title":"get_mapping_or_attr","text":"<pre><code>get_mapping_or_attr(target: Any, key: str) -&gt; Any\n</code></pre> <p>Allow mapping-or-attribute access so tool payloads can be dicts or objects.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def get_mapping_or_attr(target: Any, key: str) -&gt; Any:\n    \"\"\"Allow mapping-or-attribute access so tool payloads can be dicts or objects.\"\"\"\n    if isinstance(target, Mapping):\n        return target.get(key)\n    return getattr(target, key, None)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.extract_tool_call_id","title":"extract_tool_call_id","text":"<pre><code>extract_tool_call_id(raw: Any) -&gt; str | None\n</code></pre> <p>Return a call ID from tool call payloads or approval items.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def extract_tool_call_id(raw: Any) -&gt; str | None:\n    \"\"\"Return a call ID from tool call payloads or approval items.\"\"\"\n    # OpenAI tool call payloads are documented to include a call_id/id so outputs can be matched.\n    # See https://platform.openai.com/docs/guides/function-calling\n    # We still guard against missing IDs to avoid hard failures on malformed or non-OpenAI inputs.\n    if isinstance(raw, Mapping):\n        candidate = raw.get(\"call_id\") or raw.get(\"id\")\n        return candidate if isinstance(candidate, str) else None\n    candidate = get_mapping_or_attr(raw, \"call_id\") or get_mapping_or_attr(raw, \"id\")\n    return candidate if isinstance(candidate, str) else None\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.extract_shell_call_id","title":"extract_shell_call_id","text":"<pre><code>extract_shell_call_id(tool_call: Any) -&gt; str\n</code></pre> <p>Ensure shell calls include a call_id before executing them.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def extract_shell_call_id(tool_call: Any) -&gt; str:\n    \"\"\"Ensure shell calls include a call_id before executing them.\"\"\"\n    value = extract_tool_call_id(tool_call)\n    if not value:\n        raise ModelBehaviorError(\"Shell call is missing call_id.\")\n    return str(value)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.coerce_shell_call","title":"coerce_shell_call","text":"<pre><code>coerce_shell_call(tool_call: Any) -&gt; ShellCallData\n</code></pre> <p>Normalize a shell call payload into ShellCallData for consistent execution.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def coerce_shell_call(tool_call: Any) -&gt; ShellCallData:\n    \"\"\"Normalize a shell call payload into ShellCallData for consistent execution.\"\"\"\n    call_id = extract_shell_call_id(tool_call)\n    action_payload = get_mapping_or_attr(tool_call, \"action\")\n    if action_payload is None:\n        raise ModelBehaviorError(\"Shell call is missing an action payload.\")\n\n    commands_value = get_mapping_or_attr(action_payload, \"commands\")\n    if not isinstance(commands_value, Sequence):\n        raise ModelBehaviorError(\"Shell call action is missing commands.\")\n    commands: list[str] = []\n    for entry in commands_value:\n        if entry is None:\n            continue\n        commands.append(str(entry))\n    if not commands:\n        raise ModelBehaviorError(\"Shell call action must include at least one command.\")\n\n    timeout_value = (\n        get_mapping_or_attr(action_payload, \"timeout_ms\")\n        or get_mapping_or_attr(action_payload, \"timeoutMs\")\n        or get_mapping_or_attr(action_payload, \"timeout\")\n    )\n    timeout_ms = int(timeout_value) if isinstance(timeout_value, (int, float)) else None\n\n    max_length_value = get_mapping_or_attr(action_payload, \"max_output_length\")\n    if max_length_value is None:\n        max_length_value = get_mapping_or_attr(action_payload, \"maxOutputLength\")\n    max_output_length = (\n        int(max_length_value) if isinstance(max_length_value, (int, float)) else None\n    )\n\n    action = ShellActionRequest(\n        commands=commands,\n        timeout_ms=timeout_ms,\n        max_output_length=max_output_length,\n    )\n\n    status_value = get_mapping_or_attr(tool_call, \"status\")\n    status_literal: Literal[\"in_progress\", \"completed\"] | None = None\n    if isinstance(status_value, str):\n        lowered = status_value.lower()\n        if lowered in {\"in_progress\", \"completed\"}:\n            status_literal = cast(Literal[\"in_progress\", \"completed\"], lowered)\n\n    return ShellCallData(call_id=call_id, action=action, status=status_literal, raw=tool_call)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.parse_apply_patch_custom_input","title":"parse_apply_patch_custom_input","text":"<pre><code>parse_apply_patch_custom_input(\n    input_json: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Parse custom apply_patch tool input used when a tool passes raw JSON strings.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def parse_apply_patch_custom_input(input_json: str) -&gt; dict[str, Any]:\n    \"\"\"Parse custom apply_patch tool input used when a tool passes raw JSON strings.\"\"\"\n    return _parse_apply_patch_json(input_json, label=\"input\")\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.parse_apply_patch_function_args","title":"parse_apply_patch_function_args","text":"<pre><code>parse_apply_patch_function_args(\n    arguments: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Parse apply_patch function tool arguments from the model.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def parse_apply_patch_function_args(arguments: str) -&gt; dict[str, Any]:\n    \"\"\"Parse apply_patch function tool arguments from the model.\"\"\"\n    return _parse_apply_patch_json(arguments, label=\"arguments\")\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.extract_apply_patch_call_id","title":"extract_apply_patch_call_id","text":"<pre><code>extract_apply_patch_call_id(tool_call: Any) -&gt; str\n</code></pre> <p>Ensure apply_patch calls include a call_id for approvals and tracing.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def extract_apply_patch_call_id(tool_call: Any) -&gt; str:\n    \"\"\"Ensure apply_patch calls include a call_id for approvals and tracing.\"\"\"\n    value = extract_tool_call_id(tool_call)\n    if not value:\n        raise ModelBehaviorError(\"Apply patch call is missing call_id.\")\n    return str(value)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.coerce_apply_patch_operation","title":"coerce_apply_patch_operation","text":"<pre><code>coerce_apply_patch_operation(\n    tool_call: Any,\n    *,\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; ApplyPatchOperation\n</code></pre> <p>Normalize the tool payload into an ApplyPatchOperation the editor can consume.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def coerce_apply_patch_operation(\n    tool_call: Any, *, context_wrapper: RunContextWrapper[Any]\n) -&gt; ApplyPatchOperation:\n    \"\"\"Normalize the tool payload into an ApplyPatchOperation the editor can consume.\"\"\"\n    raw_operation = get_mapping_or_attr(tool_call, \"operation\")\n    if raw_operation is None:\n        raise ModelBehaviorError(\"Apply patch call is missing an operation payload.\")\n\n    op_type_value = str(get_mapping_or_attr(raw_operation, \"type\"))\n    if op_type_value not in {\"create_file\", \"update_file\", \"delete_file\"}:\n        raise ModelBehaviorError(f\"Unknown apply_patch operation: {op_type_value}\")\n    op_type_literal = cast(Literal[\"create_file\", \"update_file\", \"delete_file\"], op_type_value)\n\n    path = get_mapping_or_attr(raw_operation, \"path\")\n    if not isinstance(path, str) or not path:\n        raise ModelBehaviorError(\"Apply patch operation is missing a valid path.\")\n\n    diff_value = get_mapping_or_attr(raw_operation, \"diff\")\n    if op_type_literal in {\"create_file\", \"update_file\"}:\n        if not isinstance(diff_value, str) or not diff_value:\n            raise ModelBehaviorError(\n                f\"Apply patch operation {op_type_literal} is missing the required diff payload.\"\n            )\n        diff: str | None = diff_value\n    else:\n        diff = None\n\n    return ApplyPatchOperation(\n        type=op_type_literal,\n        path=str(path),\n        diff=diff,\n        ctx_wrapper=context_wrapper,\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.normalize_apply_patch_result","title":"normalize_apply_patch_result","text":"<pre><code>normalize_apply_patch_result(\n    result: ApplyPatchResult\n    | Mapping[str, Any]\n    | str\n    | None,\n) -&gt; ApplyPatchResult | None\n</code></pre> <p>Coerce editor return values into ApplyPatchResult for consistent handling.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def normalize_apply_patch_result(\n    result: ApplyPatchResult | Mapping[str, Any] | str | None,\n) -&gt; ApplyPatchResult | None:\n    \"\"\"Coerce editor return values into ApplyPatchResult for consistent handling.\"\"\"\n    if result is None:\n        return None\n    if isinstance(result, ApplyPatchResult):\n        return result\n    if isinstance(result, Mapping):\n        status = result.get(\"status\")\n        output = result.get(\"output\")\n        normalized_status = status if status in {\"completed\", \"failed\"} else None\n        normalized_output = str(output) if output is not None else None\n        return ApplyPatchResult(status=normalized_status, output=normalized_output)\n    if isinstance(result, str):\n        return ApplyPatchResult(output=result)\n    return ApplyPatchResult(output=str(result))\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.is_apply_patch_name","title":"is_apply_patch_name","text":"<pre><code>is_apply_patch_name(\n    name: str | None, tool: ApplyPatchTool | None\n) -&gt; bool\n</code></pre> <p>Allow flexible matching for apply_patch so existing names keep working.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def is_apply_patch_name(name: str | None, tool: ApplyPatchTool | None) -&gt; bool:\n    \"\"\"Allow flexible matching for apply_patch so existing names keep working.\"\"\"\n    if not name:\n        return False\n    candidate = name.strip().lower()\n    if candidate.startswith(\"apply_patch\"):\n        return True\n    if tool and candidate == tool.name.strip().lower():\n        return True\n    return False\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.normalize_shell_output","title":"normalize_shell_output","text":"<pre><code>normalize_shell_output(\n    entry: ShellCommandOutput | Mapping[str, Any],\n) -&gt; ShellCommandOutput\n</code></pre> <p>Normalize shell output into ShellCommandOutput so downstream code sees a stable shape.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def normalize_shell_output(entry: ShellCommandOutput | Mapping[str, Any]) -&gt; ShellCommandOutput:\n    \"\"\"Normalize shell output into ShellCommandOutput so downstream code sees a stable shape.\"\"\"\n    if isinstance(entry, ShellCommandOutput):\n        return entry\n\n    stdout = str(entry.get(\"stdout\", \"\") or \"\")\n    stderr = str(entry.get(\"stderr\", \"\") or \"\")\n    command_value = entry.get(\"command\")\n    provider_data_value = entry.get(\"provider_data\")\n    outcome_value = entry.get(\"outcome\")\n\n    outcome_type: Literal[\"exit\", \"timeout\"] = \"exit\"\n    exit_code_value: Any | None = None\n\n    if isinstance(outcome_value, Mapping):\n        type_value = outcome_value.get(\"type\")\n        if type_value == \"timeout\":\n            outcome_type = \"timeout\"\n        elif isinstance(type_value, str):\n            outcome_type = \"exit\"\n        exit_code_value = outcome_value.get(\"exit_code\")\n    else:\n        status_str = str(entry.get(\"status\", \"completed\") or \"completed\").lower()\n        if status_str == \"timeout\":\n            outcome_type = \"timeout\"\n        if isinstance(outcome_value, str):\n            if outcome_value == \"failure\":\n                exit_code_value = 1\n            elif outcome_value == \"success\":\n                exit_code_value = 0\n        if exit_code_value is None and \"exit_code\" in entry:\n            exit_code_value = entry.get(\"exit_code\")\n\n    outcome = ShellCallOutcome(\n        type=outcome_type,\n        exit_code=_normalize_exit_code(exit_code_value),\n    )\n\n    return ShellCommandOutput(\n        stdout=stdout,\n        stderr=stderr,\n        outcome=outcome,\n        command=str(command_value) if command_value is not None else None,\n        provider_data=cast(dict[str, Any], provider_data_value)\n        if isinstance(provider_data_value, Mapping)\n        else provider_data_value,\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.serialize_shell_output","title":"serialize_shell_output","text":"<pre><code>serialize_shell_output(\n    output: ShellCommandOutput,\n) -&gt; dict[str, Any]\n</code></pre> <p>Serialize ShellCommandOutput for persistence or cross-run transmission.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def serialize_shell_output(output: ShellCommandOutput) -&gt; dict[str, Any]:\n    \"\"\"Serialize ShellCommandOutput for persistence or cross-run transmission.\"\"\"\n    payload: dict[str, Any] = {\n        \"stdout\": output.stdout,\n        \"stderr\": output.stderr,\n        \"status\": output.status,\n        \"outcome\": {\"type\": output.outcome.type},\n    }\n    if output.outcome.type == \"exit\":\n        payload[\"outcome\"][\"exit_code\"] = output.outcome.exit_code\n        if output.outcome.exit_code is not None:\n            payload[\"exit_code\"] = output.outcome.exit_code\n    if output.command is not None:\n        payload[\"command\"] = output.command\n    if output.provider_data:\n        payload[\"provider_data\"] = output.provider_data\n    return payload\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.resolve_exit_code","title":"resolve_exit_code","text":"<pre><code>resolve_exit_code(\n    raw_exit_code: Any, outcome_status: str | None\n) -&gt; int\n</code></pre> <p>Fallback logic to produce an exit code when providers omit one.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def resolve_exit_code(raw_exit_code: Any, outcome_status: str | None) -&gt; int:\n    \"\"\"Fallback logic to produce an exit code when providers omit one.\"\"\"\n    normalized = _normalize_exit_code(raw_exit_code)\n    if normalized is not None:\n        return normalized\n\n    normalized_status = (outcome_status or \"\").lower()\n    if normalized_status == \"success\":\n        return 0\n    if normalized_status == \"failure\":\n        return 1\n    return 0\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.render_shell_outputs","title":"render_shell_outputs","text":"<pre><code>render_shell_outputs(\n    outputs: Sequence[ShellCommandOutput],\n) -&gt; str\n</code></pre> <p>Render shell outputs into human-readable text for tool responses.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def render_shell_outputs(outputs: Sequence[ShellCommandOutput]) -&gt; str:\n    \"\"\"Render shell outputs into human-readable text for tool responses.\"\"\"\n    if not outputs:\n        return \"(no output)\"\n\n    rendered_chunks: list[str] = []\n    for result in outputs:\n        chunk_lines: list[str] = []\n        if result.command:\n            chunk_lines.append(f\"$ {result.command}\")\n\n        stdout = result.stdout.rstrip(\"\\n\")\n        stderr = result.stderr.rstrip(\"\\n\")\n\n        if stdout:\n            chunk_lines.append(stdout)\n        if stderr:\n            if stdout:\n                chunk_lines.append(\"\")\n            chunk_lines.append(\"stderr:\")\n            chunk_lines.append(stderr)\n\n        if result.exit_code not in (None, 0):\n            chunk_lines.append(f\"exit code: {result.exit_code}\")\n        if result.status == \"timeout\":\n            chunk_lines.append(\"status: timeout\")\n\n        chunk = \"\\n\".join(chunk_lines).strip()\n        rendered_chunks.append(chunk if chunk else \"(no output)\")\n\n    return \"\\n\\n\".join(rendered_chunks)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.truncate_shell_outputs","title":"truncate_shell_outputs","text":"<pre><code>truncate_shell_outputs(\n    outputs: Sequence[ShellCommandOutput], max_length: int\n) -&gt; list[ShellCommandOutput]\n</code></pre> <p>Truncate shell output streams to a maximum combined length.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def truncate_shell_outputs(\n    outputs: Sequence[ShellCommandOutput], max_length: int\n) -&gt; list[ShellCommandOutput]:\n    \"\"\"Truncate shell output streams to a maximum combined length.\"\"\"\n    if max_length &lt;= 0:\n        return [\n            ShellCommandOutput(\n                stdout=\"\",\n                stderr=\"\",\n                outcome=output.outcome,\n                command=output.command,\n                provider_data=output.provider_data,\n            )\n            for output in outputs\n        ]\n\n    remaining = max_length\n    truncated: list[ShellCommandOutput] = []\n    for output in outputs:\n        stdout = \"\"\n        stderr = \"\"\n        if remaining &gt; 0 and output.stdout:\n            stdout = output.stdout[:remaining]\n            remaining -= len(stdout)\n        if remaining &gt; 0 and output.stderr:\n            stderr = output.stderr[:remaining]\n            remaining -= len(stderr)\n        truncated.append(\n            ShellCommandOutput(\n                stdout=stdout,\n                stderr=stderr,\n                outcome=output.outcome,\n                command=output.command,\n                provider_data=output.provider_data,\n            )\n        )\n\n    return truncated\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.normalize_shell_output_entries","title":"normalize_shell_output_entries","text":"<pre><code>normalize_shell_output_entries(\n    entries: Sequence[Mapping[str, Any]],\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Normalize raw shell output entries into the model-facing payload.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def normalize_shell_output_entries(\n    entries: Sequence[Mapping[str, Any]],\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Normalize raw shell output entries into the model-facing payload.\"\"\"\n    structured_output: list[dict[str, Any]] = []\n    for entry in entries:\n        sanitized = dict(entry)\n        status_value = sanitized.pop(\"status\", None)\n        sanitized.pop(\"provider_data\", None)\n        raw_exit_code = sanitized.pop(\"exit_code\", None)\n        sanitized.pop(\"command\", None)\n        outcome_value = sanitized.get(\"outcome\")\n        if isinstance(outcome_value, str):\n            resolved_type = \"exit\"\n            if status_value == \"timeout\":\n                resolved_type = \"timeout\"\n            outcome_payload: dict[str, Any] = {\"type\": resolved_type}\n            if resolved_type == \"exit\":\n                outcome_payload[\"exit_code\"] = resolve_exit_code(raw_exit_code, outcome_value)\n            sanitized[\"outcome\"] = outcome_payload\n        elif isinstance(outcome_value, dict):\n            outcome_payload = dict(outcome_value)\n            outcome_status = outcome_payload.pop(\"status\", None)\n            outcome_type = outcome_payload.get(\"type\")\n            if outcome_type != \"timeout\":\n                status_str = outcome_status if isinstance(outcome_status, str) else None\n                outcome_payload.setdefault(\n                    \"exit_code\",\n                    resolve_exit_code(raw_exit_code, status_str),\n                )\n            sanitized[\"outcome\"] = outcome_payload\n        structured_output.append(sanitized)\n    return structured_output\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.normalize_max_output_length","title":"normalize_max_output_length","text":"<pre><code>normalize_max_output_length(\n    value: int | None,\n) -&gt; int | None\n</code></pre> <p>Clamp negative max output lengths to zero while preserving None.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def normalize_max_output_length(value: int | None) -&gt; int | None:\n    \"\"\"Clamp negative max output lengths to zero while preserving None.\"\"\"\n    if value is None:\n        return None\n    return max(0, value)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.format_shell_error","title":"format_shell_error","text":"<pre><code>format_shell_error(\n    error: Exception | BaseException | Any,\n) -&gt; str\n</code></pre> <p>Best-effort stringify of shell errors to keep tool failures readable.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def format_shell_error(error: Exception | BaseException | Any) -&gt; str:\n    \"\"\"Best-effort stringify of shell errors to keep tool failures readable.\"\"\"\n    if isinstance(error, Exception):\n        message = str(error)\n        return message or error.__class__.__name__\n    try:\n        return str(error)\n    except Exception:  # pragma: no cover - fallback only\n        return repr(error)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.build_litellm_json_tool_call","title":"build_litellm_json_tool_call","text":"<pre><code>build_litellm_json_tool_call(\n    output: ResponseFunctionToolCall,\n) -&gt; FunctionTool\n</code></pre> <p>Wrap a JSON string result in a FunctionTool so LiteLLM can stream it.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def build_litellm_json_tool_call(output: ResponseFunctionToolCall) -&gt; FunctionTool:\n    \"\"\"Wrap a JSON string result in a FunctionTool so LiteLLM can stream it.\"\"\"\n\n    async def on_invoke_tool(_ctx: ToolContext[Any], value: Any) -&gt; Any:\n        \"\"\"Deserialize JSON strings so LiteLLM callers receive structured data.\"\"\"\n        if isinstance(value, str):\n            return json.loads(value)\n        return value\n\n    return FunctionTool(\n        name=output.name,\n        description=output.name,\n        params_json_schema={},\n        on_invoke_tool=on_invoke_tool,\n        strict_json_schema=True,\n        is_enabled=True,\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.resolve_approval_status","title":"resolve_approval_status  <code>async</code>","text":"<pre><code>resolve_approval_status(\n    *,\n    tool_name: str,\n    call_id: str,\n    raw_item: Any,\n    agent: Agent[Any],\n    context_wrapper: RunContextWrapper[Any],\n    on_approval: Callable[\n        [RunContextWrapper[Any], ToolApprovalItem], Any\n    ]\n    | None = None,\n) -&gt; tuple[bool | None, ToolApprovalItem]\n</code></pre> <p>Build approval item, run on_approval hook if needed, and return latest approval status.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def resolve_approval_status(\n    *,\n    tool_name: str,\n    call_id: str,\n    raw_item: Any,\n    agent: Agent[Any],\n    context_wrapper: RunContextWrapper[Any],\n    on_approval: Callable[[RunContextWrapper[Any], ToolApprovalItem], Any] | None = None,\n) -&gt; tuple[bool | None, ToolApprovalItem]:\n    \"\"\"Build approval item, run on_approval hook if needed, and return latest approval status.\"\"\"\n    approval_item = ToolApprovalItem(agent=agent, raw_item=raw_item, tool_name=tool_name)\n    approval_status = context_wrapper.get_approval_status(\n        tool_name,\n        call_id,\n        existing_pending=approval_item,\n    )\n    if approval_status is None and on_approval:\n        decision_result = on_approval(context_wrapper, approval_item)\n        if inspect.isawaitable(decision_result):\n            decision_result = await decision_result\n        if isinstance(decision_result, Mapping):\n            if decision_result.get(\"approve\") is True:\n                context_wrapper.approve_tool(approval_item)\n            elif decision_result.get(\"approve\") is False:\n                context_wrapper.reject_tool(approval_item)\n        approval_status = context_wrapper.get_approval_status(\n            tool_name,\n            call_id,\n            existing_pending=approval_item,\n        )\n    return approval_status, approval_item\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.resolve_approval_interruption","title":"resolve_approval_interruption","text":"<pre><code>resolve_approval_interruption(\n    approval_status: bool | None,\n    approval_item: ToolApprovalItem,\n    *,\n    rejection_factory: Callable[[], RunItem],\n) -&gt; RunItem | ToolApprovalItem | None\n</code></pre> <p>Return a rejection or pending approval item when approval is required.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def resolve_approval_interruption(\n    approval_status: bool | None,\n    approval_item: ToolApprovalItem,\n    *,\n    rejection_factory: Callable[[], RunItem],\n) -&gt; RunItem | ToolApprovalItem | None:\n    \"\"\"Return a rejection or pending approval item when approval is required.\"\"\"\n    if approval_status is False:\n        return rejection_factory()\n    if approval_status is not True:\n        return approval_item\n    return None\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.resolve_approval_rejection_message","title":"resolve_approval_rejection_message  <code>async</code>","text":"<pre><code>resolve_approval_rejection_message(\n    *,\n    context_wrapper: RunContextWrapper[Any],\n    run_config: RunConfig,\n    tool_type: Literal[\n        \"function\", \"computer\", \"shell\", \"apply_patch\"\n    ],\n    tool_name: str,\n    call_id: str,\n) -&gt; str\n</code></pre> <p>Resolve model-visible output text for approval rejections.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def resolve_approval_rejection_message(\n    *,\n    context_wrapper: RunContextWrapper[Any],\n    run_config: RunConfig,\n    tool_type: Literal[\"function\", \"computer\", \"shell\", \"apply_patch\"],\n    tool_name: str,\n    call_id: str,\n) -&gt; str:\n    \"\"\"Resolve model-visible output text for approval rejections.\"\"\"\n    formatter = run_config.tool_error_formatter\n    if formatter is None:\n        return REJECTION_MESSAGE\n\n    try:\n        maybe_message = formatter(\n            ToolErrorFormatterArgs(\n                kind=\"approval_rejected\",\n                tool_type=tool_type,\n                tool_name=tool_name,\n                call_id=call_id,\n                default_message=REJECTION_MESSAGE,\n                run_context=context_wrapper,\n            )\n        )\n        message = await maybe_message if inspect.isawaitable(maybe_message) else maybe_message\n    except Exception as exc:\n        logger.error(\"Tool error formatter failed for %s: %s\", tool_name, exc)\n        return REJECTION_MESSAGE\n\n    if message is None:\n        return REJECTION_MESSAGE\n\n    if not isinstance(message, str):\n        logger.error(\n            \"Tool error formatter returned non-string for %s: %s\",\n            tool_name,\n            type(message).__name__,\n        )\n        return REJECTION_MESSAGE\n\n    return message\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.function_needs_approval","title":"function_needs_approval  <code>async</code>","text":"<pre><code>function_needs_approval(\n    function_tool: FunctionTool,\n    context_wrapper: RunContextWrapper[Any],\n    tool_call: ResponseFunctionToolCall,\n) -&gt; bool\n</code></pre> <p>Evaluate a function tool's needs_approval setting with parsed args.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def function_needs_approval(\n    function_tool: FunctionTool,\n    context_wrapper: RunContextWrapper[Any],\n    tool_call: ResponseFunctionToolCall,\n) -&gt; bool:\n    \"\"\"Evaluate a function tool's needs_approval setting with parsed args.\"\"\"\n    parsed_args: dict[str, Any] = {}\n    if callable(function_tool.needs_approval):\n        try:\n            parsed_args = json.loads(tool_call.arguments or \"{}\")\n        except json.JSONDecodeError:\n            parsed_args = {}\n    needs_approval = await evaluate_needs_approval_setting(\n        function_tool.needs_approval,\n        context_wrapper,\n        parsed_args,\n        tool_call.call_id,\n    )\n    return bool(needs_approval)\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.process_hosted_mcp_approvals","title":"process_hosted_mcp_approvals","text":"<pre><code>process_hosted_mcp_approvals(\n    *,\n    original_pre_step_items: Sequence[RunItem],\n    mcp_approval_requests: Sequence[Any],\n    context_wrapper: RunContextWrapper[Any],\n    agent: Agent[Any],\n    append_item: Callable[[RunItem], None],\n) -&gt; tuple[list[ToolApprovalItem], set[str]]\n</code></pre> <p>Filter hosted MCP outputs and merge manual approvals so only coherent items remain.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def process_hosted_mcp_approvals(\n    *,\n    original_pre_step_items: Sequence[RunItem],\n    mcp_approval_requests: Sequence[Any],\n    context_wrapper: RunContextWrapper[Any],\n    agent: Agent[Any],\n    append_item: Callable[[RunItem], None],\n) -&gt; tuple[list[ToolApprovalItem], set[str]]:\n    \"\"\"Filter hosted MCP outputs and merge manual approvals so only coherent items remain.\"\"\"\n    hosted_mcp_approvals_by_id: dict[str, ToolApprovalItem] = {}\n    for item in original_pre_step_items:\n        if not isinstance(item, ToolApprovalItem):\n            continue\n        raw = item.raw_item\n        if not _is_hosted_mcp_approval_request(raw):\n            continue\n        request_id = extract_mcp_request_id(raw)\n        if request_id:\n            hosted_mcp_approvals_by_id[request_id] = item\n\n    pending_hosted_mcp_approvals: list[ToolApprovalItem] = []\n    pending_hosted_mcp_approval_ids: set[str] = set()\n\n    for mcp_run in mcp_approval_requests:\n        request_id = extract_mcp_request_id_from_run(mcp_run)\n        # MCP approval requests are documented to include an id used as approval_request_id.\n        # See https://platform.openai.com/docs/guides/tools-connectors-mcp#approvals\n        approval_item = hosted_mcp_approvals_by_id.get(request_id) if request_id else None\n        if not approval_item or not request_id:\n            continue\n\n        tool_name = RunContextWrapper._resolve_tool_name(approval_item)\n        approved = context_wrapper.get_approval_status(\n            tool_name=tool_name,\n            call_id=request_id,\n            existing_pending=approval_item,\n        )\n\n        if approved is not None:\n            raw_item: McpApprovalResponse = {\n                \"type\": \"mcp_approval_response\",\n                \"approval_request_id\": request_id,\n                \"approve\": approved,\n            }\n            response_item = MCPApprovalResponseItem(raw_item=raw_item, agent=agent)\n            append_item(response_item)\n            continue\n\n        if approval_item not in pending_hosted_mcp_approvals:\n            pending_hosted_mcp_approvals.append(approval_item)\n        pending_hosted_mcp_approval_ids.add(request_id)\n        append_item(approval_item)\n\n    return pending_hosted_mcp_approvals, pending_hosted_mcp_approval_ids\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.collect_manual_mcp_approvals","title":"collect_manual_mcp_approvals","text":"<pre><code>collect_manual_mcp_approvals(\n    *,\n    agent: Agent[Any],\n    requests: Sequence[Any],\n    context_wrapper: RunContextWrapper[Any],\n    existing_pending_by_call_id: Mapping[\n        str, ToolApprovalItem\n    ]\n    | None = None,\n) -&gt; tuple[\n    list[MCPApprovalResponseItem], list[ToolApprovalItem]\n]\n</code></pre> <p>Bridge hosted MCP approval requests with manual approvals to keep state consistent.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def collect_manual_mcp_approvals(\n    *,\n    agent: Agent[Any],\n    requests: Sequence[Any],\n    context_wrapper: RunContextWrapper[Any],\n    existing_pending_by_call_id: Mapping[str, ToolApprovalItem] | None = None,\n) -&gt; tuple[list[MCPApprovalResponseItem], list[ToolApprovalItem]]:\n    \"\"\"Bridge hosted MCP approval requests with manual approvals to keep state consistent.\"\"\"\n    pending_lookup = existing_pending_by_call_id or {}\n    approved: list[MCPApprovalResponseItem] = []\n    pending: list[ToolApprovalItem] = []\n    seen_request_ids: set[str] = set()\n\n    for request in requests:\n        request_item = get_mapping_or_attr(request, \"request_item\")\n        request_id = extract_mcp_request_id_from_run(request)\n        # The Responses API returns mcp_approval_request items with an id to correlate approvals.\n        # See https://platform.openai.com/docs/guides/tools-connectors-mcp#approvals\n        if request_id and request_id in seen_request_ids:\n            continue\n        if request_id:\n            seen_request_ids.add(request_id)\n\n        tool_name = RunContextWrapper._to_str_or_none(getattr(request_item, \"name\", None))\n        tool_name = tool_name or get_mapping_or_attr(request, \"mcp_tool\").name\n\n        existing_pending = pending_lookup.get(request_id or \"\")\n        approval_status = context_wrapper.get_approval_status(\n            tool_name, request_id or \"\", existing_pending=existing_pending\n        )\n\n        if approval_status is not None and request_id:\n            approval_response_raw: McpApprovalResponse = {\n                \"type\": \"mcp_approval_response\",\n                \"approval_request_id\": request_id,\n                \"approve\": approval_status,\n            }\n            approved.append(MCPApprovalResponseItem(raw_item=approval_response_raw, agent=agent))\n            continue\n\n        if approval_status is not None:\n            continue\n\n        pending.append(\n            existing_pending\n            or ToolApprovalItem(\n                agent=agent,\n                raw_item=request_item,\n                tool_name=tool_name,\n            )\n        )\n\n    return approved, pending\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.index_approval_items_by_call_id","title":"index_approval_items_by_call_id","text":"<pre><code>index_approval_items_by_call_id(\n    items: Sequence[RunItem],\n) -&gt; dict[str, ToolApprovalItem]\n</code></pre> <p>Build a mapping of tool call IDs to pending approval items.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def index_approval_items_by_call_id(items: Sequence[RunItem]) -&gt; dict[str, ToolApprovalItem]:\n    \"\"\"Build a mapping of tool call IDs to pending approval items.\"\"\"\n    approvals: dict[str, ToolApprovalItem] = {}\n    for item in items:\n        if not isinstance(item, ToolApprovalItem):\n            continue\n        call_id = extract_tool_call_id(item.raw_item)\n        if call_id:\n            approvals[call_id] = item\n    return approvals\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.should_keep_hosted_mcp_item","title":"should_keep_hosted_mcp_item","text":"<pre><code>should_keep_hosted_mcp_item(\n    item: RunItem,\n    *,\n    pending_hosted_mcp_approvals: Sequence[\n        ToolApprovalItem\n    ],\n    pending_hosted_mcp_approval_ids: set[str],\n) -&gt; bool\n</code></pre> <p>Keep only hosted MCP approvals that match pending requests from the provider.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>def should_keep_hosted_mcp_item(\n    item: RunItem,\n    *,\n    pending_hosted_mcp_approvals: Sequence[ToolApprovalItem],\n    pending_hosted_mcp_approval_ids: set[str],\n) -&gt; bool:\n    \"\"\"Keep only hosted MCP approvals that match pending requests from the provider.\"\"\"\n    if not isinstance(item, ToolApprovalItem):\n        return True\n    if not _is_hosted_mcp_approval_request(item.raw_item):\n        return False\n    request_id = extract_mcp_request_id(item.raw_item)\n    return item in pending_hosted_mcp_approvals or (\n        request_id is not None and request_id in pending_hosted_mcp_approval_ids\n    )\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.execute_function_tool_calls","title":"execute_function_tool_calls  <code>async</code>","text":"<pre><code>execute_function_tool_calls(\n    *,\n    agent: Agent[Any],\n    tool_runs: list[ToolRunFunction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; tuple[\n    list[FunctionToolResult],\n    list[ToolInputGuardrailResult],\n    list[ToolOutputGuardrailResult],\n]\n</code></pre> <p>Execute function tool calls with approvals, guardrails, and hooks.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_function_tool_calls(\n    *,\n    agent: Agent[Any],\n    tool_runs: list[ToolRunFunction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; tuple[\n    list[FunctionToolResult], list[ToolInputGuardrailResult], list[ToolOutputGuardrailResult]\n]:\n    \"\"\"Execute function tool calls with approvals, guardrails, and hooks.\"\"\"\n    tool_input_guardrail_results: list[ToolInputGuardrailResult] = []\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult] = []\n\n    async def run_single_tool(func_tool: FunctionTool, tool_call: ResponseFunctionToolCall) -&gt; Any:\n        with function_span(func_tool.name) as span_fn:\n            tool_context = ToolContext.from_agent_context(\n                context_wrapper,\n                tool_call.call_id,\n                tool_call=tool_call,\n            )\n            agent_hooks = agent.hooks\n            if config.trace_include_sensitive_data:\n                span_fn.span_data.input = tool_call.arguments\n            try:\n                needs_approval_result = await function_needs_approval(\n                    func_tool,\n                    context_wrapper,\n                    tool_call,\n                )\n\n                if needs_approval_result:\n                    approval_status = context_wrapper.get_approval_status(\n                        func_tool.name,\n                        tool_call.call_id,\n                    )\n\n                    if approval_status is None:\n                        approval_item = ToolApprovalItem(\n                            agent=agent, raw_item=tool_call, tool_name=func_tool.name\n                        )\n                        return FunctionToolResult(\n                            tool=func_tool, output=None, run_item=approval_item\n                        )\n\n                    if approval_status is False:\n                        rejection_message = await resolve_approval_rejection_message(\n                            context_wrapper=context_wrapper,\n                            run_config=config,\n                            tool_type=\"function\",\n                            tool_name=func_tool.name,\n                            call_id=tool_call.call_id,\n                        )\n                        span_fn.set_error(\n                            SpanError(\n                                message=rejection_message,\n                                data={\n                                    \"tool_name\": func_tool.name,\n                                    \"error\": (\n                                        f\"Tool execution for {tool_call.call_id} \"\n                                        \"was manually rejected by user.\"\n                                    ),\n                                },\n                            )\n                        )\n                        result = rejection_message\n                        span_fn.span_data.output = result\n                        return FunctionToolResult(\n                            tool=func_tool,\n                            output=result,\n                            run_item=function_rejection_item(\n                                agent,\n                                tool_call,\n                                rejection_message=rejection_message,\n                            ),\n                        )\n\n                rejected_message = await _execute_tool_input_guardrails(\n                    func_tool=func_tool,\n                    tool_context=tool_context,\n                    agent=agent,\n                    tool_input_guardrail_results=tool_input_guardrail_results,\n                )\n\n                if rejected_message is not None:\n                    final_result = rejected_message\n                else:\n                    await asyncio.gather(\n                        hooks.on_tool_start(tool_context, agent, func_tool),\n                        (\n                            agent_hooks.on_tool_start(tool_context, agent, func_tool)\n                            if agent_hooks\n                            else _coro.noop_coroutine()\n                        ),\n                    )\n                    real_result = await func_tool.on_invoke_tool(tool_context, tool_call.arguments)\n\n                    final_result = await _execute_tool_output_guardrails(\n                        func_tool=func_tool,\n                        tool_context=tool_context,\n                        agent=agent,\n                        real_result=real_result,\n                        tool_output_guardrail_results=tool_output_guardrail_results,\n                    )\n\n                    await asyncio.gather(\n                        hooks.on_tool_end(tool_context, agent, func_tool, final_result),\n                        (\n                            agent_hooks.on_tool_end(tool_context, agent, func_tool, final_result)\n                            if agent_hooks\n                            else _coro.noop_coroutine()\n                        ),\n                    )\n                result = final_result\n            except Exception as e:\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Error running tool\",\n                        data={\"tool_name\": func_tool.name, \"error\": str(e)},\n                    )\n                )\n                if isinstance(e, AgentsException):\n                    raise e\n                raise UserError(f\"Error running tool {func_tool.name}: {e}\") from e\n\n            if config.trace_include_sensitive_data:\n                span_fn.span_data.output = result\n        return result\n\n    tasks = []\n    for tool_run in tool_runs:\n        function_tool = tool_run.function_tool\n        tasks.append(run_single_tool(function_tool, tool_run.tool_call))\n\n    results = await asyncio.gather(*tasks)\n\n    function_tool_results = []\n    for tool_run, result in zip(tool_runs, results):\n        if isinstance(result, FunctionToolResult):\n            nested_run_result = consume_agent_tool_run_result(tool_run.tool_call)\n            if nested_run_result:\n                result.agent_run_result = nested_run_result\n                nested_interruptions_from_result: list[ToolApprovalItem] = (\n                    nested_run_result.interruptions\n                    if hasattr(nested_run_result, \"interruptions\")\n                    else []\n                )\n                if nested_interruptions_from_result:\n                    result.interruptions = nested_interruptions_from_result\n\n            function_tool_results.append(result)\n        else:\n            nested_run_result = peek_agent_tool_run_result(tool_run.tool_call)\n            nested_interruptions: list[ToolApprovalItem] = []\n            if nested_run_result:\n                nested_interruptions = (\n                    nested_run_result.interruptions\n                    if hasattr(nested_run_result, \"interruptions\")\n                    else []\n                )\n            if nested_run_result and not nested_interruptions:\n                nested_run_result = consume_agent_tool_run_result(tool_run.tool_call)\n            elif nested_run_result is None:\n                nested_run_result = consume_agent_tool_run_result(tool_run.tool_call)\n                if nested_run_result:\n                    nested_interruptions = (\n                        nested_run_result.interruptions\n                        if hasattr(nested_run_result, \"interruptions\")\n                        else []\n                    )\n\n            run_item: RunItem | None = None\n            if not nested_interruptions:\n                run_item = ToolCallOutputItem(\n                    output=result,\n                    raw_item=ItemHelpers.tool_call_output_item(tool_run.tool_call, result),\n                    agent=agent,\n                )\n            else:\n                # Skip tool output until nested interruptions are resolved.\n                run_item = None\n\n            function_tool_results.append(\n                FunctionToolResult(\n                    tool=tool_run.function_tool,\n                    output=result,\n                    run_item=run_item,\n                    interruptions=nested_interruptions,\n                    agent_run_result=nested_run_result,\n                )\n            )\n\n    return function_tool_results, tool_input_guardrail_results, tool_output_guardrail_results\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.execute_local_shell_calls","title":"execute_local_shell_calls  <code>async</code>","text":"<pre><code>execute_local_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunLocalShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run local shell tool calls serially and wrap outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_local_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunLocalShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run local shell tool calls serially and wrap outputs.\"\"\"\n    from .tool_actions import LocalShellAction\n\n    results: list[RunItem] = []\n    for call in calls:\n        results.append(\n            await LocalShellAction.execute(\n                agent=agent,\n                call=call,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n            )\n        )\n    return results\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.execute_shell_calls","title":"execute_shell_calls  <code>async</code>","text":"<pre><code>execute_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run shell tool calls serially and wrap outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_shell_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunShellCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run shell tool calls serially and wrap outputs.\"\"\"\n    from .tool_actions import ShellAction\n\n    results: list[RunItem] = []\n    for call in calls:\n        results.append(\n            await ShellAction.execute(\n                agent=agent,\n                call=call,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n            )\n        )\n    return results\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.execute_apply_patch_calls","title":"execute_apply_patch_calls  <code>async</code>","text":"<pre><code>execute_apply_patch_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunApplyPatchCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run apply_patch tool calls serially and normalize outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_apply_patch_calls(\n    *,\n    agent: Agent[Any],\n    calls: list[ToolRunApplyPatchCall],\n    context_wrapper: RunContextWrapper[Any],\n    hooks: RunHooks[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run apply_patch tool calls serially and normalize outputs.\"\"\"\n    from .tool_actions import ApplyPatchAction\n\n    results: list[RunItem] = []\n    for call in calls:\n        results.append(\n            await ApplyPatchAction.execute(\n                agent=agent,\n                call=call,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n            )\n        )\n    return results\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.execute_computer_actions","title":"execute_computer_actions  <code>async</code>","text":"<pre><code>execute_computer_actions(\n    *,\n    agent: Agent[Any],\n    actions: list[ToolRunComputerAction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]\n</code></pre> <p>Run computer actions serially and emit screenshot outputs.</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_computer_actions(\n    *,\n    agent: Agent[Any],\n    actions: list[ToolRunComputerAction],\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    config: RunConfig,\n) -&gt; list[RunItem]:\n    \"\"\"Run computer actions serially and emit screenshot outputs.\"\"\"\n    from .tool_actions import ComputerAction\n\n    results: list[RunItem] = []\n    for action in actions:\n        acknowledged: list[ComputerCallOutputAcknowledgedSafetyCheck] | None = None\n        if action.tool_call.pending_safety_checks and action.computer_tool.on_safety_check:\n            acknowledged = []\n            for check in action.tool_call.pending_safety_checks:\n                data = ComputerToolSafetyCheckData(\n                    ctx_wrapper=context_wrapper,\n                    agent=agent,\n                    tool_call=action.tool_call,\n                    safety_check=check,\n                )\n                maybe = action.computer_tool.on_safety_check(data)\n                ack = await maybe if inspect.isawaitable(maybe) else maybe\n                if ack:\n                    acknowledged.append(\n                        ComputerCallOutputAcknowledgedSafetyCheck(\n                            id=check.id,\n                            code=check.code,\n                            message=check.message,\n                        )\n                    )\n                else:\n                    raise UserError(\"Computer tool safety check was not acknowledged\")\n\n        results.append(\n            await ComputerAction.execute(\n                agent=agent,\n                action=action,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                config=config,\n                acknowledged_safety_checks=acknowledged,\n            )\n        )\n\n    return results\n</code></pre>"},{"location":"ref/run_internal/tool_execution/#agents.run_internal.tool_execution.execute_approved_tools","title":"execute_approved_tools  <code>async</code>","text":"<pre><code>execute_approved_tools(\n    *,\n    agent: Agent[Any],\n    interruptions: list[Any],\n    context_wrapper: RunContextWrapper[Any],\n    generated_items: list[RunItem],\n    run_config: RunConfig,\n    hooks: RunHooks[Any],\n    all_tools: list[Tool] | None = None,\n) -&gt; None\n</code></pre> <p>Execute tools that have been approved after an interruption (HITL resume path).</p> Source code in <code>src/agents/run_internal/tool_execution.py</code> <pre><code>async def execute_approved_tools(\n    *,\n    agent: Agent[Any],\n    interruptions: list[Any],\n    context_wrapper: RunContextWrapper[Any],\n    generated_items: list[RunItem],\n    run_config: RunConfig,\n    hooks: RunHooks[Any],\n    all_tools: list[Tool] | None = None,\n) -&gt; None:\n    \"\"\"Execute tools that have been approved after an interruption (HITL resume path).\"\"\"\n    tool_runs: list[ToolRunFunction] = []\n    tool_map: dict[str, Tool] = {tool.name: tool for tool in all_tools or []}\n\n    def _append_error(message: str, *, tool_call: Any, tool_name: str, call_id: str) -&gt; None:\n        append_approval_error_output(\n            message=message,\n            tool_call=tool_call,\n            tool_name=tool_name,\n            call_id=call_id,\n            generated_items=generated_items,\n            agent=agent,\n        )\n\n    async def _resolve_tool_run(\n        interruption: Any,\n    ) -&gt; tuple[ResponseFunctionToolCall, FunctionTool, str, str] | None:\n        tool_call = interruption.raw_item\n        tool_name = interruption.name or RunContextWrapper._resolve_tool_name(interruption)\n        if not tool_name:\n            _append_error(\n                message=\"Tool approval item missing tool name.\",\n                tool_call=tool_call,\n                tool_name=\"unknown\",\n                call_id=\"unknown\",\n            )\n            return None\n\n        call_id = extract_tool_call_id(tool_call)\n        if not call_id:\n            _append_error(\n                message=\"Tool approval item missing call ID.\",\n                tool_call=tool_call,\n                tool_name=tool_name,\n                call_id=\"unknown\",\n            )\n            return None\n\n        approval_status = context_wrapper.get_approval_status(\n            tool_name, call_id, existing_pending=interruption\n        )\n        if approval_status is False:\n            resolved_tool = tool_map.get(tool_name)\n            message = REJECTION_MESSAGE\n            if isinstance(resolved_tool, FunctionTool):\n                message = await resolve_approval_rejection_message(\n                    context_wrapper=context_wrapper,\n                    run_config=run_config,\n                    tool_type=\"function\",\n                    tool_name=tool_name,\n                    call_id=call_id,\n                )\n            _append_error(\n                message=message,\n                tool_call=tool_call,\n                tool_name=tool_name,\n                call_id=call_id,\n            )\n            return None\n\n        if approval_status is not True:\n            _append_error(\n                message=\"Tool approval status unclear.\",\n                tool_call=tool_call,\n                tool_name=tool_name,\n                call_id=call_id,\n            )\n            return None\n\n        tool = tool_map.get(tool_name)\n        if tool is None:\n            _append_error(\n                message=f\"Tool '{tool_name}' not found.\",\n                tool_call=tool_call,\n                tool_name=tool_name,\n                call_id=call_id,\n            )\n            return None\n\n        if not isinstance(tool, FunctionTool):\n            _append_error(\n                message=f\"Tool '{tool_name}' is not a function tool.\",\n                tool_call=tool_call,\n                tool_name=tool_name,\n                call_id=call_id,\n            )\n            return None\n\n        if not isinstance(tool_call, ResponseFunctionToolCall):\n            _append_error(\n                message=(\n                    f\"Tool '{tool_name}' approval item has invalid raw_item type for execution.\"\n                ),\n                tool_call=tool_call,\n                tool_name=tool_name,\n                call_id=call_id,\n            )\n            return None\n\n        return tool_call, tool, tool_name, call_id\n\n    for interruption in interruptions:\n        resolved = await _resolve_tool_run(interruption)\n        if resolved is None:\n            continue\n        tool_call, tool, tool_name, _ = resolved\n        tool_runs.append(ToolRunFunction(function_tool=tool, tool_call=tool_call))\n\n    if tool_runs:\n        function_results, _, _ = await execute_function_tool_calls(\n            agent=agent,\n            tool_runs=tool_runs,\n            hooks=hooks,\n            context_wrapper=context_wrapper,\n            config=run_config,\n        )\n        for result in function_results:\n            if isinstance(result.run_item, RunItemBase):\n                generated_items.append(result.run_item)\n</code></pre>"},{"location":"ref/run_internal/tool_planning/","title":"<code>Tool Planning</code>","text":""},{"location":"ref/run_internal/tool_planning/#agents.run_internal.tool_planning.ToolExecutionPlan","title":"ToolExecutionPlan  <code>dataclass</code>","text":"<p>Represents tool execution work to perform in a single turn.</p> Source code in <code>src/agents/run_internal/tool_planning.py</code> <pre><code>@_dc.dataclass\nclass ToolExecutionPlan:\n    \"\"\"Represents tool execution work to perform in a single turn.\"\"\"\n\n    function_runs: list[ToolRunFunction] = _dc.field(default_factory=list)\n    computer_actions: list[ToolRunComputerAction] = _dc.field(default_factory=list)\n    shell_calls: list[ToolRunShellCall] = _dc.field(default_factory=list)\n    apply_patch_calls: list[ToolRunApplyPatchCall] = _dc.field(default_factory=list)\n    local_shell_calls: list[ToolRunLocalShellCall] = _dc.field(default_factory=list)\n    pending_interruptions: list[ToolApprovalItem] = _dc.field(default_factory=list)\n    approved_mcp_responses: list[RunItem] = _dc.field(default_factory=list)\n    mcp_requests_with_callback: list[ToolRunMCPApprovalRequest] = _dc.field(default_factory=list)\n\n    @property\n    def has_interruptions(self) -&gt; bool:\n        return bool(self.pending_interruptions)\n</code></pre>"},{"location":"ref/run_internal/tool_planning/#agents.run_internal.tool_planning.execute_mcp_approval_requests","title":"execute_mcp_approval_requests  <code>async</code>","text":"<pre><code>execute_mcp_approval_requests(\n    *,\n    agent: Agent[Any],\n    approval_requests: list[ToolRunMCPApprovalRequest],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[RunItem]\n</code></pre> <p>Run hosted MCP approval callbacks and return approval response items.</p> Source code in <code>src/agents/run_internal/tool_planning.py</code> <pre><code>async def execute_mcp_approval_requests(\n    *,\n    agent: Agent[Any],\n    approval_requests: list[ToolRunMCPApprovalRequest],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[RunItem]:\n    \"\"\"Run hosted MCP approval callbacks and return approval response items.\"\"\"\n\n    async def run_single_approval(approval_request: ToolRunMCPApprovalRequest) -&gt; RunItem:\n        callback = approval_request.mcp_tool.on_approval_request\n        assert callback is not None, \"Callback is required for MCP approval requests\"\n        maybe_awaitable_result = callback(\n            MCPToolApprovalRequest(context_wrapper, approval_request.request_item)\n        )\n        if inspect.isawaitable(maybe_awaitable_result):\n            result = await maybe_awaitable_result\n        else:\n            result = maybe_awaitable_result\n        reason = result.get(\"reason\", None)\n        request_item = approval_request.request_item\n        request_id = (\n            request_item.id\n            if hasattr(request_item, \"id\")\n            else cast(dict[str, Any], request_item).get(\"id\", \"\")\n        )\n        raw_item: McpApprovalResponse = {\n            \"approval_request_id\": request_id,\n            \"approve\": result[\"approve\"],\n            \"type\": \"mcp_approval_response\",\n        }\n        if not result[\"approve\"] and reason:\n            raw_item[\"reason\"] = reason\n        return MCPApprovalResponseItem(\n            raw_item=raw_item,\n            agent=agent,\n        )\n\n    tasks = [run_single_approval(approval_request) for approval_request in approval_requests]\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"ref/run_internal/tool_use_tracker/","title":"<code>Tool Use Tracker</code>","text":"<p>Tool-use tracking utilities. Hosts AgentToolUseTracker and helpers to serialize/deserialize its state plus lightweight tool-call type utilities. Internal use only.</p>"},{"location":"ref/run_internal/tool_use_tracker/#agents.run_internal.tool_use_tracker.AgentToolUseTracker","title":"AgentToolUseTracker","text":"<p>Track which tools an agent has used to support model_settings resets.</p> Source code in <code>src/agents/run_internal/tool_use_tracker.py</code> <pre><code>class AgentToolUseTracker:\n    \"\"\"Track which tools an agent has used to support model_settings resets.\"\"\"\n\n    def __init__(self) -&gt; None:\n        # Name-keyed map is used for serialization/hydration only.\n        self.agent_map: dict[str, set[str]] = {}\n        # Instance-keyed list is used for runtime checks.\n        self.agent_to_tools: list[tuple[Agent[Any], list[str]]] = []\n\n    def record_used_tools(self, agent: Agent[Any], tools: list[ToolRunFunction]) -&gt; None:\n        tool_names = [tool.function_tool.name for tool in tools]\n        self.add_tool_use(agent, tool_names)\n\n    def add_tool_use(self, agent: Agent[Any], tool_names: list[str]) -&gt; None:\n        \"\"\"Maintain compatibility for callers that append tool usage directly.\"\"\"\n        agent_name = getattr(agent, \"name\", agent.__class__.__name__)\n        names_set = self.agent_map.setdefault(agent_name, set())\n        names_set.update(tool_names)\n\n        existing = next((item for item in self.agent_to_tools if item[0] is agent), None)\n        if existing:\n            existing[1].extend(tool_names)\n        else:\n            self.agent_to_tools.append((agent, list(tool_names)))\n\n    def has_used_tools(self, agent: Agent[Any]) -&gt; bool:\n        existing = next((item for item in self.agent_to_tools if item[0] is agent), None)\n        return bool(existing and existing[1])\n\n    def as_serializable(self) -&gt; dict[str, list[str]]:\n        if self.agent_map:\n            return {name: sorted(tool_names) for name, tool_names in self.agent_map.items()}\n\n        snapshot: dict[str, set[str]] = {}\n        for agent, names in self.agent_to_tools:\n            agent_name = getattr(agent, \"name\", agent.__class__.__name__)\n            snapshot.setdefault(agent_name, set()).update(names)\n        return {name: sorted(tool_names) for name, tool_names in snapshot.items()}\n\n    @classmethod\n    def from_serializable(cls, data: dict[str, list[str]]) -&gt; AgentToolUseTracker:\n        tracker = cls()\n        tracker.agent_map = {name: set(tools) for name, tools in data.items()}\n        return tracker\n</code></pre>"},{"location":"ref/run_internal/tool_use_tracker/#agents.run_internal.tool_use_tracker.AgentToolUseTracker.add_tool_use","title":"add_tool_use","text":"<pre><code>add_tool_use(\n    agent: Agent[Any], tool_names: list[str]\n) -&gt; None\n</code></pre> <p>Maintain compatibility for callers that append tool usage directly.</p> Source code in <code>src/agents/run_internal/tool_use_tracker.py</code> <pre><code>def add_tool_use(self, agent: Agent[Any], tool_names: list[str]) -&gt; None:\n    \"\"\"Maintain compatibility for callers that append tool usage directly.\"\"\"\n    agent_name = getattr(agent, \"name\", agent.__class__.__name__)\n    names_set = self.agent_map.setdefault(agent_name, set())\n    names_set.update(tool_names)\n\n    existing = next((item for item in self.agent_to_tools if item[0] is agent), None)\n    if existing:\n        existing[1].extend(tool_names)\n    else:\n        self.agent_to_tools.append((agent, list(tool_names)))\n</code></pre>"},{"location":"ref/run_internal/tool_use_tracker/#agents.run_internal.tool_use_tracker.serialize_tool_use_tracker","title":"serialize_tool_use_tracker","text":"<pre><code>serialize_tool_use_tracker(\n    tool_use_tracker: AgentToolUseTracker,\n) -&gt; dict[str, list[str]]\n</code></pre> <p>Convert the AgentToolUseTracker into a serializable snapshot.</p> Source code in <code>src/agents/run_internal/tool_use_tracker.py</code> <pre><code>def serialize_tool_use_tracker(tool_use_tracker: AgentToolUseTracker) -&gt; dict[str, list[str]]:\n    \"\"\"Convert the AgentToolUseTracker into a serializable snapshot.\"\"\"\n    snapshot: dict[str, list[str]] = {}\n    for agent, tool_names in tool_use_tracker.agent_to_tools:\n        snapshot[agent.name] = list(tool_names)\n    return snapshot\n</code></pre>"},{"location":"ref/run_internal/tool_use_tracker/#agents.run_internal.tool_use_tracker.hydrate_tool_use_tracker","title":"hydrate_tool_use_tracker","text":"<pre><code>hydrate_tool_use_tracker(\n    tool_use_tracker: AgentToolUseTracker,\n    run_state: Any,\n    starting_agent: Agent[Any],\n) -&gt; None\n</code></pre> <p>Seed a fresh AgentToolUseTracker using the snapshot stored on the RunState.</p> Source code in <code>src/agents/run_internal/tool_use_tracker.py</code> <pre><code>def hydrate_tool_use_tracker(\n    tool_use_tracker: AgentToolUseTracker,\n    run_state: Any,\n    starting_agent: Agent[Any],\n) -&gt; None:\n    \"\"\"Seed a fresh AgentToolUseTracker using the snapshot stored on the RunState.\"\"\"\n    snapshot = run_state.get_tool_use_tracker_snapshot()\n    if not snapshot:\n        return\n\n    agent_map = _build_agent_map(starting_agent)\n    for agent_name, tool_names in snapshot.items():\n        agent = agent_map.get(agent_name)\n        if agent is None:\n            continue\n        tool_use_tracker.add_tool_use(agent, list(tool_names))\n</code></pre>"},{"location":"ref/run_internal/tool_use_tracker/#agents.run_internal.tool_use_tracker.get_tool_call_types","title":"get_tool_call_types","text":"<pre><code>get_tool_call_types() -&gt; tuple[type, ...]\n</code></pre> <p>Return the concrete classes that represent tool call outputs.</p> Source code in <code>src/agents/run_internal/tool_use_tracker.py</code> <pre><code>def get_tool_call_types() -&gt; tuple[type, ...]:\n    \"\"\"Return the concrete classes that represent tool call outputs.\"\"\"\n    normalized_types: list[type] = []\n    for type_hint in get_args(ToolCallItemTypes):\n        origin = get_origin(type_hint)\n        candidate = origin or type_hint\n        if isinstance(candidate, type):\n            normalized_types.append(candidate)\n    return tuple(normalized_types)\n</code></pre>"},{"location":"ref/run_internal/turn_preparation/","title":"<code>Turn Preparation</code>","text":""},{"location":"ref/run_internal/turn_preparation/#agents.run_internal.turn_preparation.validate_run_hooks","title":"validate_run_hooks","text":"<pre><code>validate_run_hooks(\n    hooks: RunHooksBase[Any, Agent[Any]]\n    | AgentHooksBase[Any, Agent[Any]]\n    | Any\n    | None,\n) -&gt; RunHooks[Any]\n</code></pre> <p>Normalize hooks input and enforce RunHooks type.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>def validate_run_hooks(\n    hooks: RunHooksBase[Any, Agent[Any]] | AgentHooksBase[Any, Agent[Any]] | Any | None,\n) -&gt; RunHooks[Any]:\n    \"\"\"Normalize hooks input and enforce RunHooks type.\"\"\"\n    if hooks is None:\n        return RunHooks[Any]()\n    input_hook_type = type(hooks).__name__\n    if isinstance(hooks, AgentHooksBase):\n        raise TypeError(\n            \"Run hooks must be instances of RunHooks. \"\n            f\"Received agent-scoped hooks ({input_hook_type}). \"\n            \"Attach AgentHooks to an Agent via Agent(..., hooks=...).\"\n        )\n    if not isinstance(hooks, RunHooksBase):\n        raise TypeError(f\"Run hooks must be instances of RunHooks. Received {input_hook_type}.\")\n    return hooks\n</code></pre>"},{"location":"ref/run_internal/turn_preparation/#agents.run_internal.turn_preparation.maybe_filter_model_input","title":"maybe_filter_model_input  <code>async</code>","text":"<pre><code>maybe_filter_model_input(\n    *,\n    agent: Agent[TContext],\n    run_config: RunConfig,\n    context_wrapper: RunContextWrapper[TContext],\n    input_items: list[TResponseInputItem],\n    system_instructions: str | None,\n) -&gt; ModelInputData\n</code></pre> <p>Apply optional call_model_input_filter to modify model input.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>async def maybe_filter_model_input(\n    *,\n    agent: Agent[TContext],\n    run_config: RunConfig,\n    context_wrapper: RunContextWrapper[TContext],\n    input_items: list[TResponseInputItem],\n    system_instructions: str | None,\n) -&gt; ModelInputData:\n    \"\"\"Apply optional call_model_input_filter to modify model input.\"\"\"\n    effective_instructions = system_instructions\n    effective_input: list[TResponseInputItem] = input_items\n\n    if run_config.call_model_input_filter is None:\n        return ModelInputData(input=effective_input, instructions=effective_instructions)\n\n    try:\n        model_input = ModelInputData(\n            input=effective_input.copy(),\n            instructions=effective_instructions,\n        )\n        filter_payload: CallModelData[TContext] = CallModelData(\n            model_data=model_input,\n            agent=agent,\n            context=context_wrapper.context,\n        )\n        maybe_updated = run_config.call_model_input_filter(filter_payload)\n        updated = await maybe_updated if inspect.isawaitable(maybe_updated) else maybe_updated\n        if not isinstance(updated, ModelInputData):\n            raise UserError(\"call_model_input_filter must return a ModelInputData instance\")\n        return updated\n    except Exception as e:\n        _error_tracing.attach_error_to_current_span(\n            SpanError(message=\"Error in call_model_input_filter\", data={\"error\": str(e)})\n        )\n        raise\n</code></pre>"},{"location":"ref/run_internal/turn_preparation/#agents.run_internal.turn_preparation.get_handoffs","title":"get_handoffs  <code>async</code>","text":"<pre><code>get_handoffs(\n    agent: Agent[Any],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[Handoff]\n</code></pre> <p>Return enabled handoffs for the agent.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>async def get_handoffs(agent: Agent[Any], context_wrapper: RunContextWrapper[Any]) -&gt; list[Handoff]:\n    \"\"\"Return enabled handoffs for the agent.\"\"\"\n    handoffs = []\n    for handoff_item in agent.handoffs:\n        if isinstance(handoff_item, Handoff):\n            handoffs.append(handoff_item)\n        elif isinstance(handoff_item, Agent):\n            handoffs.append(handoff(handoff_item))\n\n    async def check_handoff_enabled(handoff_obj: Handoff) -&gt; bool:\n        attr = handoff_obj.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(context_wrapper, agent)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(check_handoff_enabled(h) for h in handoffs))\n    enabled: list[Handoff] = [h for h, ok in zip(handoffs, results) if ok]\n    return enabled\n</code></pre>"},{"location":"ref/run_internal/turn_preparation/#agents.run_internal.turn_preparation.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    agent: Agent[Any],\n    context_wrapper: RunContextWrapper[Any],\n) -&gt; list[Tool]\n</code></pre> <p>Fetch all tools available to the agent.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>async def get_all_tools(agent: Agent[Any], context_wrapper: RunContextWrapper[Any]) -&gt; list[Tool]:\n    \"\"\"Fetch all tools available to the agent.\"\"\"\n    return await agent.get_all_tools(context_wrapper)\n</code></pre>"},{"location":"ref/run_internal/turn_preparation/#agents.run_internal.turn_preparation.get_output_schema","title":"get_output_schema","text":"<pre><code>get_output_schema(\n    agent: Agent[Any],\n) -&gt; AgentOutputSchemaBase | None\n</code></pre> <p>Return the resolved output schema for the agent, if any.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>def get_output_schema(agent: Agent[Any]) -&gt; AgentOutputSchemaBase | None:\n    \"\"\"Return the resolved output schema for the agent, if any.\"\"\"\n    if agent.output_type is None or agent.output_type is str:\n        return None\n    elif isinstance(agent.output_type, AgentOutputSchemaBase):\n        return agent.output_type\n\n    return AgentOutputSchema(agent.output_type)\n</code></pre>"},{"location":"ref/run_internal/turn_preparation/#agents.run_internal.turn_preparation.get_model","title":"get_model","text":"<pre><code>get_model(\n    agent: Agent[Any], run_config: RunConfig\n) -&gt; Model\n</code></pre> <p>Resolve the model instance for this run.</p> Source code in <code>src/agents/run_internal/turn_preparation.py</code> <pre><code>def get_model(agent: Agent[Any], run_config: RunConfig) -&gt; Model:\n    \"\"\"Resolve the model instance for this run.\"\"\"\n    if isinstance(run_config.model, Model):\n        return run_config.model\n    elif isinstance(run_config.model, str):\n        return run_config.model_provider.get_model(run_config.model)\n    elif isinstance(agent.model, Model):\n        return agent.model\n\n    return run_config.model_provider.get_model(agent.model)\n</code></pre>"},{"location":"ref/run_internal/turn_resolution/","title":"<code>Turn Resolution</code>","text":""},{"location":"ref/run_internal/turn_resolution/#agents.run_internal.turn_resolution.execute_final_output_step","title":"execute_final_output_step  <code>async</code>","text":"<pre><code>execute_final_output_step(\n    *,\n    agent: Agent[Any],\n    original_input: str | list[TResponseInputItem],\n    new_response: ModelResponse,\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    final_output: Any,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    tool_input_guardrail_results: list[\n        ToolInputGuardrailResult\n    ],\n    tool_output_guardrail_results: list[\n        ToolOutputGuardrailResult\n    ],\n    run_final_output_hooks_fn: Callable[\n        [\n            Agent[Any],\n            RunHooks[Any],\n            RunContextWrapper[Any],\n            Any,\n        ],\n        Awaitable[None],\n    ]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Finalize a turn once final output is known and run end hooks.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def execute_final_output_step(\n    *,\n    agent: Agent[Any],\n    original_input: str | list[TResponseInputItem],\n    new_response: ModelResponse,\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    final_output: Any,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    tool_input_guardrail_results: list[ToolInputGuardrailResult],\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult],\n    run_final_output_hooks_fn: Callable[\n        [Agent[Any], RunHooks[Any], RunContextWrapper[Any], Any], Awaitable[None]\n    ]\n    | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Finalize a turn once final output is known and run end hooks.\"\"\"\n    final_output_hooks = run_final_output_hooks_fn or run_final_output_hooks\n    await final_output_hooks(agent, hooks, context_wrapper, final_output)\n\n    return SingleStepResult(\n        original_input=original_input,\n        model_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        next_step=NextStepFinalOutput(final_output),\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n        output_guardrail_results=[],\n    )\n</code></pre>"},{"location":"ref/run_internal/turn_resolution/#agents.run_internal.turn_resolution.execute_final_output","title":"execute_final_output  <code>async</code>","text":"<pre><code>execute_final_output(\n    *,\n    agent: Agent[Any],\n    original_input: str | list[TResponseInputItem],\n    new_response: ModelResponse,\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    final_output: Any,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    tool_input_guardrail_results: list[\n        ToolInputGuardrailResult\n    ],\n    tool_output_guardrail_results: list[\n        ToolOutputGuardrailResult\n    ],\n    run_final_output_hooks_fn: Callable[\n        [\n            Agent[Any],\n            RunHooks[Any],\n            RunContextWrapper[Any],\n            Any,\n        ],\n        Awaitable[None],\n    ]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Convenience wrapper to finalize a turn and run end hooks.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def execute_final_output(\n    *,\n    agent: Agent[Any],\n    original_input: str | list[TResponseInputItem],\n    new_response: ModelResponse,\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    final_output: Any,\n    hooks: RunHooks[Any],\n    context_wrapper: RunContextWrapper[Any],\n    tool_input_guardrail_results: list[ToolInputGuardrailResult],\n    tool_output_guardrail_results: list[ToolOutputGuardrailResult],\n    run_final_output_hooks_fn: Callable[\n        [Agent[Any], RunHooks[Any], RunContextWrapper[Any], Any], Awaitable[None]\n    ]\n    | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Convenience wrapper to finalize a turn and run end hooks.\"\"\"\n    return await execute_final_output_step(\n        agent=agent,\n        original_input=original_input,\n        new_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        final_output=final_output,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n        run_final_output_hooks_fn=run_final_output_hooks_fn,\n    )\n</code></pre>"},{"location":"ref/run_internal/turn_resolution/#agents.run_internal.turn_resolution.execute_handoffs","title":"execute_handoffs  <code>async</code>","text":"<pre><code>execute_handoffs(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    new_response: ModelResponse,\n    run_handoffs: list[ToolRunHandoff],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    nest_handoff_history_fn: Callable[..., HandoffInputData]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Execute a handoff and prepare the next turn for the new agent.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def execute_handoffs(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_step_items: list[RunItem],\n    new_response: ModelResponse,\n    run_handoffs: list[ToolRunHandoff],\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    nest_handoff_history_fn: Callable[..., HandoffInputData] | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Execute a handoff and prepare the next turn for the new agent.\"\"\"\n\n    def nest_history(data: HandoffInputData, mapper: Any | None = None) -&gt; HandoffInputData:\n        if nest_handoff_history_fn is None:\n            return nest_handoff_history(data, history_mapper=mapper)\n        return nest_handoff_history_fn(data, mapper)\n\n    multiple_handoffs = len(run_handoffs) &gt; 1\n    if multiple_handoffs:\n        output_message = \"Multiple handoffs detected, ignoring this one.\"\n        new_step_items.extend(\n            [\n                ToolCallOutputItem(\n                    output=output_message,\n                    raw_item=ItemHelpers.tool_call_output_item(handoff.tool_call, output_message),\n                    agent=agent,\n                )\n                for handoff in run_handoffs[1:]\n            ]\n        )\n\n    actual_handoff = run_handoffs[0]\n    with handoff_span(from_agent=agent.name) as span_handoff:\n        handoff = actual_handoff.handoff\n        new_agent: Agent[Any] = await handoff.on_invoke_handoff(\n            context_wrapper, actual_handoff.tool_call.arguments\n        )\n        span_handoff.span_data.to_agent = new_agent.name\n        if multiple_handoffs:\n            requested_agents = [handoff.handoff.agent_name for handoff in run_handoffs]\n            span_handoff.set_error(\n                SpanError(\n                    message=\"Multiple handoffs requested\",\n                    data={\n                        \"requested_agents\": requested_agents,\n                    },\n                )\n            )\n\n        new_step_items.append(\n            HandoffOutputItem(\n                agent=agent,\n                raw_item=ItemHelpers.tool_call_output_item(\n                    actual_handoff.tool_call,\n                    handoff.get_transfer_message(new_agent),\n                ),\n                source_agent=agent,\n                target_agent=new_agent,\n            )\n        )\n\n        await asyncio.gather(\n            hooks.on_handoff(\n                context=context_wrapper,\n                from_agent=agent,\n                to_agent=new_agent,\n            ),\n            (\n                agent.hooks.on_handoff(\n                    context_wrapper,\n                    agent=new_agent,\n                    source=agent,\n                )\n                if agent.hooks\n                else _coro.noop_coroutine()\n            ),\n        )\n\n        input_filter = handoff.input_filter or (\n            run_config.handoff_input_filter if run_config else None\n        )\n        handoff_nest_setting = handoff.nest_handoff_history\n        should_nest_history = (\n            handoff_nest_setting\n            if handoff_nest_setting is not None\n            else run_config.nest_handoff_history\n        )\n        handoff_input_data: HandoffInputData | None = None\n        session_step_items: list[RunItem] | None = None\n        if input_filter or should_nest_history:\n            handoff_input_data = HandoffInputData(\n                input_history=tuple(original_input)\n                if isinstance(original_input, list)\n                else original_input,\n                pre_handoff_items=tuple(pre_step_items),\n                new_items=tuple(new_step_items),\n                run_context=context_wrapper,\n            )\n\n        if input_filter and handoff_input_data is not None:\n            filter_name = getattr(input_filter, \"__qualname__\", repr(input_filter))\n            from_agent = getattr(agent, \"name\", agent.__class__.__name__)\n            to_agent = getattr(new_agent, \"name\", new_agent.__class__.__name__)\n            logger.debug(\n                \"Filtering handoff inputs with %s for %s -&gt; %s\",\n                filter_name,\n                from_agent,\n                to_agent,\n            )\n            if not callable(input_filter):\n                _error_tracing.attach_error_to_span(\n                    span_handoff,\n                    SpanError(\n                        message=\"Invalid input filter\",\n                        data={\"details\": \"not callable()\"},\n                    ),\n                )\n                raise UserError(f\"Invalid input filter: {input_filter}\")\n            filtered = input_filter(handoff_input_data)\n            if inspect.isawaitable(filtered):\n                filtered = await filtered\n            if not isinstance(filtered, HandoffInputData):\n                _error_tracing.attach_error_to_span(\n                    span_handoff,\n                    SpanError(\n                        message=\"Invalid input filter result\",\n                        data={\"details\": \"not a HandoffInputData\"},\n                    ),\n                )\n                raise UserError(f\"Invalid input filter result: {filtered}\")\n\n            original_input = (\n                filtered.input_history\n                if isinstance(filtered.input_history, str)\n                else list(filtered.input_history)\n            )\n            pre_step_items = list(filtered.pre_handoff_items)\n            new_step_items = list(filtered.new_items)\n            # For custom input filters, keep full new_items for session history and\n            # use input_items for model input when provided.\n            if filtered.input_items is not None:\n                session_step_items = list(filtered.new_items)\n                new_step_items = list(filtered.input_items)\n            else:\n                session_step_items = None\n        elif should_nest_history and handoff_input_data is not None:\n            nested = nest_history(handoff_input_data, run_config.handoff_history_mapper)\n            original_input = (\n                nested.input_history\n                if isinstance(nested.input_history, str)\n                else list(nested.input_history)\n            )\n            pre_step_items = list(nested.pre_handoff_items)\n            # Keep full new_items for session history.\n            session_step_items = list(nested.new_items)\n            # Use input_items (filtered) for model input if available.\n            if nested.input_items is not None:\n                new_step_items = list(nested.input_items)\n            else:\n                new_step_items = session_step_items\n        else:\n            # No filtering or nesting - session_step_items not needed.\n            session_step_items = None\n\n    return SingleStepResult(\n        original_input=original_input,\n        model_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        next_step=NextStepHandoff(new_agent),\n        tool_input_guardrail_results=[],\n        tool_output_guardrail_results=[],\n        session_step_items=session_step_items,\n    )\n</code></pre>"},{"location":"ref/run_internal/turn_resolution/#agents.run_internal.turn_resolution.check_for_final_output_from_tools","title":"check_for_final_output_from_tools  <code>async</code>","text":"<pre><code>check_for_final_output_from_tools(\n    agent: Agent[TContext],\n    tool_results: list[FunctionToolResult],\n    context_wrapper: RunContextWrapper[TContext],\n) -&gt; ToolsToFinalOutputResult\n</code></pre> <p>Determine if tool results should produce a final output.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def check_for_final_output_from_tools(\n    agent: Agent[TContext],\n    tool_results: list[FunctionToolResult],\n    context_wrapper: RunContextWrapper[TContext],\n) -&gt; ToolsToFinalOutputResult:\n    \"\"\"Determine if tool results should produce a final output.\"\"\"\n    if not tool_results:\n        return NOT_FINAL_OUTPUT\n\n    if agent.tool_use_behavior == \"run_llm_again\":\n        return NOT_FINAL_OUTPUT\n    elif agent.tool_use_behavior == \"stop_on_first_tool\":\n        return ToolsToFinalOutputResult(is_final_output=True, final_output=tool_results[0].output)\n    elif isinstance(agent.tool_use_behavior, dict):\n        names = agent.tool_use_behavior.get(\"stop_at_tool_names\", [])\n        for tool_result in tool_results:\n            if tool_result.tool.name in names:\n                return ToolsToFinalOutputResult(\n                    is_final_output=True, final_output=tool_result.output\n                )\n        return ToolsToFinalOutputResult(is_final_output=False, final_output=None)\n    elif callable(agent.tool_use_behavior):\n        if inspect.iscoroutinefunction(agent.tool_use_behavior):\n            return await cast(\n                Awaitable[ToolsToFinalOutputResult],\n                agent.tool_use_behavior(context_wrapper, tool_results),\n            )\n        return cast(\n            ToolsToFinalOutputResult, agent.tool_use_behavior(context_wrapper, tool_results)\n        )\n\n    logger.error(\"Invalid tool_use_behavior: %s\", agent.tool_use_behavior)\n    raise UserError(f\"Invalid tool_use_behavior: {agent.tool_use_behavior}\")\n</code></pre>"},{"location":"ref/run_internal/turn_resolution/#agents.run_internal.turn_resolution.execute_tools_and_side_effects","title":"execute_tools_and_side_effects  <code>async</code>","text":"<pre><code>execute_tools_and_side_effects(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    output_schema: AgentOutputSchemaBase | None,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n) -&gt; SingleStepResult\n</code></pre> <p>Run one turn of the loop, coordinating tools, approvals, guardrails, and handoffs.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def execute_tools_and_side_effects(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    output_schema: AgentOutputSchemaBase | None,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n) -&gt; SingleStepResult:\n    \"\"\"Run one turn of the loop, coordinating tools, approvals, guardrails, and handoffs.\"\"\"\n\n    execute_final_output_call = execute_final_output\n    execute_handoffs_call = execute_handoffs\n\n    pre_step_items = list(pre_step_items)\n    approval_items_by_call_id = index_approval_items_by_call_id(pre_step_items)\n\n    plan = _build_plan_for_fresh_turn(\n        processed_response=processed_response,\n        agent=agent,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n    )\n\n    new_step_items = _dedupe_tool_call_items(\n        existing_items=pre_step_items,\n        new_items=processed_response.new_items,\n    )\n\n    (\n        function_results,\n        tool_input_guardrail_results,\n        tool_output_guardrail_results,\n        computer_results,\n        shell_results,\n        apply_patch_results,\n        local_shell_results,\n    ) = await _execute_tool_plan(\n        plan=plan,\n        agent=agent,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        run_config=run_config,\n    )\n    new_step_items.extend(\n        _build_tool_result_items(\n            function_results=function_results,\n            computer_results=computer_results,\n            shell_results=shell_results,\n            apply_patch_results=apply_patch_results,\n            local_shell_results=local_shell_results,\n        )\n    )\n\n    interruptions = _collect_tool_interruptions(\n        function_results=function_results,\n        shell_results=shell_results,\n        apply_patch_results=apply_patch_results,\n    )\n    if plan.approved_mcp_responses:\n        new_step_items.extend(plan.approved_mcp_responses)\n    if plan.pending_interruptions:\n        interruptions.extend(plan.pending_interruptions)\n        new_step_items.extend(plan.pending_interruptions)\n\n    processed_response.interruptions = interruptions\n\n    if interruptions:\n        return SingleStepResult(\n            original_input=original_input,\n            model_response=new_response,\n            pre_step_items=pre_step_items,\n            new_step_items=new_step_items,\n            next_step=NextStepInterruption(interruptions=interruptions),\n            tool_input_guardrail_results=tool_input_guardrail_results,\n            tool_output_guardrail_results=tool_output_guardrail_results,\n            processed_response=processed_response,\n        )\n\n    await _append_mcp_callback_results(\n        agent=agent,\n        requests=plan.mcp_requests_with_callback,\n        context_wrapper=context_wrapper,\n        append_item=new_step_items.append,\n    )\n\n    if run_handoffs := processed_response.handoffs:\n        return await execute_handoffs_call(\n            agent=agent,\n            original_input=original_input,\n            pre_step_items=pre_step_items,\n            new_step_items=new_step_items,\n            new_response=new_response,\n            run_handoffs=run_handoffs,\n            hooks=hooks,\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n        )\n\n    tool_final_output = await _maybe_finalize_from_tool_results(\n        agent=agent,\n        original_input=original_input,\n        new_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        function_results=function_results,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n    if tool_final_output is not None:\n        return tool_final_output\n\n    message_items = [item for item in new_step_items if isinstance(item, MessageOutputItem)]\n    potential_final_output_text = (\n        ItemHelpers.extract_last_text(message_items[-1].raw_item) if message_items else None\n    )\n\n    if not processed_response.has_tools_or_approvals_to_run():\n        if output_schema and not output_schema.is_plain_text() and potential_final_output_text:\n            final_output = output_schema.validate_json(potential_final_output_text)\n            return await execute_final_output_call(\n                agent=agent,\n                original_input=original_input,\n                new_response=new_response,\n                pre_step_items=pre_step_items,\n                new_step_items=new_step_items,\n                final_output=final_output,\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                tool_input_guardrail_results=tool_input_guardrail_results,\n                tool_output_guardrail_results=tool_output_guardrail_results,\n            )\n        if not output_schema or output_schema.is_plain_text():\n            return await execute_final_output_call(\n                agent=agent,\n                original_input=original_input,\n                new_response=new_response,\n                pre_step_items=pre_step_items,\n                new_step_items=new_step_items,\n                final_output=potential_final_output_text or \"\",\n                hooks=hooks,\n                context_wrapper=context_wrapper,\n                tool_input_guardrail_results=tool_input_guardrail_results,\n                tool_output_guardrail_results=tool_output_guardrail_results,\n            )\n\n    return SingleStepResult(\n        original_input=original_input,\n        model_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        next_step=NextStepRunAgain(),\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n</code></pre>"},{"location":"ref/run_internal/turn_resolution/#agents.run_internal.turn_resolution.resolve_interrupted_turn","title":"resolve_interrupted_turn  <code>async</code>","text":"<pre><code>resolve_interrupted_turn(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    original_pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    run_state: RunState | None = None,\n    nest_handoff_history_fn: Callable[..., HandoffInputData]\n    | None = None,\n) -&gt; SingleStepResult\n</code></pre> <p>Continue a turn that was previously interrupted waiting for tool approval.</p> Source code in <code>src/agents/run_internal/turn_resolution.py</code> <pre><code>async def resolve_interrupted_turn(\n    *,\n    agent: Agent[TContext],\n    original_input: str | list[TResponseInputItem],\n    original_pre_step_items: list[RunItem],\n    new_response: ModelResponse,\n    processed_response: ProcessedResponse,\n    hooks: RunHooks[TContext],\n    context_wrapper: RunContextWrapper[TContext],\n    run_config: RunConfig,\n    run_state: RunState | None = None,\n    nest_handoff_history_fn: Callable[..., HandoffInputData] | None = None,\n) -&gt; SingleStepResult:\n    \"\"\"Continue a turn that was previously interrupted waiting for tool approval.\"\"\"\n\n    execute_handoffs_call = execute_handoffs\n\n    def nest_history(data: HandoffInputData, mapper: Any | None = None) -&gt; HandoffInputData:\n        if nest_handoff_history_fn is None:\n            return nest_handoff_history(data, history_mapper=mapper)\n        return nest_handoff_history_fn(data, mapper)\n\n    def _pending_approvals_from_state() -&gt; list[ToolApprovalItem]:\n        if (\n            run_state is not None\n            and hasattr(run_state, \"_current_step\")\n            and isinstance(run_state._current_step, NextStepInterruption)\n        ):\n            return [\n                item\n                for item in run_state._current_step.interruptions\n                if isinstance(item, ToolApprovalItem)\n            ]\n        return [item for item in original_pre_step_items if isinstance(item, ToolApprovalItem)]\n\n    async def _record_function_rejection(\n        call_id: str | None,\n        tool_call: ResponseFunctionToolCall,\n        function_tool: FunctionTool,\n    ) -&gt; None:\n        if isinstance(call_id, str) and call_id in rejected_function_call_ids:\n            return\n        rejection_message = REJECTION_MESSAGE\n        if call_id:\n            rejection_message = await resolve_approval_rejection_message(\n                context_wrapper=context_wrapper,\n                run_config=run_config,\n                tool_type=\"function\",\n                tool_name=function_tool.name,\n                call_id=call_id,\n            )\n        rejected_function_outputs.append(\n            function_rejection_item(agent, tool_call, rejection_message=rejection_message)\n        )\n        if isinstance(call_id, str):\n            rejected_function_call_ids.add(call_id)\n\n    async def _function_requires_approval(run: ToolRunFunction) -&gt; bool:\n        call_id = run.tool_call.call_id\n        if call_id and call_id in approval_items_by_call_id:\n            return True\n\n        try:\n            return await function_needs_approval(\n                run.function_tool,\n                context_wrapper,\n                run.tool_call,\n            )\n        except UserError:\n            raise\n        except Exception:\n            return True\n\n    try:\n        context_wrapper.turn_input = ItemHelpers.input_to_new_input_list(original_input)\n    except Exception:\n        context_wrapper.turn_input = []\n\n    pending_approval_items = _pending_approvals_from_state()\n    approval_items_by_call_id = index_approval_items_by_call_id(pending_approval_items)\n\n    rejected_function_outputs: list[RunItem] = []\n    rejected_function_call_ids: set[str] = set()\n    rerun_function_call_ids: set[str] = set()\n    pending_interruptions: list[ToolApprovalItem] = []\n    pending_interruption_keys: set[str] = set()\n\n    output_index = _build_tool_output_index(original_pre_step_items)\n\n    def _has_output_item(call_id: str, expected_type: str) -&gt; bool:\n        return (expected_type, call_id) in output_index\n\n    def _shell_call_id_from_run(run: ToolRunShellCall) -&gt; str:\n        return extract_shell_call_id(run.tool_call)\n\n    def _apply_patch_call_id_from_run(run: ToolRunApplyPatchCall) -&gt; str:\n        return extract_apply_patch_call_id(run.tool_call)\n\n    def _computer_call_id_from_run(run: ToolRunComputerAction) -&gt; str:\n        call_id = extract_tool_call_id(run.tool_call)\n        if not call_id:\n            raise ModelBehaviorError(\"Computer action is missing call_id.\")\n        return call_id\n\n    def _shell_tool_name(run: ToolRunShellCall) -&gt; str:\n        return run.shell_tool.name\n\n    def _apply_patch_tool_name(run: ToolRunApplyPatchCall) -&gt; str:\n        return run.apply_patch_tool.name\n\n    async def _build_shell_rejection(run: ToolRunShellCall, call_id: str) -&gt; RunItem:\n        rejection_message = await resolve_approval_rejection_message(\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n            tool_type=\"shell\",\n            tool_name=run.shell_tool.name,\n            call_id=call_id,\n        )\n        return cast(\n            RunItem,\n            shell_rejection_item(\n                agent,\n                call_id,\n                rejection_message=rejection_message,\n            ),\n        )\n\n    async def _build_apply_patch_rejection(run: ToolRunApplyPatchCall, call_id: str) -&gt; RunItem:\n        rejection_message = await resolve_approval_rejection_message(\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n            tool_type=\"apply_patch\",\n            tool_name=run.apply_patch_tool.name,\n            call_id=call_id,\n        )\n        return cast(\n            RunItem,\n            apply_patch_rejection_item(\n                agent,\n                call_id,\n                rejection_message=rejection_message,\n            ),\n        )\n\n    async def _shell_needs_approval(run: ToolRunShellCall) -&gt; bool:\n        shell_call = coerce_shell_call(run.tool_call)\n        return await evaluate_needs_approval_setting(\n            run.shell_tool.needs_approval,\n            context_wrapper,\n            shell_call.action,\n            shell_call.call_id,\n        )\n\n    async def _apply_patch_needs_approval(run: ToolRunApplyPatchCall) -&gt; bool:\n        operation = coerce_apply_patch_operation(\n            run.tool_call,\n            context_wrapper=context_wrapper,\n        )\n        call_id = extract_apply_patch_call_id(run.tool_call)\n        return await evaluate_needs_approval_setting(\n            run.apply_patch_tool.needs_approval, context_wrapper, operation, call_id\n        )\n\n    def _shell_output_exists(call_id: str) -&gt; bool:\n        return _has_output_item(call_id, \"shell_call_output\")\n\n    def _apply_patch_output_exists(call_id: str) -&gt; bool:\n        return _has_output_item(call_id, \"apply_patch_call_output\")\n\n    def _computer_output_exists(call_id: str) -&gt; bool:\n        return _has_output_item(call_id, \"computer_call_output\")\n\n    def _nested_interruptions_status(\n        interruptions: Sequence[ToolApprovalItem],\n    ) -&gt; Literal[\"approved\", \"pending\", \"rejected\"]:\n        has_pending = False\n        for interruption in interruptions:\n            call_id = extract_tool_call_id(interruption.raw_item)\n            if not call_id:\n                has_pending = True\n                continue\n            status = context_wrapper.get_approval_status(\n                interruption.tool_name or \"\", call_id, existing_pending=interruption\n            )\n            if status is False:\n                return \"rejected\"\n            if status is None:\n                has_pending = True\n        return \"pending\" if has_pending else \"approved\"\n\n    def _function_output_exists(run: ToolRunFunction) -&gt; bool:\n        call_id = extract_tool_call_id(run.tool_call)\n        if not call_id:\n            return False\n\n        pending_run_result = peek_agent_tool_run_result(run.tool_call)\n        if pending_run_result and getattr(pending_run_result, \"interruptions\", None):\n            status = _nested_interruptions_status(pending_run_result.interruptions)\n            if status in (\"approved\", \"rejected\"):\n                rerun_function_call_ids.add(call_id)\n                return False\n            return True\n\n        return _has_output_item(call_id, \"function_call_output\")\n\n    def _add_pending_interruption(item: ToolApprovalItem | None) -&gt; None:\n        if item is None:\n            return\n        call_id = extract_tool_call_id(item.raw_item)\n        key = call_id or f\"raw:{id(item.raw_item)}\"\n        if key in pending_interruption_keys:\n            return\n        pending_interruption_keys.add(key)\n        pending_interruptions.append(item)\n\n    def _approval_matches_agent(approval: ToolApprovalItem) -&gt; bool:\n        approval_agent = approval.agent\n        if approval_agent is None:\n            return False\n        if approval_agent is agent:\n            return True\n        return getattr(approval_agent, \"name\", None) == agent.name\n\n    async def _rebuild_function_runs_from_approvals() -&gt; list[ToolRunFunction]:\n        if not pending_approval_items:\n            return []\n        all_tools = await agent.get_all_tools(context_wrapper)\n        tool_map: dict[str, FunctionTool] = {\n            tool.name: tool for tool in all_tools if isinstance(tool, FunctionTool)\n        }\n        existing_pending_call_ids: set[str] = set()\n        for existing_pending in pending_interruptions:\n            if isinstance(existing_pending, ToolApprovalItem):\n                existing_call_id = extract_tool_call_id(existing_pending.raw_item)\n                if existing_call_id:\n                    existing_pending_call_ids.add(existing_call_id)\n        rebuilt_runs: list[ToolRunFunction] = []\n\n        def _add_unmatched_pending(approval: ToolApprovalItem) -&gt; None:\n            call_id = extract_tool_call_id(approval.raw_item)\n            if not call_id:\n                _add_pending_interruption(approval)\n                return\n            tool_name = approval.tool_name or \"\"\n            approval_status = context_wrapper.get_approval_status(\n                tool_name, call_id, existing_pending=approval\n            )\n            if approval_status is None:\n                _add_pending_interruption(approval)\n\n        for approval in pending_approval_items:\n            if not isinstance(approval, ToolApprovalItem):\n                continue\n            if not _approval_matches_agent(approval):\n                _add_unmatched_pending(approval)\n                continue\n            raw = approval.raw_item\n            raw_type = get_mapping_or_attr(raw, \"type\")\n            if raw_type != \"function_call\":\n                _add_unmatched_pending(approval)\n                continue\n            name = get_mapping_or_attr(raw, \"name\")\n            if not (isinstance(name, str) and name in tool_map):\n                _add_unmatched_pending(approval)\n                continue\n\n            rebuilt_call_id: str | None\n            arguments: str | None\n            tool_call: ResponseFunctionToolCall\n            if isinstance(raw, ResponseFunctionToolCall):\n                rebuilt_call_id = raw.call_id\n                arguments = raw.arguments\n                tool_call = raw\n            else:\n                rebuilt_call_id = extract_tool_call_id(raw)\n                arguments = get_mapping_or_attr(raw, \"arguments\") or \"{}\"\n                status = get_mapping_or_attr(raw, \"status\")\n                if not (isinstance(rebuilt_call_id, str) and isinstance(arguments, str)):\n                    _add_unmatched_pending(approval)\n                    continue\n                valid_status: Literal[\"in_progress\", \"completed\", \"incomplete\"] | None = None\n                if isinstance(status, str) and status in (\n                    \"in_progress\",\n                    \"completed\",\n                    \"incomplete\",\n                ):\n                    valid_status = status  # type: ignore[assignment]\n                tool_call = ResponseFunctionToolCall(\n                    type=\"function_call\",\n                    name=name,\n                    call_id=rebuilt_call_id,\n                    arguments=arguments,\n                    status=valid_status,\n                )\n\n            if not (isinstance(rebuilt_call_id, str) and isinstance(arguments, str)):\n                _add_unmatched_pending(approval)\n                continue\n\n            approval_status = context_wrapper.get_approval_status(\n                name, rebuilt_call_id, existing_pending=approval\n            )\n            if approval_status is False:\n                await _record_function_rejection(\n                    rebuilt_call_id,\n                    tool_call,\n                    tool_map[name],\n                )\n                continue\n            if approval_status is None:\n                if rebuilt_call_id not in existing_pending_call_ids:\n                    _add_pending_interruption(approval)\n                    existing_pending_call_ids.add(rebuilt_call_id)\n                continue\n            rebuilt_runs.append(ToolRunFunction(function_tool=tool_map[name], tool_call=tool_call))\n        return rebuilt_runs\n\n    function_tool_runs = await _select_function_tool_runs_for_resume(\n        processed_response.functions,\n        approval_items_by_call_id=approval_items_by_call_id,\n        context_wrapper=context_wrapper,\n        needs_approval_checker=_function_requires_approval,\n        output_exists_checker=_function_output_exists,\n        record_rejection=_record_function_rejection,\n        pending_interruption_adder=_add_pending_interruption,\n        pending_item_builder=lambda run: ToolApprovalItem(agent=agent, raw_item=run.tool_call),\n    )\n\n    rebuilt_function_tool_runs = await _rebuild_function_runs_from_approvals()\n    if rebuilt_function_tool_runs:\n        existing_call_ids: set[str] = set()\n        for run in function_tool_runs:\n            call_id = extract_tool_call_id(run.tool_call)\n            if call_id:\n                existing_call_ids.add(call_id)\n        for run in rebuilt_function_tool_runs:\n            call_id = extract_tool_call_id(run.tool_call)\n            if call_id and call_id in existing_call_ids:\n                continue\n            function_tool_runs.append(run)\n            if call_id:\n                existing_call_ids.add(call_id)\n\n    pending_computer_actions: list[ToolRunComputerAction] = []\n    for action in processed_response.computer_actions:\n        call_id = _computer_call_id_from_run(action)\n        if _computer_output_exists(call_id):\n            continue\n        pending_computer_actions.append(action)\n\n    approved_shell_calls, rejected_shell_results = await _collect_runs_by_approval(\n        processed_response.shell_calls,\n        call_id_extractor=_shell_call_id_from_run,\n        tool_name_resolver=_shell_tool_name,\n        rejection_builder=_build_shell_rejection,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n        agent=agent,\n        pending_interruption_adder=_add_pending_interruption,\n        needs_approval_checker=_shell_needs_approval,\n        output_exists_checker=_shell_output_exists,\n    )\n\n    approved_apply_patch_calls, rejected_apply_patch_results = await _collect_runs_by_approval(\n        processed_response.apply_patch_calls,\n        call_id_extractor=_apply_patch_call_id_from_run,\n        tool_name_resolver=_apply_patch_tool_name,\n        rejection_builder=_build_apply_patch_rejection,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n        agent=agent,\n        pending_interruption_adder=_add_pending_interruption,\n        needs_approval_checker=_apply_patch_needs_approval,\n        output_exists_checker=_apply_patch_output_exists,\n    )\n\n    plan = _build_plan_for_resume_turn(\n        processed_response=processed_response,\n        agent=agent,\n        context_wrapper=context_wrapper,\n        approval_items_by_call_id=approval_items_by_call_id,\n        pending_interruptions=pending_interruptions,\n        pending_interruption_adder=_add_pending_interruption,\n        function_runs=function_tool_runs,\n        computer_actions=pending_computer_actions,\n        shell_calls=approved_shell_calls,\n        apply_patch_calls=approved_apply_patch_calls,\n    )\n\n    (\n        function_results,\n        tool_input_guardrail_results,\n        tool_output_guardrail_results,\n        computer_results,\n        shell_results,\n        apply_patch_results,\n        _local_shell_results,\n    ) = await _execute_tool_plan(\n        plan=plan,\n        agent=agent,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        run_config=run_config,\n    )\n\n    for interruption in _collect_tool_interruptions(\n        function_results=function_results,\n        shell_results=[],\n        apply_patch_results=[],\n    ):\n        _add_pending_interruption(interruption)\n\n    new_items, append_if_new = _make_unique_item_appender(original_pre_step_items)\n\n    for item in _build_tool_result_items(\n        function_results=function_results,\n        computer_results=computer_results,\n        shell_results=shell_results,\n        apply_patch_results=apply_patch_results,\n        local_shell_results=[],\n    ):\n        append_if_new(item)\n    for rejection_item in rejected_function_outputs:\n        append_if_new(rejection_item)\n    for pending_item in pending_interruptions:\n        if pending_item:\n            append_if_new(pending_item)\n    for shell_rejection in rejected_shell_results:\n        append_if_new(shell_rejection)\n    for apply_patch_rejection in rejected_apply_patch_results:\n        append_if_new(apply_patch_rejection)\n    for approved_response in plan.approved_mcp_responses:\n        append_if_new(approved_response)\n\n    processed_response.interruptions = pending_interruptions\n    if pending_interruptions:\n        return SingleStepResult(\n            original_input=original_input,\n            model_response=new_response,\n            pre_step_items=original_pre_step_items,\n            new_step_items=new_items,\n            next_step=NextStepInterruption(\n                interruptions=[item for item in pending_interruptions if item]\n            ),\n            tool_input_guardrail_results=tool_input_guardrail_results,\n            tool_output_guardrail_results=tool_output_guardrail_results,\n            processed_response=processed_response,\n        )\n\n    await _append_mcp_callback_results(\n        agent=agent,\n        requests=plan.mcp_requests_with_callback,\n        context_wrapper=context_wrapper,\n        append_item=append_if_new,\n    )\n\n    (\n        pending_hosted_mcp_approvals,\n        pending_hosted_mcp_approval_ids,\n    ) = process_hosted_mcp_approvals(\n        original_pre_step_items=original_pre_step_items,\n        mcp_approval_requests=processed_response.mcp_approval_requests,\n        context_wrapper=context_wrapper,\n        agent=agent,\n        append_item=append_if_new,\n    )\n\n    pre_step_items = [\n        item\n        for item in original_pre_step_items\n        if should_keep_hosted_mcp_item(\n            item,\n            pending_hosted_mcp_approvals=pending_hosted_mcp_approvals,\n            pending_hosted_mcp_approval_ids=pending_hosted_mcp_approval_ids,\n        )\n    ]\n\n    if rejected_function_call_ids:\n        pre_step_items = [\n            item\n            for item in pre_step_items\n            if not (\n                item.type == \"tool_call_output_item\"\n                and (\n                    extract_tool_call_id(getattr(item, \"raw_item\", None))\n                    in rejected_function_call_ids\n                )\n            )\n        ]\n\n    if rerun_function_call_ids:\n        pre_step_items = [\n            item\n            for item in pre_step_items\n            if not (\n                item.type == \"tool_call_output_item\"\n                and (\n                    extract_tool_call_id(getattr(item, \"raw_item\", None)) in rerun_function_call_ids\n                )\n            )\n        ]\n\n    executed_handoff_call_ids: set[str] = set()\n    for item in original_pre_step_items:\n        if isinstance(item, HandoffCallItem):\n            handoff_call_id = extract_tool_call_id(item.raw_item)\n            if handoff_call_id:\n                executed_handoff_call_ids.add(handoff_call_id)\n\n    pending_handoffs = [\n        handoff\n        for handoff in processed_response.handoffs\n        if not handoff.tool_call.call_id\n        or handoff.tool_call.call_id not in executed_handoff_call_ids\n    ]\n\n    if pending_handoffs:\n        return await execute_handoffs_call(\n            agent=agent,\n            original_input=original_input,\n            pre_step_items=pre_step_items,\n            new_step_items=new_items,\n            new_response=new_response,\n            run_handoffs=pending_handoffs,\n            hooks=hooks,\n            context_wrapper=context_wrapper,\n            run_config=run_config,\n            nest_handoff_history_fn=nest_history,\n        )\n\n    tool_final_output = await _maybe_finalize_from_tool_results(\n        agent=agent,\n        original_input=original_input,\n        new_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_items,\n        function_results=function_results,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n    if tool_final_output is not None:\n        return tool_final_output\n\n    return SingleStepResult(\n        original_input=original_input,\n        model_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_items,\n        next_step=NextStepRunAgain(),\n        tool_input_guardrail_results=tool_input_guardrail_results,\n        tool_output_guardrail_results=tool_output_guardrail_results,\n    )\n</code></pre>"},{"location":"ref/tracing/","title":"Tracing module","text":""},{"location":"ref/tracing/#agents.tracing.TracingConfig","title":"TracingConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for tracing export.</p> Source code in <code>src/agents/tracing/config.py</code> <pre><code>class TracingConfig(TypedDict, total=False):\n    \"\"\"Configuration for tracing export.\"\"\"\n\n    api_key: str\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceCtxManager","title":"TraceCtxManager","text":"<p>Create a trace when none exists and manage its lifecycle for a run.</p> Source code in <code>src/agents/tracing/context.py</code> <pre><code>class TraceCtxManager:\n    \"\"\"Create a trace when none exists and manage its lifecycle for a run.\"\"\"\n\n    def __init__(\n        self,\n        workflow_name: str,\n        trace_id: str | None,\n        group_id: str | None,\n        metadata: dict[str, Any] | None,\n        tracing: TracingConfig | None,\n        disabled: bool,\n    ):\n        self.trace: Trace | None = None\n        self.workflow_name = workflow_name\n        self.trace_id = trace_id\n        self.group_id = group_id\n        self.metadata = metadata\n        self.tracing = tracing\n        self.disabled = disabled\n\n    def __enter__(self) -&gt; TraceCtxManager:\n        current_trace = get_current_trace()\n        if not current_trace:\n            self.trace = trace(\n                workflow_name=self.workflow_name,\n                trace_id=self.trace_id,\n                group_id=self.group_id,\n                metadata=self.metadata,\n                tracing=self.tracing,\n                disabled=self.disabled,\n            )\n            assert self.trace is not None\n            self.trace.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.trace:\n            self.trace.finish(reset_current=True)\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor","title":"TracingProcessor","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for processing and monitoring traces and spans in the OpenAI Agents system.</p> <p>This abstract class defines the interface that all tracing processors must implement. Processors receive notifications when traces and spans start and end, allowing them to collect, process, and export tracing data.</p> Example <pre><code>class CustomProcessor(TracingProcessor):\n    def __init__(self):\n        self.active_traces = {}\n        self.active_spans = {}\n\n    def on_trace_start(self, trace):\n        self.active_traces[trace.trace_id] = trace\n\n    def on_trace_end(self, trace):\n        # Process completed trace\n        del self.active_traces[trace.trace_id]\n\n    def on_span_start(self, span):\n        self.active_spans[span.span_id] = span\n\n    def on_span_end(self, span):\n        # Process completed span\n        del self.active_spans[span.span_id]\n\n    def shutdown(self):\n        # Clean up resources\n        self.active_traces.clear()\n        self.active_spans.clear()\n\n    def force_flush(self):\n        # Force processing of any queued items\n        pass\n</code></pre> Notes <ul> <li>All methods should be thread-safe</li> <li>Methods should not block for long periods</li> <li>Handle errors gracefully to prevent disrupting agent execution</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>class TracingProcessor(abc.ABC):\n    \"\"\"Interface for processing and monitoring traces and spans in the OpenAI Agents system.\n\n    This abstract class defines the interface that all tracing processors must implement.\n    Processors receive notifications when traces and spans start and end, allowing them\n    to collect, process, and export tracing data.\n\n    Example:\n        ```python\n        class CustomProcessor(TracingProcessor):\n            def __init__(self):\n                self.active_traces = {}\n                self.active_spans = {}\n\n            def on_trace_start(self, trace):\n                self.active_traces[trace.trace_id] = trace\n\n            def on_trace_end(self, trace):\n                # Process completed trace\n                del self.active_traces[trace.trace_id]\n\n            def on_span_start(self, span):\n                self.active_spans[span.span_id] = span\n\n            def on_span_end(self, span):\n                # Process completed span\n                del self.active_spans[span.span_id]\n\n            def shutdown(self):\n                # Clean up resources\n                self.active_traces.clear()\n                self.active_spans.clear()\n\n            def force_flush(self):\n                # Force processing of any queued items\n                pass\n        ```\n\n    Notes:\n        - All methods should be thread-safe\n        - Methods should not block for long periods\n        - Handle errors gracefully to prevent disrupting agent execution\n    \"\"\"\n\n    @abc.abstractmethod\n    def on_trace_start(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a new trace begins execution.\n\n        Args:\n            trace: The trace that started. Contains workflow name and metadata.\n\n        Notes:\n            - Called synchronously on trace start\n            - Should return quickly to avoid blocking execution\n            - Any errors should be caught and handled internally\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_trace_end(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a trace completes execution.\n\n        Args:\n            trace: The completed trace containing all spans and results.\n\n        Notes:\n            - Called synchronously when trace finishes\n            - Good time to export/process the complete trace\n            - Should handle cleanup of any trace-specific resources\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_start(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a new span begins execution.\n\n        Args:\n            span: The span that started. Contains operation details and context.\n\n        Notes:\n            - Called synchronously on span start\n            - Should return quickly to avoid blocking execution\n            - Spans are automatically nested under current trace/span\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_end(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a span completes execution.\n\n        Args:\n            span: The completed span containing execution results.\n\n        Notes:\n            - Called synchronously when span finishes\n            - Should not block or raise exceptions\n            - Good time to export/process the individual span\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def shutdown(self) -&gt; None:\n        \"\"\"Called when the application stops to clean up resources.\n\n        Should perform any necessary cleanup like:\n        - Flushing queued traces/spans\n        - Closing connections\n        - Releasing resources\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def force_flush(self) -&gt; None:\n        \"\"\"Forces immediate processing of any queued traces/spans.\n\n        Notes:\n            - Should process all queued items before returning\n            - Useful before shutdown or when immediate processing is needed\n            - May block while processing completes\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_trace_start","title":"on_trace_start  <code>abstractmethod</code>","text":"<pre><code>on_trace_start(trace: Trace) -&gt; None\n</code></pre> <p>Called when a new trace begins execution.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace that started. Contains workflow name and metadata.</p> required Notes <ul> <li>Called synchronously on trace start</li> <li>Should return quickly to avoid blocking execution</li> <li>Any errors should be caught and handled internally</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_start(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a new trace begins execution.\n\n    Args:\n        trace: The trace that started. Contains workflow name and metadata.\n\n    Notes:\n        - Called synchronously on trace start\n        - Should return quickly to avoid blocking execution\n        - Any errors should be caught and handled internally\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_trace_end","title":"on_trace_end  <code>abstractmethod</code>","text":"<pre><code>on_trace_end(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace completes execution.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The completed trace containing all spans and results.</p> required Notes <ul> <li>Called synchronously when trace finishes</li> <li>Good time to export/process the complete trace</li> <li>Should handle cleanup of any trace-specific resources</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_end(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a trace completes execution.\n\n    Args:\n        trace: The completed trace containing all spans and results.\n\n    Notes:\n        - Called synchronously when trace finishes\n        - Good time to export/process the complete trace\n        - Should handle cleanup of any trace-specific resources\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_span_start","title":"on_span_start  <code>abstractmethod</code>","text":"<pre><code>on_span_start(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a new span begins execution.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The span that started. Contains operation details and context.</p> required Notes <ul> <li>Called synchronously on span start</li> <li>Should return quickly to avoid blocking execution</li> <li>Spans are automatically nested under current trace/span</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_start(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a new span begins execution.\n\n    Args:\n        span: The span that started. Contains operation details and context.\n\n    Notes:\n        - Called synchronously on span start\n        - Should return quickly to avoid blocking execution\n        - Spans are automatically nested under current trace/span\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_span_end","title":"on_span_end  <code>abstractmethod</code>","text":"<pre><code>on_span_end(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span completes execution.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The completed span containing execution results.</p> required Notes <ul> <li>Called synchronously when span finishes</li> <li>Should not block or raise exceptions</li> <li>Good time to export/process the individual span</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_end(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a span completes execution.\n\n    Args:\n        span: The completed span containing execution results.\n\n    Notes:\n        - Called synchronously when span finishes\n        - Should not block or raise exceptions\n        - Good time to export/process the individual span\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.shutdown","title":"shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Called when the application stops to clean up resources.</p> <p>Should perform any necessary cleanup like: - Flushing queued traces/spans - Closing connections - Releasing resources</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"Called when the application stops to clean up resources.\n\n    Should perform any necessary cleanup like:\n    - Flushing queued traces/spans\n    - Closing connections\n    - Releasing resources\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.force_flush","title":"force_flush  <code>abstractmethod</code>","text":"<pre><code>force_flush() -&gt; None\n</code></pre> <p>Forces immediate processing of any queued traces/spans.</p> Notes <ul> <li>Should process all queued items before returning</li> <li>Useful before shutdown or when immediate processing is needed</li> <li>May block while processing completes</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef force_flush(self) -&gt; None:\n    \"\"\"Forces immediate processing of any queued traces/spans.\n\n    Notes:\n        - Should process all queued items before returning\n        - Useful before shutdown or when immediate processing is needed\n        - May block while processing completes\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider","title":"TraceProvider","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for creating traces and spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>class TraceProvider(ABC):\n    \"\"\"Interface for creating traces and spans.\"\"\"\n\n    @abstractmethod\n    def register_processor(self, processor: TracingProcessor) -&gt; None:\n        \"\"\"Add a processor that will receive all traces and spans.\"\"\"\n\n    @abstractmethod\n    def set_processors(self, processors: list[TracingProcessor]) -&gt; None:\n        \"\"\"Replace the list of processors with ``processors``.\"\"\"\n\n    @abstractmethod\n    def get_current_trace(self) -&gt; Trace | None:\n        \"\"\"Return the currently active trace, if any.\"\"\"\n\n    @abstractmethod\n    def get_current_span(self) -&gt; Span[Any] | None:\n        \"\"\"Return the currently active span, if any.\"\"\"\n\n    @abstractmethod\n    def set_disabled(self, disabled: bool) -&gt; None:\n        \"\"\"Enable or disable tracing globally.\"\"\"\n\n    @abstractmethod\n    def time_iso(self) -&gt; str:\n        \"\"\"Return the current time in ISO 8601 format.\"\"\"\n\n    @abstractmethod\n    def gen_trace_id(self) -&gt; str:\n        \"\"\"Generate a new trace identifier.\"\"\"\n\n    @abstractmethod\n    def gen_span_id(self) -&gt; str:\n        \"\"\"Generate a new span identifier.\"\"\"\n\n    @abstractmethod\n    def gen_group_id(self) -&gt; str:\n        \"\"\"Generate a new group identifier.\"\"\"\n\n    @abstractmethod\n    def create_trace(\n        self,\n        name: str,\n        trace_id: str | None = None,\n        group_id: str | None = None,\n        metadata: dict[str, Any] | None = None,\n        disabled: bool = False,\n        tracing: TracingConfig | None = None,\n    ) -&gt; Trace:\n        \"\"\"Create a new trace.\"\"\"\n\n    @abstractmethod\n    def create_span(\n        self,\n        span_data: TSpanData,\n        span_id: str | None = None,\n        parent: Trace | Span[Any] | None = None,\n        disabled: bool = False,\n    ) -&gt; Span[TSpanData]:\n        \"\"\"Create a new span.\"\"\"\n\n    @abstractmethod\n    def shutdown(self) -&gt; None:\n        \"\"\"Clean up any resources used by the provider.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.register_processor","title":"register_processor  <code>abstractmethod</code>","text":"<pre><code>register_processor(processor: TracingProcessor) -&gt; None\n</code></pre> <p>Add a processor that will receive all traces and spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef register_processor(self, processor: TracingProcessor) -&gt; None:\n    \"\"\"Add a processor that will receive all traces and spans.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.set_processors","title":"set_processors  <code>abstractmethod</code>","text":"<pre><code>set_processors(processors: list[TracingProcessor]) -&gt; None\n</code></pre> <p>Replace the list of processors with <code>processors</code>.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef set_processors(self, processors: list[TracingProcessor]) -&gt; None:\n    \"\"\"Replace the list of processors with ``processors``.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.get_current_trace","title":"get_current_trace  <code>abstractmethod</code>","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Return the currently active trace, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef get_current_trace(self) -&gt; Trace | None:\n    \"\"\"Return the currently active trace, if any.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.get_current_span","title":"get_current_span  <code>abstractmethod</code>","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Return the currently active span, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef get_current_span(self) -&gt; Span[Any] | None:\n    \"\"\"Return the currently active span, if any.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.set_disabled","title":"set_disabled  <code>abstractmethod</code>","text":"<pre><code>set_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Enable or disable tracing globally.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef set_disabled(self, disabled: bool) -&gt; None:\n    \"\"\"Enable or disable tracing globally.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.time_iso","title":"time_iso  <code>abstractmethod</code>","text":"<pre><code>time_iso() -&gt; str\n</code></pre> <p>Return the current time in ISO 8601 format.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef time_iso(self) -&gt; str:\n    \"\"\"Return the current time in ISO 8601 format.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.gen_trace_id","title":"gen_trace_id  <code>abstractmethod</code>","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_trace_id(self) -&gt; str:\n    \"\"\"Generate a new trace identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.gen_span_id","title":"gen_span_id  <code>abstractmethod</code>","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_span_id(self) -&gt; str:\n    \"\"\"Generate a new span identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.gen_group_id","title":"gen_group_id  <code>abstractmethod</code>","text":"<pre><code>gen_group_id() -&gt; str\n</code></pre> <p>Generate a new group identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_group_id(self) -&gt; str:\n    \"\"\"Generate a new group identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.create_trace","title":"create_trace  <code>abstractmethod</code>","text":"<pre><code>create_trace(\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n    tracing: TracingConfig | None = None,\n) -&gt; Trace\n</code></pre> <p>Create a new trace.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef create_trace(\n    self,\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n    tracing: TracingConfig | None = None,\n) -&gt; Trace:\n    \"\"\"Create a new trace.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.create_span","title":"create_span  <code>abstractmethod</code>","text":"<pre><code>create_span(\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]\n</code></pre> <p>Create a new span.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef create_span(\n    self,\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]:\n    \"\"\"Create a new span.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.shutdown","title":"shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Clean up any resources used by the provider.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"Clean up any resources used by the provider.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.AgentSpanData","title":"AgentSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an Agent Span in the trace. Includes name, handoffs, tools, and output type.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class AgentSpanData(SpanData):\n    \"\"\"\n    Represents an Agent Span in the trace.\n    Includes name, handoffs, tools, and output type.\n    \"\"\"\n\n    __slots__ = (\"name\", \"handoffs\", \"tools\", \"output_type\")\n\n    def __init__(\n        self,\n        name: str,\n        handoffs: list[str] | None = None,\n        tools: list[str] | None = None,\n        output_type: str | None = None,\n    ):\n        self.name = name\n        self.handoffs: list[str] | None = handoffs\n        self.tools: list[str] | None = tools\n        self.output_type: str | None = output_type\n\n    @property\n    def type(self) -&gt; str:\n        return \"agent\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"handoffs\": self.handoffs,\n            \"tools\": self.tools,\n            \"output_type\": self.output_type,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.CustomSpanData","title":"CustomSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Custom Span in the trace. Includes name and data property bag.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class CustomSpanData(SpanData):\n    \"\"\"\n    Represents a Custom Span in the trace.\n    Includes name and data property bag.\n    \"\"\"\n\n    __slots__ = (\"name\", \"data\")\n\n    def __init__(self, name: str, data: dict[str, Any]):\n        self.name = name\n        self.data = data\n\n    @property\n    def type(self) -&gt; str:\n        return \"custom\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"data\": self.data,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.FunctionSpanData","title":"FunctionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Function Span in the trace. Includes input, output and MCP data (if applicable).</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class FunctionSpanData(SpanData):\n    \"\"\"\n    Represents a Function Span in the trace.\n    Includes input, output and MCP data (if applicable).\n    \"\"\"\n\n    __slots__ = (\"name\", \"input\", \"output\", \"mcp_data\")\n\n    def __init__(\n        self,\n        name: str,\n        input: str | None,\n        output: Any | None,\n        mcp_data: dict[str, Any] | None = None,\n    ):\n        self.name = name\n        self.input = input\n        self.output = output\n        self.mcp_data = mcp_data\n\n    @property\n    def type(self) -&gt; str:\n        return \"function\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"input\": self.input,\n            \"output\": str(self.output) if self.output else None,\n            \"mcp_data\": self.mcp_data,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.GenerationSpanData","title":"GenerationSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Generation Span in the trace. Includes input, output, model, model configuration, and usage.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GenerationSpanData(SpanData):\n    \"\"\"\n    Represents a Generation Span in the trace.\n    Includes input, output, model, model configuration, and usage.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n        \"usage\",\n    )\n\n    def __init__(\n        self,\n        input: Sequence[Mapping[str, Any]] | None = None,\n        output: Sequence[Mapping[str, Any]] | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        usage: dict[str, Any] | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n        self.usage = usage\n\n    @property\n    def type(self) -&gt; str:\n        return \"generation\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"usage\": self.usage,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.GuardrailSpanData","title":"GuardrailSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Guardrail Span in the trace. Includes name and triggered status.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GuardrailSpanData(SpanData):\n    \"\"\"\n    Represents a Guardrail Span in the trace.\n    Includes name and triggered status.\n    \"\"\"\n\n    __slots__ = (\"name\", \"triggered\")\n\n    def __init__(self, name: str, triggered: bool = False):\n        self.name = name\n        self.triggered = triggered\n\n    @property\n    def type(self) -&gt; str:\n        return \"guardrail\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"triggered\": self.triggered,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.HandoffSpanData","title":"HandoffSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Handoff Span in the trace. Includes source and destination agents.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class HandoffSpanData(SpanData):\n    \"\"\"\n    Represents a Handoff Span in the trace.\n    Includes source and destination agents.\n    \"\"\"\n\n    __slots__ = (\"from_agent\", \"to_agent\")\n\n    def __init__(self, from_agent: str | None, to_agent: str | None):\n        self.from_agent = from_agent\n        self.to_agent = to_agent\n\n    @property\n    def type(self) -&gt; str:\n        return \"handoff\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"from_agent\": self.from_agent,\n            \"to_agent\": self.to_agent,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.MCPListToolsSpanData","title":"MCPListToolsSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an MCP List Tools Span in the trace. Includes server and result.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class MCPListToolsSpanData(SpanData):\n    \"\"\"\n    Represents an MCP List Tools Span in the trace.\n    Includes server and result.\n    \"\"\"\n\n    __slots__ = (\n        \"server\",\n        \"result\",\n    )\n\n    def __init__(self, server: str | None = None, result: list[str] | None = None):\n        self.server = server\n        self.result = result\n\n    @property\n    def type(self) -&gt; str:\n        return \"mcp_tools\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"server\": self.server,\n            \"result\": self.result,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.ResponseSpanData","title":"ResponseSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Response Span in the trace. Includes response and input.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class ResponseSpanData(SpanData):\n    \"\"\"\n    Represents a Response Span in the trace.\n    Includes response and input.\n    \"\"\"\n\n    __slots__ = (\"response\", \"input\")\n\n    def __init__(\n        self,\n        response: Response | None = None,\n        input: str | list[ResponseInputItemParam] | None = None,\n    ) -&gt; None:\n        self.response = response\n        # This is not used by the OpenAI trace processors, but is useful for other tracing\n        # processor implementations\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"response\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"response_id\": self.response.id if self.response else None,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpanData","title":"SpanData","text":"<p>               Bases: <code>ABC</code></p> <p>Represents span data in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpanData(abc.ABC):\n    \"\"\"\n    Represents span data in the trace.\n    \"\"\"\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any]:\n        \"\"\"Export the span data as a dictionary.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def type(self) -&gt; str:\n        \"\"\"Return the type of the span.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpanData.type","title":"type  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>Return the type of the span.</p>"},{"location":"ref/tracing/#agents.tracing.SpanData.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any]\n</code></pre> <p>Export the span data as a dictionary.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any]:\n    \"\"\"Export the span data as a dictionary.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpeechGroupSpanData","title":"SpeechGroupSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Group Span in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechGroupSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Group Span in the trace.\n    \"\"\"\n\n    __slots__ = \"input\"\n\n    def __init__(\n        self,\n        input: str | None = None,\n    ):\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech_group\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpeechSpanData","title":"SpeechSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Span in the trace. Includes input, output, model, model configuration, and first content timestamp.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Span in the trace.\n    Includes input, output, model, model configuration, and first content timestamp.\n    \"\"\"\n\n    __slots__ = (\"input\", \"output\", \"model\", \"model_config\", \"first_content_at\")\n\n    def __init__(\n        self,\n        input: str | None = None,\n        output: str | None = None,\n        output_format: str | None = \"pcm\",\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        first_content_at: str | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.output_format = output_format\n        self.model = model\n        self.model_config = model_config\n        self.first_content_at = first_content_at\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": {\n                \"data\": self.output or \"\",\n                \"format\": self.output_format,\n            },\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"first_content_at\": self.first_content_at,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TranscriptionSpanData","title":"TranscriptionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Transcription Span in the trace. Includes input, output, model, and model configuration.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class TranscriptionSpanData(SpanData):\n    \"\"\"\n    Represents a Transcription Span in the trace.\n    Includes input, output, model, and model configuration.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n    )\n\n    def __init__(\n        self,\n        input: str | None = None,\n        input_format: str | None = \"pcm\",\n        output: str | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n    ):\n        self.input = input\n        self.input_format = input_format\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n\n    @property\n    def type(self) -&gt; str:\n        return \"transcription\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": {\n                \"data\": self.input or \"\",\n                \"format\": self.input_format,\n            },\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Span","title":"Span","text":"<p>               Bases: <code>ABC</code>, <code>Generic[TSpanData]</code></p> <p>Base class for representing traceable operations with timing and context.</p> <p>A span represents a single operation within a trace (e.g., an LLM call, tool execution, or agent run). Spans track timing, relationships between operations, and operation-specific data.</p> Example <pre><code># Creating a custom span\nwith custom_span(\"database_query\", {\n    \"operation\": \"SELECT\",\n    \"table\": \"users\"\n}) as span:\n    results = await db.query(\"SELECT * FROM users\")\n    span.set_output({\"count\": len(results)})\n\n# Handling errors in spans\nwith custom_span(\"risky_operation\") as span:\n    try:\n        result = perform_risky_operation()\n    except Exception as e:\n        span.set_error({\n            \"message\": str(e),\n            \"data\": {\"operation\": \"risky_operation\"}\n        })\n        raise\n</code></pre> <p>Notes: - Spans automatically nest under the current trace - Use context managers for reliable start/finish - Include relevant data but avoid sensitive information - Handle errors properly using set_error()</p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class Span(abc.ABC, Generic[TSpanData]):\n    \"\"\"Base class for representing traceable operations with timing and context.\n\n    A span represents a single operation within a trace (e.g., an LLM call, tool execution,\n    or agent run). Spans track timing, relationships between operations, and operation-specific\n    data.\n\n    Type Args:\n        TSpanData: The type of span-specific data this span contains.\n\n    Example:\n        ```python\n        # Creating a custom span\n        with custom_span(\"database_query\", {\n            \"operation\": \"SELECT\",\n            \"table\": \"users\"\n        }) as span:\n            results = await db.query(\"SELECT * FROM users\")\n            span.set_output({\"count\": len(results)})\n\n        # Handling errors in spans\n        with custom_span(\"risky_operation\") as span:\n            try:\n                result = perform_risky_operation()\n            except Exception as e:\n                span.set_error({\n                    \"message\": str(e),\n                    \"data\": {\"operation\": \"risky_operation\"}\n                })\n                raise\n        ```\n\n        Notes:\n        - Spans automatically nest under the current trace\n        - Use context managers for reliable start/finish\n        - Include relevant data but avoid sensitive information\n        - Handle errors properly using set_error()\n    \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        \"\"\"The ID of the trace this span belongs to.\n\n        Returns:\n            str: Unique identifier of the parent trace.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_id(self) -&gt; str:\n        \"\"\"Unique identifier for this span.\n\n        Returns:\n            str: The span's unique ID within its trace.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_data(self) -&gt; TSpanData:\n        \"\"\"Operation-specific data for this span.\n\n        Returns:\n            TSpanData: Data specific to this type of span (e.g., LLM generation data).\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"\n        Start the span.\n\n        Args:\n            mark_as_current: If true, the span will be marked as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False) -&gt; None:\n        \"\"\"\n        Finish the span.\n\n        Args:\n            reset_current: If true, the span will be reset as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Span[TSpanData]:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def parent_id(self) -&gt; str | None:\n        \"\"\"ID of the parent span, if any.\n\n        Returns:\n            str | None: The parent span's ID, or None if this is a root span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def set_error(self, error: SpanError) -&gt; None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def error(self) -&gt; SpanError | None:\n        \"\"\"Any error that occurred during span execution.\n\n        Returns:\n            SpanError | None: Error details if an error occurred, None otherwise.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def started_at(self) -&gt; str | None:\n        \"\"\"When the span started execution.\n\n        Returns:\n            str | None: ISO format timestamp of span start, None if not started.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def ended_at(self) -&gt; str | None:\n        \"\"\"When the span finished execution.\n\n        Returns:\n            str | None: ISO format timestamp of span end, None if not finished.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def tracing_api_key(self) -&gt; str | None:\n        \"\"\"The API key to use when exporting this span.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Span.trace_id","title":"trace_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>trace_id: str\n</code></pre> <p>The ID of the trace this span belongs to.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique identifier of the parent trace.</p>"},{"location":"ref/tracing/#agents.tracing.Span.span_id","title":"span_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>span_id: str\n</code></pre> <p>Unique identifier for this span.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The span's unique ID within its trace.</p>"},{"location":"ref/tracing/#agents.tracing.Span.span_data","title":"span_data  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>span_data: TSpanData\n</code></pre> <p>Operation-specific data for this span.</p> <p>Returns:</p> Name Type Description <code>TSpanData</code> <code>TSpanData</code> <p>Data specific to this type of span (e.g., LLM generation data).</p>"},{"location":"ref/tracing/#agents.tracing.Span.parent_id","title":"parent_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>parent_id: str | None\n</code></pre> <p>ID of the parent span, if any.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The parent span's ID, or None if this is a root span.</p>"},{"location":"ref/tracing/#agents.tracing.Span.error","title":"error  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>error: SpanError | None\n</code></pre> <p>Any error that occurred during span execution.</p> <p>Returns:</p> Type Description <code>SpanError | None</code> <p>SpanError | None: Error details if an error occurred, None otherwise.</p>"},{"location":"ref/tracing/#agents.tracing.Span.started_at","title":"started_at  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>started_at: str | None\n</code></pre> <p>When the span started execution.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: ISO format timestamp of span start, None if not started.</p>"},{"location":"ref/tracing/#agents.tracing.Span.ended_at","title":"ended_at  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ended_at: str | None\n</code></pre> <p>When the span finished execution.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: ISO format timestamp of span end, None if not finished.</p>"},{"location":"ref/tracing/#agents.tracing.Span.tracing_api_key","title":"tracing_api_key  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tracing_api_key: str | None\n</code></pre> <p>The API key to use when exporting this span.</p>"},{"location":"ref/tracing/#agents.tracing.Span.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the span.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, the span will be marked as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"\n    Start the span.\n\n    Args:\n        mark_as_current: If true, the span will be marked as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Span.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False) -&gt; None\n</code></pre> <p>Finish the span.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, the span will be reset as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False) -&gt; None:\n    \"\"\"\n    Finish the span.\n\n    Args:\n        reset_current: If true, the span will be reset as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpanError","title":"SpanError","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents an error that occurred during span execution.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>A human-readable error description</p> <code>data</code> <code>dict[str, Any] | None</code> <p>Optional dictionary containing additional error context</p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class SpanError(TypedDict):\n    \"\"\"Represents an error that occurred during span execution.\n\n    Attributes:\n        message: A human-readable error description\n        data: Optional dictionary containing additional error context\n    \"\"\"\n\n    message: str\n    data: dict[str, Any] | None\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace","title":"Trace","text":"<p>               Bases: <code>ABC</code></p> <p>A complete end-to-end workflow containing related spans and metadata.</p> <p>A trace represents a logical workflow or operation (e.g., \"Customer Service Query\" or \"Code Generation\") and contains all the spans (individual operations) that occur during that workflow.</p> Example <pre><code># Basic trace usage\nwith trace(\"Order Processing\") as t:\n    validation_result = await Runner.run(validator, order_data)\n    if validation_result.approved:\n        await Runner.run(processor, order_data)\n\n# Trace with metadata and grouping\nwith trace(\n    \"Customer Service\",\n    group_id=\"chat_123\",\n    metadata={\"customer\": \"user_456\"}\n) as t:\n    result = await Runner.run(support_agent, query)\n</code></pre> Notes <ul> <li>Use descriptive workflow names</li> <li>Group related traces with consistent group_ids</li> <li>Add relevant metadata for filtering/analysis</li> <li>Use context managers for reliable cleanup</li> <li>Consider privacy when adding trace data</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class Trace(abc.ABC):\n    \"\"\"A complete end-to-end workflow containing related spans and metadata.\n\n    A trace represents a logical workflow or operation (e.g., \"Customer Service Query\"\n    or \"Code Generation\") and contains all the spans (individual operations) that occur\n    during that workflow.\n\n    Example:\n        ```python\n        # Basic trace usage\n        with trace(\"Order Processing\") as t:\n            validation_result = await Runner.run(validator, order_data)\n            if validation_result.approved:\n                await Runner.run(processor, order_data)\n\n        # Trace with metadata and grouping\n        with trace(\n            \"Customer Service\",\n            group_id=\"chat_123\",\n            metadata={\"customer\": \"user_456\"}\n        ) as t:\n            result = await Runner.run(support_agent, query)\n        ```\n\n    Notes:\n        - Use descriptive workflow names\n        - Group related traces with consistent group_ids\n        - Add relevant metadata for filtering/analysis\n        - Use context managers for reliable cleanup\n        - Consider privacy when adding trace data\n    \"\"\"\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Trace:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"Start the trace and optionally mark it as the current trace.\n\n        Args:\n            mark_as_current: If true, marks this trace as the current trace\n                in the execution context.\n\n        Notes:\n            - Must be called before any spans can be added\n            - Only one trace can be current at a time\n            - Thread-safe when using mark_as_current\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False):\n        \"\"\"Finish the trace and optionally reset the current trace.\n\n        Args:\n            reset_current: If true, resets the current trace to the previous\n                trace in the execution context.\n\n        Notes:\n            - Must be called to complete the trace\n            - Finalizes all open spans\n            - Thread-safe when using reset_current\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        \"\"\"Get the unique identifier for this trace.\n\n        Returns:\n            str: The trace's unique ID in the format 'trace_&lt;32_alphanumeric&gt;'\n\n        Notes:\n            - IDs are globally unique\n            - Used to link spans to their parent trace\n            - Can be used to look up traces in the dashboard\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Get the human-readable name of this workflow trace.\n\n        Returns:\n            str: The workflow name (e.g., \"Customer Service\", \"Data Processing\")\n\n        Notes:\n            - Should be descriptive and meaningful\n            - Used for grouping and filtering in the dashboard\n            - Helps identify the purpose of the trace\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        \"\"\"Export the trace data as a serializable dictionary.\n\n        Returns:\n            dict | None: Dictionary containing trace data, or None if tracing is disabled.\n\n        Notes:\n            - Includes all spans and their data\n            - Used for sending traces to backends\n            - May include metadata and group ID\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def tracing_api_key(self) -&gt; str | None:\n        \"\"\"The API key to use when exporting this trace and its spans.\"\"\"\n        pass\n\n    def to_json(self, *, include_tracing_api_key: bool = False) -&gt; dict[str, Any] | None:\n        \"\"\"Serialize trace metadata for persistence or transport.\n\n        Args:\n            include_tracing_api_key: When True, include the tracing API key. Defaults to False\n                to avoid persisting secrets unintentionally.\n        \"\"\"\n        exported = self.export()\n        if exported is None:\n            return None\n        payload = dict(exported)\n        if include_tracing_api_key and self.tracing_api_key:\n            payload[\"tracing_api_key\"] = self.tracing_api_key\n        return payload\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace.trace_id","title":"trace_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>trace_id: str\n</code></pre> <p>Get the unique identifier for this trace.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The trace's unique ID in the format 'trace_&lt;32_alphanumeric&gt;'</p> Notes <ul> <li>IDs are globally unique</li> <li>Used to link spans to their parent trace</li> <li>Can be used to look up traces in the dashboard</li> </ul>"},{"location":"ref/tracing/#agents.tracing.Trace.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Get the human-readable name of this workflow trace.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The workflow name (e.g., \"Customer Service\", \"Data Processing\")</p> Notes <ul> <li>Should be descriptive and meaningful</li> <li>Used for grouping and filtering in the dashboard</li> <li>Helps identify the purpose of the trace</li> </ul>"},{"location":"ref/tracing/#agents.tracing.Trace.tracing_api_key","title":"tracing_api_key  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tracing_api_key: str | None\n</code></pre> <p>The API key to use when exporting this trace and its spans.</p>"},{"location":"ref/tracing/#agents.tracing.Trace.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the trace and optionally mark it as the current trace.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, marks this trace as the current trace in the execution context.</p> <code>False</code> Notes <ul> <li>Must be called before any spans can be added</li> <li>Only one trace can be current at a time</li> <li>Thread-safe when using mark_as_current</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"Start the trace and optionally mark it as the current trace.\n\n    Args:\n        mark_as_current: If true, marks this trace as the current trace\n            in the execution context.\n\n    Notes:\n        - Must be called before any spans can be added\n        - Only one trace can be current at a time\n        - Thread-safe when using mark_as_current\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False)\n</code></pre> <p>Finish the trace and optionally reset the current trace.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, resets the current trace to the previous trace in the execution context.</p> <code>False</code> Notes <ul> <li>Must be called to complete the trace</li> <li>Finalizes all open spans</li> <li>Thread-safe when using reset_current</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False):\n    \"\"\"Finish the trace and optionally reset the current trace.\n\n    Args:\n        reset_current: If true, resets the current trace to the previous\n            trace in the execution context.\n\n    Notes:\n        - Must be called to complete the trace\n        - Finalizes all open spans\n        - Thread-safe when using reset_current\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any] | None\n</code></pre> <p>Export the trace data as a serializable dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>dict | None: Dictionary containing trace data, or None if tracing is disabled.</p> Notes <ul> <li>Includes all spans and their data</li> <li>Used for sending traces to backends</li> <li>May include metadata and group ID</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any] | None:\n    \"\"\"Export the trace data as a serializable dictionary.\n\n    Returns:\n        dict | None: Dictionary containing trace data, or None if tracing is disabled.\n\n    Notes:\n        - Includes all spans and their data\n        - Used for sending traces to backends\n        - May include metadata and group ID\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace.to_json","title":"to_json","text":"<pre><code>to_json(\n    *, include_tracing_api_key: bool = False\n) -&gt; dict[str, Any] | None\n</code></pre> <p>Serialize trace metadata for persistence or transport.</p> <p>Parameters:</p> Name Type Description Default <code>include_tracing_api_key</code> <code>bool</code> <p>When True, include the tracing API key. Defaults to False to avoid persisting secrets unintentionally.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>def to_json(self, *, include_tracing_api_key: bool = False) -&gt; dict[str, Any] | None:\n    \"\"\"Serialize trace metadata for persistence or transport.\n\n    Args:\n        include_tracing_api_key: When True, include the tracing API key. Defaults to False\n            to avoid persisting secrets unintentionally.\n    \"\"\"\n    exported = self.export()\n    if exported is None:\n        return None\n    payload = dict(exported)\n    if include_tracing_api_key and self.tracing_api_key:\n        payload[\"tracing_api_key\"] = self.tracing_api_key\n    return payload\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.agent_span","title":"agent_span","text":"<pre><code>agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]\n</code></pre> <p>Create a new agent span. The span will not be started automatically, you should either do <code>with agent_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the agent.</p> required <code>handoffs</code> <code>list[str] | None</code> <p>Optional list of agent names to which this agent could hand off control.</p> <code>None</code> <code>tools</code> <code>list[str] | None</code> <p>Optional list of tool names available to this agent.</p> <code>None</code> <code>output_type</code> <code>str | None</code> <p>Optional name of the output type produced by the agent.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[AgentSpanData]</code> <p>The newly created agent span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]:\n    \"\"\"Create a new agent span. The span will not be started automatically, you should either do\n    `with agent_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the agent.\n        handoffs: Optional list of agent names to which this agent could hand off control.\n        tools: Optional list of tool names available to this agent.\n        output_type: Optional name of the output type produced by the agent.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created agent span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=AgentSpanData(name=name, handoffs=handoffs, tools=tools, output_type=output_type),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.custom_span","title":"custom_span","text":"<pre><code>custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]\n</code></pre> <p>Create a new custom span, to which you can add your own metadata. The span will not be started automatically, you should either do <code>with custom_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom span.</p> required <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary structured data to associate with the span.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[CustomSpanData]</code> <p>The newly created custom span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]:\n    \"\"\"Create a new custom span, to which you can add your own metadata. The span will not be\n    started automatically, you should either do `with custom_span() ...` or call\n    `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the custom span.\n        data: Arbitrary structured data to associate with the span.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created custom span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=CustomSpanData(name=name, data=data or {}),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.function_span","title":"function_span","text":"<pre><code>function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]\n</code></pre> <p>Create a new function span. The span will not be started automatically, you should either do <code>with function_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the function.</p> required <code>input</code> <code>str | None</code> <p>The input to the function.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The output of the function.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[FunctionSpanData]</code> <p>The newly created function span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]:\n    \"\"\"Create a new function span. The span will not be started automatically, you should either do\n    `with function_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the function.\n        input: The input to the function.\n        output: The output of the function.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created function span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=FunctionSpanData(name=name, input=input, output=output),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.generation_span","title":"generation_span","text":"<pre><code>generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]\n</code></pre> <p>Create a new generation span. The span will not be started automatically, you should either do <code>with generation_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>This span captures the details of a model generation, including the input message sequence, any generated outputs, the model name and configuration, and usage data. If you only need to capture a model response identifier, use <code>response_span()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of input messages sent to the model.</p> <code>None</code> <code>output</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of output messages received from the model.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>The model identifier used for the generation.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>usage</code> <code>dict[str, Any] | None</code> <p>A dictionary of usage information (input tokens, output tokens, etc.).</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[GenerationSpanData]</code> <p>The newly created generation span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]:\n    \"\"\"Create a new generation span. The span will not be started automatically, you should either\n    do `with generation_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    This span captures the details of a model generation, including the\n    input message sequence, any generated outputs, the model name and\n    configuration, and usage data. If you only need to capture a model\n    response identifier, use `response_span()` instead.\n\n    Args:\n        input: The sequence of input messages sent to the model.\n        output: The sequence of output messages received from the model.\n        model: The model identifier used for the generation.\n        model_config: The model configuration (hyperparameters) used.\n        usage: A dictionary of usage information (input tokens, output tokens, etc.).\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created generation span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GenerationSpanData(\n            input=input,\n            output=output,\n            model=model,\n            model_config=model_config,\n            usage=usage,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.get_current_span","title":"get_current_span","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Returns the currently active span, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_span() -&gt; Span[Any] | None:\n    \"\"\"Returns the currently active span, if present.\"\"\"\n    return get_trace_provider().get_current_span()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.get_current_trace","title":"get_current_trace","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Returns the currently active trace, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_trace() -&gt; Trace | None:\n    \"\"\"Returns the currently active trace, if present.\"\"\"\n    return get_trace_provider().get_current_trace()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.guardrail_span","title":"guardrail_span","text":"<pre><code>guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]\n</code></pre> <p>Create a new guardrail span. The span will not be started automatically, you should either do <code>with guardrail_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the guardrail.</p> required <code>triggered</code> <code>bool</code> <p>Whether the guardrail was triggered.</p> <code>False</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]:\n    \"\"\"Create a new guardrail span. The span will not be started automatically, you should either\n    do `with guardrail_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the guardrail.\n        triggered: Whether the guardrail was triggered.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GuardrailSpanData(name=name, triggered=triggered),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.handoff_span","title":"handoff_span","text":"<pre><code>handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]\n</code></pre> <p>Create a new handoff span. The span will not be started automatically, you should either do <code>with handoff_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>from_agent</code> <code>str | None</code> <p>The name of the agent that is handing off.</p> <code>None</code> <code>to_agent</code> <code>str | None</code> <p>The name of the agent that is receiving the handoff.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[HandoffSpanData]</code> <p>The newly created handoff span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]:\n    \"\"\"Create a new handoff span. The span will not be started automatically, you should either do\n    `with handoff_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        from_agent: The name of the agent that is handing off.\n        to_agent: The name of the agent that is receiving the handoff.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created handoff span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=HandoffSpanData(from_agent=from_agent, to_agent=to_agent),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.mcp_tools_span","title":"mcp_tools_span","text":"<pre><code>mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]\n</code></pre> <p>Create a new MCP list tools span. The span will not be started automatically, you should either do <code>with mcp_tools_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str | None</code> <p>The name of the MCP server.</p> <code>None</code> <code>result</code> <code>list[str] | None</code> <p>The result of the MCP list tools call.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]:\n    \"\"\"Create a new MCP list tools span. The span will not be started automatically, you should\n    either do `with mcp_tools_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        server: The name of the MCP server.\n        result: The result of the MCP list tools call.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=MCPListToolsSpanData(server=server, result=result),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.response_span","title":"response_span","text":"<pre><code>response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]\n</code></pre> <p>Create a new response span. The span will not be started automatically, you should either do <code>with response_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response | None</code> <p>The OpenAI Response object.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]:\n    \"\"\"Create a new response span. The span will not be started automatically, you should either do\n    `with response_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        response: The OpenAI Response object.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=ResponseSpanData(response=response),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.speech_group_span","title":"speech_group_span","text":"<pre><code>speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]\n</code></pre> <p>Create a new speech group span. The span will not be started automatically, you should either do <code>with speech_group_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str | None</code> <p>The input text used for the speech request.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]:\n    \"\"\"Create a new speech group span. The span will not be started automatically, you should\n    either do `with speech_group_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        input: The input text used for the speech request.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechGroupSpanData(input=input),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.speech_span","title":"speech_span","text":"<pre><code>speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]\n</code></pre> <p>Create a new speech span. The span will not be started automatically, you should either do <code>with speech_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the text-to-speech.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The text input of the text-to-speech.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.</p> <code>None</code> <code>output_format</code> <code>str | None</code> <p>The format of the audio output (defaults to \"pcm\").</p> <code>'pcm'</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>first_content_at</code> <code>str | None</code> <p>The time of the first byte of the audio output.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]:\n    \"\"\"Create a new speech span. The span will not be started automatically, you should either do\n    `with speech_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the text-to-speech.\n        input: The text input of the text-to-speech.\n        output: The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\n        output_format: The format of the audio output (defaults to \"pcm\").\n        model_config: The model configuration (hyperparameters) used.\n        first_content_at: The time of the first byte of the audio output.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechSpanData(\n            model=model,\n            input=input,\n            output=output,\n            output_format=output_format,\n            model_config=model_config,\n            first_content_at=first_content_at,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.trace","title":"trace","text":"<pre><code>trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    tracing: TracingConfig | None = None,\n    disabled: bool = False,\n) -&gt; Trace\n</code></pre> <p>Create a new trace. The trace will not be started automatically; you should either use it as a context manager (<code>with trace(...):</code>) or call <code>trace.start()</code> + <code>trace.finish()</code> manually.</p> <p>In addition to the workflow name and optional grouping identifier, you can provide an arbitrary metadata dictionary to attach additional user-defined information to the trace.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_name</code> <code>str</code> <p>The name of the logical app or workflow. For example, you might provide \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.</p> required <code>trace_id</code> <code>str | None</code> <p>The ID of the trace. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_trace_id()</code> to generate a trace ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>group_id</code> <code>str | None</code> <p>Optional grouping identifier to link multiple traces from the same conversation or process. For instance, you might use a chat thread ID.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of additional metadata to attach to the trace.</p> <code>None</code> <code>tracing</code> <code>TracingConfig | None</code> <p>Optional tracing configuration for exporting this trace.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Trace but the Trace will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Trace</code> <p>The newly created trace object.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    tracing: TracingConfig | None = None,\n    disabled: bool = False,\n) -&gt; Trace:\n    \"\"\"\n    Create a new trace. The trace will not be started automatically; you should either use\n    it as a context manager (`with trace(...):`) or call `trace.start()` + `trace.finish()`\n    manually.\n\n    In addition to the workflow name and optional grouping identifier, you can provide\n    an arbitrary metadata dictionary to attach additional user-defined information to\n    the trace.\n\n    Args:\n        workflow_name: The name of the logical app or workflow. For example, you might provide\n            \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\n        trace_id: The ID of the trace. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_trace_id()` to generate a trace ID, to guarantee that IDs are\n            correctly formatted.\n        group_id: Optional grouping identifier to link multiple traces from the same conversation\n            or process. For instance, you might use a chat thread ID.\n        metadata: Optional dictionary of additional metadata to attach to the trace.\n        tracing: Optional tracing configuration for exporting this trace.\n        disabled: If True, we will return a Trace but the Trace will not be recorded.\n\n    Returns:\n        The newly created trace object.\n    \"\"\"\n    current_trace = get_trace_provider().get_current_trace()\n    if current_trace:\n        logger.warning(\n            \"Trace already exists. Creating a new trace, but this is probably a mistake.\"\n        )\n\n    return get_trace_provider().create_trace(\n        name=workflow_name,\n        trace_id=trace_id,\n        group_id=group_id,\n        metadata=metadata,\n        tracing=tracing,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.transcription_span","title":"transcription_span","text":"<pre><code>transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]\n</code></pre> <p>Create a new transcription span. The span will not be started automatically, you should either do <code>with transcription_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the speech-to-text.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The audio input of the speech-to-text transcription, as a base64 encoded string of audio bytes.</p> <code>None</code> <code>input_format</code> <code>str | None</code> <p>The format of the audio input (defaults to \"pcm\").</p> <code>'pcm'</code> <code>output</code> <code>str | None</code> <p>The output of the speech-to-text transcription.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[TranscriptionSpanData]</code> <p>The newly created speech-to-text span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]:\n    \"\"\"Create a new transcription span. The span will not be started automatically, you should\n    either do `with transcription_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the speech-to-text.\n        input: The audio input of the speech-to-text transcription, as a base64 encoded string of\n            audio bytes.\n        input_format: The format of the audio input (defaults to \"pcm\").\n        output: The output of the speech-to-text transcription.\n        model_config: The model configuration (hyperparameters) used.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created speech-to-text span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=TranscriptionSpanData(\n            input=input,\n            input_format=input_format,\n            output=output,\n            model=model,\n            model_config=model_config,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.get_trace_provider","title":"get_trace_provider","text":"<pre><code>get_trace_provider() -&gt; TraceProvider\n</code></pre> <p>Get the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def get_trace_provider() -&gt; TraceProvider:\n    \"\"\"Get the global trace provider used by tracing utilities.\"\"\"\n    if GLOBAL_TRACE_PROVIDER is None:\n        raise RuntimeError(\"Trace provider not set\")\n    return GLOBAL_TRACE_PROVIDER\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_trace_provider","title":"set_trace_provider","text":"<pre><code>set_trace_provider(provider: TraceProvider) -&gt; None\n</code></pre> <p>Set the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def set_trace_provider(provider: TraceProvider) -&gt; None:\n    \"\"\"Set the global trace provider used by tracing utilities.\"\"\"\n    global GLOBAL_TRACE_PROVIDER\n    GLOBAL_TRACE_PROVIDER = provider\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.gen_span_id","title":"gen_span_id","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_span_id() -&gt; str:\n    \"\"\"Generate a new span ID.\"\"\"\n    return get_trace_provider().gen_span_id()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.gen_trace_id","title":"gen_trace_id","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_trace_id() -&gt; str:\n    \"\"\"Generate a new trace ID.\"\"\"\n    return get_trace_provider().gen_trace_id()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.add_trace_processor","title":"add_trace_processor","text":"<pre><code>add_trace_processor(\n    span_processor: TracingProcessor,\n) -&gt; None\n</code></pre> <p>Adds a new trace processor. This processor will receive all traces/spans.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def add_trace_processor(span_processor: TracingProcessor) -&gt; None:\n    \"\"\"\n    Adds a new trace processor. This processor will receive all traces/spans.\n    \"\"\"\n    get_trace_provider().register_processor(span_processor)\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_trace_processors","title":"set_trace_processors","text":"<pre><code>set_trace_processors(\n    processors: list[TracingProcessor],\n) -&gt; None\n</code></pre> <p>Set the list of trace processors. This will replace the current list of processors.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_trace_processors(processors: list[TracingProcessor]) -&gt; None:\n    \"\"\"\n    Set the list of trace processors. This will replace the current list of processors.\n    \"\"\"\n    get_trace_provider().set_processors(processors)\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_tracing_disabled","title":"set_tracing_disabled","text":"<pre><code>set_tracing_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Set whether tracing is globally disabled.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_disabled(disabled: bool) -&gt; None:\n    \"\"\"\n    Set whether tracing is globally disabled.\n    \"\"\"\n    get_trace_provider().set_disabled(disabled)\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_tracing_export_api_key","title":"set_tracing_export_api_key","text":"<pre><code>set_tracing_export_api_key(api_key: str) -&gt; None\n</code></pre> <p>Set the OpenAI API key for the backend exporter.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_export_api_key(api_key: str) -&gt; None:\n    \"\"\"\n    Set the OpenAI API key for the backend exporter.\n    \"\"\"\n    default_exporter().set_api_key(api_key)\n</code></pre>"},{"location":"ref/tracing/config/","title":"<code>Config</code>","text":""},{"location":"ref/tracing/config/#agents.tracing.config.TracingConfig","title":"TracingConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for tracing export.</p> Source code in <code>src/agents/tracing/config.py</code> <pre><code>class TracingConfig(TypedDict, total=False):\n    \"\"\"Configuration for tracing export.\"\"\"\n\n    api_key: str\n</code></pre>"},{"location":"ref/tracing/context/","title":"<code>Context</code>","text":""},{"location":"ref/tracing/context/#agents.tracing.context.TraceCtxManager","title":"TraceCtxManager","text":"<p>Create a trace when none exists and manage its lifecycle for a run.</p> Source code in <code>src/agents/tracing/context.py</code> <pre><code>class TraceCtxManager:\n    \"\"\"Create a trace when none exists and manage its lifecycle for a run.\"\"\"\n\n    def __init__(\n        self,\n        workflow_name: str,\n        trace_id: str | None,\n        group_id: str | None,\n        metadata: dict[str, Any] | None,\n        tracing: TracingConfig | None,\n        disabled: bool,\n    ):\n        self.trace: Trace | None = None\n        self.workflow_name = workflow_name\n        self.trace_id = trace_id\n        self.group_id = group_id\n        self.metadata = metadata\n        self.tracing = tracing\n        self.disabled = disabled\n\n    def __enter__(self) -&gt; TraceCtxManager:\n        current_trace = get_current_trace()\n        if not current_trace:\n            self.trace = trace(\n                workflow_name=self.workflow_name,\n                trace_id=self.trace_id,\n                group_id=self.group_id,\n                metadata=self.metadata,\n                tracing=self.tracing,\n                disabled=self.disabled,\n            )\n            assert self.trace is not None\n            self.trace.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.trace:\n            self.trace.finish(reset_current=True)\n</code></pre>"},{"location":"ref/tracing/create/","title":"<code>Creating traces/spans</code>","text":""},{"location":"ref/tracing/create/#agents.tracing.create.trace","title":"trace","text":"<pre><code>trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    tracing: TracingConfig | None = None,\n    disabled: bool = False,\n) -&gt; Trace\n</code></pre> <p>Create a new trace. The trace will not be started automatically; you should either use it as a context manager (<code>with trace(...):</code>) or call <code>trace.start()</code> + <code>trace.finish()</code> manually.</p> <p>In addition to the workflow name and optional grouping identifier, you can provide an arbitrary metadata dictionary to attach additional user-defined information to the trace.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_name</code> <code>str</code> <p>The name of the logical app or workflow. For example, you might provide \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.</p> required <code>trace_id</code> <code>str | None</code> <p>The ID of the trace. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_trace_id()</code> to generate a trace ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>group_id</code> <code>str | None</code> <p>Optional grouping identifier to link multiple traces from the same conversation or process. For instance, you might use a chat thread ID.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of additional metadata to attach to the trace.</p> <code>None</code> <code>tracing</code> <code>TracingConfig | None</code> <p>Optional tracing configuration for exporting this trace.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Trace but the Trace will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Trace</code> <p>The newly created trace object.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    tracing: TracingConfig | None = None,\n    disabled: bool = False,\n) -&gt; Trace:\n    \"\"\"\n    Create a new trace. The trace will not be started automatically; you should either use\n    it as a context manager (`with trace(...):`) or call `trace.start()` + `trace.finish()`\n    manually.\n\n    In addition to the workflow name and optional grouping identifier, you can provide\n    an arbitrary metadata dictionary to attach additional user-defined information to\n    the trace.\n\n    Args:\n        workflow_name: The name of the logical app or workflow. For example, you might provide\n            \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\n        trace_id: The ID of the trace. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_trace_id()` to generate a trace ID, to guarantee that IDs are\n            correctly formatted.\n        group_id: Optional grouping identifier to link multiple traces from the same conversation\n            or process. For instance, you might use a chat thread ID.\n        metadata: Optional dictionary of additional metadata to attach to the trace.\n        tracing: Optional tracing configuration for exporting this trace.\n        disabled: If True, we will return a Trace but the Trace will not be recorded.\n\n    Returns:\n        The newly created trace object.\n    \"\"\"\n    current_trace = get_trace_provider().get_current_trace()\n    if current_trace:\n        logger.warning(\n            \"Trace already exists. Creating a new trace, but this is probably a mistake.\"\n        )\n\n    return get_trace_provider().create_trace(\n        name=workflow_name,\n        trace_id=trace_id,\n        group_id=group_id,\n        metadata=metadata,\n        tracing=tracing,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.get_current_trace","title":"get_current_trace","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Returns the currently active trace, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_trace() -&gt; Trace | None:\n    \"\"\"Returns the currently active trace, if present.\"\"\"\n    return get_trace_provider().get_current_trace()\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.get_current_span","title":"get_current_span","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Returns the currently active span, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_span() -&gt; Span[Any] | None:\n    \"\"\"Returns the currently active span, if present.\"\"\"\n    return get_trace_provider().get_current_span()\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.agent_span","title":"agent_span","text":"<pre><code>agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]\n</code></pre> <p>Create a new agent span. The span will not be started automatically, you should either do <code>with agent_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the agent.</p> required <code>handoffs</code> <code>list[str] | None</code> <p>Optional list of agent names to which this agent could hand off control.</p> <code>None</code> <code>tools</code> <code>list[str] | None</code> <p>Optional list of tool names available to this agent.</p> <code>None</code> <code>output_type</code> <code>str | None</code> <p>Optional name of the output type produced by the agent.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[AgentSpanData]</code> <p>The newly created agent span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]:\n    \"\"\"Create a new agent span. The span will not be started automatically, you should either do\n    `with agent_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the agent.\n        handoffs: Optional list of agent names to which this agent could hand off control.\n        tools: Optional list of tool names available to this agent.\n        output_type: Optional name of the output type produced by the agent.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created agent span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=AgentSpanData(name=name, handoffs=handoffs, tools=tools, output_type=output_type),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.function_span","title":"function_span","text":"<pre><code>function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]\n</code></pre> <p>Create a new function span. The span will not be started automatically, you should either do <code>with function_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the function.</p> required <code>input</code> <code>str | None</code> <p>The input to the function.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The output of the function.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[FunctionSpanData]</code> <p>The newly created function span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]:\n    \"\"\"Create a new function span. The span will not be started automatically, you should either do\n    `with function_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the function.\n        input: The input to the function.\n        output: The output of the function.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created function span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=FunctionSpanData(name=name, input=input, output=output),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.generation_span","title":"generation_span","text":"<pre><code>generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]\n</code></pre> <p>Create a new generation span. The span will not be started automatically, you should either do <code>with generation_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>This span captures the details of a model generation, including the input message sequence, any generated outputs, the model name and configuration, and usage data. If you only need to capture a model response identifier, use <code>response_span()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of input messages sent to the model.</p> <code>None</code> <code>output</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of output messages received from the model.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>The model identifier used for the generation.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>usage</code> <code>dict[str, Any] | None</code> <p>A dictionary of usage information (input tokens, output tokens, etc.).</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[GenerationSpanData]</code> <p>The newly created generation span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]:\n    \"\"\"Create a new generation span. The span will not be started automatically, you should either\n    do `with generation_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    This span captures the details of a model generation, including the\n    input message sequence, any generated outputs, the model name and\n    configuration, and usage data. If you only need to capture a model\n    response identifier, use `response_span()` instead.\n\n    Args:\n        input: The sequence of input messages sent to the model.\n        output: The sequence of output messages received from the model.\n        model: The model identifier used for the generation.\n        model_config: The model configuration (hyperparameters) used.\n        usage: A dictionary of usage information (input tokens, output tokens, etc.).\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created generation span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GenerationSpanData(\n            input=input,\n            output=output,\n            model=model,\n            model_config=model_config,\n            usage=usage,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.response_span","title":"response_span","text":"<pre><code>response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]\n</code></pre> <p>Create a new response span. The span will not be started automatically, you should either do <code>with response_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response | None</code> <p>The OpenAI Response object.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]:\n    \"\"\"Create a new response span. The span will not be started automatically, you should either do\n    `with response_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        response: The OpenAI Response object.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=ResponseSpanData(response=response),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.handoff_span","title":"handoff_span","text":"<pre><code>handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]\n</code></pre> <p>Create a new handoff span. The span will not be started automatically, you should either do <code>with handoff_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>from_agent</code> <code>str | None</code> <p>The name of the agent that is handing off.</p> <code>None</code> <code>to_agent</code> <code>str | None</code> <p>The name of the agent that is receiving the handoff.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[HandoffSpanData]</code> <p>The newly created handoff span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]:\n    \"\"\"Create a new handoff span. The span will not be started automatically, you should either do\n    `with handoff_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        from_agent: The name of the agent that is handing off.\n        to_agent: The name of the agent that is receiving the handoff.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created handoff span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=HandoffSpanData(from_agent=from_agent, to_agent=to_agent),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.custom_span","title":"custom_span","text":"<pre><code>custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]\n</code></pre> <p>Create a new custom span, to which you can add your own metadata. The span will not be started automatically, you should either do <code>with custom_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom span.</p> required <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary structured data to associate with the span.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[CustomSpanData]</code> <p>The newly created custom span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]:\n    \"\"\"Create a new custom span, to which you can add your own metadata. The span will not be\n    started automatically, you should either do `with custom_span() ...` or call\n    `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the custom span.\n        data: Arbitrary structured data to associate with the span.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created custom span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=CustomSpanData(name=name, data=data or {}),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.guardrail_span","title":"guardrail_span","text":"<pre><code>guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]\n</code></pre> <p>Create a new guardrail span. The span will not be started automatically, you should either do <code>with guardrail_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the guardrail.</p> required <code>triggered</code> <code>bool</code> <p>Whether the guardrail was triggered.</p> <code>False</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]:\n    \"\"\"Create a new guardrail span. The span will not be started automatically, you should either\n    do `with guardrail_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the guardrail.\n        triggered: Whether the guardrail was triggered.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GuardrailSpanData(name=name, triggered=triggered),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.transcription_span","title":"transcription_span","text":"<pre><code>transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]\n</code></pre> <p>Create a new transcription span. The span will not be started automatically, you should either do <code>with transcription_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the speech-to-text.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The audio input of the speech-to-text transcription, as a base64 encoded string of audio bytes.</p> <code>None</code> <code>input_format</code> <code>str | None</code> <p>The format of the audio input (defaults to \"pcm\").</p> <code>'pcm'</code> <code>output</code> <code>str | None</code> <p>The output of the speech-to-text transcription.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[TranscriptionSpanData]</code> <p>The newly created speech-to-text span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]:\n    \"\"\"Create a new transcription span. The span will not be started automatically, you should\n    either do `with transcription_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the speech-to-text.\n        input: The audio input of the speech-to-text transcription, as a base64 encoded string of\n            audio bytes.\n        input_format: The format of the audio input (defaults to \"pcm\").\n        output: The output of the speech-to-text transcription.\n        model_config: The model configuration (hyperparameters) used.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created speech-to-text span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=TranscriptionSpanData(\n            input=input,\n            input_format=input_format,\n            output=output,\n            model=model,\n            model_config=model_config,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.speech_span","title":"speech_span","text":"<pre><code>speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]\n</code></pre> <p>Create a new speech span. The span will not be started automatically, you should either do <code>with speech_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the text-to-speech.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The text input of the text-to-speech.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.</p> <code>None</code> <code>output_format</code> <code>str | None</code> <p>The format of the audio output (defaults to \"pcm\").</p> <code>'pcm'</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>first_content_at</code> <code>str | None</code> <p>The time of the first byte of the audio output.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]:\n    \"\"\"Create a new speech span. The span will not be started automatically, you should either do\n    `with speech_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the text-to-speech.\n        input: The text input of the text-to-speech.\n        output: The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\n        output_format: The format of the audio output (defaults to \"pcm\").\n        model_config: The model configuration (hyperparameters) used.\n        first_content_at: The time of the first byte of the audio output.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechSpanData(\n            model=model,\n            input=input,\n            output=output,\n            output_format=output_format,\n            model_config=model_config,\n            first_content_at=first_content_at,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.speech_group_span","title":"speech_group_span","text":"<pre><code>speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]\n</code></pre> <p>Create a new speech group span. The span will not be started automatically, you should either do <code>with speech_group_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str | None</code> <p>The input text used for the speech request.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]:\n    \"\"\"Create a new speech group span. The span will not be started automatically, you should\n    either do `with speech_group_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        input: The input text used for the speech request.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechGroupSpanData(input=input),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.mcp_tools_span","title":"mcp_tools_span","text":"<pre><code>mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]\n</code></pre> <p>Create a new MCP list tools span. The span will not be started automatically, you should either do <code>with mcp_tools_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str | None</code> <p>The name of the MCP server.</p> <code>None</code> <code>result</code> <code>list[str] | None</code> <p>The result of the MCP list tools call.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]:\n    \"\"\"Create a new MCP list tools span. The span will not be started automatically, you should\n    either do `with mcp_tools_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        server: The name of the MCP server.\n        result: The result of the MCP list tools call.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=MCPListToolsSpanData(server=server, result=result),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/logger/","title":"<code>Logger</code>","text":""},{"location":"ref/tracing/model_tracing/","title":"<code>Model Tracing</code>","text":""},{"location":"ref/tracing/model_tracing/#agents.tracing.model_tracing.get_model_tracing_impl","title":"get_model_tracing_impl","text":"<pre><code>get_model_tracing_impl(\n    tracing_disabled: bool,\n    trace_include_sensitive_data: bool,\n) -&gt; ModelTracing\n</code></pre> <p>Return the ModelTracing setting based on run-level tracing configuration.</p> Source code in <code>src/agents/tracing/model_tracing.py</code> <pre><code>def get_model_tracing_impl(\n    tracing_disabled: bool, trace_include_sensitive_data: bool\n) -&gt; ModelTracing:\n    \"\"\"Return the ModelTracing setting based on run-level tracing configuration.\"\"\"\n    if tracing_disabled:\n        return ModelTracing.DISABLED\n    if trace_include_sensitive_data:\n        return ModelTracing.ENABLED\n    return ModelTracing.ENABLED_WITHOUT_DATA\n</code></pre>"},{"location":"ref/tracing/processor_interface/","title":"<code>Processor interface</code>","text":""},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor","title":"TracingProcessor","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for processing and monitoring traces and spans in the OpenAI Agents system.</p> <p>This abstract class defines the interface that all tracing processors must implement. Processors receive notifications when traces and spans start and end, allowing them to collect, process, and export tracing data.</p> Example <pre><code>class CustomProcessor(TracingProcessor):\n    def __init__(self):\n        self.active_traces = {}\n        self.active_spans = {}\n\n    def on_trace_start(self, trace):\n        self.active_traces[trace.trace_id] = trace\n\n    def on_trace_end(self, trace):\n        # Process completed trace\n        del self.active_traces[trace.trace_id]\n\n    def on_span_start(self, span):\n        self.active_spans[span.span_id] = span\n\n    def on_span_end(self, span):\n        # Process completed span\n        del self.active_spans[span.span_id]\n\n    def shutdown(self):\n        # Clean up resources\n        self.active_traces.clear()\n        self.active_spans.clear()\n\n    def force_flush(self):\n        # Force processing of any queued items\n        pass\n</code></pre> Notes <ul> <li>All methods should be thread-safe</li> <li>Methods should not block for long periods</li> <li>Handle errors gracefully to prevent disrupting agent execution</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>class TracingProcessor(abc.ABC):\n    \"\"\"Interface for processing and monitoring traces and spans in the OpenAI Agents system.\n\n    This abstract class defines the interface that all tracing processors must implement.\n    Processors receive notifications when traces and spans start and end, allowing them\n    to collect, process, and export tracing data.\n\n    Example:\n        ```python\n        class CustomProcessor(TracingProcessor):\n            def __init__(self):\n                self.active_traces = {}\n                self.active_spans = {}\n\n            def on_trace_start(self, trace):\n                self.active_traces[trace.trace_id] = trace\n\n            def on_trace_end(self, trace):\n                # Process completed trace\n                del self.active_traces[trace.trace_id]\n\n            def on_span_start(self, span):\n                self.active_spans[span.span_id] = span\n\n            def on_span_end(self, span):\n                # Process completed span\n                del self.active_spans[span.span_id]\n\n            def shutdown(self):\n                # Clean up resources\n                self.active_traces.clear()\n                self.active_spans.clear()\n\n            def force_flush(self):\n                # Force processing of any queued items\n                pass\n        ```\n\n    Notes:\n        - All methods should be thread-safe\n        - Methods should not block for long periods\n        - Handle errors gracefully to prevent disrupting agent execution\n    \"\"\"\n\n    @abc.abstractmethod\n    def on_trace_start(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a new trace begins execution.\n\n        Args:\n            trace: The trace that started. Contains workflow name and metadata.\n\n        Notes:\n            - Called synchronously on trace start\n            - Should return quickly to avoid blocking execution\n            - Any errors should be caught and handled internally\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_trace_end(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a trace completes execution.\n\n        Args:\n            trace: The completed trace containing all spans and results.\n\n        Notes:\n            - Called synchronously when trace finishes\n            - Good time to export/process the complete trace\n            - Should handle cleanup of any trace-specific resources\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_start(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a new span begins execution.\n\n        Args:\n            span: The span that started. Contains operation details and context.\n\n        Notes:\n            - Called synchronously on span start\n            - Should return quickly to avoid blocking execution\n            - Spans are automatically nested under current trace/span\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_end(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a span completes execution.\n\n        Args:\n            span: The completed span containing execution results.\n\n        Notes:\n            - Called synchronously when span finishes\n            - Should not block or raise exceptions\n            - Good time to export/process the individual span\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def shutdown(self) -&gt; None:\n        \"\"\"Called when the application stops to clean up resources.\n\n        Should perform any necessary cleanup like:\n        - Flushing queued traces/spans\n        - Closing connections\n        - Releasing resources\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def force_flush(self) -&gt; None:\n        \"\"\"Forces immediate processing of any queued traces/spans.\n\n        Notes:\n            - Should process all queued items before returning\n            - Useful before shutdown or when immediate processing is needed\n            - May block while processing completes\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_trace_start","title":"on_trace_start  <code>abstractmethod</code>","text":"<pre><code>on_trace_start(trace: Trace) -&gt; None\n</code></pre> <p>Called when a new trace begins execution.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace that started. Contains workflow name and metadata.</p> required Notes <ul> <li>Called synchronously on trace start</li> <li>Should return quickly to avoid blocking execution</li> <li>Any errors should be caught and handled internally</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_start(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a new trace begins execution.\n\n    Args:\n        trace: The trace that started. Contains workflow name and metadata.\n\n    Notes:\n        - Called synchronously on trace start\n        - Should return quickly to avoid blocking execution\n        - Any errors should be caught and handled internally\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_trace_end","title":"on_trace_end  <code>abstractmethod</code>","text":"<pre><code>on_trace_end(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace completes execution.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The completed trace containing all spans and results.</p> required Notes <ul> <li>Called synchronously when trace finishes</li> <li>Good time to export/process the complete trace</li> <li>Should handle cleanup of any trace-specific resources</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_end(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a trace completes execution.\n\n    Args:\n        trace: The completed trace containing all spans and results.\n\n    Notes:\n        - Called synchronously when trace finishes\n        - Good time to export/process the complete trace\n        - Should handle cleanup of any trace-specific resources\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_span_start","title":"on_span_start  <code>abstractmethod</code>","text":"<pre><code>on_span_start(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a new span begins execution.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The span that started. Contains operation details and context.</p> required Notes <ul> <li>Called synchronously on span start</li> <li>Should return quickly to avoid blocking execution</li> <li>Spans are automatically nested under current trace/span</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_start(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a new span begins execution.\n\n    Args:\n        span: The span that started. Contains operation details and context.\n\n    Notes:\n        - Called synchronously on span start\n        - Should return quickly to avoid blocking execution\n        - Spans are automatically nested under current trace/span\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_span_end","title":"on_span_end  <code>abstractmethod</code>","text":"<pre><code>on_span_end(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span completes execution.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The completed span containing execution results.</p> required Notes <ul> <li>Called synchronously when span finishes</li> <li>Should not block or raise exceptions</li> <li>Good time to export/process the individual span</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_end(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a span completes execution.\n\n    Args:\n        span: The completed span containing execution results.\n\n    Notes:\n        - Called synchronously when span finishes\n        - Should not block or raise exceptions\n        - Good time to export/process the individual span\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.shutdown","title":"shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Called when the application stops to clean up resources.</p> <p>Should perform any necessary cleanup like: - Flushing queued traces/spans - Closing connections - Releasing resources</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"Called when the application stops to clean up resources.\n\n    Should perform any necessary cleanup like:\n    - Flushing queued traces/spans\n    - Closing connections\n    - Releasing resources\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.force_flush","title":"force_flush  <code>abstractmethod</code>","text":"<pre><code>force_flush() -&gt; None\n</code></pre> <p>Forces immediate processing of any queued traces/spans.</p> Notes <ul> <li>Should process all queued items before returning</li> <li>Useful before shutdown or when immediate processing is needed</li> <li>May block while processing completes</li> </ul> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef force_flush(self) -&gt; None:\n    \"\"\"Forces immediate processing of any queued traces/spans.\n\n    Notes:\n        - Should process all queued items before returning\n        - Useful before shutdown or when immediate processing is needed\n        - May block while processing completes\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingExporter","title":"TracingExporter","text":"<p>               Bases: <code>ABC</code></p> <p>Exports traces and spans. For example, could log them or send them to a backend.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>class TracingExporter(abc.ABC):\n    \"\"\"Exports traces and spans. For example, could log them or send them to a backend.\"\"\"\n\n    @abc.abstractmethod\n    def export(self, items: list[\"Trace | Span[Any]\"]) -&gt; None:\n        \"\"\"Exports a list of traces and spans.\n\n        Args:\n            items: The items to export.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingExporter.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export(items: list[Trace | Span[Any]]) -&gt; None\n</code></pre> <p>Exports a list of traces and spans.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[Trace | Span[Any]]</code> <p>The items to export.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef export(self, items: list[\"Trace | Span[Any]\"]) -&gt; None:\n    \"\"\"Exports a list of traces and spans.\n\n    Args:\n        items: The items to export.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processors/","title":"<code>Processors</code>","text":""},{"location":"ref/tracing/processors/#agents.tracing.processors.ConsoleSpanExporter","title":"ConsoleSpanExporter","text":"<p>               Bases: <code>TracingExporter</code></p> <p>Prints the traces and spans to the console.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>class ConsoleSpanExporter(TracingExporter):\n    \"\"\"Prints the traces and spans to the console.\"\"\"\n\n    def export(self, items: list[Trace | Span[Any]]) -&gt; None:\n        for item in items:\n            if isinstance(item, Trace):\n                print(f\"[Exporter] Export trace_id={item.trace_id}, name={item.name}\")\n            else:\n                print(f\"[Exporter] Export span: {item.export()}\")\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter","title":"BackendSpanExporter","text":"<p>               Bases: <code>TracingExporter</code></p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>class BackendSpanExporter(TracingExporter):\n    def __init__(\n        self,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        endpoint: str = \"https://api.openai.com/v1/traces/ingest\",\n        max_retries: int = 3,\n        base_delay: float = 1.0,\n        max_delay: float = 30.0,\n    ):\n        \"\"\"\n        Args:\n            api_key: The API key for the \"Authorization\" header. Defaults to\n                `os.environ[\"OPENAI_API_KEY\"]` if not provided.\n            organization: The OpenAI organization to use. Defaults to\n                `os.environ[\"OPENAI_ORG_ID\"]` if not provided.\n            project: The OpenAI project to use. Defaults to\n                `os.environ[\"OPENAI_PROJECT_ID\"]` if not provided.\n            endpoint: The HTTP endpoint to which traces/spans are posted.\n            max_retries: Maximum number of retries upon failures.\n            base_delay: Base delay (in seconds) for the first backoff.\n            max_delay: Maximum delay (in seconds) for backoff growth.\n        \"\"\"\n        self._api_key = api_key\n        self._organization = organization\n        self._project = project\n        self.endpoint = endpoint\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n\n        # Keep a client open for connection pooling across multiple export calls\n        self._client = httpx.Client(timeout=httpx.Timeout(timeout=60, connect=5.0))\n\n    def set_api_key(self, api_key: str):\n        \"\"\"Set the OpenAI API key for the exporter.\n\n        Args:\n            api_key: The OpenAI API key to use. This is the same key used by the OpenAI Python\n                client.\n        \"\"\"\n        # Clear the cached property if it exists\n        if \"api_key\" in self.__dict__:\n            del self.__dict__[\"api_key\"]\n\n        # Update the private attribute\n        self._api_key = api_key\n\n    @cached_property\n    def api_key(self):\n        return self._api_key or os.environ.get(\"OPENAI_API_KEY\")\n\n    @cached_property\n    def organization(self):\n        return self._organization or os.environ.get(\"OPENAI_ORG_ID\")\n\n    @cached_property\n    def project(self):\n        return self._project or os.environ.get(\"OPENAI_PROJECT_ID\")\n\n    def export(self, items: list[Trace | Span[Any]]) -&gt; None:\n        if not items:\n            return\n\n        grouped_items: dict[str | None, list[Trace | Span[Any]]] = {}\n        for item in items:\n            key = item.tracing_api_key\n            grouped_items.setdefault(key, []).append(item)\n\n        for item_key, grouped in grouped_items.items():\n            api_key = item_key or self.api_key\n            if not api_key:\n                logger.warning(\"OPENAI_API_KEY is not set, skipping trace export\")\n                continue\n\n            data = [item.export() for item in grouped if item.export()]\n            payload = {\"data\": data}\n\n            headers = {\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\",\n                \"OpenAI-Beta\": \"traces=v1\",\n            }\n\n            if self.organization:\n                headers[\"OpenAI-Organization\"] = self.organization\n\n            if self.project:\n                headers[\"OpenAI-Project\"] = self.project\n\n            # Exponential backoff loop\n            attempt = 0\n            delay = self.base_delay\n            while True:\n                attempt += 1\n                try:\n                    response = self._client.post(url=self.endpoint, headers=headers, json=payload)\n\n                    # If the response is successful, break out of the loop\n                    if response.status_code &lt; 300:\n                        logger.debug(f\"Exported {len(grouped)} items\")\n                        break\n\n                    # If the response is a client error (4xx), we won't retry\n                    if 400 &lt;= response.status_code &lt; 500:\n                        logger.error(\n                            \"[non-fatal] Tracing client error %s: %s\",\n                            response.status_code,\n                            response.text,\n                        )\n                        break\n\n                    # For 5xx or other unexpected codes, treat it as transient and retry\n                    logger.warning(\n                        f\"[non-fatal] Tracing: server error {response.status_code}, retrying.\"\n                    )\n                except httpx.RequestError as exc:\n                    # Network or other I/O error, we'll retry\n                    logger.warning(f\"[non-fatal] Tracing: request failed: {exc}\")\n\n                # If we reach here, we need to retry or give up\n                if attempt &gt;= self.max_retries:\n                    logger.error(\n                        \"[non-fatal] Tracing: max retries reached, giving up on this batch.\"\n                    )\n                    break\n\n                # Exponential backoff + jitter\n                sleep_time = delay + random.uniform(0, 0.1 * delay)  # 10% jitter\n                time.sleep(sleep_time)\n                delay = min(delay * 2, self.max_delay)\n\n    def close(self):\n        \"\"\"Close the underlying HTTP client.\"\"\"\n        self._client.close()\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter.__init__","title":"__init__","text":"<pre><code>__init__(\n    api_key: str | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n    endpoint: str = \"https://api.openai.com/v1/traces/ingest\",\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 30.0,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>The API key for the \"Authorization\" header. Defaults to <code>os.environ[\"OPENAI_API_KEY\"]</code> if not provided.</p> <code>None</code> <code>organization</code> <code>str | None</code> <p>The OpenAI organization to use. Defaults to <code>os.environ[\"OPENAI_ORG_ID\"]</code> if not provided.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>The OpenAI project to use. Defaults to <code>os.environ[\"OPENAI_PROJECT_ID\"]</code> if not provided.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>The HTTP endpoint to which traces/spans are posted.</p> <code>'https://api.openai.com/v1/traces/ingest'</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries upon failures.</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Base delay (in seconds) for the first backoff.</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay (in seconds) for backoff growth.</p> <code>30.0</code> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def __init__(\n    self,\n    api_key: str | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n    endpoint: str = \"https://api.openai.com/v1/traces/ingest\",\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 30.0,\n):\n    \"\"\"\n    Args:\n        api_key: The API key for the \"Authorization\" header. Defaults to\n            `os.environ[\"OPENAI_API_KEY\"]` if not provided.\n        organization: The OpenAI organization to use. Defaults to\n            `os.environ[\"OPENAI_ORG_ID\"]` if not provided.\n        project: The OpenAI project to use. Defaults to\n            `os.environ[\"OPENAI_PROJECT_ID\"]` if not provided.\n        endpoint: The HTTP endpoint to which traces/spans are posted.\n        max_retries: Maximum number of retries upon failures.\n        base_delay: Base delay (in seconds) for the first backoff.\n        max_delay: Maximum delay (in seconds) for backoff growth.\n    \"\"\"\n    self._api_key = api_key\n    self._organization = organization\n    self._project = project\n    self.endpoint = endpoint\n    self.max_retries = max_retries\n    self.base_delay = base_delay\n    self.max_delay = max_delay\n\n    # Keep a client open for connection pooling across multiple export calls\n    self._client = httpx.Client(timeout=httpx.Timeout(timeout=60, connect=5.0))\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter.set_api_key","title":"set_api_key","text":"<pre><code>set_api_key(api_key: str)\n</code></pre> <p>Set the OpenAI API key for the exporter.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The OpenAI API key to use. This is the same key used by the OpenAI Python client.</p> required Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def set_api_key(self, api_key: str):\n    \"\"\"Set the OpenAI API key for the exporter.\n\n    Args:\n        api_key: The OpenAI API key to use. This is the same key used by the OpenAI Python\n            client.\n    \"\"\"\n    # Clear the cached property if it exists\n    if \"api_key\" in self.__dict__:\n        del self.__dict__[\"api_key\"]\n\n    # Update the private attribute\n    self._api_key = api_key\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the underlying HTTP client.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def close(self):\n    \"\"\"Close the underlying HTTP client.\"\"\"\n    self._client.close()\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor","title":"BatchTraceProcessor","text":"<p>               Bases: <code>TracingProcessor</code></p> <p>Some implementation notes: 1. Using Queue, which is thread-safe. 2. Using a background thread to export spans, to minimize any performance issues. 3. Spans are stored in memory until they are exported.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>class BatchTraceProcessor(TracingProcessor):\n    \"\"\"Some implementation notes:\n    1. Using Queue, which is thread-safe.\n    2. Using a background thread to export spans, to minimize any performance issues.\n    3. Spans are stored in memory until they are exported.\n    \"\"\"\n\n    def __init__(\n        self,\n        exporter: TracingExporter,\n        max_queue_size: int = 8192,\n        max_batch_size: int = 128,\n        schedule_delay: float = 5.0,\n        export_trigger_ratio: float = 0.7,\n    ):\n        \"\"\"\n        Args:\n            exporter: The exporter to use.\n            max_queue_size: The maximum number of spans to store in the queue. After this, we will\n                start dropping spans.\n            max_batch_size: The maximum number of spans to export in a single batch.\n            schedule_delay: The delay between checks for new spans to export.\n            export_trigger_ratio: The ratio of the queue size at which we will trigger an export.\n        \"\"\"\n        self._exporter = exporter\n        self._queue: queue.Queue[Trace | Span[Any]] = queue.Queue(maxsize=max_queue_size)\n        self._max_queue_size = max_queue_size\n        self._max_batch_size = max_batch_size\n        self._schedule_delay = schedule_delay\n        self._shutdown_event = threading.Event()\n\n        # The queue size threshold at which we export immediately.\n        self._export_trigger_size = max(1, int(max_queue_size * export_trigger_ratio))\n\n        # Track when we next *must* perform a scheduled export\n        self._next_export_time = time.time() + self._schedule_delay\n\n        # We lazily start the background worker thread the first time a span/trace is queued.\n        self._worker_thread: threading.Thread | None = None\n        self._thread_start_lock = threading.Lock()\n\n    def _ensure_thread_started(self) -&gt; None:\n        # Fast path without holding the lock\n        if self._worker_thread and self._worker_thread.is_alive():\n            return\n\n        # Double-checked locking to avoid starting multiple threads\n        with self._thread_start_lock:\n            if self._worker_thread and self._worker_thread.is_alive():\n                return\n\n            self._worker_thread = threading.Thread(target=self._run, daemon=True)\n            self._worker_thread.start()\n\n    def on_trace_start(self, trace: Trace) -&gt; None:\n        # Ensure the background worker is running before we enqueue anything.\n        self._ensure_thread_started()\n\n        try:\n            self._queue.put_nowait(trace)\n        except queue.Full:\n            logger.warning(\"Queue is full, dropping trace.\")\n\n    def on_trace_end(self, trace: Trace) -&gt; None:\n        # We send traces via on_trace_start, so we don't need to do anything here.\n        pass\n\n    def on_span_start(self, span: Span[Any]) -&gt; None:\n        # We send spans via on_span_end, so we don't need to do anything here.\n        pass\n\n    def on_span_end(self, span: Span[Any]) -&gt; None:\n        # Ensure the background worker is running before we enqueue anything.\n        self._ensure_thread_started()\n\n        try:\n            self._queue.put_nowait(span)\n        except queue.Full:\n            logger.warning(\"Queue is full, dropping span.\")\n\n    def shutdown(self, timeout: float | None = None):\n        \"\"\"\n        Called when the application stops. We signal our thread to stop, then join it.\n        \"\"\"\n        self._shutdown_event.set()\n\n        # Only join if we ever started the background thread; otherwise flush synchronously.\n        if self._worker_thread and self._worker_thread.is_alive():\n            self._worker_thread.join(timeout=timeout)\n        else:\n            # No background thread: process any remaining items synchronously.\n            self._export_batches(force=True)\n\n    def force_flush(self):\n        \"\"\"\n        Forces an immediate flush of all queued spans.\n        \"\"\"\n        self._export_batches(force=True)\n\n    def _run(self):\n        while not self._shutdown_event.is_set():\n            current_time = time.time()\n            queue_size = self._queue.qsize()\n\n            # If it's time for a scheduled flush or queue is above the trigger threshold\n            if current_time &gt;= self._next_export_time or queue_size &gt;= self._export_trigger_size:\n                self._export_batches(force=False)\n                # Reset the next scheduled flush time\n                self._next_export_time = time.time() + self._schedule_delay\n            else:\n                # Sleep a short interval so we don't busy-wait.\n                time.sleep(0.2)\n\n        # Final drain after shutdown\n        self._export_batches(force=True)\n\n    def _export_batches(self, force: bool = False):\n        \"\"\"Drains the queue and exports in batches. If force=True, export everything.\n        Otherwise, export up to `max_batch_size` repeatedly until the queue is completely empty.\n        \"\"\"\n        while True:\n            items_to_export: list[Span[Any] | Trace] = []\n\n            # Gather a batch of spans up to max_batch_size\n            while not self._queue.empty() and (\n                force or len(items_to_export) &lt; self._max_batch_size\n            ):\n                try:\n                    items_to_export.append(self._queue.get_nowait())\n                except queue.Empty:\n                    # Another thread might have emptied the queue between checks\n                    break\n\n            # If we collected nothing, we're done\n            if not items_to_export:\n                break\n\n            # Export the batch\n            self._exporter.export(items_to_export)\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor.__init__","title":"__init__","text":"<pre><code>__init__(\n    exporter: TracingExporter,\n    max_queue_size: int = 8192,\n    max_batch_size: int = 128,\n    schedule_delay: float = 5.0,\n    export_trigger_ratio: float = 0.7,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>exporter</code> <code>TracingExporter</code> <p>The exporter to use.</p> required <code>max_queue_size</code> <code>int</code> <p>The maximum number of spans to store in the queue. After this, we will start dropping spans.</p> <code>8192</code> <code>max_batch_size</code> <code>int</code> <p>The maximum number of spans to export in a single batch.</p> <code>128</code> <code>schedule_delay</code> <code>float</code> <p>The delay between checks for new spans to export.</p> <code>5.0</code> <code>export_trigger_ratio</code> <code>float</code> <p>The ratio of the queue size at which we will trigger an export.</p> <code>0.7</code> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def __init__(\n    self,\n    exporter: TracingExporter,\n    max_queue_size: int = 8192,\n    max_batch_size: int = 128,\n    schedule_delay: float = 5.0,\n    export_trigger_ratio: float = 0.7,\n):\n    \"\"\"\n    Args:\n        exporter: The exporter to use.\n        max_queue_size: The maximum number of spans to store in the queue. After this, we will\n            start dropping spans.\n        max_batch_size: The maximum number of spans to export in a single batch.\n        schedule_delay: The delay between checks for new spans to export.\n        export_trigger_ratio: The ratio of the queue size at which we will trigger an export.\n    \"\"\"\n    self._exporter = exporter\n    self._queue: queue.Queue[Trace | Span[Any]] = queue.Queue(maxsize=max_queue_size)\n    self._max_queue_size = max_queue_size\n    self._max_batch_size = max_batch_size\n    self._schedule_delay = schedule_delay\n    self._shutdown_event = threading.Event()\n\n    # The queue size threshold at which we export immediately.\n    self._export_trigger_size = max(1, int(max_queue_size * export_trigger_ratio))\n\n    # Track when we next *must* perform a scheduled export\n    self._next_export_time = time.time() + self._schedule_delay\n\n    # We lazily start the background worker thread the first time a span/trace is queued.\n    self._worker_thread: threading.Thread | None = None\n    self._thread_start_lock = threading.Lock()\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor.shutdown","title":"shutdown","text":"<pre><code>shutdown(timeout: float | None = None)\n</code></pre> <p>Called when the application stops. We signal our thread to stop, then join it.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def shutdown(self, timeout: float | None = None):\n    \"\"\"\n    Called when the application stops. We signal our thread to stop, then join it.\n    \"\"\"\n    self._shutdown_event.set()\n\n    # Only join if we ever started the background thread; otherwise flush synchronously.\n    if self._worker_thread and self._worker_thread.is_alive():\n        self._worker_thread.join(timeout=timeout)\n    else:\n        # No background thread: process any remaining items synchronously.\n        self._export_batches(force=True)\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor.force_flush","title":"force_flush","text":"<pre><code>force_flush()\n</code></pre> <p>Forces an immediate flush of all queued spans.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def force_flush(self):\n    \"\"\"\n    Forces an immediate flush of all queued spans.\n    \"\"\"\n    self._export_batches(force=True)\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.default_exporter","title":"default_exporter","text":"<pre><code>default_exporter() -&gt; BackendSpanExporter\n</code></pre> <p>The default exporter, which exports traces and spans to the backend in batches.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def default_exporter() -&gt; BackendSpanExporter:\n    \"\"\"The default exporter, which exports traces and spans to the backend in batches.\"\"\"\n    return _global_exporter\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.default_processor","title":"default_processor","text":"<pre><code>default_processor() -&gt; BatchTraceProcessor\n</code></pre> <p>The default processor, which exports traces and spans to the backend in batches.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def default_processor() -&gt; BatchTraceProcessor:\n    \"\"\"The default processor, which exports traces and spans to the backend in batches.\"\"\"\n    return _global_processor\n</code></pre>"},{"location":"ref/tracing/provider/","title":"<code>Provider</code>","text":""},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor","title":"SynchronousMultiTracingProcessor","text":"<p>               Bases: <code>TracingProcessor</code></p> <p>Forwards all calls to a list of TracingProcessors, in order of registration.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>class SynchronousMultiTracingProcessor(TracingProcessor):\n    \"\"\"\n    Forwards all calls to a list of TracingProcessors, in order of registration.\n    \"\"\"\n\n    def __init__(self):\n        # Using a tuple to avoid race conditions when iterating over processors\n        self._processors: tuple[TracingProcessor, ...] = ()\n        self._lock = threading.Lock()\n\n    def add_tracing_processor(self, tracing_processor: TracingProcessor):\n        \"\"\"\n        Add a processor to the list of processors. Each processor will receive all traces/spans.\n        \"\"\"\n        with self._lock:\n            self._processors += (tracing_processor,)\n\n    def set_processors(self, processors: list[TracingProcessor]):\n        \"\"\"\n        Set the list of processors. This will replace the current list of processors.\n        \"\"\"\n        with self._lock:\n            self._processors = tuple(processors)\n\n    def on_trace_start(self, trace: Trace) -&gt; None:\n        \"\"\"\n        Called when a trace is started.\n        \"\"\"\n        for processor in self._processors:\n            try:\n                processor.on_trace_start(trace)\n            except Exception as e:\n                logger.error(f\"Error in trace processor {processor} during on_trace_start: {e}\")\n\n    def on_trace_end(self, trace: Trace) -&gt; None:\n        \"\"\"\n        Called when a trace is finished.\n        \"\"\"\n        for processor in self._processors:\n            try:\n                processor.on_trace_end(trace)\n            except Exception as e:\n                logger.error(f\"Error in trace processor {processor} during on_trace_end: {e}\")\n\n    def on_span_start(self, span: Span[Any]) -&gt; None:\n        \"\"\"\n        Called when a span is started.\n        \"\"\"\n        for processor in self._processors:\n            try:\n                processor.on_span_start(span)\n            except Exception as e:\n                logger.error(f\"Error in trace processor {processor} during on_span_start: {e}\")\n\n    def on_span_end(self, span: Span[Any]) -&gt; None:\n        \"\"\"\n        Called when a span is finished.\n        \"\"\"\n        for processor in self._processors:\n            try:\n                processor.on_span_end(span)\n            except Exception as e:\n                logger.error(f\"Error in trace processor {processor} during on_span_end: {e}\")\n\n    def shutdown(self) -&gt; None:\n        \"\"\"\n        Called when the application stops.\n        \"\"\"\n        for processor in self._processors:\n            _safe_debug(f\"Shutting down trace processor {processor}\")\n            try:\n                processor.shutdown()\n            except Exception as e:\n                logger.error(f\"Error shutting down trace processor {processor}: {e}\")\n\n    def force_flush(self):\n        \"\"\"\n        Force the processors to flush their buffers.\n        \"\"\"\n        for processor in self._processors:\n            try:\n                processor.force_flush()\n            except Exception as e:\n                logger.error(f\"Error flushing trace processor {processor}: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.add_tracing_processor","title":"add_tracing_processor","text":"<pre><code>add_tracing_processor(tracing_processor: TracingProcessor)\n</code></pre> <p>Add a processor to the list of processors. Each processor will receive all traces/spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def add_tracing_processor(self, tracing_processor: TracingProcessor):\n    \"\"\"\n    Add a processor to the list of processors. Each processor will receive all traces/spans.\n    \"\"\"\n    with self._lock:\n        self._processors += (tracing_processor,)\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.set_processors","title":"set_processors","text":"<pre><code>set_processors(processors: list[TracingProcessor])\n</code></pre> <p>Set the list of processors. This will replace the current list of processors.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def set_processors(self, processors: list[TracingProcessor]):\n    \"\"\"\n    Set the list of processors. This will replace the current list of processors.\n    \"\"\"\n    with self._lock:\n        self._processors = tuple(processors)\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.on_trace_start","title":"on_trace_start","text":"<pre><code>on_trace_start(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace is started.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def on_trace_start(self, trace: Trace) -&gt; None:\n    \"\"\"\n    Called when a trace is started.\n    \"\"\"\n    for processor in self._processors:\n        try:\n            processor.on_trace_start(trace)\n        except Exception as e:\n            logger.error(f\"Error in trace processor {processor} during on_trace_start: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.on_trace_end","title":"on_trace_end","text":"<pre><code>on_trace_end(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace is finished.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def on_trace_end(self, trace: Trace) -&gt; None:\n    \"\"\"\n    Called when a trace is finished.\n    \"\"\"\n    for processor in self._processors:\n        try:\n            processor.on_trace_end(trace)\n        except Exception as e:\n            logger.error(f\"Error in trace processor {processor} during on_trace_end: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.on_span_start","title":"on_span_start","text":"<pre><code>on_span_start(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span is started.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def on_span_start(self, span: Span[Any]) -&gt; None:\n    \"\"\"\n    Called when a span is started.\n    \"\"\"\n    for processor in self._processors:\n        try:\n            processor.on_span_start(span)\n        except Exception as e:\n            logger.error(f\"Error in trace processor {processor} during on_span_start: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.on_span_end","title":"on_span_end","text":"<pre><code>on_span_end(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span is finished.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def on_span_end(self, span: Span[Any]) -&gt; None:\n    \"\"\"\n    Called when a span is finished.\n    \"\"\"\n    for processor in self._processors:\n        try:\n            processor.on_span_end(span)\n        except Exception as e:\n            logger.error(f\"Error in trace processor {processor} during on_span_end: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.shutdown","title":"shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Called when the application stops.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"\n    Called when the application stops.\n    \"\"\"\n    for processor in self._processors:\n        _safe_debug(f\"Shutting down trace processor {processor}\")\n        try:\n            processor.shutdown()\n        except Exception as e:\n            logger.error(f\"Error shutting down trace processor {processor}: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.SynchronousMultiTracingProcessor.force_flush","title":"force_flush","text":"<pre><code>force_flush()\n</code></pre> <p>Force the processors to flush their buffers.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def force_flush(self):\n    \"\"\"\n    Force the processors to flush their buffers.\n    \"\"\"\n    for processor in self._processors:\n        try:\n            processor.force_flush()\n        except Exception as e:\n            logger.error(f\"Error flushing trace processor {processor}: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider","title":"TraceProvider","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for creating traces and spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>class TraceProvider(ABC):\n    \"\"\"Interface for creating traces and spans.\"\"\"\n\n    @abstractmethod\n    def register_processor(self, processor: TracingProcessor) -&gt; None:\n        \"\"\"Add a processor that will receive all traces and spans.\"\"\"\n\n    @abstractmethod\n    def set_processors(self, processors: list[TracingProcessor]) -&gt; None:\n        \"\"\"Replace the list of processors with ``processors``.\"\"\"\n\n    @abstractmethod\n    def get_current_trace(self) -&gt; Trace | None:\n        \"\"\"Return the currently active trace, if any.\"\"\"\n\n    @abstractmethod\n    def get_current_span(self) -&gt; Span[Any] | None:\n        \"\"\"Return the currently active span, if any.\"\"\"\n\n    @abstractmethod\n    def set_disabled(self, disabled: bool) -&gt; None:\n        \"\"\"Enable or disable tracing globally.\"\"\"\n\n    @abstractmethod\n    def time_iso(self) -&gt; str:\n        \"\"\"Return the current time in ISO 8601 format.\"\"\"\n\n    @abstractmethod\n    def gen_trace_id(self) -&gt; str:\n        \"\"\"Generate a new trace identifier.\"\"\"\n\n    @abstractmethod\n    def gen_span_id(self) -&gt; str:\n        \"\"\"Generate a new span identifier.\"\"\"\n\n    @abstractmethod\n    def gen_group_id(self) -&gt; str:\n        \"\"\"Generate a new group identifier.\"\"\"\n\n    @abstractmethod\n    def create_trace(\n        self,\n        name: str,\n        trace_id: str | None = None,\n        group_id: str | None = None,\n        metadata: dict[str, Any] | None = None,\n        disabled: bool = False,\n        tracing: TracingConfig | None = None,\n    ) -&gt; Trace:\n        \"\"\"Create a new trace.\"\"\"\n\n    @abstractmethod\n    def create_span(\n        self,\n        span_data: TSpanData,\n        span_id: str | None = None,\n        parent: Trace | Span[Any] | None = None,\n        disabled: bool = False,\n    ) -&gt; Span[TSpanData]:\n        \"\"\"Create a new span.\"\"\"\n\n    @abstractmethod\n    def shutdown(self) -&gt; None:\n        \"\"\"Clean up any resources used by the provider.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.register_processor","title":"register_processor  <code>abstractmethod</code>","text":"<pre><code>register_processor(processor: TracingProcessor) -&gt; None\n</code></pre> <p>Add a processor that will receive all traces and spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef register_processor(self, processor: TracingProcessor) -&gt; None:\n    \"\"\"Add a processor that will receive all traces and spans.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.set_processors","title":"set_processors  <code>abstractmethod</code>","text":"<pre><code>set_processors(processors: list[TracingProcessor]) -&gt; None\n</code></pre> <p>Replace the list of processors with <code>processors</code>.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef set_processors(self, processors: list[TracingProcessor]) -&gt; None:\n    \"\"\"Replace the list of processors with ``processors``.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.get_current_trace","title":"get_current_trace  <code>abstractmethod</code>","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Return the currently active trace, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef get_current_trace(self) -&gt; Trace | None:\n    \"\"\"Return the currently active trace, if any.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.get_current_span","title":"get_current_span  <code>abstractmethod</code>","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Return the currently active span, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef get_current_span(self) -&gt; Span[Any] | None:\n    \"\"\"Return the currently active span, if any.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.set_disabled","title":"set_disabled  <code>abstractmethod</code>","text":"<pre><code>set_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Enable or disable tracing globally.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef set_disabled(self, disabled: bool) -&gt; None:\n    \"\"\"Enable or disable tracing globally.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.time_iso","title":"time_iso  <code>abstractmethod</code>","text":"<pre><code>time_iso() -&gt; str\n</code></pre> <p>Return the current time in ISO 8601 format.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef time_iso(self) -&gt; str:\n    \"\"\"Return the current time in ISO 8601 format.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.gen_trace_id","title":"gen_trace_id  <code>abstractmethod</code>","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_trace_id(self) -&gt; str:\n    \"\"\"Generate a new trace identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.gen_span_id","title":"gen_span_id  <code>abstractmethod</code>","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_span_id(self) -&gt; str:\n    \"\"\"Generate a new span identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.gen_group_id","title":"gen_group_id  <code>abstractmethod</code>","text":"<pre><code>gen_group_id() -&gt; str\n</code></pre> <p>Generate a new group identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_group_id(self) -&gt; str:\n    \"\"\"Generate a new group identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.create_trace","title":"create_trace  <code>abstractmethod</code>","text":"<pre><code>create_trace(\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n    tracing: TracingConfig | None = None,\n) -&gt; Trace\n</code></pre> <p>Create a new trace.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef create_trace(\n    self,\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n    tracing: TracingConfig | None = None,\n) -&gt; Trace:\n    \"\"\"Create a new trace.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.create_span","title":"create_span  <code>abstractmethod</code>","text":"<pre><code>create_span(\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]\n</code></pre> <p>Create a new span.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef create_span(\n    self,\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]:\n    \"\"\"Create a new span.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.TraceProvider.shutdown","title":"shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Clean up any resources used by the provider.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"Clean up any resources used by the provider.\"\"\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider","title":"DefaultTraceProvider","text":"<p>               Bases: <code>TraceProvider</code></p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>class DefaultTraceProvider(TraceProvider):\n    def __init__(self) -&gt; None:\n        self._multi_processor = SynchronousMultiTracingProcessor()\n        # Lazily read env flag on first use to honor env set after import but before first trace.\n        self._env_disabled: bool | None = None\n        self._manual_disabled: bool | None = None\n        self._disabled = False\n\n    def register_processor(self, processor: TracingProcessor):\n        \"\"\"\n        Add a processor to the list of processors. Each processor will receive all traces/spans.\n        \"\"\"\n        self._multi_processor.add_tracing_processor(processor)\n\n    def set_processors(self, processors: list[TracingProcessor]):\n        \"\"\"\n        Set the list of processors. This will replace the current list of processors.\n        \"\"\"\n        self._multi_processor.set_processors(processors)\n\n    def get_current_trace(self) -&gt; Trace | None:\n        \"\"\"\n        Returns the currently active trace, if any.\n        \"\"\"\n        return Scope.get_current_trace()\n\n    def get_current_span(self) -&gt; Span[Any] | None:\n        \"\"\"\n        Returns the currently active span, if any.\n        \"\"\"\n        return Scope.get_current_span()\n\n    def set_disabled(self, disabled: bool) -&gt; None:\n        \"\"\"\n        Set whether tracing is disabled.\n        \"\"\"\n        self._manual_disabled = disabled\n        self._refresh_disabled_flag()\n\n    def _refresh_disabled_flag(self) -&gt; None:\n        \"\"\"Refresh disabled flag from cached env value and manual override.\n\n        The env flag is read once on first use to avoid surprises mid-run; further env\n        changes are ignored after the manual flag is set via set_disabled, which always\n        takes precedence over the env value.\n        \"\"\"\n        if self._env_disabled is None:\n            self._env_disabled = os.environ.get(\n                \"OPENAI_AGENTS_DISABLE_TRACING\", \"false\"\n            ).lower() in (\n                \"true\",\n                \"1\",\n            )\n        if self._manual_disabled is None:\n            self._disabled = bool(self._env_disabled)\n        else:\n            self._disabled = self._manual_disabled\n\n    def time_iso(self) -&gt; str:\n        \"\"\"Return the current time in ISO 8601 format.\"\"\"\n        return datetime.now(timezone.utc).isoformat()\n\n    def gen_trace_id(self) -&gt; str:\n        \"\"\"Generate a new trace ID.\"\"\"\n        return f\"trace_{uuid.uuid4().hex}\"\n\n    def gen_span_id(self) -&gt; str:\n        \"\"\"Generate a new span ID.\"\"\"\n        return f\"span_{uuid.uuid4().hex[:24]}\"\n\n    def gen_group_id(self) -&gt; str:\n        \"\"\"Generate a new group ID.\"\"\"\n        return f\"group_{uuid.uuid4().hex[:24]}\"\n\n    def create_trace(\n        self,\n        name: str,\n        trace_id: str | None = None,\n        group_id: str | None = None,\n        metadata: dict[str, Any] | None = None,\n        disabled: bool = False,\n        tracing: TracingConfig | None = None,\n    ) -&gt; Trace:\n        \"\"\"\n        Create a new trace.\n        \"\"\"\n        self._refresh_disabled_flag()\n        if self._disabled or disabled:\n            logger.debug(f\"Tracing is disabled. Not creating trace {name}\")\n            return NoOpTrace()\n\n        trace_id = trace_id or self.gen_trace_id()\n\n        logger.debug(f\"Creating trace {name} with id {trace_id}\")\n\n        return TraceImpl(\n            name=name,\n            trace_id=trace_id,\n            group_id=group_id,\n            metadata=metadata,\n            processor=self._multi_processor,\n            tracing_api_key=tracing.get(\"api_key\") if tracing else None,\n        )\n\n    def create_span(\n        self,\n        span_data: TSpanData,\n        span_id: str | None = None,\n        parent: Trace | Span[Any] | None = None,\n        disabled: bool = False,\n    ) -&gt; Span[TSpanData]:\n        \"\"\"\n        Create a new span.\n        \"\"\"\n        self._refresh_disabled_flag()\n        tracing_api_key: str | None = None\n        if self._disabled or disabled:\n            logger.debug(f\"Tracing is disabled. Not creating span {span_data}\")\n            return NoOpSpan(span_data)\n\n        if not parent:\n            current_span = Scope.get_current_span()\n            current_trace = Scope.get_current_trace()\n            if current_trace is None:\n                logger.error(\n                    \"No active trace. Make sure to start a trace with `trace()` first \"\n                    \"Returning NoOpSpan.\"\n                )\n                return NoOpSpan(span_data)\n            elif isinstance(current_trace, NoOpTrace) or isinstance(current_span, NoOpSpan):\n                logger.debug(\n                    f\"Parent {current_span} or {current_trace} is no-op, returning NoOpSpan\"\n                )\n                return NoOpSpan(span_data)\n\n            parent_id = current_span.span_id if current_span else None\n            trace_id = current_trace.trace_id\n            tracing_api_key = current_trace.tracing_api_key\n\n        elif isinstance(parent, Trace):\n            if isinstance(parent, NoOpTrace):\n                logger.debug(f\"Parent {parent} is no-op, returning NoOpSpan\")\n                return NoOpSpan(span_data)\n            trace_id = parent.trace_id\n            parent_id = None\n            tracing_api_key = parent.tracing_api_key\n        elif isinstance(parent, Span):\n            if isinstance(parent, NoOpSpan):\n                logger.debug(f\"Parent {parent} is no-op, returning NoOpSpan\")\n                return NoOpSpan(span_data)\n            parent_id = parent.span_id\n            trace_id = parent.trace_id\n            tracing_api_key = parent.tracing_api_key\n\n        logger.debug(f\"Creating span {span_data} with id {span_id}\")\n\n        return SpanImpl(\n            trace_id=trace_id,\n            span_id=span_id or self.gen_span_id(),\n            parent_id=parent_id,\n            processor=self._multi_processor,\n            span_data=span_data,\n            tracing_api_key=tracing_api_key,\n        )\n\n    def shutdown(self) -&gt; None:\n        if self._disabled:\n            return\n\n        try:\n            _safe_debug(\"Shutting down trace provider\")\n            self._multi_processor.shutdown()\n        except Exception as e:\n            logger.error(f\"Error shutting down trace provider: {e}\")\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.register_processor","title":"register_processor","text":"<pre><code>register_processor(processor: TracingProcessor)\n</code></pre> <p>Add a processor to the list of processors. Each processor will receive all traces/spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def register_processor(self, processor: TracingProcessor):\n    \"\"\"\n    Add a processor to the list of processors. Each processor will receive all traces/spans.\n    \"\"\"\n    self._multi_processor.add_tracing_processor(processor)\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.set_processors","title":"set_processors","text":"<pre><code>set_processors(processors: list[TracingProcessor])\n</code></pre> <p>Set the list of processors. This will replace the current list of processors.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def set_processors(self, processors: list[TracingProcessor]):\n    \"\"\"\n    Set the list of processors. This will replace the current list of processors.\n    \"\"\"\n    self._multi_processor.set_processors(processors)\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.get_current_trace","title":"get_current_trace","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Returns the currently active trace, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def get_current_trace(self) -&gt; Trace | None:\n    \"\"\"\n    Returns the currently active trace, if any.\n    \"\"\"\n    return Scope.get_current_trace()\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.get_current_span","title":"get_current_span","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Returns the currently active span, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def get_current_span(self) -&gt; Span[Any] | None:\n    \"\"\"\n    Returns the currently active span, if any.\n    \"\"\"\n    return Scope.get_current_span()\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.set_disabled","title":"set_disabled","text":"<pre><code>set_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Set whether tracing is disabled.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def set_disabled(self, disabled: bool) -&gt; None:\n    \"\"\"\n    Set whether tracing is disabled.\n    \"\"\"\n    self._manual_disabled = disabled\n    self._refresh_disabled_flag()\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.time_iso","title":"time_iso","text":"<pre><code>time_iso() -&gt; str\n</code></pre> <p>Return the current time in ISO 8601 format.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def time_iso(self) -&gt; str:\n    \"\"\"Return the current time in ISO 8601 format.\"\"\"\n    return datetime.now(timezone.utc).isoformat()\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.gen_trace_id","title":"gen_trace_id","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace ID.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def gen_trace_id(self) -&gt; str:\n    \"\"\"Generate a new trace ID.\"\"\"\n    return f\"trace_{uuid.uuid4().hex}\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.gen_span_id","title":"gen_span_id","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span ID.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def gen_span_id(self) -&gt; str:\n    \"\"\"Generate a new span ID.\"\"\"\n    return f\"span_{uuid.uuid4().hex[:24]}\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.gen_group_id","title":"gen_group_id","text":"<pre><code>gen_group_id() -&gt; str\n</code></pre> <p>Generate a new group ID.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def gen_group_id(self) -&gt; str:\n    \"\"\"Generate a new group ID.\"\"\"\n    return f\"group_{uuid.uuid4().hex[:24]}\"\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.create_trace","title":"create_trace","text":"<pre><code>create_trace(\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n    tracing: TracingConfig | None = None,\n) -&gt; Trace\n</code></pre> <p>Create a new trace.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def create_trace(\n    self,\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n    tracing: TracingConfig | None = None,\n) -&gt; Trace:\n    \"\"\"\n    Create a new trace.\n    \"\"\"\n    self._refresh_disabled_flag()\n    if self._disabled or disabled:\n        logger.debug(f\"Tracing is disabled. Not creating trace {name}\")\n        return NoOpTrace()\n\n    trace_id = trace_id or self.gen_trace_id()\n\n    logger.debug(f\"Creating trace {name} with id {trace_id}\")\n\n    return TraceImpl(\n        name=name,\n        trace_id=trace_id,\n        group_id=group_id,\n        metadata=metadata,\n        processor=self._multi_processor,\n        tracing_api_key=tracing.get(\"api_key\") if tracing else None,\n    )\n</code></pre>"},{"location":"ref/tracing/provider/#agents.tracing.provider.DefaultTraceProvider.create_span","title":"create_span","text":"<pre><code>create_span(\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]\n</code></pre> <p>Create a new span.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>def create_span(\n    self,\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]:\n    \"\"\"\n    Create a new span.\n    \"\"\"\n    self._refresh_disabled_flag()\n    tracing_api_key: str | None = None\n    if self._disabled or disabled:\n        logger.debug(f\"Tracing is disabled. Not creating span {span_data}\")\n        return NoOpSpan(span_data)\n\n    if not parent:\n        current_span = Scope.get_current_span()\n        current_trace = Scope.get_current_trace()\n        if current_trace is None:\n            logger.error(\n                \"No active trace. Make sure to start a trace with `trace()` first \"\n                \"Returning NoOpSpan.\"\n            )\n            return NoOpSpan(span_data)\n        elif isinstance(current_trace, NoOpTrace) or isinstance(current_span, NoOpSpan):\n            logger.debug(\n                f\"Parent {current_span} or {current_trace} is no-op, returning NoOpSpan\"\n            )\n            return NoOpSpan(span_data)\n\n        parent_id = current_span.span_id if current_span else None\n        trace_id = current_trace.trace_id\n        tracing_api_key = current_trace.tracing_api_key\n\n    elif isinstance(parent, Trace):\n        if isinstance(parent, NoOpTrace):\n            logger.debug(f\"Parent {parent} is no-op, returning NoOpSpan\")\n            return NoOpSpan(span_data)\n        trace_id = parent.trace_id\n        parent_id = None\n        tracing_api_key = parent.tracing_api_key\n    elif isinstance(parent, Span):\n        if isinstance(parent, NoOpSpan):\n            logger.debug(f\"Parent {parent} is no-op, returning NoOpSpan\")\n            return NoOpSpan(span_data)\n        parent_id = parent.span_id\n        trace_id = parent.trace_id\n        tracing_api_key = parent.tracing_api_key\n\n    logger.debug(f\"Creating span {span_data} with id {span_id}\")\n\n    return SpanImpl(\n        trace_id=trace_id,\n        span_id=span_id or self.gen_span_id(),\n        parent_id=parent_id,\n        processor=self._multi_processor,\n        span_data=span_data,\n        tracing_api_key=tracing_api_key,\n    )\n</code></pre>"},{"location":"ref/tracing/scope/","title":"<code>Scope</code>","text":""},{"location":"ref/tracing/scope/#agents.tracing.scope.Scope","title":"Scope","text":"<p>Manages the current span and trace in the context.</p> Source code in <code>src/agents/tracing/scope.py</code> <pre><code>class Scope:\n    \"\"\"\n    Manages the current span and trace in the context.\n    \"\"\"\n\n    @classmethod\n    def get_current_span(cls) -&gt; \"Span[Any] | None\":\n        return _current_span.get()\n\n    @classmethod\n    def set_current_span(cls, span: \"Span[Any] | None\") -&gt; \"contextvars.Token[Span[Any] | None]\":\n        return _current_span.set(span)\n\n    @classmethod\n    def reset_current_span(cls, token: \"contextvars.Token[Span[Any] | None]\") -&gt; None:\n        _current_span.reset(token)\n\n    @classmethod\n    def get_current_trace(cls) -&gt; \"Trace | None\":\n        return _current_trace.get()\n\n    @classmethod\n    def set_current_trace(cls, trace: \"Trace | None\") -&gt; \"contextvars.Token[Trace | None]\":\n        logger.debug(f\"Setting current trace: {trace.trace_id if trace else None}\")\n        return _current_trace.set(trace)\n\n    @classmethod\n    def reset_current_trace(cls, token: \"contextvars.Token[Trace | None]\") -&gt; None:\n        logger.debug(\"Resetting current trace\")\n        _current_trace.reset(token)\n</code></pre>"},{"location":"ref/tracing/setup/","title":"<code>Setup</code>","text":""},{"location":"ref/tracing/setup/#agents.tracing.setup.set_trace_provider","title":"set_trace_provider","text":"<pre><code>set_trace_provider(provider: TraceProvider) -&gt; None\n</code></pre> <p>Set the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def set_trace_provider(provider: TraceProvider) -&gt; None:\n    \"\"\"Set the global trace provider used by tracing utilities.\"\"\"\n    global GLOBAL_TRACE_PROVIDER\n    GLOBAL_TRACE_PROVIDER = provider\n</code></pre>"},{"location":"ref/tracing/setup/#agents.tracing.setup.get_trace_provider","title":"get_trace_provider","text":"<pre><code>get_trace_provider() -&gt; TraceProvider\n</code></pre> <p>Get the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def get_trace_provider() -&gt; TraceProvider:\n    \"\"\"Get the global trace provider used by tracing utilities.\"\"\"\n    if GLOBAL_TRACE_PROVIDER is None:\n        raise RuntimeError(\"Trace provider not set\")\n    return GLOBAL_TRACE_PROVIDER\n</code></pre>"},{"location":"ref/tracing/span_data/","title":"<code>Span data</code>","text":""},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpanData","title":"SpanData","text":"<p>               Bases: <code>ABC</code></p> <p>Represents span data in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpanData(abc.ABC):\n    \"\"\"\n    Represents span data in the trace.\n    \"\"\"\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any]:\n        \"\"\"Export the span data as a dictionary.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def type(self) -&gt; str:\n        \"\"\"Return the type of the span.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpanData.type","title":"type  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>Return the type of the span.</p>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpanData.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any]\n</code></pre> <p>Export the span data as a dictionary.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any]:\n    \"\"\"Export the span data as a dictionary.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.AgentSpanData","title":"AgentSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an Agent Span in the trace. Includes name, handoffs, tools, and output type.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class AgentSpanData(SpanData):\n    \"\"\"\n    Represents an Agent Span in the trace.\n    Includes name, handoffs, tools, and output type.\n    \"\"\"\n\n    __slots__ = (\"name\", \"handoffs\", \"tools\", \"output_type\")\n\n    def __init__(\n        self,\n        name: str,\n        handoffs: list[str] | None = None,\n        tools: list[str] | None = None,\n        output_type: str | None = None,\n    ):\n        self.name = name\n        self.handoffs: list[str] | None = handoffs\n        self.tools: list[str] | None = tools\n        self.output_type: str | None = output_type\n\n    @property\n    def type(self) -&gt; str:\n        return \"agent\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"handoffs\": self.handoffs,\n            \"tools\": self.tools,\n            \"output_type\": self.output_type,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.FunctionSpanData","title":"FunctionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Function Span in the trace. Includes input, output and MCP data (if applicable).</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class FunctionSpanData(SpanData):\n    \"\"\"\n    Represents a Function Span in the trace.\n    Includes input, output and MCP data (if applicable).\n    \"\"\"\n\n    __slots__ = (\"name\", \"input\", \"output\", \"mcp_data\")\n\n    def __init__(\n        self,\n        name: str,\n        input: str | None,\n        output: Any | None,\n        mcp_data: dict[str, Any] | None = None,\n    ):\n        self.name = name\n        self.input = input\n        self.output = output\n        self.mcp_data = mcp_data\n\n    @property\n    def type(self) -&gt; str:\n        return \"function\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"input\": self.input,\n            \"output\": str(self.output) if self.output else None,\n            \"mcp_data\": self.mcp_data,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.GenerationSpanData","title":"GenerationSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Generation Span in the trace. Includes input, output, model, model configuration, and usage.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GenerationSpanData(SpanData):\n    \"\"\"\n    Represents a Generation Span in the trace.\n    Includes input, output, model, model configuration, and usage.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n        \"usage\",\n    )\n\n    def __init__(\n        self,\n        input: Sequence[Mapping[str, Any]] | None = None,\n        output: Sequence[Mapping[str, Any]] | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        usage: dict[str, Any] | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n        self.usage = usage\n\n    @property\n    def type(self) -&gt; str:\n        return \"generation\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"usage\": self.usage,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.ResponseSpanData","title":"ResponseSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Response Span in the trace. Includes response and input.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class ResponseSpanData(SpanData):\n    \"\"\"\n    Represents a Response Span in the trace.\n    Includes response and input.\n    \"\"\"\n\n    __slots__ = (\"response\", \"input\")\n\n    def __init__(\n        self,\n        response: Response | None = None,\n        input: str | list[ResponseInputItemParam] | None = None,\n    ) -&gt; None:\n        self.response = response\n        # This is not used by the OpenAI trace processors, but is useful for other tracing\n        # processor implementations\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"response\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"response_id\": self.response.id if self.response else None,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.HandoffSpanData","title":"HandoffSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Handoff Span in the trace. Includes source and destination agents.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class HandoffSpanData(SpanData):\n    \"\"\"\n    Represents a Handoff Span in the trace.\n    Includes source and destination agents.\n    \"\"\"\n\n    __slots__ = (\"from_agent\", \"to_agent\")\n\n    def __init__(self, from_agent: str | None, to_agent: str | None):\n        self.from_agent = from_agent\n        self.to_agent = to_agent\n\n    @property\n    def type(self) -&gt; str:\n        return \"handoff\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"from_agent\": self.from_agent,\n            \"to_agent\": self.to_agent,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.CustomSpanData","title":"CustomSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Custom Span in the trace. Includes name and data property bag.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class CustomSpanData(SpanData):\n    \"\"\"\n    Represents a Custom Span in the trace.\n    Includes name and data property bag.\n    \"\"\"\n\n    __slots__ = (\"name\", \"data\")\n\n    def __init__(self, name: str, data: dict[str, Any]):\n        self.name = name\n        self.data = data\n\n    @property\n    def type(self) -&gt; str:\n        return \"custom\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"data\": self.data,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.GuardrailSpanData","title":"GuardrailSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Guardrail Span in the trace. Includes name and triggered status.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GuardrailSpanData(SpanData):\n    \"\"\"\n    Represents a Guardrail Span in the trace.\n    Includes name and triggered status.\n    \"\"\"\n\n    __slots__ = (\"name\", \"triggered\")\n\n    def __init__(self, name: str, triggered: bool = False):\n        self.name = name\n        self.triggered = triggered\n\n    @property\n    def type(self) -&gt; str:\n        return \"guardrail\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"triggered\": self.triggered,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.TranscriptionSpanData","title":"TranscriptionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Transcription Span in the trace. Includes input, output, model, and model configuration.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class TranscriptionSpanData(SpanData):\n    \"\"\"\n    Represents a Transcription Span in the trace.\n    Includes input, output, model, and model configuration.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n    )\n\n    def __init__(\n        self,\n        input: str | None = None,\n        input_format: str | None = \"pcm\",\n        output: str | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n    ):\n        self.input = input\n        self.input_format = input_format\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n\n    @property\n    def type(self) -&gt; str:\n        return \"transcription\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": {\n                \"data\": self.input or \"\",\n                \"format\": self.input_format,\n            },\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpeechSpanData","title":"SpeechSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Span in the trace. Includes input, output, model, model configuration, and first content timestamp.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Span in the trace.\n    Includes input, output, model, model configuration, and first content timestamp.\n    \"\"\"\n\n    __slots__ = (\"input\", \"output\", \"model\", \"model_config\", \"first_content_at\")\n\n    def __init__(\n        self,\n        input: str | None = None,\n        output: str | None = None,\n        output_format: str | None = \"pcm\",\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        first_content_at: str | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.output_format = output_format\n        self.model = model\n        self.model_config = model_config\n        self.first_content_at = first_content_at\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": {\n                \"data\": self.output or \"\",\n                \"format\": self.output_format,\n            },\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"first_content_at\": self.first_content_at,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpeechGroupSpanData","title":"SpeechGroupSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Group Span in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechGroupSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Group Span in the trace.\n    \"\"\"\n\n    __slots__ = \"input\"\n\n    def __init__(\n        self,\n        input: str | None = None,\n    ):\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech_group\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.MCPListToolsSpanData","title":"MCPListToolsSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an MCP List Tools Span in the trace. Includes server and result.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class MCPListToolsSpanData(SpanData):\n    \"\"\"\n    Represents an MCP List Tools Span in the trace.\n    Includes server and result.\n    \"\"\"\n\n    __slots__ = (\n        \"server\",\n        \"result\",\n    )\n\n    def __init__(self, server: str | None = None, result: list[str] | None = None):\n        self.server = server\n        self.result = result\n\n    @property\n    def type(self) -&gt; str:\n        return \"mcp_tools\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"server\": self.server,\n            \"result\": self.result,\n        }\n</code></pre>"},{"location":"ref/tracing/spans/","title":"<code>Spans</code>","text":""},{"location":"ref/tracing/spans/#agents.tracing.spans.Span","title":"Span","text":"<p>               Bases: <code>ABC</code>, <code>Generic[TSpanData]</code></p> <p>Base class for representing traceable operations with timing and context.</p> <p>A span represents a single operation within a trace (e.g., an LLM call, tool execution, or agent run). Spans track timing, relationships between operations, and operation-specific data.</p> Example <pre><code># Creating a custom span\nwith custom_span(\"database_query\", {\n    \"operation\": \"SELECT\",\n    \"table\": \"users\"\n}) as span:\n    results = await db.query(\"SELECT * FROM users\")\n    span.set_output({\"count\": len(results)})\n\n# Handling errors in spans\nwith custom_span(\"risky_operation\") as span:\n    try:\n        result = perform_risky_operation()\n    except Exception as e:\n        span.set_error({\n            \"message\": str(e),\n            \"data\": {\"operation\": \"risky_operation\"}\n        })\n        raise\n</code></pre> <p>Notes: - Spans automatically nest under the current trace - Use context managers for reliable start/finish - Include relevant data but avoid sensitive information - Handle errors properly using set_error()</p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class Span(abc.ABC, Generic[TSpanData]):\n    \"\"\"Base class for representing traceable operations with timing and context.\n\n    A span represents a single operation within a trace (e.g., an LLM call, tool execution,\n    or agent run). Spans track timing, relationships between operations, and operation-specific\n    data.\n\n    Type Args:\n        TSpanData: The type of span-specific data this span contains.\n\n    Example:\n        ```python\n        # Creating a custom span\n        with custom_span(\"database_query\", {\n            \"operation\": \"SELECT\",\n            \"table\": \"users\"\n        }) as span:\n            results = await db.query(\"SELECT * FROM users\")\n            span.set_output({\"count\": len(results)})\n\n        # Handling errors in spans\n        with custom_span(\"risky_operation\") as span:\n            try:\n                result = perform_risky_operation()\n            except Exception as e:\n                span.set_error({\n                    \"message\": str(e),\n                    \"data\": {\"operation\": \"risky_operation\"}\n                })\n                raise\n        ```\n\n        Notes:\n        - Spans automatically nest under the current trace\n        - Use context managers for reliable start/finish\n        - Include relevant data but avoid sensitive information\n        - Handle errors properly using set_error()\n    \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        \"\"\"The ID of the trace this span belongs to.\n\n        Returns:\n            str: Unique identifier of the parent trace.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_id(self) -&gt; str:\n        \"\"\"Unique identifier for this span.\n\n        Returns:\n            str: The span's unique ID within its trace.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_data(self) -&gt; TSpanData:\n        \"\"\"Operation-specific data for this span.\n\n        Returns:\n            TSpanData: Data specific to this type of span (e.g., LLM generation data).\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"\n        Start the span.\n\n        Args:\n            mark_as_current: If true, the span will be marked as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False) -&gt; None:\n        \"\"\"\n        Finish the span.\n\n        Args:\n            reset_current: If true, the span will be reset as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Span[TSpanData]:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def parent_id(self) -&gt; str | None:\n        \"\"\"ID of the parent span, if any.\n\n        Returns:\n            str | None: The parent span's ID, or None if this is a root span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def set_error(self, error: SpanError) -&gt; None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def error(self) -&gt; SpanError | None:\n        \"\"\"Any error that occurred during span execution.\n\n        Returns:\n            SpanError | None: Error details if an error occurred, None otherwise.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def started_at(self) -&gt; str | None:\n        \"\"\"When the span started execution.\n\n        Returns:\n            str | None: ISO format timestamp of span start, None if not started.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def ended_at(self) -&gt; str | None:\n        \"\"\"When the span finished execution.\n\n        Returns:\n            str | None: ISO format timestamp of span end, None if not finished.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def tracing_api_key(self) -&gt; str | None:\n        \"\"\"The API key to use when exporting this span.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.trace_id","title":"trace_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>trace_id: str\n</code></pre> <p>The ID of the trace this span belongs to.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique identifier of the parent trace.</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.span_id","title":"span_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>span_id: str\n</code></pre> <p>Unique identifier for this span.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The span's unique ID within its trace.</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.span_data","title":"span_data  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>span_data: TSpanData\n</code></pre> <p>Operation-specific data for this span.</p> <p>Returns:</p> Name Type Description <code>TSpanData</code> <code>TSpanData</code> <p>Data specific to this type of span (e.g., LLM generation data).</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.parent_id","title":"parent_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>parent_id: str | None\n</code></pre> <p>ID of the parent span, if any.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The parent span's ID, or None if this is a root span.</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.error","title":"error  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>error: SpanError | None\n</code></pre> <p>Any error that occurred during span execution.</p> <p>Returns:</p> Type Description <code>SpanError | None</code> <p>SpanError | None: Error details if an error occurred, None otherwise.</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.started_at","title":"started_at  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>started_at: str | None\n</code></pre> <p>When the span started execution.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: ISO format timestamp of span start, None if not started.</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.ended_at","title":"ended_at  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ended_at: str | None\n</code></pre> <p>When the span finished execution.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: ISO format timestamp of span end, None if not finished.</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.tracing_api_key","title":"tracing_api_key  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tracing_api_key: str | None\n</code></pre> <p>The API key to use when exporting this span.</p>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the span.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, the span will be marked as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"\n    Start the span.\n\n    Args:\n        mark_as_current: If true, the span will be marked as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False) -&gt; None\n</code></pre> <p>Finish the span.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, the span will be reset as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False) -&gt; None:\n    \"\"\"\n    Finish the span.\n\n    Args:\n        reset_current: If true, the span will be reset as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.NoOpSpan","title":"NoOpSpan","text":"<p>               Bases: <code>Span[TSpanData]</code></p> <p>A no-op implementation of Span that doesn't record any data.</p> <p>Used when tracing is disabled but span operations still need to work.</p> <p>Parameters:</p> Name Type Description Default <code>span_data</code> <code>TSpanData</code> <p>The operation-specific data for this span.</p> required Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class NoOpSpan(Span[TSpanData]):\n    \"\"\"A no-op implementation of Span that doesn't record any data.\n\n    Used when tracing is disabled but span operations still need to work.\n\n    Args:\n        span_data: The operation-specific data for this span.\n    \"\"\"\n\n    __slots__ = (\"_span_data\", \"_prev_span_token\")\n\n    def __init__(self, span_data: TSpanData):\n        self._span_data = span_data\n        self._prev_span_token: contextvars.Token[Span[TSpanData] | None] | None = None\n\n    @property\n    def trace_id(self) -&gt; str:\n        return \"no-op\"\n\n    @property\n    def span_id(self) -&gt; str:\n        return \"no-op\"\n\n    @property\n    def span_data(self) -&gt; TSpanData:\n        return self._span_data\n\n    @property\n    def parent_id(self) -&gt; str | None:\n        return None\n\n    def start(self, mark_as_current: bool = False):\n        if mark_as_current:\n            self._prev_span_token = Scope.set_current_span(self)\n\n    def finish(self, reset_current: bool = False) -&gt; None:\n        if reset_current and self._prev_span_token is not None:\n            Scope.reset_current_span(self._prev_span_token)\n            self._prev_span_token = None\n\n    def __enter__(self) -&gt; Span[TSpanData]:\n        self.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        reset_current = True\n        if exc_type is GeneratorExit:\n            logger.debug(\"GeneratorExit, skipping span reset\")\n            reset_current = False\n\n        self.finish(reset_current=reset_current)\n\n    def set_error(self, error: SpanError) -&gt; None:\n        pass\n\n    @property\n    def error(self) -&gt; SpanError | None:\n        return None\n\n    def export(self) -&gt; dict[str, Any] | None:\n        return None\n\n    @property\n    def started_at(self) -&gt; str | None:\n        return None\n\n    @property\n    def ended_at(self) -&gt; str | None:\n        return None\n\n    @property\n    def tracing_api_key(self) -&gt; str | None:\n        return None\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.SpanImpl","title":"SpanImpl","text":"<p>               Bases: <code>Span[TSpanData]</code></p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class SpanImpl(Span[TSpanData]):\n    __slots__ = (\n        \"_trace_id\",\n        \"_span_id\",\n        \"_parent_id\",\n        \"_started_at\",\n        \"_ended_at\",\n        \"_error\",\n        \"_prev_span_token\",\n        \"_processor\",\n        \"_span_data\",\n        \"_tracing_api_key\",\n    )\n\n    def __init__(\n        self,\n        trace_id: str,\n        span_id: str | None,\n        parent_id: str | None,\n        processor: TracingProcessor,\n        span_data: TSpanData,\n        tracing_api_key: str | None,\n    ):\n        self._trace_id = trace_id\n        self._span_id = span_id or util.gen_span_id()\n        self._parent_id = parent_id\n        self._started_at: str | None = None\n        self._ended_at: str | None = None\n        self._processor = processor\n        self._error: SpanError | None = None\n        self._prev_span_token: contextvars.Token[Span[TSpanData] | None] | None = None\n        self._span_data = span_data\n        self._tracing_api_key = tracing_api_key\n\n    @property\n    def trace_id(self) -&gt; str:\n        return self._trace_id\n\n    @property\n    def span_id(self) -&gt; str:\n        return self._span_id\n\n    @property\n    def span_data(self) -&gt; TSpanData:\n        return self._span_data\n\n    @property\n    def parent_id(self) -&gt; str | None:\n        return self._parent_id\n\n    def start(self, mark_as_current: bool = False):\n        if self.started_at is not None:\n            logger.warning(\"Span already started\")\n            return\n\n        self._started_at = util.time_iso()\n        self._processor.on_span_start(self)\n        if mark_as_current:\n            self._prev_span_token = Scope.set_current_span(self)\n\n    def finish(self, reset_current: bool = False) -&gt; None:\n        if self.ended_at is not None:\n            logger.warning(\"Span already finished\")\n            return\n\n        self._ended_at = util.time_iso()\n        self._processor.on_span_end(self)\n        if reset_current and self._prev_span_token is not None:\n            Scope.reset_current_span(self._prev_span_token)\n            self._prev_span_token = None\n\n    def __enter__(self) -&gt; Span[TSpanData]:\n        self.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        reset_current = True\n        if exc_type is GeneratorExit:\n            logger.debug(\"GeneratorExit, skipping span reset\")\n            reset_current = False\n\n        self.finish(reset_current=reset_current)\n\n    def set_error(self, error: SpanError) -&gt; None:\n        self._error = error\n\n    @property\n    def error(self) -&gt; SpanError | None:\n        return self._error\n\n    @property\n    def started_at(self) -&gt; str | None:\n        return self._started_at\n\n    @property\n    def ended_at(self) -&gt; str | None:\n        return self._ended_at\n\n    @property\n    def tracing_api_key(self) -&gt; str | None:\n        return self._tracing_api_key\n\n    def export(self) -&gt; dict[str, Any] | None:\n        return {\n            \"object\": \"trace.span\",\n            \"id\": self.span_id,\n            \"trace_id\": self.trace_id,\n            \"parent_id\": self._parent_id,\n            \"started_at\": self._started_at,\n            \"ended_at\": self._ended_at,\n            \"span_data\": self.span_data.export(),\n            \"error\": self._error,\n        }\n</code></pre>"},{"location":"ref/tracing/traces/","title":"<code>Traces</code>","text":""},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace","title":"Trace","text":"<p>               Bases: <code>ABC</code></p> <p>A complete end-to-end workflow containing related spans and metadata.</p> <p>A trace represents a logical workflow or operation (e.g., \"Customer Service Query\" or \"Code Generation\") and contains all the spans (individual operations) that occur during that workflow.</p> Example <pre><code># Basic trace usage\nwith trace(\"Order Processing\") as t:\n    validation_result = await Runner.run(validator, order_data)\n    if validation_result.approved:\n        await Runner.run(processor, order_data)\n\n# Trace with metadata and grouping\nwith trace(\n    \"Customer Service\",\n    group_id=\"chat_123\",\n    metadata={\"customer\": \"user_456\"}\n) as t:\n    result = await Runner.run(support_agent, query)\n</code></pre> Notes <ul> <li>Use descriptive workflow names</li> <li>Group related traces with consistent group_ids</li> <li>Add relevant metadata for filtering/analysis</li> <li>Use context managers for reliable cleanup</li> <li>Consider privacy when adding trace data</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class Trace(abc.ABC):\n    \"\"\"A complete end-to-end workflow containing related spans and metadata.\n\n    A trace represents a logical workflow or operation (e.g., \"Customer Service Query\"\n    or \"Code Generation\") and contains all the spans (individual operations) that occur\n    during that workflow.\n\n    Example:\n        ```python\n        # Basic trace usage\n        with trace(\"Order Processing\") as t:\n            validation_result = await Runner.run(validator, order_data)\n            if validation_result.approved:\n                await Runner.run(processor, order_data)\n\n        # Trace with metadata and grouping\n        with trace(\n            \"Customer Service\",\n            group_id=\"chat_123\",\n            metadata={\"customer\": \"user_456\"}\n        ) as t:\n            result = await Runner.run(support_agent, query)\n        ```\n\n    Notes:\n        - Use descriptive workflow names\n        - Group related traces with consistent group_ids\n        - Add relevant metadata for filtering/analysis\n        - Use context managers for reliable cleanup\n        - Consider privacy when adding trace data\n    \"\"\"\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Trace:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"Start the trace and optionally mark it as the current trace.\n\n        Args:\n            mark_as_current: If true, marks this trace as the current trace\n                in the execution context.\n\n        Notes:\n            - Must be called before any spans can be added\n            - Only one trace can be current at a time\n            - Thread-safe when using mark_as_current\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False):\n        \"\"\"Finish the trace and optionally reset the current trace.\n\n        Args:\n            reset_current: If true, resets the current trace to the previous\n                trace in the execution context.\n\n        Notes:\n            - Must be called to complete the trace\n            - Finalizes all open spans\n            - Thread-safe when using reset_current\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        \"\"\"Get the unique identifier for this trace.\n\n        Returns:\n            str: The trace's unique ID in the format 'trace_&lt;32_alphanumeric&gt;'\n\n        Notes:\n            - IDs are globally unique\n            - Used to link spans to their parent trace\n            - Can be used to look up traces in the dashboard\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Get the human-readable name of this workflow trace.\n\n        Returns:\n            str: The workflow name (e.g., \"Customer Service\", \"Data Processing\")\n\n        Notes:\n            - Should be descriptive and meaningful\n            - Used for grouping and filtering in the dashboard\n            - Helps identify the purpose of the trace\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        \"\"\"Export the trace data as a serializable dictionary.\n\n        Returns:\n            dict | None: Dictionary containing trace data, or None if tracing is disabled.\n\n        Notes:\n            - Includes all spans and their data\n            - Used for sending traces to backends\n            - May include metadata and group ID\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def tracing_api_key(self) -&gt; str | None:\n        \"\"\"The API key to use when exporting this trace and its spans.\"\"\"\n        pass\n\n    def to_json(self, *, include_tracing_api_key: bool = False) -&gt; dict[str, Any] | None:\n        \"\"\"Serialize trace metadata for persistence or transport.\n\n        Args:\n            include_tracing_api_key: When True, include the tracing API key. Defaults to False\n                to avoid persisting secrets unintentionally.\n        \"\"\"\n        exported = self.export()\n        if exported is None:\n            return None\n        payload = dict(exported)\n        if include_tracing_api_key and self.tracing_api_key:\n            payload[\"tracing_api_key\"] = self.tracing_api_key\n        return payload\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.trace_id","title":"trace_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>trace_id: str\n</code></pre> <p>Get the unique identifier for this trace.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The trace's unique ID in the format 'trace_&lt;32_alphanumeric&gt;'</p> Notes <ul> <li>IDs are globally unique</li> <li>Used to link spans to their parent trace</li> <li>Can be used to look up traces in the dashboard</li> </ul>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Get the human-readable name of this workflow trace.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The workflow name (e.g., \"Customer Service\", \"Data Processing\")</p> Notes <ul> <li>Should be descriptive and meaningful</li> <li>Used for grouping and filtering in the dashboard</li> <li>Helps identify the purpose of the trace</li> </ul>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.tracing_api_key","title":"tracing_api_key  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tracing_api_key: str | None\n</code></pre> <p>The API key to use when exporting this trace and its spans.</p>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the trace and optionally mark it as the current trace.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, marks this trace as the current trace in the execution context.</p> <code>False</code> Notes <ul> <li>Must be called before any spans can be added</li> <li>Only one trace can be current at a time</li> <li>Thread-safe when using mark_as_current</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"Start the trace and optionally mark it as the current trace.\n\n    Args:\n        mark_as_current: If true, marks this trace as the current trace\n            in the execution context.\n\n    Notes:\n        - Must be called before any spans can be added\n        - Only one trace can be current at a time\n        - Thread-safe when using mark_as_current\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False)\n</code></pre> <p>Finish the trace and optionally reset the current trace.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, resets the current trace to the previous trace in the execution context.</p> <code>False</code> Notes <ul> <li>Must be called to complete the trace</li> <li>Finalizes all open spans</li> <li>Thread-safe when using reset_current</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False):\n    \"\"\"Finish the trace and optionally reset the current trace.\n\n    Args:\n        reset_current: If true, resets the current trace to the previous\n            trace in the execution context.\n\n    Notes:\n        - Must be called to complete the trace\n        - Finalizes all open spans\n        - Thread-safe when using reset_current\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any] | None\n</code></pre> <p>Export the trace data as a serializable dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>dict | None: Dictionary containing trace data, or None if tracing is disabled.</p> Notes <ul> <li>Includes all spans and their data</li> <li>Used for sending traces to backends</li> <li>May include metadata and group ID</li> </ul> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any] | None:\n    \"\"\"Export the trace data as a serializable dictionary.\n\n    Returns:\n        dict | None: Dictionary containing trace data, or None if tracing is disabled.\n\n    Notes:\n        - Includes all spans and their data\n        - Used for sending traces to backends\n        - May include metadata and group ID\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.to_json","title":"to_json","text":"<pre><code>to_json(\n    *, include_tracing_api_key: bool = False\n) -&gt; dict[str, Any] | None\n</code></pre> <p>Serialize trace metadata for persistence or transport.</p> <p>Parameters:</p> Name Type Description Default <code>include_tracing_api_key</code> <code>bool</code> <p>When True, include the tracing API key. Defaults to False to avoid persisting secrets unintentionally.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>def to_json(self, *, include_tracing_api_key: bool = False) -&gt; dict[str, Any] | None:\n    \"\"\"Serialize trace metadata for persistence or transport.\n\n    Args:\n        include_tracing_api_key: When True, include the tracing API key. Defaults to False\n            to avoid persisting secrets unintentionally.\n    \"\"\"\n    exported = self.export()\n    if exported is None:\n        return None\n    payload = dict(exported)\n    if include_tracing_api_key and self.tracing_api_key:\n        payload[\"tracing_api_key\"] = self.tracing_api_key\n    return payload\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.TraceState","title":"TraceState  <code>dataclass</code>","text":"<p>Serializable trace metadata for run state persistence.</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@dataclass\nclass TraceState:\n    \"\"\"Serializable trace metadata for run state persistence.\"\"\"\n\n    trace_id: str | None = None\n    workflow_name: str | None = None\n    group_id: str | None = None\n    metadata: dict[str, Any] | None = None\n    tracing_api_key: str | None = None\n    object_type: str | None = None\n    extra: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_trace(cls, trace: Trace | None) -&gt; TraceState | None:\n        if trace is None:\n            return None\n        payload = trace.to_json(include_tracing_api_key=True)\n        return cls.from_json(payload)\n\n    @classmethod\n    def from_json(cls, payload: Mapping[str, Any] | None) -&gt; TraceState | None:\n        if not payload:\n            return None\n        data = dict(payload)\n        object_type = data.pop(\"object\", None)\n        trace_id = data.pop(\"id\", None) or data.pop(\"trace_id\", None)\n        workflow_name = data.pop(\"workflow_name\", None)\n        group_id = data.pop(\"group_id\", None)\n        metadata_value = data.pop(\"metadata\", None)\n        metadata = metadata_value if isinstance(metadata_value, dict) else None\n        tracing_api_key = data.pop(\"tracing_api_key\", None)\n        return cls(\n            trace_id=trace_id if isinstance(trace_id, str) else None,\n            workflow_name=workflow_name if isinstance(workflow_name, str) else None,\n            group_id=group_id if isinstance(group_id, str) else None,\n            metadata=metadata,\n            tracing_api_key=tracing_api_key if isinstance(tracing_api_key, str) else None,\n            object_type=object_type if isinstance(object_type, str) else None,\n            extra=data,\n        )\n\n    def to_json(self, *, include_tracing_api_key: bool = False) -&gt; dict[str, Any] | None:\n        if (\n            self.trace_id is None\n            and self.workflow_name is None\n            and self.group_id is None\n            and self.metadata is None\n            and self.tracing_api_key is None\n            and self.object_type is None\n            and not self.extra\n        ):\n            return None\n        payload: dict[str, Any] = {}\n        if self.object_type:\n            payload[\"object\"] = self.object_type\n        if self.trace_id:\n            payload[\"id\"] = self.trace_id\n        if self.workflow_name is not None:\n            payload[\"workflow_name\"] = self.workflow_name\n        if self.group_id is not None:\n            payload[\"group_id\"] = self.group_id\n        if self.metadata is not None:\n            payload[\"metadata\"] = dict(self.metadata)\n        if include_tracing_api_key and self.tracing_api_key:\n            payload[\"tracing_api_key\"] = self.tracing_api_key\n        for key, value in self.extra.items():\n            if key not in payload:\n                payload[key] = value\n        return payload\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.NoOpTrace","title":"NoOpTrace","text":"<p>               Bases: <code>Trace</code></p> <p>A no-op implementation of Trace that doesn't record any data.</p> <p>Used when tracing is disabled but trace operations still need to work. Maintains proper context management but doesn't store or export any data.</p> Example <pre><code># When tracing is disabled, traces become NoOpTrace\nwith trace(\"Disabled Workflow\") as t:\n    # Operations still work but nothing is recorded\n    await Runner.run(agent, \"query\")\n</code></pre> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class NoOpTrace(Trace):\n    \"\"\"A no-op implementation of Trace that doesn't record any data.\n\n    Used when tracing is disabled but trace operations still need to work.\n    Maintains proper context management but doesn't store or export any data.\n\n    Example:\n        ```python\n        # When tracing is disabled, traces become NoOpTrace\n        with trace(\"Disabled Workflow\") as t:\n            # Operations still work but nothing is recorded\n            await Runner.run(agent, \"query\")\n        ```\n    \"\"\"\n\n    def __init__(self):\n        self._started = False\n        self._prev_context_token: contextvars.Token[Trace | None] | None = None\n\n    def __enter__(self) -&gt; Trace:\n        if self._started:\n            if not self._prev_context_token:\n                logger.error(\"Trace already started but no context token set\")\n            return self\n\n        self._started = True\n        self.start(mark_as_current=True)\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.finish(reset_current=True)\n\n    def start(self, mark_as_current: bool = False):\n        if mark_as_current:\n            self._prev_context_token = Scope.set_current_trace(self)\n\n    def finish(self, reset_current: bool = False):\n        if reset_current and self._prev_context_token is not None:\n            Scope.reset_current_trace(self._prev_context_token)\n            self._prev_context_token = None\n\n    @property\n    def trace_id(self) -&gt; str:\n        \"\"\"The trace's unique identifier.\n\n        Returns:\n            str: A unique ID for this trace.\n        \"\"\"\n        return \"no-op\"\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The workflow name for this trace.\n\n        Returns:\n            str: Human-readable name describing this workflow.\n        \"\"\"\n        return \"no-op\"\n\n    def export(self) -&gt; dict[str, Any] | None:\n        \"\"\"Export the trace data as a dictionary.\n\n        Returns:\n            dict | None: Trace data in exportable format, or None if no data.\n        \"\"\"\n        return None\n\n    @property\n    def tracing_api_key(self) -&gt; str | None:\n        return None\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.NoOpTrace.trace_id","title":"trace_id  <code>property</code>","text":"<pre><code>trace_id: str\n</code></pre> <p>The trace's unique identifier.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique ID for this trace.</p>"},{"location":"ref/tracing/traces/#agents.tracing.traces.NoOpTrace.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The workflow name for this trace.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Human-readable name describing this workflow.</p>"},{"location":"ref/tracing/traces/#agents.tracing.traces.NoOpTrace.export","title":"export","text":"<pre><code>export() -&gt; dict[str, Any] | None\n</code></pre> <p>Export the trace data as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>dict | None: Trace data in exportable format, or None if no data.</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>def export(self) -&gt; dict[str, Any] | None:\n    \"\"\"Export the trace data as a dictionary.\n\n    Returns:\n        dict | None: Trace data in exportable format, or None if no data.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.NoOpTrace.to_json","title":"to_json","text":"<pre><code>to_json(\n    *, include_tracing_api_key: bool = False\n) -&gt; dict[str, Any] | None\n</code></pre> <p>Serialize trace metadata for persistence or transport.</p> <p>Parameters:</p> Name Type Description Default <code>include_tracing_api_key</code> <code>bool</code> <p>When True, include the tracing API key. Defaults to False to avoid persisting secrets unintentionally.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>def to_json(self, *, include_tracing_api_key: bool = False) -&gt; dict[str, Any] | None:\n    \"\"\"Serialize trace metadata for persistence or transport.\n\n    Args:\n        include_tracing_api_key: When True, include the tracing API key. Defaults to False\n            to avoid persisting secrets unintentionally.\n    \"\"\"\n    exported = self.export()\n    if exported is None:\n        return None\n    payload = dict(exported)\n    if include_tracing_api_key and self.tracing_api_key:\n        payload[\"tracing_api_key\"] = self.tracing_api_key\n    return payload\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.TraceImpl","title":"TraceImpl","text":"<p>               Bases: <code>Trace</code></p> <p>A trace that will be recorded by the tracing library.</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class TraceImpl(Trace):\n    \"\"\"\n    A trace that will be recorded by the tracing library.\n    \"\"\"\n\n    __slots__ = (\n        \"_name\",\n        \"_trace_id\",\n        \"_tracing_api_key\",\n        \"group_id\",\n        \"metadata\",\n        \"_prev_context_token\",\n        \"_processor\",\n        \"_started\",\n    )\n\n    def __init__(\n        self,\n        name: str,\n        trace_id: str | None,\n        group_id: str | None,\n        metadata: dict[str, Any] | None,\n        processor: TracingProcessor,\n        tracing_api_key: str | None = None,\n    ):\n        self._name = name\n        self._trace_id = trace_id or util.gen_trace_id()\n        self._tracing_api_key = tracing_api_key\n        self.group_id = group_id\n        self.metadata = metadata\n        self._prev_context_token: contextvars.Token[Trace | None] | None = None\n        self._processor = processor\n        self._started = False\n\n    @property\n    def trace_id(self) -&gt; str:\n        return self._trace_id\n\n    @property\n    def name(self) -&gt; str:\n        return self._name\n\n    @property\n    def tracing_api_key(self) -&gt; str | None:\n        return self._tracing_api_key\n\n    def start(self, mark_as_current: bool = False):\n        if self._started:\n            return\n\n        self._started = True\n        self._processor.on_trace_start(self)\n\n        if mark_as_current:\n            self._prev_context_token = Scope.set_current_trace(self)\n\n    def finish(self, reset_current: bool = False):\n        if not self._started:\n            return\n\n        self._processor.on_trace_end(self)\n\n        if reset_current and self._prev_context_token is not None:\n            Scope.reset_current_trace(self._prev_context_token)\n            self._prev_context_token = None\n\n    def __enter__(self) -&gt; Trace:\n        if self._started:\n            if not self._prev_context_token:\n                logger.error(\"Trace already started but no context token set\")\n            return self\n\n        self.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.finish(reset_current=exc_type is not GeneratorExit)\n\n    def export(self) -&gt; dict[str, Any] | None:\n        return {\n            \"object\": \"trace\",\n            \"id\": self.trace_id,\n            \"workflow_name\": self.name,\n            \"group_id\": self.group_id,\n            \"metadata\": self.metadata,\n        }\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.TraceImpl.to_json","title":"to_json","text":"<pre><code>to_json(\n    *, include_tracing_api_key: bool = False\n) -&gt; dict[str, Any] | None\n</code></pre> <p>Serialize trace metadata for persistence or transport.</p> <p>Parameters:</p> Name Type Description Default <code>include_tracing_api_key</code> <code>bool</code> <p>When True, include the tracing API key. Defaults to False to avoid persisting secrets unintentionally.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>def to_json(self, *, include_tracing_api_key: bool = False) -&gt; dict[str, Any] | None:\n    \"\"\"Serialize trace metadata for persistence or transport.\n\n    Args:\n        include_tracing_api_key: When True, include the tracing API key. Defaults to False\n            to avoid persisting secrets unintentionally.\n    \"\"\"\n    exported = self.export()\n    if exported is None:\n        return None\n    payload = dict(exported)\n    if include_tracing_api_key and self.tracing_api_key:\n        payload[\"tracing_api_key\"] = self.tracing_api_key\n    return payload\n</code></pre>"},{"location":"ref/tracing/util/","title":"<code>Util</code>","text":""},{"location":"ref/tracing/util/#agents.tracing.util.time_iso","title":"time_iso","text":"<pre><code>time_iso() -&gt; str\n</code></pre> <p>Return the current time in ISO 8601 format.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def time_iso() -&gt; str:\n    \"\"\"Return the current time in ISO 8601 format.\"\"\"\n    return get_trace_provider().time_iso()\n</code></pre>"},{"location":"ref/tracing/util/#agents.tracing.util.gen_trace_id","title":"gen_trace_id","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_trace_id() -&gt; str:\n    \"\"\"Generate a new trace ID.\"\"\"\n    return get_trace_provider().gen_trace_id()\n</code></pre>"},{"location":"ref/tracing/util/#agents.tracing.util.gen_span_id","title":"gen_span_id","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_span_id() -&gt; str:\n    \"\"\"Generate a new span ID.\"\"\"\n    return get_trace_provider().gen_span_id()\n</code></pre>"},{"location":"ref/tracing/util/#agents.tracing.util.gen_group_id","title":"gen_group_id","text":"<pre><code>gen_group_id() -&gt; str\n</code></pre> <p>Generate a new group ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_group_id() -&gt; str:\n    \"\"\"Generate a new group ID.\"\"\"\n    return get_trace_provider().gen_group_id()\n</code></pre>"},{"location":"ref/voice/events/","title":"<code>Events</code>","text":""},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEvent","title":"VoiceStreamEvent  <code>module-attribute</code>","text":"<pre><code>VoiceStreamEvent: TypeAlias = Union[\n    VoiceStreamEventAudio,\n    VoiceStreamEventLifecycle,\n    VoiceStreamEventError,\n]\n</code></pre> <p>An event from the <code>VoicePipeline</code>, streamed via <code>StreamedAudioResult.stream()</code>.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventAudio","title":"VoiceStreamEventAudio  <code>dataclass</code>","text":"<p>Streaming event from the VoicePipeline</p> Source code in <code>src/agents/voice/events.py</code> <pre><code>@dataclass\nclass VoiceStreamEventAudio:\n    \"\"\"Streaming event from the VoicePipeline\"\"\"\n\n    data: npt.NDArray[np.int16 | np.float32] | None\n    \"\"\"The audio data.\"\"\"\n\n    type: Literal[\"voice_stream_event_audio\"] = \"voice_stream_event_audio\"\n    \"\"\"The type of event.\"\"\"\n</code></pre>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventAudio.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: NDArray[int16 | float32] | None\n</code></pre> <p>The audio data.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventAudio.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\"voice_stream_event_audio\"] = (\n    \"voice_stream_event_audio\"\n)\n</code></pre> <p>The type of event.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventLifecycle","title":"VoiceStreamEventLifecycle  <code>dataclass</code>","text":"<p>Streaming event from the VoicePipeline</p> Source code in <code>src/agents/voice/events.py</code> <pre><code>@dataclass\nclass VoiceStreamEventLifecycle:\n    \"\"\"Streaming event from the VoicePipeline\"\"\"\n\n    event: Literal[\"turn_started\", \"turn_ended\", \"session_ended\"]\n    \"\"\"The event that occurred.\"\"\"\n\n    type: Literal[\"voice_stream_event_lifecycle\"] = \"voice_stream_event_lifecycle\"\n    \"\"\"The type of event.\"\"\"\n</code></pre>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventLifecycle.event","title":"event  <code>instance-attribute</code>","text":"<pre><code>event: Literal[\n    \"turn_started\", \"turn_ended\", \"session_ended\"\n]\n</code></pre> <p>The event that occurred.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventLifecycle.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\"voice_stream_event_lifecycle\"] = (\n    \"voice_stream_event_lifecycle\"\n)\n</code></pre> <p>The type of event.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventError","title":"VoiceStreamEventError  <code>dataclass</code>","text":"<p>Streaming event from the VoicePipeline</p> Source code in <code>src/agents/voice/events.py</code> <pre><code>@dataclass\nclass VoiceStreamEventError:\n    \"\"\"Streaming event from the VoicePipeline\"\"\"\n\n    error: Exception\n    \"\"\"The error that occurred.\"\"\"\n\n    type: Literal[\"voice_stream_event_error\"] = \"voice_stream_event_error\"\n    \"\"\"The type of event.\"\"\"\n</code></pre>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventError.error","title":"error  <code>instance-attribute</code>","text":"<pre><code>error: Exception\n</code></pre> <p>The error that occurred.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventError.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\"voice_stream_event_error\"] = (\n    \"voice_stream_event_error\"\n)\n</code></pre> <p>The type of event.</p>"},{"location":"ref/voice/exceptions/","title":"<code>Exceptions</code>","text":""},{"location":"ref/voice/exceptions/#agents.voice.exceptions.STTWebsocketConnectionError","title":"STTWebsocketConnectionError","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the STT websocket connection fails.</p> Source code in <code>src/agents/voice/exceptions.py</code> <pre><code>class STTWebsocketConnectionError(AgentsException):\n    \"\"\"Exception raised when the STT websocket connection fails.\"\"\"\n\n    def __init__(self, message: str):\n        self.message = message\n</code></pre>"},{"location":"ref/voice/imports/","title":"<code>Imports</code>","text":""},{"location":"ref/voice/input/","title":"<code>Input</code>","text":""},{"location":"ref/voice/input/#agents.voice.input.AudioInput","title":"AudioInput  <code>dataclass</code>","text":"<p>Static audio to be used as input for the VoicePipeline.</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>@dataclass\nclass AudioInput:\n    \"\"\"Static audio to be used as input for the VoicePipeline.\"\"\"\n\n    buffer: npt.NDArray[np.int16 | np.float32]\n    \"\"\"\n    A buffer containing the audio data for the agent. Must be a numpy array of int16 or float32.\n    \"\"\"\n\n    frame_rate: int = DEFAULT_SAMPLE_RATE\n    \"\"\"The sample rate of the audio data. Defaults to 24000.\"\"\"\n\n    sample_width: int = 2\n    \"\"\"The sample width of the audio data. Defaults to 2.\"\"\"\n\n    channels: int = 1\n    \"\"\"The number of channels in the audio data. Defaults to 1.\"\"\"\n\n    def to_audio_file(self) -&gt; tuple[str, io.BytesIO, str]:\n        \"\"\"Returns a tuple of (filename, bytes, content_type)\"\"\"\n        return _buffer_to_audio_file(self.buffer, self.frame_rate, self.sample_width, self.channels)\n\n    def to_base64(self) -&gt; str:\n        \"\"\"Returns the audio data as a base64 encoded string.\"\"\"\n        if self.buffer.dtype == np.float32:\n            # convert to int16\n            self.buffer = np.clip(self.buffer, -1.0, 1.0)\n            self.buffer = (self.buffer * 32767).astype(np.int16)\n        elif self.buffer.dtype != np.int16:\n            raise UserError(\"Buffer must be a numpy array of int16 or float32\")\n\n        return base64.b64encode(self.buffer.tobytes()).decode(\"utf-8\")\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.buffer","title":"buffer  <code>instance-attribute</code>","text":"<pre><code>buffer: NDArray[int16 | float32]\n</code></pre> <p>A buffer containing the audio data for the agent. Must be a numpy array of int16 or float32.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.frame_rate","title":"frame_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frame_rate: int = DEFAULT_SAMPLE_RATE\n</code></pre> <p>The sample rate of the audio data. Defaults to 24000.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.sample_width","title":"sample_width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_width: int = 2\n</code></pre> <p>The sample width of the audio data. Defaults to 2.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.channels","title":"channels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>channels: int = 1\n</code></pre> <p>The number of channels in the audio data. Defaults to 1.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.to_audio_file","title":"to_audio_file","text":"<pre><code>to_audio_file() -&gt; tuple[str, BytesIO, str]\n</code></pre> <p>Returns a tuple of (filename, bytes, content_type)</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>def to_audio_file(self) -&gt; tuple[str, io.BytesIO, str]:\n    \"\"\"Returns a tuple of (filename, bytes, content_type)\"\"\"\n    return _buffer_to_audio_file(self.buffer, self.frame_rate, self.sample_width, self.channels)\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.to_base64","title":"to_base64","text":"<pre><code>to_base64() -&gt; str\n</code></pre> <p>Returns the audio data as a base64 encoded string.</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>def to_base64(self) -&gt; str:\n    \"\"\"Returns the audio data as a base64 encoded string.\"\"\"\n    if self.buffer.dtype == np.float32:\n        # convert to int16\n        self.buffer = np.clip(self.buffer, -1.0, 1.0)\n        self.buffer = (self.buffer * 32767).astype(np.int16)\n    elif self.buffer.dtype != np.int16:\n        raise UserError(\"Buffer must be a numpy array of int16 or float32\")\n\n    return base64.b64encode(self.buffer.tobytes()).decode(\"utf-8\")\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.StreamedAudioInput","title":"StreamedAudioInput","text":"<p>Audio input represented as a stream of audio data. You can pass this to the <code>VoicePipeline</code> and then push audio data into the queue using the <code>add_audio</code> method.</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>class StreamedAudioInput:\n    \"\"\"Audio input represented as a stream of audio data. You can pass this to the `VoicePipeline`\n    and then push audio data into the queue using the `add_audio` method.\n    \"\"\"\n\n    def __init__(self):\n        self.queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32] | None] = asyncio.Queue()\n\n    async def add_audio(self, audio: npt.NDArray[np.int16 | np.float32] | None):\n        \"\"\"Adds more audio data to the stream.\n\n        Args:\n            audio: The audio data to add. Must be a numpy array of int16 or float32 or None.\n              If None passed, it indicates the end of the stream.\n        \"\"\"\n        await self.queue.put(audio)\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.StreamedAudioInput.add_audio","title":"add_audio  <code>async</code>","text":"<pre><code>add_audio(audio: NDArray[int16 | float32] | None)\n</code></pre> <p>Adds more audio data to the stream.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>NDArray[int16 | float32] | None</code> <p>The audio data to add. Must be a numpy array of int16 or float32 or None. If None passed, it indicates the end of the stream.</p> required Source code in <code>src/agents/voice/input.py</code> <pre><code>async def add_audio(self, audio: npt.NDArray[np.int16 | np.float32] | None):\n    \"\"\"Adds more audio data to the stream.\n\n    Args:\n        audio: The audio data to add. Must be a numpy array of int16 or float32 or None.\n          If None passed, it indicates the end of the stream.\n    \"\"\"\n    await self.queue.put(audio)\n</code></pre>"},{"location":"ref/voice/model/","title":"<code>Model</code>","text":""},{"location":"ref/voice/model/#agents.voice.model.TTSVoice","title":"TTSVoice  <code>module-attribute</code>","text":"<pre><code>TTSVoice = Literal[\n    \"alloy\",\n    \"ash\",\n    \"coral\",\n    \"echo\",\n    \"fable\",\n    \"onyx\",\n    \"nova\",\n    \"sage\",\n    \"shimmer\",\n]\n</code></pre> <p>Exportable type for the TTSModelSettings voice enum</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings","title":"TTSModelSettings  <code>dataclass</code>","text":"<p>Settings for a TTS model.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@dataclass\nclass TTSModelSettings:\n    \"\"\"Settings for a TTS model.\"\"\"\n\n    voice: TTSVoice | None = None\n    \"\"\"\n    The voice to use for the TTS model. If not provided, the default voice for the respective model\n    will be used.\n    \"\"\"\n\n    buffer_size: int = 120\n    \"\"\"The minimal size of the chunks of audio data that are being streamed out.\"\"\"\n\n    dtype: npt.DTypeLike = np.int16\n    \"\"\"The data type for the audio data to be returned in.\"\"\"\n\n    transform_data: (\n        Callable[[npt.NDArray[np.int16 | np.float32]], npt.NDArray[np.int16 | np.float32]] | None\n    ) = None\n    \"\"\"\n    A function to transform the data from the TTS model. This is useful if you want the resulting\n    audio stream to have the data in a specific shape already.\n    \"\"\"\n\n    instructions: str = (\n        \"You will receive partial sentences. Do not complete the sentence just read out the text.\"\n    )\n    \"\"\"\n    The instructions to use for the TTS model. This is useful if you want to control the tone of the\n    audio output.\n    \"\"\"\n\n    text_splitter: Callable[[str], tuple[str, str]] = get_sentence_based_splitter()\n    \"\"\"\n    A function to split the text into chunks. This is useful if you want to split the text into\n    chunks before sending it to the TTS model rather than waiting for the whole text to be\n    processed.\n    \"\"\"\n\n    speed: float | None = None\n    \"\"\"The speed with which the TTS model will read the text. Between 0.25 and 4.0.\"\"\"\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.voice","title":"voice  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>voice: TTSVoice | None = None\n</code></pre> <p>The voice to use for the TTS model. If not provided, the default voice for the respective model will be used.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.buffer_size","title":"buffer_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer_size: int = 120\n</code></pre> <p>The minimal size of the chunks of audio data that are being streamed out.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.dtype","title":"dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dtype: DTypeLike = int16\n</code></pre> <p>The data type for the audio data to be returned in.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.transform_data","title":"transform_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transform_data: (\n    Callable[\n        [NDArray[int16 | float32]], NDArray[int16 | float32]\n    ]\n    | None\n) = None\n</code></pre> <p>A function to transform the data from the TTS model. This is useful if you want the resulting audio stream to have the data in a specific shape already.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.instructions","title":"instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instructions: str = \"You will receive partial sentences. Do not complete the sentence just read out the text.\"\n</code></pre> <p>The instructions to use for the TTS model. This is useful if you want to control the tone of the audio output.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.text_splitter","title":"text_splitter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_splitter: Callable[[str], tuple[str, str]] = (\n    get_sentence_based_splitter()\n)\n</code></pre> <p>A function to split the text into chunks. This is useful if you want to split the text into chunks before sending it to the TTS model rather than waiting for the whole text to be processed.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.speed","title":"speed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>speed: float | None = None\n</code></pre> <p>The speed with which the TTS model will read the text. Between 0.25 and 4.0.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModel","title":"TTSModel","text":"<p>               Bases: <code>ABC</code></p> <p>A text-to-speech model that can convert text into audio output.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class TTSModel(abc.ABC):\n    \"\"\"A text-to-speech model that can convert text into audio output.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def model_name(self) -&gt; str:\n        \"\"\"The name of the TTS model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n        \"\"\"Given a text string, produces a stream of audio bytes, in PCM format.\n\n        Args:\n            text: The text to convert to audio.\n\n        Returns:\n            An async iterator of audio bytes, in PCM format.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.TTSModel.model_name","title":"model_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>The name of the TTS model.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModel.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(\n    text: str, settings: TTSModelSettings\n) -&gt; AsyncIterator[bytes]\n</code></pre> <p>Given a text string, produces a stream of audio bytes, in PCM format.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to convert to audio.</p> required <p>Returns:</p> Type Description <code>AsyncIterator[bytes]</code> <p>An async iterator of audio bytes, in PCM format.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n    \"\"\"Given a text string, produces a stream of audio bytes, in PCM format.\n\n    Args:\n        text: The text to convert to audio.\n\n    Returns:\n        An async iterator of audio bytes, in PCM format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.StreamedTranscriptionSession","title":"StreamedTranscriptionSession","text":"<p>               Bases: <code>ABC</code></p> <p>A streamed transcription of audio input.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class StreamedTranscriptionSession(abc.ABC):\n    \"\"\"A streamed transcription of audio input.\"\"\"\n\n    @abc.abstractmethod\n    def transcribe_turns(self) -&gt; AsyncIterator[str]:\n        \"\"\"Yields a stream of text transcriptions. Each transcription is a turn in the conversation.\n\n        This method is expected to return only after `close()` is called.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def close(self) -&gt; None:\n        \"\"\"Closes the session.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.StreamedTranscriptionSession.transcribe_turns","title":"transcribe_turns  <code>abstractmethod</code>","text":"<pre><code>transcribe_turns() -&gt; AsyncIterator[str]\n</code></pre> <p>Yields a stream of text transcriptions. Each transcription is a turn in the conversation.</p> <p>This method is expected to return only after <code>close()</code> is called.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef transcribe_turns(self) -&gt; AsyncIterator[str]:\n    \"\"\"Yields a stream of text transcriptions. Each transcription is a turn in the conversation.\n\n    This method is expected to return only after `close()` is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.StreamedTranscriptionSession.close","title":"close  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the session.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Closes the session.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings","title":"STTModelSettings  <code>dataclass</code>","text":"<p>Settings for a speech-to-text model.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@dataclass\nclass STTModelSettings:\n    \"\"\"Settings for a speech-to-text model.\"\"\"\n\n    prompt: str | None = None\n    \"\"\"Instructions for the model to follow.\"\"\"\n\n    language: str | None = None\n    \"\"\"The language of the audio input.\"\"\"\n\n    temperature: float | None = None\n    \"\"\"The temperature of the model.\"\"\"\n\n    turn_detection: dict[str, Any] | None = None\n    \"\"\"The turn detection settings for the model when using streamed audio input.\"\"\"\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.prompt","title":"prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt: str | None = None\n</code></pre> <p>Instructions for the model to follow.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.language","title":"language  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>language: str | None = None\n</code></pre> <p>The language of the audio input.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float | None = None\n</code></pre> <p>The temperature of the model.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.turn_detection","title":"turn_detection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>turn_detection: dict[str, Any] | None = None\n</code></pre> <p>The turn detection settings for the model when using streamed audio input.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModel","title":"STTModel","text":"<p>               Bases: <code>ABC</code></p> <p>A speech-to-text model that can convert audio input into text.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class STTModel(abc.ABC):\n    \"\"\"A speech-to-text model that can convert audio input into text.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def model_name(self) -&gt; str:\n        \"\"\"The name of the STT model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def transcribe(\n        self,\n        input: AudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; str:\n        \"\"\"Given an audio input, produces a text transcription.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n            trace_include_sensitive_data: Whether to include sensitive data in traces.\n            trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n        Returns:\n            The text transcription of the audio input.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def create_session(\n        self,\n        input: StreamedAudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; StreamedTranscriptionSession:\n        \"\"\"Creates a new transcription session, which you can push audio to, and receive a stream\n        of text transcriptions.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n            trace_include_sensitive_data: Whether to include sensitive data in traces.\n            trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n        Returns:\n            A new transcription session.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModel.model_name","title":"model_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>The name of the STT model.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModel.transcribe","title":"transcribe  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>transcribe(\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str\n</code></pre> <p>Given an audio input, produces a text transcription.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>AudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <code>trace_include_sensitive_data</code> <code>bool</code> <p>Whether to include sensitive data in traces.</p> required <code>trace_include_sensitive_audio_data</code> <code>bool</code> <p>Whether to include sensitive audio data in traces.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text transcription of the audio input.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\nasync def transcribe(\n    self,\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str:\n    \"\"\"Given an audio input, produces a text transcription.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n        trace_include_sensitive_data: Whether to include sensitive data in traces.\n        trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n    Returns:\n        The text transcription of the audio input.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModel.create_session","title":"create_session  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>create_session(\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession\n</code></pre> <p>Creates a new transcription session, which you can push audio to, and receive a stream of text transcriptions.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StreamedAudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <code>trace_include_sensitive_data</code> <code>bool</code> <p>Whether to include sensitive data in traces.</p> required <code>trace_include_sensitive_audio_data</code> <code>bool</code> <p>Whether to include sensitive audio data in traces.</p> required <p>Returns:</p> Type Description <code>StreamedTranscriptionSession</code> <p>A new transcription session.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\nasync def create_session(\n    self,\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession:\n    \"\"\"Creates a new transcription session, which you can push audio to, and receive a stream\n    of text transcriptions.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n        trace_include_sensitive_data: Whether to include sensitive data in traces.\n        trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n    Returns:\n        A new transcription session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.VoiceModelProvider","title":"VoiceModelProvider","text":"<p>               Bases: <code>ABC</code></p> <p>The base interface for a voice model provider.</p> <p>A model provider is responsible for creating speech-to-text and text-to-speech models, given a name.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class VoiceModelProvider(abc.ABC):\n    \"\"\"The base interface for a voice model provider.\n\n    A model provider is responsible for creating speech-to-text and text-to-speech models, given a\n    name.\n    \"\"\"\n\n    @abc.abstractmethod\n    def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n        \"\"\"Get a speech-to-text model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The speech-to-text model.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n        \"\"\"Get a text-to-speech model by name.\"\"\"\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.VoiceModelProvider.get_stt_model","title":"get_stt_model  <code>abstractmethod</code>","text":"<pre><code>get_stt_model(model_name: str | None) -&gt; STTModel\n</code></pre> <p>Get a speech-to-text model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>STTModel</code> <p>The speech-to-text model.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef get_stt_model(self, model_name: str | None) -&gt; STTModel:\n    \"\"\"Get a speech-to-text model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The speech-to-text model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.VoiceModelProvider.get_tts_model","title":"get_tts_model  <code>abstractmethod</code>","text":"<pre><code>get_tts_model(model_name: str | None) -&gt; TTSModel\n</code></pre> <p>Get a text-to-speech model by name.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n    \"\"\"Get a text-to-speech model by name.\"\"\"\n</code></pre>"},{"location":"ref/voice/pipeline/","title":"<code>Pipeline</code>","text":""},{"location":"ref/voice/pipeline/#agents.voice.pipeline.VoicePipeline","title":"VoicePipeline","text":"<p>An opinionated voice agent pipeline. It works in three steps: 1. Transcribe audio input into text. 2. Run the provided <code>workflow</code>, which produces a sequence of text responses. 3. Convert the text responses into streaming audio output.</p> Source code in <code>src/agents/voice/pipeline.py</code> <pre><code>class VoicePipeline:\n    \"\"\"An opinionated voice agent pipeline. It works in three steps:\n    1. Transcribe audio input into text.\n    2. Run the provided `workflow`, which produces a sequence of text responses.\n    3. Convert the text responses into streaming audio output.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        workflow: VoiceWorkflowBase,\n        stt_model: STTModel | str | None = None,\n        tts_model: TTSModel | str | None = None,\n        config: VoicePipelineConfig | None = None,\n    ):\n        \"\"\"Create a new voice pipeline.\n\n        Args:\n            workflow: The workflow to run. See `VoiceWorkflowBase`.\n            stt_model: The speech-to-text model to use. If not provided, a default OpenAI\n                model will be used.\n            tts_model: The text-to-speech model to use. If not provided, a default OpenAI\n                model will be used.\n            config: The pipeline configuration. If not provided, a default configuration will be\n                used.\n        \"\"\"\n        self.workflow = workflow\n        self.stt_model = stt_model if isinstance(stt_model, STTModel) else None\n        self.tts_model = tts_model if isinstance(tts_model, TTSModel) else None\n        self._stt_model_name = stt_model if isinstance(stt_model, str) else None\n        self._tts_model_name = tts_model if isinstance(tts_model, str) else None\n        self.config = config or VoicePipelineConfig()\n\n    async def run(self, audio_input: AudioInput | StreamedAudioInput) -&gt; StreamedAudioResult:\n        \"\"\"Run the voice pipeline.\n\n        Args:\n            audio_input: The audio input to process. This can either be an `AudioInput` instance,\n                which is a single static buffer, or a `StreamedAudioInput` instance, which is a\n                stream of audio data that you can append to.\n\n        Returns:\n            A `StreamedAudioResult` instance. You can use this object to stream audio events and\n            play them out.\n        \"\"\"\n        if isinstance(audio_input, AudioInput):\n            return await self._run_single_turn(audio_input)\n        elif isinstance(audio_input, StreamedAudioInput):\n            return await self._run_multi_turn(audio_input)\n        else:\n            raise UserError(f\"Unsupported audio input type: {type(audio_input)}\")\n\n    def _get_tts_model(self) -&gt; TTSModel:\n        if not self.tts_model:\n            self.tts_model = self.config.model_provider.get_tts_model(self._tts_model_name)\n        return self.tts_model\n\n    def _get_stt_model(self) -&gt; STTModel:\n        if not self.stt_model:\n            self.stt_model = self.config.model_provider.get_stt_model(self._stt_model_name)\n        return self.stt_model\n\n    async def _process_audio_input(self, audio_input: AudioInput) -&gt; str:\n        model = self._get_stt_model()\n        return await model.transcribe(\n            audio_input,\n            self.config.stt_settings,\n            self.config.trace_include_sensitive_data,\n            self.config.trace_include_sensitive_audio_data,\n        )\n\n    async def _run_single_turn(self, audio_input: AudioInput) -&gt; StreamedAudioResult:\n        # Since this is single turn, we can use the TraceCtxManager to manage starting/ending the\n        # trace\n        with TraceCtxManager(\n            workflow_name=self.config.workflow_name or \"Voice Agent\",\n            trace_id=None,  # Automatically generated\n            group_id=self.config.group_id,\n            metadata=self.config.trace_metadata,\n            tracing=self.config.tracing,\n            disabled=self.config.tracing_disabled,\n        ):\n            input_text = await self._process_audio_input(audio_input)\n\n            output = StreamedAudioResult(\n                self._get_tts_model(), self.config.tts_settings, self.config\n            )\n\n            async def stream_events():\n                try:\n                    async for text_event in self.workflow.run(input_text):\n                        await output._add_text(text_event)\n                    await output._turn_done()\n                    await output._done()\n                except Exception as e:\n                    logger.error(f\"Error processing single turn: {e}\")\n                    await output._add_error(e)\n                    raise e\n\n            output._set_task(asyncio.create_task(stream_events()))\n            return output\n\n    async def _run_multi_turn(self, audio_input: StreamedAudioInput) -&gt; StreamedAudioResult:\n        with TraceCtxManager(\n            workflow_name=self.config.workflow_name or \"Voice Agent\",\n            trace_id=None,\n            group_id=self.config.group_id,\n            metadata=self.config.trace_metadata,\n            tracing=self.config.tracing,\n            disabled=self.config.tracing_disabled,\n        ):\n            output = StreamedAudioResult(\n                self._get_tts_model(), self.config.tts_settings, self.config\n            )\n\n            try:\n                async for intro_text in self.workflow.on_start():\n                    await output._add_text(intro_text)\n            except Exception as e:\n                logger.warning(f\"on_start() failed: {e}\")\n\n            transcription_session = await self._get_stt_model().create_session(\n                audio_input,\n                self.config.stt_settings,\n                self.config.trace_include_sensitive_data,\n                self.config.trace_include_sensitive_audio_data,\n            )\n\n            async def process_turns():\n                try:\n                    async for input_text in transcription_session.transcribe_turns():\n                        result = self.workflow.run(input_text)\n                        async for text_event in result:\n                            await output._add_text(text_event)\n                        await output._turn_done()\n                except Exception as e:\n                    logger.error(f\"Error processing turns: {e}\")\n                    await output._add_error(e)\n                    raise e\n                finally:\n                    await transcription_session.close()\n                    await output._done()\n\n            output._set_task(asyncio.create_task(process_turns()))\n            return output\n</code></pre>"},{"location":"ref/voice/pipeline/#agents.voice.pipeline.VoicePipeline.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    workflow: VoiceWorkflowBase,\n    stt_model: STTModel | str | None = None,\n    tts_model: TTSModel | str | None = None,\n    config: VoicePipelineConfig | None = None,\n)\n</code></pre> <p>Create a new voice pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>VoiceWorkflowBase</code> <p>The workflow to run. See <code>VoiceWorkflowBase</code>.</p> required <code>stt_model</code> <code>STTModel | str | None</code> <p>The speech-to-text model to use. If not provided, a default OpenAI model will be used.</p> <code>None</code> <code>tts_model</code> <code>TTSModel | str | None</code> <p>The text-to-speech model to use. If not provided, a default OpenAI model will be used.</p> <code>None</code> <code>config</code> <code>VoicePipelineConfig | None</code> <p>The pipeline configuration. If not provided, a default configuration will be used.</p> <code>None</code> Source code in <code>src/agents/voice/pipeline.py</code> <pre><code>def __init__(\n    self,\n    *,\n    workflow: VoiceWorkflowBase,\n    stt_model: STTModel | str | None = None,\n    tts_model: TTSModel | str | None = None,\n    config: VoicePipelineConfig | None = None,\n):\n    \"\"\"Create a new voice pipeline.\n\n    Args:\n        workflow: The workflow to run. See `VoiceWorkflowBase`.\n        stt_model: The speech-to-text model to use. If not provided, a default OpenAI\n            model will be used.\n        tts_model: The text-to-speech model to use. If not provided, a default OpenAI\n            model will be used.\n        config: The pipeline configuration. If not provided, a default configuration will be\n            used.\n    \"\"\"\n    self.workflow = workflow\n    self.stt_model = stt_model if isinstance(stt_model, STTModel) else None\n    self.tts_model = tts_model if isinstance(tts_model, TTSModel) else None\n    self._stt_model_name = stt_model if isinstance(stt_model, str) else None\n    self._tts_model_name = tts_model if isinstance(tts_model, str) else None\n    self.config = config or VoicePipelineConfig()\n</code></pre>"},{"location":"ref/voice/pipeline/#agents.voice.pipeline.VoicePipeline.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    audio_input: AudioInput | StreamedAudioInput,\n) -&gt; StreamedAudioResult\n</code></pre> <p>Run the voice pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>audio_input</code> <code>AudioInput | StreamedAudioInput</code> <p>The audio input to process. This can either be an <code>AudioInput</code> instance, which is a single static buffer, or a <code>StreamedAudioInput</code> instance, which is a stream of audio data that you can append to.</p> required <p>Returns:</p> Type Description <code>StreamedAudioResult</code> <p>A <code>StreamedAudioResult</code> instance. You can use this object to stream audio events and</p> <code>StreamedAudioResult</code> <p>play them out.</p> Source code in <code>src/agents/voice/pipeline.py</code> <pre><code>async def run(self, audio_input: AudioInput | StreamedAudioInput) -&gt; StreamedAudioResult:\n    \"\"\"Run the voice pipeline.\n\n    Args:\n        audio_input: The audio input to process. This can either be an `AudioInput` instance,\n            which is a single static buffer, or a `StreamedAudioInput` instance, which is a\n            stream of audio data that you can append to.\n\n    Returns:\n        A `StreamedAudioResult` instance. You can use this object to stream audio events and\n        play them out.\n    \"\"\"\n    if isinstance(audio_input, AudioInput):\n        return await self._run_single_turn(audio_input)\n    elif isinstance(audio_input, StreamedAudioInput):\n        return await self._run_multi_turn(audio_input)\n    else:\n        raise UserError(f\"Unsupported audio input type: {type(audio_input)}\")\n</code></pre>"},{"location":"ref/voice/pipeline_config/","title":"<code>Pipeline Config</code>","text":""},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig","title":"VoicePipelineConfig  <code>dataclass</code>","text":"<p>Configuration for a <code>VoicePipeline</code>.</p> Source code in <code>src/agents/voice/pipeline_config.py</code> <pre><code>@dataclass\nclass VoicePipelineConfig:\n    \"\"\"Configuration for a `VoicePipeline`.\"\"\"\n\n    model_provider: VoiceModelProvider = field(default_factory=OpenAIVoiceModelProvider)\n    \"\"\"The voice model provider to use for the pipeline. Defaults to OpenAI.\"\"\"\n\n    tracing_disabled: bool = False\n    \"\"\"Whether to disable tracing of the pipeline. Defaults to `False`.\"\"\"\n\n    tracing: TracingConfig | None = None\n    \"\"\"Tracing configuration for this pipeline.\"\"\"\n\n    trace_include_sensitive_data: bool = True\n    \"\"\"Whether to include sensitive data in traces. Defaults to `True`. This is specifically for the\n      voice pipeline, and not for anything that goes on inside your Workflow.\"\"\"\n\n    trace_include_sensitive_audio_data: bool = True\n    \"\"\"Whether to include audio data in traces. Defaults to `True`.\"\"\"\n\n    workflow_name: str = \"Voice Agent\"\n    \"\"\"The name of the workflow to use for tracing. Defaults to `Voice Agent`.\"\"\"\n\n    group_id: str = field(default_factory=gen_group_id)\n    \"\"\"\n    A grouping identifier to use for tracing, to link multiple traces from the same conversation\n    or process. If not provided, we will create a random group ID.\n    \"\"\"\n\n    trace_metadata: dict[str, Any] | None = None\n    \"\"\"\n    An optional dictionary of additional metadata to include with the trace.\n    \"\"\"\n\n    stt_settings: STTModelSettings = field(default_factory=STTModelSettings)\n    \"\"\"The settings to use for the STT model.\"\"\"\n\n    tts_settings: TTSModelSettings = field(default_factory=TTSModelSettings)\n    \"\"\"The settings to use for the TTS model.\"\"\"\n</code></pre>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.model_provider","title":"model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_provider: VoiceModelProvider = field(\n    default_factory=OpenAIVoiceModelProvider\n)\n</code></pre> <p>The voice model provider to use for the pipeline. Defaults to OpenAI.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled","title":"tracing_disabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing_disabled: bool = False\n</code></pre> <p>Whether to disable tracing of the pipeline. Defaults to <code>False</code>.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.tracing","title":"tracing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing: TracingConfig | None = None\n</code></pre> <p>Tracing configuration for this pipeline.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data","title":"trace_include_sensitive_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_include_sensitive_data: bool = True\n</code></pre> <p>Whether to include sensitive data in traces. Defaults to <code>True</code>. This is specifically for the voice pipeline, and not for anything that goes on inside your Workflow.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data","title":"trace_include_sensitive_audio_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_include_sensitive_audio_data: bool = True\n</code></pre> <p>Whether to include audio data in traces. Defaults to <code>True</code>.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.workflow_name","title":"workflow_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workflow_name: str = 'Voice Agent'\n</code></pre> <p>The name of the workflow to use for tracing. Defaults to <code>Voice Agent</code>.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.group_id","title":"group_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>group_id: str = field(default_factory=gen_group_id)\n</code></pre> <p>A grouping identifier to use for tracing, to link multiple traces from the same conversation or process. If not provided, we will create a random group ID.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.trace_metadata","title":"trace_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_metadata: dict[str, Any] | None = None\n</code></pre> <p>An optional dictionary of additional metadata to include with the trace.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.stt_settings","title":"stt_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stt_settings: STTModelSettings = field(\n    default_factory=STTModelSettings\n)\n</code></pre> <p>The settings to use for the STT model.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.tts_settings","title":"tts_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tts_settings: TTSModelSettings = field(\n    default_factory=TTSModelSettings\n)\n</code></pre> <p>The settings to use for the TTS model.</p>"},{"location":"ref/voice/result/","title":"<code>Result</code>","text":""},{"location":"ref/voice/result/#agents.voice.result.StreamedAudioResult","title":"StreamedAudioResult","text":"<p>The output of a <code>VoicePipeline</code>. Streams events and audio data as they're generated.</p> Source code in <code>src/agents/voice/result.py</code> <pre><code>class StreamedAudioResult:\n    \"\"\"The output of a `VoicePipeline`. Streams events and audio data as they're generated.\"\"\"\n\n    def __init__(\n        self,\n        tts_model: TTSModel,\n        tts_settings: TTSModelSettings,\n        voice_pipeline_config: VoicePipelineConfig,\n    ):\n        \"\"\"Create a new `StreamedAudioResult` instance.\n\n        Args:\n            tts_model: The TTS model to use.\n            tts_settings: The TTS settings to use.\n            voice_pipeline_config: The voice pipeline config to use.\n        \"\"\"\n        self.tts_model = tts_model\n        self.tts_settings = tts_settings\n        self.total_output_text = \"\"\n        self.instructions = tts_settings.instructions\n        self.text_generation_task: asyncio.Task[Any] | None = None\n\n        self._voice_pipeline_config = voice_pipeline_config\n        self._text_buffer = \"\"\n        self._turn_text_buffer = \"\"\n        self._queue: asyncio.Queue[VoiceStreamEvent] = asyncio.Queue()\n        self._tasks: list[asyncio.Task[Any]] = []\n        self._ordered_tasks: list[\n            asyncio.Queue[VoiceStreamEvent | None]\n        ] = []  # New: list to hold local queues for each text segment\n        self._dispatcher_task: asyncio.Task[Any] | None = (\n            None  # Task to dispatch audio chunks in order\n        )\n\n        self._done_processing = False\n        self._buffer_size = tts_settings.buffer_size\n        self._started_processing_turn = False\n        self._first_byte_received = False\n        self._generation_start_time: str | None = None\n        self._completed_session = False\n        self._stored_exception: BaseException | None = None\n        self._tracing_span: Span[SpeechGroupSpanData] | None = None\n\n    async def _start_turn(self):\n        if self._started_processing_turn:\n            return\n\n        self._tracing_span = speech_group_span()\n        self._tracing_span.start()\n        self._started_processing_turn = True\n        self._first_byte_received = False\n        self._generation_start_time = time_iso()\n        await self._queue.put(VoiceStreamEventLifecycle(event=\"turn_started\"))\n\n    def _set_task(self, task: asyncio.Task[Any]):\n        self.text_generation_task = task\n\n    async def _add_error(self, error: Exception):\n        await self._queue.put(VoiceStreamEventError(error))\n\n    def _transform_audio_buffer(\n        self, buffer: list[bytes], output_dtype: npt.DTypeLike\n    ) -&gt; npt.NDArray[np.int16 | np.float32]:\n        np_array = np.frombuffer(b\"\".join(buffer), dtype=np.int16)\n\n        if output_dtype == np.int16:\n            return np_array\n        elif output_dtype == np.float32:\n            return (np_array.astype(np.float32) / 32767.0).reshape(-1, 1)\n        else:\n            raise UserError(\"Invalid output dtype\")\n\n    async def _stream_audio(\n        self,\n        text: str,\n        local_queue: asyncio.Queue[VoiceStreamEvent | None],\n        finish_turn: bool = False,\n    ):\n        with speech_span(\n            model=self.tts_model.model_name,\n            input=text if self._voice_pipeline_config.trace_include_sensitive_data else \"\",\n            model_config={\n                \"voice\": self.tts_settings.voice,\n                \"instructions\": self.instructions,\n                \"speed\": self.tts_settings.speed,\n            },\n            output_format=\"pcm\",\n            parent=self._tracing_span,\n        ) as tts_span:\n            try:\n                first_byte_received = False\n                buffer: list[bytes] = []\n                full_audio_data: list[bytes] = []\n\n                async for chunk in self.tts_model.run(text, self.tts_settings):\n                    if not first_byte_received:\n                        first_byte_received = True\n                        tts_span.span_data.first_content_at = time_iso()\n\n                    if chunk:\n                        buffer.append(chunk)\n                        full_audio_data.append(chunk)\n                        if len(buffer) &gt;= self._buffer_size:\n                            audio_np = self._transform_audio_buffer(buffer, self.tts_settings.dtype)\n                            if self.tts_settings.transform_data:\n                                audio_np = self.tts_settings.transform_data(audio_np)\n                            await local_queue.put(\n                                VoiceStreamEventAudio(data=audio_np)\n                            )  # Use local queue\n                            buffer = []\n                if buffer:\n                    audio_np = self._transform_audio_buffer(buffer, self.tts_settings.dtype)\n                    if self.tts_settings.transform_data:\n                        audio_np = self.tts_settings.transform_data(audio_np)\n                    await local_queue.put(VoiceStreamEventAudio(data=audio_np))  # Use local queue\n\n                if self._voice_pipeline_config.trace_include_sensitive_audio_data:\n                    tts_span.span_data.output = _audio_to_base64(full_audio_data)\n                else:\n                    tts_span.span_data.output = \"\"\n\n                if finish_turn:\n                    await local_queue.put(VoiceStreamEventLifecycle(event=\"turn_ended\"))\n                else:\n                    await local_queue.put(None)  # Signal completion for this segment\n            except Exception as e:\n                tts_span.set_error(\n                    {\n                        \"message\": str(e),\n                        \"data\": {\n                            \"text\": text\n                            if self._voice_pipeline_config.trace_include_sensitive_data\n                            else \"\",\n                        },\n                    }\n                )\n                logger.error(f\"Error streaming audio: {e}\")\n\n                # Signal completion for whole session because of error\n                await local_queue.put(VoiceStreamEventLifecycle(event=\"session_ended\"))\n                raise e\n\n    async def _add_text(self, text: str):\n        await self._start_turn()\n\n        self._text_buffer += text\n        self.total_output_text += text\n        self._turn_text_buffer += text\n\n        combined_sentences, self._text_buffer = self.tts_settings.text_splitter(self._text_buffer)\n\n        if len(combined_sentences) &gt;= 20:\n            local_queue: asyncio.Queue[VoiceStreamEvent | None] = asyncio.Queue()\n            self._ordered_tasks.append(local_queue)\n            self._tasks.append(\n                asyncio.create_task(self._stream_audio(combined_sentences, local_queue))\n            )\n            if self._dispatcher_task is None:\n                self._dispatcher_task = asyncio.create_task(self._dispatch_audio())\n\n    async def _turn_done(self):\n        if self._text_buffer:\n            local_queue: asyncio.Queue[VoiceStreamEvent | None] = asyncio.Queue()\n            self._ordered_tasks.append(local_queue)  # Append the local queue for the final segment\n            self._tasks.append(\n                asyncio.create_task(\n                    self._stream_audio(self._text_buffer, local_queue, finish_turn=True)\n                )\n            )\n            self._text_buffer = \"\"\n        self._done_processing = True\n        if self._dispatcher_task is None:\n            self._dispatcher_task = asyncio.create_task(self._dispatch_audio())\n        await asyncio.gather(*self._tasks)\n\n    def _finish_turn(self):\n        if self._tracing_span:\n            if self._voice_pipeline_config.trace_include_sensitive_data:\n                self._tracing_span.span_data.input = self._turn_text_buffer\n            else:\n                self._tracing_span.span_data.input = \"\"\n\n            self._tracing_span.finish()\n            self._tracing_span = None\n        self._turn_text_buffer = \"\"\n        self._started_processing_turn = False\n\n    async def _done(self):\n        self._completed_session = True\n        await self._wait_for_completion()\n\n    async def _dispatch_audio(self):\n        # Dispatch audio chunks from each segment in the order they were added\n        while True:\n            if len(self._ordered_tasks) == 0:\n                if self._completed_session:\n                    break\n                await asyncio.sleep(0)\n                continue\n            local_queue = self._ordered_tasks.pop(0)\n            while True:\n                chunk = await local_queue.get()\n                if chunk is None:\n                    break\n                await self._queue.put(chunk)\n                if isinstance(chunk, VoiceStreamEventLifecycle):\n                    local_queue.task_done()\n                    if chunk.event == \"turn_ended\":\n                        self._finish_turn()\n                        break\n        await self._queue.put(VoiceStreamEventLifecycle(event=\"session_ended\"))\n\n    async def _wait_for_completion(self):\n        tasks: list[asyncio.Task[Any]] = self._tasks\n        if self._dispatcher_task is not None:\n            tasks.append(self._dispatcher_task)\n        await asyncio.gather(*tasks)\n\n    def _cleanup_tasks(self):\n        self._finish_turn()\n\n        for task in self._tasks:\n            if not task.done():\n                task.cancel()\n\n        if self._dispatcher_task and not self._dispatcher_task.done():\n            self._dispatcher_task.cancel()\n\n        if self.text_generation_task and not self.text_generation_task.done():\n            self.text_generation_task.cancel()\n\n    def _check_errors(self):\n        for task in self._tasks:\n            if task.done():\n                if task.exception():\n                    self._stored_exception = task.exception()\n                    break\n\n    async def stream(self) -&gt; AsyncIterator[VoiceStreamEvent]:\n        \"\"\"Stream the events and audio data as they're generated.\"\"\"\n        while True:\n            try:\n                event = await self._queue.get()\n            except asyncio.CancelledError:\n                break\n            if isinstance(event, VoiceStreamEventError):\n                self._stored_exception = event.error\n                logger.error(f\"Error processing output: {event.error}\")\n                break\n            if event is None:\n                break\n            yield event\n            if event.type == \"voice_stream_event_lifecycle\" and event.event == \"session_ended\":\n                break\n\n        self._check_errors()\n        self._cleanup_tasks()\n\n        if self._stored_exception:\n            raise self._stored_exception\n</code></pre>"},{"location":"ref/voice/result/#agents.voice.result.StreamedAudioResult.__init__","title":"__init__","text":"<pre><code>__init__(\n    tts_model: TTSModel,\n    tts_settings: TTSModelSettings,\n    voice_pipeline_config: VoicePipelineConfig,\n)\n</code></pre> <p>Create a new <code>StreamedAudioResult</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>tts_model</code> <code>TTSModel</code> <p>The TTS model to use.</p> required <code>tts_settings</code> <code>TTSModelSettings</code> <p>The TTS settings to use.</p> required <code>voice_pipeline_config</code> <code>VoicePipelineConfig</code> <p>The voice pipeline config to use.</p> required Source code in <code>src/agents/voice/result.py</code> <pre><code>def __init__(\n    self,\n    tts_model: TTSModel,\n    tts_settings: TTSModelSettings,\n    voice_pipeline_config: VoicePipelineConfig,\n):\n    \"\"\"Create a new `StreamedAudioResult` instance.\n\n    Args:\n        tts_model: The TTS model to use.\n        tts_settings: The TTS settings to use.\n        voice_pipeline_config: The voice pipeline config to use.\n    \"\"\"\n    self.tts_model = tts_model\n    self.tts_settings = tts_settings\n    self.total_output_text = \"\"\n    self.instructions = tts_settings.instructions\n    self.text_generation_task: asyncio.Task[Any] | None = None\n\n    self._voice_pipeline_config = voice_pipeline_config\n    self._text_buffer = \"\"\n    self._turn_text_buffer = \"\"\n    self._queue: asyncio.Queue[VoiceStreamEvent] = asyncio.Queue()\n    self._tasks: list[asyncio.Task[Any]] = []\n    self._ordered_tasks: list[\n        asyncio.Queue[VoiceStreamEvent | None]\n    ] = []  # New: list to hold local queues for each text segment\n    self._dispatcher_task: asyncio.Task[Any] | None = (\n        None  # Task to dispatch audio chunks in order\n    )\n\n    self._done_processing = False\n    self._buffer_size = tts_settings.buffer_size\n    self._started_processing_turn = False\n    self._first_byte_received = False\n    self._generation_start_time: str | None = None\n    self._completed_session = False\n    self._stored_exception: BaseException | None = None\n    self._tracing_span: Span[SpeechGroupSpanData] | None = None\n</code></pre>"},{"location":"ref/voice/result/#agents.voice.result.StreamedAudioResult.stream","title":"stream  <code>async</code>","text":"<pre><code>stream() -&gt; AsyncIterator[VoiceStreamEvent]\n</code></pre> <p>Stream the events and audio data as they're generated.</p> Source code in <code>src/agents/voice/result.py</code> <pre><code>async def stream(self) -&gt; AsyncIterator[VoiceStreamEvent]:\n    \"\"\"Stream the events and audio data as they're generated.\"\"\"\n    while True:\n        try:\n            event = await self._queue.get()\n        except asyncio.CancelledError:\n            break\n        if isinstance(event, VoiceStreamEventError):\n            self._stored_exception = event.error\n            logger.error(f\"Error processing output: {event.error}\")\n            break\n        if event is None:\n            break\n        yield event\n        if event.type == \"voice_stream_event_lifecycle\" and event.event == \"session_ended\":\n            break\n\n    self._check_errors()\n    self._cleanup_tasks()\n\n    if self._stored_exception:\n        raise self._stored_exception\n</code></pre>"},{"location":"ref/voice/utils/","title":"<code>Utils</code>","text":""},{"location":"ref/voice/utils/#agents.voice.utils.get_sentence_based_splitter","title":"get_sentence_based_splitter","text":"<pre><code>get_sentence_based_splitter(\n    min_sentence_length: int = 20,\n) -&gt; Callable[[str], tuple[str, str]]\n</code></pre> <p>Returns a function that splits text into chunks based on sentence boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>min_sentence_length</code> <code>int</code> <p>The minimum length of a sentence to be included in a chunk.</p> <code>20</code> <p>Returns:</p> Type Description <code>Callable[[str], tuple[str, str]]</code> <p>A function that splits text into chunks based on sentence boundaries.</p> Source code in <code>src/agents/voice/utils.py</code> <pre><code>def get_sentence_based_splitter(\n    min_sentence_length: int = 20,\n) -&gt; Callable[[str], tuple[str, str]]:\n    \"\"\"Returns a function that splits text into chunks based on sentence boundaries.\n\n    Args:\n        min_sentence_length: The minimum length of a sentence to be included in a chunk.\n\n    Returns:\n        A function that splits text into chunks based on sentence boundaries.\n    \"\"\"\n\n    def sentence_based_text_splitter(text_buffer: str) -&gt; tuple[str, str]:\n        \"\"\"\n        A function to split the text into chunks. This is useful if you want to split the text into\n        chunks before sending it to the TTS model rather than waiting for the whole text to be\n        processed.\n\n        Args:\n            text_buffer: The text to split.\n\n        Returns:\n            A tuple of the text to process and the remaining text buffer.\n        \"\"\"\n        sentences = re.split(r\"(?&lt;=[.!?])\\s+\", text_buffer.strip())\n        if len(sentences) &gt;= 1:\n            combined_sentences = \" \".join(sentences[:-1])\n            if len(combined_sentences) &gt;= min_sentence_length:\n                remaining_text_buffer = sentences[-1]\n                return combined_sentences, remaining_text_buffer\n        return \"\", text_buffer\n\n    return sentence_based_text_splitter\n</code></pre>"},{"location":"ref/voice/workflow/","title":"<code>Workflow</code>","text":""},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowBase","title":"VoiceWorkflowBase","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for a voice workflow. You must implement the <code>run</code> method. A \"workflow\" is any code you want, that receives a transcription and yields text that will be turned into speech by a text-to-speech model. In most cases, you'll create <code>Agent</code>s and use <code>Runner.run_streamed()</code> to run them, returning some or all of the text events from the stream. You can use the <code>VoiceWorkflowHelper</code> class to help with extracting text events from the stream. If you have a simple workflow that has a single starting agent and no custom logic, you can use <code>SingleAgentVoiceWorkflow</code> directly.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class VoiceWorkflowBase(abc.ABC):\n    \"\"\"\n    A base class for a voice workflow. You must implement the `run` method. A \"workflow\" is any\n    code you want, that receives a transcription and yields text that will be turned into speech\n    by a text-to-speech model.\n    In most cases, you'll create `Agent`s and use `Runner.run_streamed()` to run them, returning\n    some or all of the text events from the stream. You can use the `VoiceWorkflowHelper` class to\n    help with extracting text events from the stream.\n    If you have a simple workflow that has a single starting agent and no custom logic, you can\n    use `SingleAgentVoiceWorkflow` directly.\n    \"\"\"\n\n    @abc.abstractmethod\n    def run(self, transcription: str) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Run the voice workflow. You will receive an input transcription, and must yield text that\n        will be spoken to the user. You can run whatever logic you want here. In most cases, the\n        final logic will involve calling `Runner.run_streamed()` and yielding any text events from\n        the stream.\n        \"\"\"\n        pass\n\n    async def on_start(self) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Optional method that runs before any user input is received. Can be used\n        to deliver a greeting or instruction via TTS. Defaults to doing nothing.\n        \"\"\"\n        return\n        yield\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowBase.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(transcription: str) -&gt; AsyncIterator[str]\n</code></pre> <p>Run the voice workflow. You will receive an input transcription, and must yield text that will be spoken to the user. You can run whatever logic you want here. In most cases, the final logic will involve calling <code>Runner.run_streamed()</code> and yielding any text events from the stream.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>@abc.abstractmethod\ndef run(self, transcription: str) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Run the voice workflow. You will receive an input transcription, and must yield text that\n    will be spoken to the user. You can run whatever logic you want here. In most cases, the\n    final logic will involve calling `Runner.run_streamed()` and yielding any text events from\n    the stream.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowBase.on_start","title":"on_start  <code>async</code>","text":"<pre><code>on_start() -&gt; AsyncIterator[str]\n</code></pre> <p>Optional method that runs before any user input is received. Can be used to deliver a greeting or instruction via TTS. Defaults to doing nothing.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>async def on_start(self) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Optional method that runs before any user input is received. Can be used\n    to deliver a greeting or instruction via TTS. Defaults to doing nothing.\n    \"\"\"\n    return\n    yield\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowHelper","title":"VoiceWorkflowHelper","text":"Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class VoiceWorkflowHelper:\n    @classmethod\n    async def stream_text_from(cls, result: RunResultStreaming) -&gt; AsyncIterator[str]:\n        \"\"\"Wraps a `RunResultStreaming` object and yields text events from the stream.\"\"\"\n        async for event in result.stream_events():\n            if (\n                event.type == \"raw_response_event\"\n                and event.data.type == \"response.output_text.delta\"\n            ):\n                yield event.data.delta\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowHelper.stream_text_from","title":"stream_text_from  <code>async</code> <code>classmethod</code>","text":"<pre><code>stream_text_from(\n    result: RunResultStreaming,\n) -&gt; AsyncIterator[str]\n</code></pre> <p>Wraps a <code>RunResultStreaming</code> object and yields text events from the stream.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>@classmethod\nasync def stream_text_from(cls, result: RunResultStreaming) -&gt; AsyncIterator[str]:\n    \"\"\"Wraps a `RunResultStreaming` object and yields text events from the stream.\"\"\"\n    async for event in result.stream_events():\n        if (\n            event.type == \"raw_response_event\"\n            and event.data.type == \"response.output_text.delta\"\n        ):\n            yield event.data.delta\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentWorkflowCallbacks","title":"SingleAgentWorkflowCallbacks","text":"Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class SingleAgentWorkflowCallbacks:\n    def on_run(self, workflow: SingleAgentVoiceWorkflow, transcription: str) -&gt; None:\n        \"\"\"Called when the workflow is run.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentWorkflowCallbacks.on_run","title":"on_run","text":"<pre><code>on_run(\n    workflow: SingleAgentVoiceWorkflow, transcription: str\n) -&gt; None\n</code></pre> <p>Called when the workflow is run.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>def on_run(self, workflow: SingleAgentVoiceWorkflow, transcription: str) -&gt; None:\n    \"\"\"Called when the workflow is run.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentVoiceWorkflow","title":"SingleAgentVoiceWorkflow","text":"<p>               Bases: <code>VoiceWorkflowBase</code></p> <p>A simple voice workflow that runs a single agent. Each transcription and result is added to the input history. For more complex workflows (e.g. multiple Runner calls, custom message history, custom logic, custom configs), subclass <code>VoiceWorkflowBase</code> and implement your own logic.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class SingleAgentVoiceWorkflow(VoiceWorkflowBase):\n    \"\"\"A simple voice workflow that runs a single agent. Each transcription and result is added to\n    the input history.\n    For more complex workflows (e.g. multiple Runner calls, custom message history, custom logic,\n    custom configs), subclass `VoiceWorkflowBase` and implement your own logic.\n    \"\"\"\n\n    def __init__(self, agent: Agent[Any], callbacks: SingleAgentWorkflowCallbacks | None = None):\n        \"\"\"Create a new single agent voice workflow.\n\n        Args:\n            agent: The agent to run.\n            callbacks: Optional callbacks to call during the workflow.\n        \"\"\"\n        self._input_history: list[TResponseInputItem] = []\n        self._current_agent = agent\n        self._callbacks = callbacks\n\n    async def run(self, transcription: str) -&gt; AsyncIterator[str]:\n        if self._callbacks:\n            self._callbacks.on_run(self, transcription)\n\n        # Add the transcription to the input history\n        self._input_history.append(\n            {\n                \"role\": \"user\",\n                \"content\": transcription,\n            }\n        )\n\n        # Run the agent\n        result = Runner.run_streamed(self._current_agent, self._input_history)\n\n        # Stream the text from the result\n        async for chunk in VoiceWorkflowHelper.stream_text_from(result):\n            yield chunk\n\n        # Update the input history and current agent\n        self._input_history = result.to_input_list()\n        self._current_agent = result.last_agent\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentVoiceWorkflow.__init__","title":"__init__","text":"<pre><code>__init__(\n    agent: Agent[Any],\n    callbacks: SingleAgentWorkflowCallbacks | None = None,\n)\n</code></pre> <p>Create a new single agent voice workflow.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent[Any]</code> <p>The agent to run.</p> required <code>callbacks</code> <code>SingleAgentWorkflowCallbacks | None</code> <p>Optional callbacks to call during the workflow.</p> <code>None</code> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>def __init__(self, agent: Agent[Any], callbacks: SingleAgentWorkflowCallbacks | None = None):\n    \"\"\"Create a new single agent voice workflow.\n\n    Args:\n        agent: The agent to run.\n        callbacks: Optional callbacks to call during the workflow.\n    \"\"\"\n    self._input_history: list[TResponseInputItem] = []\n    self._current_agent = agent\n    self._callbacks = callbacks\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentVoiceWorkflow.on_start","title":"on_start  <code>async</code>","text":"<pre><code>on_start() -&gt; AsyncIterator[str]\n</code></pre> <p>Optional method that runs before any user input is received. Can be used to deliver a greeting or instruction via TTS. Defaults to doing nothing.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>async def on_start(self) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Optional method that runs before any user input is received. Can be used\n    to deliver a greeting or instruction via TTS. Defaults to doing nothing.\n    \"\"\"\n    return\n    yield\n</code></pre>"},{"location":"ref/voice/models/openai_model_provider/","title":"<code>OpenAI Model Provider</code>","text":""},{"location":"ref/voice/models/openai_model_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider","title":"OpenAIVoiceModelProvider","text":"<p>               Bases: <code>VoiceModelProvider</code></p> <p>A voice model provider that uses OpenAI models.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>class OpenAIVoiceModelProvider(VoiceModelProvider):\n    \"\"\"A voice model provider that uses OpenAI models.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        base_url: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n    ) -&gt; None:\n        \"\"\"Create a new OpenAI voice model provider.\n\n        Args:\n            api_key: The API key to use for the OpenAI client. If not provided, we will use the\n                default API key.\n            base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n                default base URL.\n            openai_client: An optional OpenAI client to use. If not provided, we will create a new\n                OpenAI client using the api_key and base_url.\n            organization: The organization to use for the OpenAI client.\n            project: The project to use for the OpenAI client.\n        \"\"\"\n        if openai_client is not None:\n            assert api_key is None and base_url is None, (\n                \"Don't provide api_key or base_url if you provide openai_client\"\n            )\n            self._client: AsyncOpenAI | None = openai_client\n        else:\n            self._client = None\n            self._stored_api_key = api_key\n            self._stored_base_url = base_url\n            self._stored_organization = organization\n            self._stored_project = project\n\n    # We lazy load the client in case you never actually use OpenAIProvider(). Otherwise\n    # AsyncOpenAI() raises an error if you don't have an API key set.\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = _openai_shared.get_default_openai_client() or AsyncOpenAI(\n                api_key=self._stored_api_key or _openai_shared.get_default_openai_key(),\n                base_url=self._stored_base_url,\n                organization=self._stored_organization,\n                project=self._stored_project,\n                http_client=shared_http_client(),\n            )\n\n        return self._client\n\n    def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n        \"\"\"Get a speech-to-text model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The speech-to-text model.\n        \"\"\"\n        return OpenAISTTModel(model_name or DEFAULT_STT_MODEL, self._get_client())\n\n    def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n        \"\"\"Get a text-to-speech model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The text-to-speech model.\n        \"\"\"\n        return OpenAITTSModel(model_name or DEFAULT_TTS_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_model_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n) -&gt; None\n</code></pre> <p>Create a new OpenAI voice model provider.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>The API key to use for the OpenAI client. If not provided, we will use the default API key.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>The base URL to use for the OpenAI client. If not provided, we will use the default base URL.</p> <code>None</code> <code>openai_client</code> <code>AsyncOpenAI | None</code> <p>An optional OpenAI client to use. If not provided, we will create a new OpenAI client using the api_key and base_url.</p> <code>None</code> <code>organization</code> <code>str | None</code> <p>The organization to use for the OpenAI client.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>The project to use for the OpenAI client.</p> <code>None</code> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n) -&gt; None:\n    \"\"\"Create a new OpenAI voice model provider.\n\n    Args:\n        api_key: The API key to use for the OpenAI client. If not provided, we will use the\n            default API key.\n        base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n            default base URL.\n        openai_client: An optional OpenAI client to use. If not provided, we will create a new\n            OpenAI client using the api_key and base_url.\n        organization: The organization to use for the OpenAI client.\n        project: The project to use for the OpenAI client.\n    \"\"\"\n    if openai_client is not None:\n        assert api_key is None and base_url is None, (\n            \"Don't provide api_key or base_url if you provide openai_client\"\n        )\n        self._client: AsyncOpenAI | None = openai_client\n    else:\n        self._client = None\n        self._stored_api_key = api_key\n        self._stored_base_url = base_url\n        self._stored_organization = organization\n        self._stored_project = project\n</code></pre>"},{"location":"ref/voice/models/openai_model_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.get_stt_model","title":"get_stt_model","text":"<pre><code>get_stt_model(model_name: str | None) -&gt; STTModel\n</code></pre> <p>Get a speech-to-text model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>STTModel</code> <p>The speech-to-text model.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n    \"\"\"Get a speech-to-text model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The speech-to-text model.\n    \"\"\"\n    return OpenAISTTModel(model_name or DEFAULT_STT_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_model_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.get_tts_model","title":"get_tts_model","text":"<pre><code>get_tts_model(model_name: str | None) -&gt; TTSModel\n</code></pre> <p>Get a text-to-speech model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>TTSModel</code> <p>The text-to-speech model.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n    \"\"\"Get a text-to-speech model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The text-to-speech model.\n    \"\"\"\n    return OpenAITTSModel(model_name or DEFAULT_TTS_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_provider/","title":"<code>OpenAIVoiceModelProvider</code>","text":""},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider","title":"OpenAIVoiceModelProvider","text":"<p>               Bases: <code>VoiceModelProvider</code></p> <p>A voice model provider that uses OpenAI models.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>class OpenAIVoiceModelProvider(VoiceModelProvider):\n    \"\"\"A voice model provider that uses OpenAI models.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        base_url: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n    ) -&gt; None:\n        \"\"\"Create a new OpenAI voice model provider.\n\n        Args:\n            api_key: The API key to use for the OpenAI client. If not provided, we will use the\n                default API key.\n            base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n                default base URL.\n            openai_client: An optional OpenAI client to use. If not provided, we will create a new\n                OpenAI client using the api_key and base_url.\n            organization: The organization to use for the OpenAI client.\n            project: The project to use for the OpenAI client.\n        \"\"\"\n        if openai_client is not None:\n            assert api_key is None and base_url is None, (\n                \"Don't provide api_key or base_url if you provide openai_client\"\n            )\n            self._client: AsyncOpenAI | None = openai_client\n        else:\n            self._client = None\n            self._stored_api_key = api_key\n            self._stored_base_url = base_url\n            self._stored_organization = organization\n            self._stored_project = project\n\n    # We lazy load the client in case you never actually use OpenAIProvider(). Otherwise\n    # AsyncOpenAI() raises an error if you don't have an API key set.\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = _openai_shared.get_default_openai_client() or AsyncOpenAI(\n                api_key=self._stored_api_key or _openai_shared.get_default_openai_key(),\n                base_url=self._stored_base_url,\n                organization=self._stored_organization,\n                project=self._stored_project,\n                http_client=shared_http_client(),\n            )\n\n        return self._client\n\n    def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n        \"\"\"Get a speech-to-text model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The speech-to-text model.\n        \"\"\"\n        return OpenAISTTModel(model_name or DEFAULT_STT_MODEL, self._get_client())\n\n    def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n        \"\"\"Get a text-to-speech model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The text-to-speech model.\n        \"\"\"\n        return OpenAITTSModel(model_name or DEFAULT_TTS_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n) -&gt; None\n</code></pre> <p>Create a new OpenAI voice model provider.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>The API key to use for the OpenAI client. If not provided, we will use the default API key.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>The base URL to use for the OpenAI client. If not provided, we will use the default base URL.</p> <code>None</code> <code>openai_client</code> <code>AsyncOpenAI | None</code> <p>An optional OpenAI client to use. If not provided, we will create a new OpenAI client using the api_key and base_url.</p> <code>None</code> <code>organization</code> <code>str | None</code> <p>The organization to use for the OpenAI client.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>The project to use for the OpenAI client.</p> <code>None</code> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n) -&gt; None:\n    \"\"\"Create a new OpenAI voice model provider.\n\n    Args:\n        api_key: The API key to use for the OpenAI client. If not provided, we will use the\n            default API key.\n        base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n            default base URL.\n        openai_client: An optional OpenAI client to use. If not provided, we will create a new\n            OpenAI client using the api_key and base_url.\n        organization: The organization to use for the OpenAI client.\n        project: The project to use for the OpenAI client.\n    \"\"\"\n    if openai_client is not None:\n        assert api_key is None and base_url is None, (\n            \"Don't provide api_key or base_url if you provide openai_client\"\n        )\n        self._client: AsyncOpenAI | None = openai_client\n    else:\n        self._client = None\n        self._stored_api_key = api_key\n        self._stored_base_url = base_url\n        self._stored_organization = organization\n        self._stored_project = project\n</code></pre>"},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.get_stt_model","title":"get_stt_model","text":"<pre><code>get_stt_model(model_name: str | None) -&gt; STTModel\n</code></pre> <p>Get a speech-to-text model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>STTModel</code> <p>The speech-to-text model.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n    \"\"\"Get a speech-to-text model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The speech-to-text model.\n    \"\"\"\n    return OpenAISTTModel(model_name or DEFAULT_STT_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.get_tts_model","title":"get_tts_model","text":"<pre><code>get_tts_model(model_name: str | None) -&gt; TTSModel\n</code></pre> <p>Get a text-to-speech model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>TTSModel</code> <p>The text-to-speech model.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n    \"\"\"Get a text-to-speech model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The text-to-speech model.\n    \"\"\"\n    return OpenAITTSModel(model_name or DEFAULT_TTS_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_stt/","title":"<code>OpenAI STT</code>","text":""},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTTranscriptionSession","title":"OpenAISTTTranscriptionSession","text":"<p>               Bases: <code>StreamedTranscriptionSession</code></p> <p>A transcription session for OpenAI's STT model.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>class OpenAISTTTranscriptionSession(StreamedTranscriptionSession):\n    \"\"\"A transcription session for OpenAI's STT model.\"\"\"\n\n    def __init__(\n        self,\n        input: StreamedAudioInput,\n        client: AsyncOpenAI,\n        model: str,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ):\n        self.connected: bool = False\n        self._client = client\n        self._model = model\n        self._settings = settings\n        self._turn_detection = settings.turn_detection or DEFAULT_TURN_DETECTION\n        self._trace_include_sensitive_data = trace_include_sensitive_data\n        self._trace_include_sensitive_audio_data = trace_include_sensitive_audio_data\n\n        self._input_queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32] | None] = input.queue\n        self._output_queue: asyncio.Queue[str | ErrorSentinel | SessionCompleteSentinel] = (\n            asyncio.Queue()\n        )\n        self._websocket: websockets.ClientConnection | None = None\n        self._event_queue: asyncio.Queue[dict[str, Any] | WebsocketDoneSentinel] = asyncio.Queue()\n        self._state_queue: asyncio.Queue[dict[str, Any]] = asyncio.Queue()\n        self._turn_audio_buffer: list[npt.NDArray[np.int16 | np.float32]] = []\n        self._tracing_span: Span[TranscriptionSpanData] | None = None\n\n        # tasks\n        self._listener_task: asyncio.Task[Any] | None = None\n        self._process_events_task: asyncio.Task[Any] | None = None\n        self._stream_audio_task: asyncio.Task[Any] | None = None\n        self._connection_task: asyncio.Task[Any] | None = None\n        self._stored_exception: Exception | None = None\n\n    def _start_turn(self) -&gt; None:\n        self._tracing_span = transcription_span(\n            model=self._model,\n            model_config={\n                \"temperature\": self._settings.temperature,\n                \"language\": self._settings.language,\n                \"prompt\": self._settings.prompt,\n                \"turn_detection\": self._turn_detection,\n            },\n        )\n        self._tracing_span.start()\n\n    def _end_turn(self, _transcript: str) -&gt; None:\n        if len(_transcript) &lt; 1:\n            return\n\n        if self._tracing_span:\n            # Only encode audio if tracing is enabled AND buffer is not empty\n            if self._trace_include_sensitive_audio_data and self._turn_audio_buffer:\n                self._tracing_span.span_data.input = _audio_to_base64(self._turn_audio_buffer)\n\n            self._tracing_span.span_data.input_format = \"pcm\"\n\n            if self._trace_include_sensitive_data:\n                self._tracing_span.span_data.output = _transcript\n\n            self._tracing_span.finish()\n            self._turn_audio_buffer = []\n            self._tracing_span = None\n\n    async def _event_listener(self) -&gt; None:\n        assert self._websocket is not None, \"Websocket not initialized\"\n\n        async for message in self._websocket:\n            try:\n                event = json.loads(message)\n\n                if event.get(\"type\") == \"error\":\n                    raise STTWebsocketConnectionError(f\"Error event: {event.get('error')}\")\n\n                if event.get(\"type\") in [\n                    \"session.updated\",\n                    \"transcription_session.updated\",\n                    \"session.created\",\n                    \"transcription_session.created\",\n                ]:\n                    await self._state_queue.put(event)\n\n                await self._event_queue.put(event)\n            except Exception as e:\n                await self._output_queue.put(ErrorSentinel(e))\n                raise STTWebsocketConnectionError(\"Error parsing events\") from e\n        await self._event_queue.put(WebsocketDoneSentinel())\n\n    async def _configure_session(self) -&gt; None:\n        assert self._websocket is not None, \"Websocket not initialized\"\n        await self._websocket.send(\n            json.dumps(\n                {\n                    \"type\": \"session.update\",\n                    \"session\": {\n                        \"type\": \"transcription\",\n                        \"audio\": {\n                            \"input\": {\n                                \"format\": {\"type\": \"audio/pcm\", \"rate\": 24000},\n                                \"transcription\": {\"model\": self._model},\n                                \"turn_detection\": self._turn_detection,\n                            }\n                        },\n                    },\n                }\n            )\n        )\n\n    async def _setup_connection(self, ws: websockets.ClientConnection) -&gt; None:\n        self._websocket = ws\n        self._listener_task = asyncio.create_task(self._event_listener())\n\n        try:\n            event = await _wait_for_event(\n                self._state_queue,\n                [\"session.created\", \"transcription_session.created\"],\n                SESSION_CREATION_TIMEOUT,\n            )\n        except TimeoutError as e:\n            wrapped_err = STTWebsocketConnectionError(\n                \"Timeout waiting for transcription_session.created event\"\n            )\n            await self._output_queue.put(ErrorSentinel(wrapped_err))\n            raise wrapped_err from e\n        except Exception as e:\n            await self._output_queue.put(ErrorSentinel(e))\n            raise e\n\n        await self._configure_session()\n\n        try:\n            event = await _wait_for_event(\n                self._state_queue,\n                [\"session.updated\", \"transcription_session.updated\"],\n                SESSION_UPDATE_TIMEOUT,\n            )\n            if _debug.DONT_LOG_MODEL_DATA:\n                logger.debug(\"Session updated\")\n            else:\n                logger.debug(f\"Session updated: {event}\")\n        except TimeoutError as e:\n            wrapped_err = STTWebsocketConnectionError(\n                \"Timeout waiting for transcription_session.updated event\"\n            )\n            await self._output_queue.put(ErrorSentinel(wrapped_err))\n            raise wrapped_err from e\n        except Exception as e:\n            await self._output_queue.put(ErrorSentinel(e))\n            raise\n\n    async def _handle_events(self) -&gt; None:\n        while True:\n            try:\n                event = await asyncio.wait_for(\n                    self._event_queue.get(), timeout=EVENT_INACTIVITY_TIMEOUT\n                )\n                if isinstance(event, WebsocketDoneSentinel):\n                    # processed all events and websocket is done\n                    break\n\n                event_type = event.get(\"type\", \"unknown\")\n                if event_type in [\n                    \"input_audio_transcription_completed\",  # legacy\n                    \"conversation.item.input_audio_transcription.completed\",\n                ]:\n                    transcript = cast(str, event.get(\"transcript\", \"\"))\n                    if len(transcript) &gt; 0:\n                        self._end_turn(transcript)\n                        self._start_turn()\n                        await self._output_queue.put(transcript)\n                await asyncio.sleep(0)  # yield control\n            except asyncio.TimeoutError:\n                # No new events for a while. Assume the session is done.\n                break\n            except Exception as e:\n                await self._output_queue.put(ErrorSentinel(e))\n                raise e\n        await self._output_queue.put(SessionCompleteSentinel())\n\n    async def _stream_audio(\n        self, audio_queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32] | None]\n    ) -&gt; None:\n        assert self._websocket is not None, \"Websocket not initialized\"\n        self._start_turn()\n        while True:\n            buffer = await audio_queue.get()\n            if buffer is None:\n                break\n\n            self._turn_audio_buffer.append(buffer)\n            try:\n                await self._websocket.send(\n                    json.dumps(\n                        {\n                            \"type\": \"input_audio_buffer.append\",\n                            \"audio\": base64.b64encode(buffer.tobytes()).decode(\"utf-8\"),\n                        }\n                    )\n                )\n            except websockets.ConnectionClosed:\n                break\n            except Exception as e:\n                await self._output_queue.put(ErrorSentinel(e))\n                raise e\n\n            await asyncio.sleep(0)  # yield control\n\n    async def _process_websocket_connection(self) -&gt; None:\n        try:\n            async with websockets.connect(\n                \"wss://api.openai.com/v1/realtime?intent=transcription\",\n                additional_headers={\n                    \"Authorization\": f\"Bearer {self._client.api_key}\",\n                    \"OpenAI-Log-Session\": \"1\",\n                },\n            ) as ws:\n                await self._setup_connection(ws)\n                self._process_events_task = asyncio.create_task(self._handle_events())\n                self._stream_audio_task = asyncio.create_task(self._stream_audio(self._input_queue))\n                self.connected = True\n                if self._listener_task:\n                    await self._listener_task\n                else:\n                    logger.error(\"Listener task not initialized\")\n                    raise AgentsException(\"Listener task not initialized\")\n        except Exception as e:\n            await self._output_queue.put(ErrorSentinel(e))\n            raise e\n\n    def _check_errors(self) -&gt; None:\n        if self._connection_task and self._connection_task.done():\n            exc = self._connection_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n        if self._process_events_task and self._process_events_task.done():\n            exc = self._process_events_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n        if self._stream_audio_task and self._stream_audio_task.done():\n            exc = self._stream_audio_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n        if self._listener_task and self._listener_task.done():\n            exc = self._listener_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n    def _cleanup_tasks(self) -&gt; None:\n        if self._listener_task and not self._listener_task.done():\n            self._listener_task.cancel()\n\n        if self._process_events_task and not self._process_events_task.done():\n            self._process_events_task.cancel()\n\n        if self._stream_audio_task and not self._stream_audio_task.done():\n            self._stream_audio_task.cancel()\n\n        if self._connection_task and not self._connection_task.done():\n            self._connection_task.cancel()\n\n    async def transcribe_turns(self) -&gt; AsyncIterator[str]:\n        self._connection_task = asyncio.create_task(self._process_websocket_connection())\n\n        while True:\n            try:\n                turn = await self._output_queue.get()\n            except asyncio.CancelledError:\n                break\n\n            if (\n                turn is None\n                or isinstance(turn, ErrorSentinel)\n                or isinstance(turn, SessionCompleteSentinel)\n            ):\n                self._output_queue.task_done()\n                break\n            yield turn\n            self._output_queue.task_done()\n\n        if self._tracing_span:\n            self._end_turn(\"\")\n\n        if self._websocket:\n            await self._websocket.close()\n\n        self._check_errors()\n        if self._stored_exception:\n            raise self._stored_exception\n\n    async def close(self) -&gt; None:\n        if self._websocket:\n            await self._websocket.close()\n\n        self._cleanup_tasks()\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel","title":"OpenAISTTModel","text":"<p>               Bases: <code>STTModel</code></p> <p>A speech-to-text model for OpenAI.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>class OpenAISTTModel(STTModel):\n    \"\"\"A speech-to-text model for OpenAI.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        openai_client: AsyncOpenAI,\n    ):\n        \"\"\"Create a new OpenAI speech-to-text model.\n\n        Args:\n            model: The name of the model to use.\n            openai_client: The OpenAI client to use.\n        \"\"\"\n        self.model = model\n        self._client = openai_client\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.model\n\n    def _non_null_or_not_given(self, value: Any) -&gt; Any:\n        return value if value is not None else None  # NOT_GIVEN\n\n    async def transcribe(\n        self,\n        input: AudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; str:\n        \"\"\"Transcribe an audio input.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n\n        Returns:\n            The transcribed text.\n        \"\"\"\n        with transcription_span(\n            model=self.model,\n            input=input.to_base64() if trace_include_sensitive_audio_data else \"\",\n            input_format=\"pcm\",\n            model_config={\n                \"temperature\": self._non_null_or_not_given(settings.temperature),\n                \"language\": self._non_null_or_not_given(settings.language),\n                \"prompt\": self._non_null_or_not_given(settings.prompt),\n            },\n        ) as span:\n            try:\n                response = await self._client.audio.transcriptions.create(\n                    model=self.model,\n                    file=input.to_audio_file(),\n                    prompt=self._non_null_or_not_given(settings.prompt),\n                    language=self._non_null_or_not_given(settings.language),\n                    temperature=self._non_null_or_not_given(settings.temperature),\n                )\n                if trace_include_sensitive_data:\n                    span.span_data.output = response.text\n                return response.text\n            except Exception as e:\n                span.span_data.output = \"\"\n                span.set_error(SpanError(message=str(e), data={}))\n                raise e\n\n    async def create_session(\n        self,\n        input: StreamedAudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; StreamedTranscriptionSession:\n        \"\"\"Create a new transcription session.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n            trace_include_sensitive_data: Whether to include sensitive data in traces.\n            trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n        Returns:\n            A new transcription session.\n        \"\"\"\n        return OpenAISTTTranscriptionSession(\n            input,\n            self._client,\n            self.model,\n            settings,\n            trace_include_sensitive_data,\n            trace_include_sensitive_audio_data,\n        )\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel.__init__","title":"__init__","text":"<pre><code>__init__(model: str, openai_client: AsyncOpenAI)\n</code></pre> <p>Create a new OpenAI speech-to-text model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model to use.</p> required <code>openai_client</code> <code>AsyncOpenAI</code> <p>The OpenAI client to use.</p> required Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    openai_client: AsyncOpenAI,\n):\n    \"\"\"Create a new OpenAI speech-to-text model.\n\n    Args:\n        model: The name of the model to use.\n        openai_client: The OpenAI client to use.\n    \"\"\"\n    self.model = model\n    self._client = openai_client\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel.transcribe","title":"transcribe  <code>async</code>","text":"<pre><code>transcribe(\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str\n</code></pre> <p>Transcribe an audio input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>AudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transcribed text.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>async def transcribe(\n    self,\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str:\n    \"\"\"Transcribe an audio input.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n\n    Returns:\n        The transcribed text.\n    \"\"\"\n    with transcription_span(\n        model=self.model,\n        input=input.to_base64() if trace_include_sensitive_audio_data else \"\",\n        input_format=\"pcm\",\n        model_config={\n            \"temperature\": self._non_null_or_not_given(settings.temperature),\n            \"language\": self._non_null_or_not_given(settings.language),\n            \"prompt\": self._non_null_or_not_given(settings.prompt),\n        },\n    ) as span:\n        try:\n            response = await self._client.audio.transcriptions.create(\n                model=self.model,\n                file=input.to_audio_file(),\n                prompt=self._non_null_or_not_given(settings.prompt),\n                language=self._non_null_or_not_given(settings.language),\n                temperature=self._non_null_or_not_given(settings.temperature),\n            )\n            if trace_include_sensitive_data:\n                span.span_data.output = response.text\n            return response.text\n        except Exception as e:\n            span.span_data.output = \"\"\n            span.set_error(SpanError(message=str(e), data={}))\n            raise e\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel.create_session","title":"create_session  <code>async</code>","text":"<pre><code>create_session(\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession\n</code></pre> <p>Create a new transcription session.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StreamedAudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <code>trace_include_sensitive_data</code> <code>bool</code> <p>Whether to include sensitive data in traces.</p> required <code>trace_include_sensitive_audio_data</code> <code>bool</code> <p>Whether to include sensitive audio data in traces.</p> required <p>Returns:</p> Type Description <code>StreamedTranscriptionSession</code> <p>A new transcription session.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>async def create_session(\n    self,\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession:\n    \"\"\"Create a new transcription session.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n        trace_include_sensitive_data: Whether to include sensitive data in traces.\n        trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n    Returns:\n        A new transcription session.\n    \"\"\"\n    return OpenAISTTTranscriptionSession(\n        input,\n        self._client,\n        self.model,\n        settings,\n        trace_include_sensitive_data,\n        trace_include_sensitive_audio_data,\n    )\n</code></pre>"},{"location":"ref/voice/models/openai_tts/","title":"<code>OpenAI TTS</code>","text":""},{"location":"ref/voice/models/openai_tts/#agents.voice.models.openai_tts.OpenAITTSModel","title":"OpenAITTSModel","text":"<p>               Bases: <code>TTSModel</code></p> <p>A text-to-speech model for OpenAI.</p> Source code in <code>src/agents/voice/models/openai_tts.py</code> <pre><code>class OpenAITTSModel(TTSModel):\n    \"\"\"A text-to-speech model for OpenAI.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        openai_client: AsyncOpenAI,\n    ):\n        \"\"\"Create a new OpenAI text-to-speech model.\n\n        Args:\n            model: The name of the model to use.\n            openai_client: The OpenAI client to use.\n        \"\"\"\n        self.model = model\n        self._client = openai_client\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.model\n\n    async def run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n        \"\"\"Run the text-to-speech model.\n\n        Args:\n            text: The text to convert to speech.\n            settings: The settings to use for the text-to-speech model.\n\n        Returns:\n            An iterator of audio chunks.\n        \"\"\"\n        response = self._client.audio.speech.with_streaming_response.create(\n            model=self.model,\n            voice=settings.voice or DEFAULT_VOICE,\n            input=text,\n            response_format=\"pcm\",\n            extra_body={\n                \"instructions\": settings.instructions,\n            },\n        )\n\n        async with response as stream:\n            async for chunk in stream.iter_bytes(chunk_size=1024):\n                yield chunk\n</code></pre>"},{"location":"ref/voice/models/openai_tts/#agents.voice.models.openai_tts.OpenAITTSModel.__init__","title":"__init__","text":"<pre><code>__init__(model: str, openai_client: AsyncOpenAI)\n</code></pre> <p>Create a new OpenAI text-to-speech model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model to use.</p> required <code>openai_client</code> <code>AsyncOpenAI</code> <p>The OpenAI client to use.</p> required Source code in <code>src/agents/voice/models/openai_tts.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    openai_client: AsyncOpenAI,\n):\n    \"\"\"Create a new OpenAI text-to-speech model.\n\n    Args:\n        model: The name of the model to use.\n        openai_client: The OpenAI client to use.\n    \"\"\"\n    self.model = model\n    self._client = openai_client\n</code></pre>"},{"location":"ref/voice/models/openai_tts/#agents.voice.models.openai_tts.OpenAITTSModel.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    text: str, settings: TTSModelSettings\n) -&gt; AsyncIterator[bytes]\n</code></pre> <p>Run the text-to-speech model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to convert to speech.</p> required <code>settings</code> <code>TTSModelSettings</code> <p>The settings to use for the text-to-speech model.</p> required <p>Returns:</p> Type Description <code>AsyncIterator[bytes]</code> <p>An iterator of audio chunks.</p> Source code in <code>src/agents/voice/models/openai_tts.py</code> <pre><code>async def run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n    \"\"\"Run the text-to-speech model.\n\n    Args:\n        text: The text to convert to speech.\n        settings: The settings to use for the text-to-speech model.\n\n    Returns:\n        An iterator of audio chunks.\n    \"\"\"\n    response = self._client.audio.speech.with_streaming_response.create(\n        model=self.model,\n        voice=settings.voice or DEFAULT_VOICE,\n        input=text,\n        response_format=\"pcm\",\n        extra_body={\n            \"instructions\": settings.instructions,\n        },\n    )\n\n    async with response as stream:\n        async for chunk in stream.iter_bytes(chunk_size=1024):\n            yield chunk\n</code></pre>"},{"location":"sessions/","title":"Sessions","text":"<p>The Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle <code>.to_input_list()</code> between turns.</p> <p>Sessions stores conversation history for a specific session, allowing agents to maintain context without requiring explicit manual memory management. This is particularly useful for building chat applications or multi-turn conversations where you want the agent to remember previous interactions.</p>"},{"location":"sessions/#quick-start","title":"Quick start","text":"<pre><code>from agents import Agent, Runner, SQLiteSession\n\n# Create agent\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"Reply very concisely.\",\n)\n\n# Create a session instance with a session ID\nsession = SQLiteSession(\"conversation_123\")\n\n# First turn\nresult = await Runner.run(\n    agent,\n    \"What city is the Golden Gate Bridge in?\",\n    session=session\n)\nprint(result.final_output)  # \"San Francisco\"\n\n# Second turn - agent automatically remembers previous context\nresult = await Runner.run(\n    agent,\n    \"What state is it in?\",\n    session=session\n)\nprint(result.final_output)  # \"California\"\n\n# Also works with synchronous runner\nresult = Runner.run_sync(\n    agent,\n    \"What's the population?\",\n    session=session\n)\nprint(result.final_output)  # \"Approximately 39 million\"\n</code></pre>"},{"location":"sessions/#how-it-works","title":"How it works","text":"<p>When session memory is enabled:</p> <ol> <li>Before each run: The runner automatically retrieves the conversation history for the session and prepends it to the input items.</li> <li>After each run: All new items generated during the run (user input, assistant responses, tool calls, etc.) are automatically stored in the session.</li> <li>Context preservation: Each subsequent run with the same session includes the full conversation history, allowing the agent to maintain context.</li> </ol> <p>This eliminates the need to manually call <code>.to_input_list()</code> and manage conversation state between runs.</p>"},{"location":"sessions/#memory-operations","title":"Memory operations","text":""},{"location":"sessions/#basic-operations","title":"Basic operations","text":"<p>Sessions supports several operations for managing conversation history:</p> <pre><code>from agents import SQLiteSession\n\nsession = SQLiteSession(\"user_123\", \"conversations.db\")\n\n# Get all items in a session\nitems = await session.get_items()\n\n# Add new items to a session\nnew_items = [\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n]\nawait session.add_items(new_items)\n\n# Remove and return the most recent item\nlast_item = await session.pop_item()\nprint(last_item)  # {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n\n# Clear all items from a session\nawait session.clear_session()\n</code></pre>"},{"location":"sessions/#using-pop_item-for-corrections","title":"Using pop_item for corrections","text":"<p>The <code>pop_item</code> method is particularly useful when you want to undo or modify the last item in a conversation:</p> <pre><code>from agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\")\nsession = SQLiteSession(\"correction_example\")\n\n# Initial conversation\nresult = await Runner.run(\n    agent,\n    \"What's 2 + 2?\",\n    session=session\n)\nprint(f\"Agent: {result.final_output}\")\n\n# User wants to correct their question\nassistant_item = await session.pop_item()  # Remove agent's response\nuser_item = await session.pop_item()  # Remove user's question\n\n# Ask a corrected question\nresult = await Runner.run(\n    agent,\n    \"What's 2 + 3?\",\n    session=session\n)\nprint(f\"Agent: {result.final_output}\")\n</code></pre>"},{"location":"sessions/#session-types","title":"Session types","text":"<p>The SDK provides several session implementations for different use cases:</p>"},{"location":"sessions/#openai-conversations-api-sessions","title":"OpenAI Conversations API sessions","text":"<p>Use OpenAI's Conversations API through <code>OpenAIConversationsSession</code>.</p> <pre><code>from agents import Agent, Runner, OpenAIConversationsSession\n\n# Create agent\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"Reply very concisely.\",\n)\n\n# Create a new conversation\nsession = OpenAIConversationsSession()\n\n# Optionally resume a previous conversation by passing a conversation ID\n# session = OpenAIConversationsSession(conversation_id=\"conv_123\")\n\n# Start conversation\nresult = await Runner.run(\n    agent,\n    \"What city is the Golden Gate Bridge in?\",\n    session=session\n)\nprint(result.final_output)  # \"San Francisco\"\n\n# Continue the conversation\nresult = await Runner.run(\n    agent,\n    \"What state is it in?\",\n    session=session\n)\nprint(result.final_output)  # \"California\"\n</code></pre>"},{"location":"sessions/#openai-responses-compaction-sessions","title":"OpenAI Responses compaction sessions","text":"<p>Use <code>OpenAIResponsesCompactionSession</code> to compact session history with the Responses API (<code>responses.compact</code>). It wraps an underlying session and can automatically compact after each turn based on <code>should_trigger_compaction</code>.</p>"},{"location":"sessions/#typical-usage-auto-compaction","title":"Typical usage (auto-compaction)","text":"<pre><code>from agents import Agent, Runner, SQLiteSession\nfrom agents.memory import OpenAIResponsesCompactionSession\n\nunderlying = SQLiteSession(\"conversation_123\")\nsession = OpenAIResponsesCompactionSession(\n    session_id=\"conversation_123\",\n    underlying_session=underlying,\n)\n\nagent = Agent(name=\"Assistant\")\nresult = await Runner.run(agent, \"Hello\", session=session)\nprint(result.final_output)\n</code></pre> <p>By default, compaction runs after each turn once the candidate threshold is reached.</p>"},{"location":"sessions/#auto-compaction-can-block-streaming","title":"auto-compaction can block streaming","text":"<p>Compaction clears and rewrites the session history, so the SDK waits for compaction to finish before considering the run complete. In streaming mode, this means <code>run.stream_events()</code> can stay open for a few seconds after the last output token if compaction is heavy.</p> <p>If you want low-latency streaming or fast turn-taking, disable auto-compaction and call <code>run_compaction()</code> yourself between turns (or during idle time). You can decide when to force compaction based on your own criteria.</p> <pre><code>from agents import Agent, Runner, SQLiteSession\nfrom agents.memory import OpenAIResponsesCompactionSession\n\nunderlying = SQLiteSession(\"conversation_123\")\nsession = OpenAIResponsesCompactionSession(\n    session_id=\"conversation_123\",\n    underlying_session=underlying,\n    # Disable triggering the auto compaction\n    should_trigger_compaction=lambda _: False,\n)\n\nagent = Agent(name=\"Assistant\")\nresult = await Runner.run(agent, \"Hello\", session=session)\n\n# Decide when to compact (e.g., on idle, every N turns, or size thresholds).\nawait session.run_compaction({\"force\": True})\n</code></pre>"},{"location":"sessions/#sqlite-sessions","title":"SQLite sessions","text":"<p>The default, lightweight session implementation using SQLite:</p> <pre><code>from agents import SQLiteSession\n\n# In-memory database (lost when process ends)\nsession = SQLiteSession(\"user_123\")\n\n# Persistent file-based database\nsession = SQLiteSession(\"user_123\", \"conversations.db\")\n\n# Use the session\nresult = await Runner.run(\n    agent,\n    \"Hello\",\n    session=session\n)\n</code></pre>"},{"location":"sessions/#sqlalchemy-sessions","title":"SQLAlchemy sessions","text":"<p>Production-ready sessions using any SQLAlchemy-supported database:</p> <pre><code>from agents.extensions.memory import SQLAlchemySession\n\n# Using database URL\nsession = SQLAlchemySession.from_url(\n    \"user_123\",\n    url=\"postgresql+asyncpg://user:pass@localhost/db\",\n    create_tables=True\n)\n\n# Using existing engine\nfrom sqlalchemy.ext.asyncio import create_async_engine\nengine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db\")\nsession = SQLAlchemySession(\"user_123\", engine=engine, create_tables=True)\n</code></pre> <p>See SQLAlchemy Sessions for detailed documentation.</p>"},{"location":"sessions/#advanced-sqlite-sessions","title":"Advanced SQLite sessions","text":"<p>Enhanced SQLite sessions with conversation branching, usage analytics, and structured queries:</p> <pre><code>from agents.extensions.memory import AdvancedSQLiteSession\n\n# Create with advanced features\nsession = AdvancedSQLiteSession(\n    session_id=\"user_123\",\n    db_path=\"conversations.db\",\n    create_tables=True\n)\n\n# Automatic usage tracking\nresult = await Runner.run(agent, \"Hello\", session=session)\nawait session.store_run_usage(result)  # Track token usage\n\n# Conversation branching\nawait session.create_branch_from_turn(2)  # Branch from turn 2\n</code></pre> <p>See Advanced SQLite Sessions for detailed documentation.</p>"},{"location":"sessions/#encrypted-sessions","title":"Encrypted sessions","text":"<p>Transparent encryption wrapper for any session implementation:</p> <pre><code>from agents.extensions.memory import EncryptedSession, SQLAlchemySession\n\n# Create underlying session\nunderlying_session = SQLAlchemySession.from_url(\n    \"user_123\",\n    url=\"sqlite+aiosqlite:///conversations.db\",\n    create_tables=True\n)\n\n# Wrap with encryption and TTL\nsession = EncryptedSession(\n    session_id=\"user_123\",\n    underlying_session=underlying_session,\n    encryption_key=\"your-secret-key\",\n    ttl=600  # 10 minutes\n)\n\nresult = await Runner.run(agent, \"Hello\", session=session)\n</code></pre> <p>See Encrypted Sessions for detailed documentation.</p>"},{"location":"sessions/#other-session-types","title":"Other session types","text":"<p>There are a few more built-in options. Please refer to <code>examples/memory/</code> and source code under <code>extensions/memory/</code>.</p>"},{"location":"sessions/#session-management","title":"Session management","text":""},{"location":"sessions/#session-id-naming","title":"Session ID naming","text":"<p>Use meaningful session IDs that help you organize conversations:</p> <ul> <li>User-based: <code>\"user_12345\"</code></li> <li>Thread-based: <code>\"thread_abc123\"</code></li> <li>Context-based: <code>\"support_ticket_456\"</code></li> </ul>"},{"location":"sessions/#memory-persistence","title":"Memory persistence","text":"<ul> <li>Use in-memory SQLite (<code>SQLiteSession(\"session_id\")</code>) for temporary conversations</li> <li>Use file-based SQLite (<code>SQLiteSession(\"session_id\", \"path/to/db.sqlite\")</code>) for persistent conversations</li> <li>Use SQLAlchemy-powered sessions (<code>SQLAlchemySession(\"session_id\", engine=engine, create_tables=True)</code>) for production systems with existing databases supported by SQLAlchemy</li> <li>Use Dapr state store sessions (<code>DaprSession.from_address(\"session_id\", state_store_name=\"statestore\", dapr_address=\"localhost:50001\")</code>) for production cloud-native deployments with support for  30+ database backends with built-in telemetry, tracing, and data isolation</li> <li>Use OpenAI-hosted storage (<code>OpenAIConversationsSession()</code>) when you prefer to store history in the OpenAI Conversations API</li> <li>Use encrypted sessions (<code>EncryptedSession(session_id, underlying_session, encryption_key)</code>) to wrap any session with transparent encryption and TTL-based expiration</li> <li>Consider implementing custom session backends for other production systems (Redis, Django, etc.) for more advanced use cases</li> </ul>"},{"location":"sessions/#multiple-sessions","title":"Multiple sessions","text":"<pre><code>from agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\")\n\n# Different sessions maintain separate conversation histories\nsession_1 = SQLiteSession(\"user_123\", \"conversations.db\")\nsession_2 = SQLiteSession(\"user_456\", \"conversations.db\")\n\nresult1 = await Runner.run(\n    agent,\n    \"Help me with my account\",\n    session=session_1\n)\nresult2 = await Runner.run(\n    agent,\n    \"What are my charges?\",\n    session=session_2\n)\n</code></pre>"},{"location":"sessions/#session-sharing","title":"Session sharing","text":"<pre><code># Different agents can share the same session\nsupport_agent = Agent(name=\"Support\")\nbilling_agent = Agent(name=\"Billing\")\nsession = SQLiteSession(\"user_123\")\n\n# Both agents will see the same conversation history\nresult1 = await Runner.run(\n    support_agent,\n    \"Help me with my account\",\n    session=session\n)\nresult2 = await Runner.run(\n    billing_agent,\n    \"What are my charges?\",\n    session=session\n)\n</code></pre>"},{"location":"sessions/#complete-example","title":"Complete example","text":"<p>Here's a complete example showing session memory in action:</p> <pre><code>import asyncio\nfrom agents import Agent, Runner, SQLiteSession\n\n\nasync def main():\n    # Create an agent\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"Reply very concisely.\",\n    )\n\n    # Create a session instance that will persist across runs\n    session = SQLiteSession(\"conversation_123\", \"conversation_history.db\")\n\n    print(\"=== Sessions Example ===\")\n    print(\"The agent will remember previous messages automatically.\\n\")\n\n    # First turn\n    print(\"First turn:\")\n    print(\"User: What city is the Golden Gate Bridge in?\")\n    result = await Runner.run(\n        agent,\n        \"What city is the Golden Gate Bridge in?\",\n        session=session\n    )\n    print(f\"Assistant: {result.final_output}\")\n    print()\n\n    # Second turn - the agent will remember the previous conversation\n    print(\"Second turn:\")\n    print(\"User: What state is it in?\")\n    result = await Runner.run(\n        agent,\n        \"What state is it in?\",\n        session=session\n    )\n    print(f\"Assistant: {result.final_output}\")\n    print()\n\n    # Third turn - continuing the conversation\n    print(\"Third turn:\")\n    print(\"User: What's the population of that state?\")\n    result = await Runner.run(\n        agent,\n        \"What's the population of that state?\",\n        session=session\n    )\n    print(f\"Assistant: {result.final_output}\")\n    print()\n\n    print(\"=== Conversation Complete ===\")\n    print(\"Notice how the agent remembered the context from previous turns!\")\n    print(\"Sessions automatically handles conversation history.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"sessions/#custom-session-implementations","title":"Custom session implementations","text":"<p>You can implement your own session memory by creating a class that follows the <code>Session</code> protocol:</p> <pre><code>from agents.memory.session import SessionABC\nfrom agents.items import TResponseInputItem\nfrom typing import List\n\nclass MyCustomSession(SessionABC):\n    \"\"\"Custom session implementation following the Session protocol.\"\"\"\n\n    def __init__(self, session_id: str):\n        self.session_id = session_id\n        # Your initialization here\n\n    async def get_items(self, limit: int | None = None) -&gt; List[TResponseInputItem]:\n        \"\"\"Retrieve conversation history for this session.\"\"\"\n        # Your implementation here\n        pass\n\n    async def add_items(self, items: List[TResponseInputItem]) -&gt; None:\n        \"\"\"Store new items for this session.\"\"\"\n        # Your implementation here\n        pass\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from this session.\"\"\"\n        # Your implementation here\n        pass\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        # Your implementation here\n        pass\n\n# Use your custom session\nagent = Agent(name=\"Assistant\")\nresult = await Runner.run(\n    agent,\n    \"Hello\",\n    session=MyCustomSession(\"my_session\")\n)\n</code></pre>"},{"location":"sessions/#community-session-implementations","title":"Community session implementations","text":"<p>The community has developed additional session implementations:</p> Package Description openai-django-sessions Django ORM-based sessions for any Django-supported database (PostgreSQL, MySQL, SQLite, and more) <p>If you've built a session implementation, please feel free to submit a documentation PR to add it here!</p>"},{"location":"sessions/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see:</p> <ul> <li><code>Session</code> - Protocol interface</li> <li><code>OpenAIConversationsSession</code> - OpenAI Conversations API implementation</li> <li><code>OpenAIResponsesCompactionSession</code> - Responses API compaction wrapper</li> <li><code>SQLiteSession</code> - Basic SQLite implementation</li> <li><code>SQLAlchemySession</code> - SQLAlchemy-powered implementation</li> <li><code>DaprSession</code> - Dapr state store implementation</li> <li><code>AdvancedSQLiteSession</code> - Enhanced SQLite with branching and analytics</li> <li><code>EncryptedSession</code> - Encrypted wrapper for any session</li> </ul>"},{"location":"sessions/advanced_sqlite_session/","title":"Advanced SQLite Sessions","text":"<p><code>AdvancedSQLiteSession</code> is an enhanced version of the basic <code>SQLiteSession</code> that provides advanced conversation management capabilities including conversation branching, detailed usage analytics, and structured conversation queries.</p>"},{"location":"sessions/advanced_sqlite_session/#features","title":"Features","text":"<ul> <li>Conversation branching: Create alternative conversation paths from any user message</li> <li>Usage tracking: Detailed token usage analytics per turn with full JSON breakdowns</li> <li>Structured queries: Get conversations by turns, tool usage statistics, and more</li> <li>Branch management: Independent branch switching and management</li> <li>Message structure metadata: Track message types, tool usage, and conversation flow</li> </ul>"},{"location":"sessions/advanced_sqlite_session/#quick-start","title":"Quick start","text":"<pre><code>from agents import Agent, Runner\nfrom agents.extensions.memory import AdvancedSQLiteSession\n\n# Create agent\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"Reply very concisely.\",\n)\n\n# Create an advanced session\nsession = AdvancedSQLiteSession(\n    session_id=\"conversation_123\",\n    db_path=\"conversations.db\",\n    create_tables=True\n)\n\n# First conversation turn\nresult = await Runner.run(\n    agent,\n    \"What city is the Golden Gate Bridge in?\",\n    session=session\n)\nprint(result.final_output)  # \"San Francisco\"\n\n# IMPORTANT: Store usage data\nawait session.store_run_usage(result)\n\n# Continue conversation\nresult = await Runner.run(\n    agent,\n    \"What state is it in?\",\n    session=session\n)\nprint(result.final_output)  # \"California\"\nawait session.store_run_usage(result)\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#initialization","title":"Initialization","text":"<pre><code>from agents.extensions.memory import AdvancedSQLiteSession\n\n# Basic initialization\nsession = AdvancedSQLiteSession(\n    session_id=\"my_conversation\",\n    create_tables=True  # Auto-create advanced tables\n)\n\n# With persistent storage\nsession = AdvancedSQLiteSession(\n    session_id=\"user_123\",\n    db_path=\"path/to/conversations.db\",\n    create_tables=True\n)\n\n# With custom logger\nimport logging\nlogger = logging.getLogger(\"my_app\")\nsession = AdvancedSQLiteSession(\n    session_id=\"session_456\",\n    create_tables=True,\n    logger=logger\n)\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#parameters","title":"Parameters","text":"<ul> <li><code>session_id</code> (str): Unique identifier for the conversation session</li> <li><code>db_path</code> (str | Path): Path to SQLite database file. Defaults to <code>:memory:</code> for in-memory storage</li> <li><code>create_tables</code> (bool): Whether to automatically create the advanced tables. Defaults to <code>False</code></li> <li><code>logger</code> (logging.Logger | None): Custom logger for the session. Defaults to module logger</li> </ul>"},{"location":"sessions/advanced_sqlite_session/#usage-tracking","title":"Usage tracking","text":"<p>AdvancedSQLiteSession provides detailed usage analytics by storing token usage data per conversation turn. This is entirely dependent on the <code>store_run_usage</code> method being called after each agent run.</p>"},{"location":"sessions/advanced_sqlite_session/#storing-usage-data","title":"Storing usage data","text":"<pre><code># After each agent run, store the usage data\nresult = await Runner.run(agent, \"Hello\", session=session)\nawait session.store_run_usage(result)\n\n# This stores:\n# - Total tokens used\n# - Input/output token breakdown\n# - Request count\n# - Detailed JSON token information (if available)\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#retrieving-usage-statistics","title":"Retrieving usage statistics","text":"<pre><code># Get session-level usage (all branches)\nsession_usage = await session.get_session_usage()\nif session_usage:\n    print(f\"Total requests: {session_usage['requests']}\")\n    print(f\"Total tokens: {session_usage['total_tokens']}\")\n    print(f\"Input tokens: {session_usage['input_tokens']}\")\n    print(f\"Output tokens: {session_usage['output_tokens']}\")\n    print(f\"Total turns: {session_usage['total_turns']}\")\n\n# Get usage for specific branch\nbranch_usage = await session.get_session_usage(branch_id=\"main\")\n\n# Get usage by turn\nturn_usage = await session.get_turn_usage()\nfor turn_data in turn_usage:\n    print(f\"Turn {turn_data['user_turn_number']}: {turn_data['total_tokens']} tokens\")\n    if turn_data['input_tokens_details']:\n        print(f\"  Input details: {turn_data['input_tokens_details']}\")\n    if turn_data['output_tokens_details']:\n        print(f\"  Output details: {turn_data['output_tokens_details']}\")\n\n# Get usage for specific turn\nturn_2_usage = await session.get_turn_usage(user_turn_number=2)\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#conversation-branching","title":"Conversation branching","text":"<p>One of the key features of AdvancedSQLiteSession is the ability to create conversation branches from any user message, allowing you to explore alternative conversation paths.</p>"},{"location":"sessions/advanced_sqlite_session/#creating-branches","title":"Creating branches","text":"<pre><code># Get available turns for branching\nturns = await session.get_conversation_turns()\nfor turn in turns:\n    print(f\"Turn {turn['turn']}: {turn['content']}\")\n    print(f\"Can branch: {turn['can_branch']}\")\n\n# Create a branch from turn 2\nbranch_id = await session.create_branch_from_turn(2)\nprint(f\"Created branch: {branch_id}\")\n\n# Create a branch with custom name\nbranch_id = await session.create_branch_from_turn(\n    2, \n    branch_name=\"alternative_path\"\n)\n\n# Create branch by searching for content\nbranch_id = await session.create_branch_from_content(\n    \"weather\", \n    branch_name=\"weather_focus\"\n)\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#branch-management","title":"Branch management","text":"<pre><code># List all branches\nbranches = await session.list_branches()\nfor branch in branches:\n    current = \" (current)\" if branch[\"is_current\"] else \"\"\n    print(f\"{branch['branch_id']}: {branch['user_turns']} turns, {branch['message_count']} messages{current}\")\n\n# Switch between branches\nawait session.switch_to_branch(\"main\")\nawait session.switch_to_branch(branch_id)\n\n# Delete a branch\nawait session.delete_branch(branch_id, force=True)  # force=True allows deleting current branch\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#branch-workflow-example","title":"Branch workflow example","text":"<pre><code># Original conversation\nresult = await Runner.run(agent, \"What's the capital of France?\", session=session)\nawait session.store_run_usage(result)\n\nresult = await Runner.run(agent, \"What's the weather like there?\", session=session)\nawait session.store_run_usage(result)\n\n# Create branch from turn 2 (weather question)\nbranch_id = await session.create_branch_from_turn(2, \"weather_focus\")\n\n# Continue in new branch with different question\nresult = await Runner.run(\n    agent, \n    \"What are the main tourist attractions in Paris?\", \n    session=session\n)\nawait session.store_run_usage(result)\n\n# Switch back to main branch\nawait session.switch_to_branch(\"main\")\n\n# Continue original conversation\nresult = await Runner.run(\n    agent, \n    \"How expensive is it to visit?\", \n    session=session\n)\nawait session.store_run_usage(result)\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#structured-queries","title":"Structured queries","text":"<p>AdvancedSQLiteSession provides several methods for analyzing conversation structure and content.</p>"},{"location":"sessions/advanced_sqlite_session/#conversation-analysis","title":"Conversation analysis","text":"<pre><code># Get conversation organized by turns\nconversation_by_turns = await session.get_conversation_by_turns()\nfor turn_num, items in conversation_by_turns.items():\n    print(f\"Turn {turn_num}: {len(items)} items\")\n    for item in items:\n        if item[\"tool_name\"]:\n            print(f\"  - {item['type']} (tool: {item['tool_name']})\")\n        else:\n            print(f\"  - {item['type']}\")\n\n# Get tool usage statistics\ntool_usage = await session.get_tool_usage()\nfor tool_name, count, turn in tool_usage:\n    print(f\"{tool_name}: used {count} times in turn {turn}\")\n\n# Find turns by content\nmatching_turns = await session.find_turns_by_content(\"weather\")\nfor turn in matching_turns:\n    print(f\"Turn {turn['turn']}: {turn['content']}\")\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#message-structure","title":"Message structure","text":"<p>The session automatically tracks message structure including:</p> <ul> <li>Message types (user, assistant, tool_call, etc.)</li> <li>Tool names for tool calls</li> <li>Turn numbers and sequence numbers</li> <li>Branch associations</li> <li>Timestamps</li> </ul>"},{"location":"sessions/advanced_sqlite_session/#database-schema","title":"Database schema","text":"<p>AdvancedSQLiteSession extends the basic SQLite schema with two additional tables:</p>"},{"location":"sessions/advanced_sqlite_session/#message_structure-table","title":"message_structure table","text":"<pre><code>CREATE TABLE message_structure (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    session_id TEXT NOT NULL,\n    message_id INTEGER NOT NULL,\n    branch_id TEXT NOT NULL DEFAULT 'main',\n    message_type TEXT NOT NULL,\n    sequence_number INTEGER NOT NULL,\n    user_turn_number INTEGER,\n    branch_turn_number INTEGER,\n    tool_name TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (session_id) REFERENCES agent_sessions(session_id) ON DELETE CASCADE,\n    FOREIGN KEY (message_id) REFERENCES agent_messages(id) ON DELETE CASCADE\n);\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#turn_usage-table","title":"turn_usage table","text":"<pre><code>CREATE TABLE turn_usage (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    session_id TEXT NOT NULL,\n    branch_id TEXT NOT NULL DEFAULT 'main',\n    user_turn_number INTEGER NOT NULL,\n    requests INTEGER DEFAULT 0,\n    input_tokens INTEGER DEFAULT 0,\n    output_tokens INTEGER DEFAULT 0,\n    total_tokens INTEGER DEFAULT 0,\n    input_tokens_details JSON,\n    output_tokens_details JSON,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (session_id) REFERENCES agent_sessions(session_id) ON DELETE CASCADE,\n    UNIQUE(session_id, branch_id, user_turn_number)\n);\n</code></pre>"},{"location":"sessions/advanced_sqlite_session/#complete-example","title":"Complete example","text":"<p>Check out the complete example for a comprehensive demonstration of all features.</p>"},{"location":"sessions/advanced_sqlite_session/#api-reference","title":"API Reference","text":"<ul> <li><code>AdvancedSQLiteSession</code> - Main class</li> <li><code>Session</code> - Base session protocol</li> </ul>"},{"location":"sessions/encrypted_session/","title":"Encrypted Sessions","text":"<p><code>EncryptedSession</code> provides transparent encryption for any session implementation, securing conversation data with automatic expiration of old items.</p>"},{"location":"sessions/encrypted_session/#features","title":"Features","text":"<ul> <li>Transparent encryption: Wraps any session with Fernet encryption</li> <li>Per-session keys: Uses HKDF key derivation for unique encryption per session</li> <li>Automatic expiration: Old items are silently skipped when TTL expires</li> <li>Drop-in replacement: Works with any existing session implementation</li> </ul>"},{"location":"sessions/encrypted_session/#installation","title":"Installation","text":"<p>Encrypted sessions require the <code>encrypt</code> extra:</p> <pre><code>pip install openai-agents[encrypt]\n</code></pre>"},{"location":"sessions/encrypted_session/#quick-start","title":"Quick start","text":"<pre><code>import asyncio\nfrom agents import Agent, Runner\nfrom agents.extensions.memory import EncryptedSession, SQLAlchemySession\n\nasync def main():\n    agent = Agent(\"Assistant\")\n\n    # Create underlying session\n    underlying_session = SQLAlchemySession.from_url(\n        \"user-123\",\n        url=\"sqlite+aiosqlite:///:memory:\",\n        create_tables=True\n    )\n\n    # Wrap with encryption\n    session = EncryptedSession(\n        session_id=\"user-123\",\n        underlying_session=underlying_session,\n        encryption_key=\"your-secret-key-here\",\n        ttl=600  # 10 minutes\n    )\n\n    result = await Runner.run(agent, \"Hello\", session=session)\n    print(result.final_output)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"sessions/encrypted_session/#configuration","title":"Configuration","text":""},{"location":"sessions/encrypted_session/#encryption-key","title":"Encryption key","text":"<p>The encryption key can be either a Fernet key or any string:</p> <pre><code>from agents.extensions.memory import EncryptedSession\n\n# Using a Fernet key (base64-encoded)\nsession = EncryptedSession(\n    session_id=\"user-123\",\n    underlying_session=underlying_session,\n    encryption_key=\"your-fernet-key-here\",\n    ttl=600\n)\n\n# Using a raw string (will be derived to a key)\nsession = EncryptedSession(\n    session_id=\"user-123\", \n    underlying_session=underlying_session,\n    encryption_key=\"my-secret-password\",\n    ttl=600\n)\n</code></pre>"},{"location":"sessions/encrypted_session/#ttl-time-to-live","title":"TTL (Time To Live)","text":"<p>Set how long encrypted items remain valid:</p> <pre><code># Items expire after 1 hour\nsession = EncryptedSession(\n    session_id=\"user-123\",\n    underlying_session=underlying_session,\n    encryption_key=\"secret\",\n    ttl=3600  # 1 hour in seconds\n)\n\n# Items expire after 1 day\nsession = EncryptedSession(\n    session_id=\"user-123\",\n    underlying_session=underlying_session,\n    encryption_key=\"secret\", \n    ttl=86400  # 24 hours in seconds\n)\n</code></pre>"},{"location":"sessions/encrypted_session/#usage-with-different-session-types","title":"Usage with different session types","text":""},{"location":"sessions/encrypted_session/#with-sqlite-sessions","title":"With SQLite sessions","text":"<pre><code>from agents import SQLiteSession\nfrom agents.extensions.memory import EncryptedSession\n\n# Create encrypted SQLite session\nunderlying = SQLiteSession(\"user-123\", \"conversations.db\")\n\nsession = EncryptedSession(\n    session_id=\"user-123\",\n    underlying_session=underlying,\n    encryption_key=\"secret-key\"\n)\n</code></pre>"},{"location":"sessions/encrypted_session/#with-sqlalchemy-sessions","title":"With SQLAlchemy sessions","text":"<pre><code>from agents.extensions.memory import EncryptedSession, SQLAlchemySession\n\n# Create encrypted SQLAlchemy session\nunderlying = SQLAlchemySession.from_url(\n    \"user-123\",\n    url=\"postgresql+asyncpg://user:pass@localhost/db\",\n    create_tables=True\n)\n\nsession = EncryptedSession(\n    session_id=\"user-123\",\n    underlying_session=underlying,\n    encryption_key=\"secret-key\"\n)\n</code></pre> <p>Advanced Session Features</p> <p>When using <code>EncryptedSession</code> with advanced session implementations like <code>AdvancedSQLiteSession</code>, note that:</p> <ul> <li>Methods like <code>find_turns_by_content()</code> won't work effectively since message content is encrypted</li> <li>Content-based searches operate on encrypted data, limiting their effectiveness</li> </ul>"},{"location":"sessions/encrypted_session/#key-derivation","title":"Key derivation","text":"<p>EncryptedSession uses HKDF (HMAC-based Key Derivation Function) to derive unique encryption keys per session:</p> <ul> <li>Master key: Your provided encryption key</li> <li>Session salt: The session ID</li> <li>Info string: <code>\"agents.session-store.hkdf.v1\"</code></li> <li>Output: 32-byte Fernet key</li> </ul> <p>This ensures that: - Each session has a unique encryption key - Keys cannot be derived without the master key - Session data cannot be decrypted across different sessions</p>"},{"location":"sessions/encrypted_session/#automatic-expiration","title":"Automatic expiration","text":"<p>When items exceed the TTL, they are automatically skipped during retrieval:</p> <pre><code># Items older than TTL are silently ignored\nitems = await session.get_items()  # Only returns non-expired items\n\n# Expired items don't affect session behavior\nresult = await Runner.run(agent, \"Continue conversation\", session=session)\n</code></pre>"},{"location":"sessions/encrypted_session/#api-reference","title":"API Reference","text":"<ul> <li><code>EncryptedSession</code> - Main class</li> <li><code>Session</code> - Base session protocol</li> </ul>"},{"location":"sessions/sqlalchemy_session/","title":"SQLAlchemy Sessions","text":"<p><code>SQLAlchemySession</code> uses SQLAlchemy to provide a production-ready session implementation, allowing you to use any database supported by SQLAlchemy (PostgreSQL, MySQL, SQLite, etc.) for session storage.</p>"},{"location":"sessions/sqlalchemy_session/#installation","title":"Installation","text":"<p>SQLAlchemy sessions require the <code>sqlalchemy</code> extra:</p> <pre><code>pip install openai-agents[sqlalchemy]\n</code></pre>"},{"location":"sessions/sqlalchemy_session/#quick-start","title":"Quick start","text":""},{"location":"sessions/sqlalchemy_session/#using-database-url","title":"Using database URL","text":"<p>The simplest way to get started:</p> <pre><code>import asyncio\nfrom agents import Agent, Runner\nfrom agents.extensions.memory import SQLAlchemySession\n\nasync def main():\n    agent = Agent(\"Assistant\")\n\n    # Create session using database URL\n    session = SQLAlchemySession.from_url(\n        \"user-123\",\n        url=\"sqlite+aiosqlite:///:memory:\",\n        create_tables=True\n    )\n\n    result = await Runner.run(agent, \"Hello\", session=session)\n    print(result.final_output)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"sessions/sqlalchemy_session/#using-existing-engine","title":"Using existing engine","text":"<p>For applications with existing SQLAlchemy engines:</p> <pre><code>import asyncio\nfrom agents import Agent, Runner\nfrom agents.extensions.memory import SQLAlchemySession\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\nasync def main():\n    # Create your database engine\n    engine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db\")\n\n    agent = Agent(\"Assistant\")\n    session = SQLAlchemySession(\n        \"user-456\",\n        engine=engine,\n        create_tables=True\n    )\n\n    result = await Runner.run(agent, \"Hello\", session=session)\n    print(result.final_output)\n\n    # Clean up\n    await engine.dispose()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"sessions/sqlalchemy_session/#api-reference","title":"API Reference","text":"<ul> <li><code>SQLAlchemySession</code> - Main class</li> <li><code>Session</code> - Base session protocol</li> </ul>"},{"location":"voice/pipeline/","title":"Pipelines and workflows","text":"<p><code>VoicePipeline</code> is a class that makes it easy to turn your agentic workflows into a voice app. You pass in a workflow to run, and the pipeline takes care of transcribing input audio, detecting when the audio ends, calling your workflow at the right time, and turning the workflow output back into audio.</p> <pre><code>graph LR\n    %% Input\n    A[\"\ud83c\udfa4 Audio Input\"]\n\n    %% Voice Pipeline\n    subgraph Voice_Pipeline [Voice Pipeline]\n        direction TB\n        B[\"Transcribe (speech-to-text)\"]\n        C[\"Your Code\"]:::highlight\n        D[\"Text-to-speech\"]\n        B --&gt; C --&gt; D\n    end\n\n    %% Output\n    E[\"\ud83c\udfa7 Audio Output\"]\n\n    %% Flow\n    A --&gt; Voice_Pipeline\n    Voice_Pipeline --&gt; E\n\n    %% Custom styling\n    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;\n</code></pre>"},{"location":"voice/pipeline/#configuring-a-pipeline","title":"Configuring a pipeline","text":"<p>When you create a pipeline, you can set a few things:</p> <ol> <li>The <code>workflow</code>, which is the code that runs each time new audio is transcribed.</li> <li>The <code>speech-to-text</code> and <code>text-to-speech</code> models used</li> <li>The <code>config</code>, which lets you configure things like:<ul> <li>A model provider, which can map model names to models</li> <li>Tracing, including whether to disable tracing, whether audio files are uploaded, the workflow name, trace IDs etc.</li> <li>Settings on the TTS and STT models, like the prompt, language and data types used.</li> </ul> </li> </ol>"},{"location":"voice/pipeline/#running-a-pipeline","title":"Running a pipeline","text":"<p>You can run a pipeline via the <code>run()</code> method, which lets you pass in audio input in two forms:</p> <ol> <li><code>AudioInput</code> is used when you have a full audio transcript, and just want to produce a result for it. This is useful in cases where you don't need to detect when a speaker is done speaking; for example, when you have pre-recorded audio or in push-to-talk apps where it's clear when the user is done speaking.</li> <li><code>StreamedAudioInput</code> is used when you might need to detect when a user is done speaking. It allows you to push audio chunks as they are detected, and the voice pipeline will automatically run the agent workflow at the right time, via a process called \"activity detection\".</li> </ol>"},{"location":"voice/pipeline/#results","title":"Results","text":"<p>The result of a voice pipeline run is a <code>StreamedAudioResult</code>. This is an object that lets you stream events as they occur. There are a few kinds of <code>VoiceStreamEvent</code>, including:</p> <ol> <li><code>VoiceStreamEventAudio</code>, which contains a chunk of audio.</li> <li><code>VoiceStreamEventLifecycle</code>, which informs you of lifecycle events like a turn starting or ending.</li> <li><code>VoiceStreamEventError</code>, is an error event.</li> </ol> <pre><code>result = await pipeline.run(input)\n\nasync for event in result.stream():\n    if event.type == \"voice_stream_event_audio\":\n        # play audio\n    elif event.type == \"voice_stream_event_lifecycle\":\n        # lifecycle\n    elif event.type == \"voice_stream_event_error\"\n        # error\n    ...\n</code></pre>"},{"location":"voice/pipeline/#best-practices","title":"Best practices","text":""},{"location":"voice/pipeline/#interruptions","title":"Interruptions","text":"<p>The Agents SDK currently does not support any built-in interruptions support for <code>StreamedAudioInput</code>. Instead for every detected turn it will trigger a separate run of your workflow. If you want to handle interruptions inside your application you can listen to the <code>VoiceStreamEventLifecycle</code> events. <code>turn_started</code> will indicate that a new turn was transcribed and processing is beginning. <code>turn_ended</code> will trigger after all the audio was dispatched for a respective turn. You could use these events to mute the microphone of the speaker when the model starts a turn and unmute it after you flushed all the related audio for a turn.</p>"},{"location":"voice/quickstart/","title":"Quickstart","text":""},{"location":"voice/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure you've followed the base quickstart instructions for the Agents SDK, and set up a virtual environment. Then, install the optional voice dependencies from the SDK:</p> <pre><code>pip install 'openai-agents[voice]'\n</code></pre>"},{"location":"voice/quickstart/#concepts","title":"Concepts","text":"<p>The main concept to know about is a <code>VoicePipeline</code>, which is a 3 step process:</p> <ol> <li>Run a speech-to-text model to turn audio into text.</li> <li>Run your code, which is usually an agentic workflow, to produce a result.</li> <li>Run a text-to-speech model to turn the result text back into audio.</li> </ol> <pre><code>graph LR\n    %% Input\n    A[\"\ud83c\udfa4 Audio Input\"]\n\n    %% Voice Pipeline\n    subgraph Voice_Pipeline [Voice Pipeline]\n        direction TB\n        B[\"Transcribe (speech-to-text)\"]\n        C[\"Your Code\"]:::highlight\n        D[\"Text-to-speech\"]\n        B --&gt; C --&gt; D\n    end\n\n    %% Output\n    E[\"\ud83c\udfa7 Audio Output\"]\n\n    %% Flow\n    A --&gt; Voice_Pipeline\n    Voice_Pipeline --&gt; E\n\n    %% Custom styling\n    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;\n</code></pre>"},{"location":"voice/quickstart/#agents","title":"Agents","text":"<p>First, let's set up some Agents. This should feel familiar to you if you've built any agents with this SDK. We'll have a couple of Agents, a handoff, and a tool.</p> <pre><code>import asyncio\nimport random\n\nfrom agents import (\n    Agent,\n    function_tool,\n)\nfrom agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    print(f\"[debug] get_weather called with city: {city}\")\n    choices = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n    return f\"The weather in {city} is {random.choice(choices)}.\"\n\n\nspanish_agent = Agent(\n    name=\"Spanish\",\n    handoff_description=\"A spanish speaking agent.\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. Speak in Spanish.\",\n    ),\n    model=\"gpt-5.2\",\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\",\n    ),\n    model=\"gpt-5.2\",\n    handoffs=[spanish_agent],\n    tools=[get_weather],\n)\n</code></pre>"},{"location":"voice/quickstart/#voice-pipeline","title":"Voice pipeline","text":"<p>We'll set up a simple voice pipeline, using <code>SingleAgentVoiceWorkflow</code> as the workflow.</p> <pre><code>from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline\npipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n</code></pre>"},{"location":"voice/quickstart/#run-the-pipeline","title":"Run the pipeline","text":"<pre><code>import numpy as np\nimport sounddevice as sd\nfrom agents.voice import AudioInput\n\n# For simplicity, we'll just create 3 seconds of silence\n# In reality, you'd get microphone data\nbuffer = np.zeros(24000 * 3, dtype=np.int16)\naudio_input = AudioInput(buffer=buffer)\n\nresult = await pipeline.run(audio_input)\n\n# Create an audio player using `sounddevice`\nplayer = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\nplayer.start()\n\n# Play the audio stream as it comes in\nasync for event in result.stream():\n    if event.type == \"voice_stream_event_audio\":\n        player.write(event.data)\n</code></pre>"},{"location":"voice/quickstart/#put-it-all-together","title":"Put it all together","text":"<pre><code>import asyncio\nimport random\n\nimport numpy as np\nimport sounddevice as sd\n\nfrom agents import (\n    Agent,\n    function_tool,\n    set_tracing_disabled,\n)\nfrom agents.voice import (\n    AudioInput,\n    SingleAgentVoiceWorkflow,\n    VoicePipeline,\n)\nfrom agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    print(f\"[debug] get_weather called with city: {city}\")\n    choices = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n    return f\"The weather in {city} is {random.choice(choices)}.\"\n\n\nspanish_agent = Agent(\n    name=\"Spanish\",\n    handoff_description=\"A spanish speaking agent.\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. Speak in Spanish.\",\n    ),\n    model=\"gpt-5.2\",\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\",\n    ),\n    model=\"gpt-5.2\",\n    handoffs=[spanish_agent],\n    tools=[get_weather],\n)\n\n\nasync def main():\n    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n    buffer = np.zeros(24000 * 3, dtype=np.int16)\n    audio_input = AudioInput(buffer=buffer)\n\n    result = await pipeline.run(audio_input)\n\n    # Create an audio player using `sounddevice`\n    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n    player.start()\n\n    # Play the audio stream as it comes in\n    async for event in result.stream():\n        if event.type == \"voice_stream_event_audio\":\n            player.write(event.data)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>If you run this example, the agent will speak to you! Check out the example in examples/voice/static to see a demo where you can speak to the agent yourself.</p>"},{"location":"voice/tracing/","title":"Tracing","text":"<p>Just like the way agents are traced, voice pipelines are also automatically traced.</p> <p>You can read the tracing doc above for basic tracing information, but you can additionally configure tracing of a pipeline via <code>VoicePipelineConfig</code>.</p> <p>Key tracing related fields are:</p> <ul> <li><code>tracing_disabled</code>: controls whether tracing is disabled. By default, tracing is enabled.</li> <li><code>trace_include_sensitive_data</code>: controls whether traces include potentially sensitive data, like audio transcripts. This is specifically for the voice pipeline, and not for anything that goes on inside your Workflow.</li> <li><code>trace_include_sensitive_audio_data</code>: controls whether traces include audio data.</li> <li><code>workflow_name</code>: The name of the trace workflow.</li> <li><code>group_id</code>: The <code>group_id</code> of the trace, which lets you link multiple traces.</li> <li><code>trace_metadata</code>: Additional metadata to include with the trace.</li> </ul>"},{"location":"ko/ref/voice/models/openai_tts/#agents.voice.models.openai_tts.OpenAITTSModel.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    text: str, settings: TTSModelSettings\n) -&gt; AsyncIterator[bytes]\n</code></pre> <p>Run the text-to-speech model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to convert to speech.</p> required <code>settings</code> <code>TTSModelSettings</code> <p>The settings to use for the text-to-speech model.</p> required <p>Returns:</p> Type Description <code>AsyncIterator[bytes]</code> <p>An iterator of audio chunks.</p> Source code in <code>src/agents/voice/models/openai_tts.py</code> <pre><code>async def run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n    \"\"\"Run the text-to-speech model.\n\n    Args:\n        text: The text to convert to speech.\n        settings: The settings to use for the text-to-speech model.\n\n    Returns:\n        An iterator of audio chunks.\n    \"\"\"\n    response = self._client.audio.speech.with_streaming_response.create(\n        model=self.model,\n        voice=settings.voice or DEFAULT_VOICE,\n        input=text,\n        response_format=\"pcm\",\n        extra_body={\n            \"instructions\": settings.instructions,\n        },\n    )\n\n    async with response as stream:\n        async for chunk in stream.iter_bytes(chunk_size=1024):\n            yield chunk\n</code></pre>"}]}